2026-01-27 10:19:46,288 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-27 10:19:46,321 | INFO | Vocabulary size: 350
2026-01-27 10:19:46,982 | INFO | Gradient checkpoint layers: []
2026-01-27 10:19:49,315 | INFO | BatchBeamSearch implementation is selected.
2026-01-27 10:19:49,318 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-27 10:19:49,318 | INFO | Decoding device=cuda, dtype=float32
2026-01-27 10:19:49,319 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-27 10:19:49,319 | WARNING | The file /vol/corpora/Rhapsodie/wav16k_corrected/Rhap-D0001.wav does not exist
2026-01-27 10:19:49,320 | INFO |  File duration: 290.2293125 seconds
2026-01-27 10:20:02,534 | INFO | speech length: 3474239
2026-01-27 10:20:04,972 | INFO | decoder input length: 5427
2026-01-27 10:20:04,972 | INFO | max output length: 5427
2026-01-27 10:20:04,972 | INFO | min output length: 542
2026-01-27 10:40:33,722 | INFO | end detected at 1918
2026-01-27 10:40:33,723 | INFO | -6232.93 * 0.5 = -3116.47 for decoder
2026-01-27 10:40:33,724 | INFO | -2965.33 * 0.5 = -1482.66 for ctc
2026-01-27 10:40:33,724 | INFO | total log probability: -4599.13
2026-01-27 10:40:33,724 | INFO | normalized log probability: -2.41
2026-01-27 10:40:33,724 | INFO | total number of ended hypotheses: 158
2026-01-27 10:40:33,745 | INFO | best hypo: ▁j'avais▁la▁chance▁d'avoir▁des▁enfants▁qui▁travaillent▁je▁dirai▁ce▁qui▁préférait▁rigoler▁que▁travaillers▁ils▁étaient▁tout▁à▁fait▁et▁normaux▁mais▁je▁vous▁diris▁ons▁à▁normalement▁et▁c'est▁vrai▁que▁bons▁abitants▁d'enensemble▁de▁paris▁et▁ons▁aucun▁problèmes▁mais▁j'aimes▁aucun▁problèmes▁enfants▁et▁j'ai▁j'ai▁amais▁très▁bien▁compris▁les▁gens▁qui▁ne▁connais▁saient▁déjà▁ce▁qui▁mettaient▁leurs▁enfants▁et▁d'abords▁dans▁le▁publics▁puis▁dans▁le▁privés▁pourqu'ils▁puissent▁penser▁à▁éventuellements▁avec▁soient▁et▁plus▁forts▁et▁'és▁et▁très▁étonnés▁parce▁qu'ils▁qu'ibleurs▁en▁prépants▁et▁prenait▁aprs▁les▁gens▁qu'ils▁n'és▁publes▁vous▁apparaîtormalements▁je▁vous▁dire▁e▁les▁lyecés▁je▁pense▁que▁l'intreurs▁de▁paris▁eux▁globalements▁sicles▁sont▁bon▁d'un▁bons▁n'z▁vous▁pense▁ouichois▁dans▁leurs▁enfants▁qu'ellements▁le▁lyecés▁'établisants▁et▁'en▁et▁moins▁bons▁alors▁le▁problème▁si▁vous▁voulez▁qu'posent▁à▁paris▁et▁jouies▁d'un▁mathimes▁le▁problèmes▁écoles▁maternelles▁et▁praires▁dans▁lequels▁quand▁ons▁n'avait▁e▁fectitivements▁pour▁un▁certain▁nombre▁d'enfants▁qui▁ne▁sont▁pas▁et▁ne▁parles▁pas▁qu'anse▁pour▁les▁enseignants▁c'est▁pas▁facles▁et▁moi▁je▁vois▁et▁mamanches▁je▁travailles▁dans▁le▁matait▁ils▁faudrai▁et▁j'étais▁fils▁'és▁l'édcations▁nationales▁je▁metaitis▁beaucoupe▁plus▁d'enseignants▁dans▁les▁maternels▁pour▁qu'un▁enfants▁et▁sortent▁de▁maternels▁parles▁obligatoirement▁en▁français▁parce▁que▁tours▁'és▁enfants▁qu'arrivent▁ils▁vont▁à▁maternels▁dons▁c'est▁pas▁dans▁norbles▁qu'ils▁rivent▁en▁c'pés▁ne▁parlants▁pas▁français▁mais▁ils▁rivants▁et▁ne▁parlants▁pas▁français▁parce▁qu'ils▁ons▁beaucoup▁trop▁de▁nombre▁ternants▁on▁ne▁peutent▁pas▁leurs▁apprendre▁pour▁réglers▁mamants▁et▁ballangets▁pourraient▁imagner▁de▁becings▁oui▁oui▁et▁d'abords▁be▁j'ai▁voueux▁que▁'a▁commencer▁à▁faires▁à▁ajours▁ças▁'és▁'un▁c'és▁'une▁des▁meshures▁de▁plans▁baneu▁le▁bocye▁mais▁je▁pense▁surtouts▁que▁dans▁leurs▁simes▁rons▁siements▁les▁maternelles▁u▁et▁'affismement▁d'enseignants▁mais▁que▁mais'est▁que▁dans▁leurs▁ils▁faudrait▁qu'er▁qu'ons▁s'és▁pasare▁et▁'ils▁les▁cos▁de▁fais▁pour▁les▁enfants▁qui▁parlent▁pas▁français▁s'est▁pas▁compliqués▁quand▁même▁c'est▁pas▁fible▁d'appendre▁le▁français▁qu'ils▁flas▁ças▁seulements▁dons▁les▁mamants▁ne▁parle▁pas▁français▁qu'ils▁ne▁voient▁pas▁survent▁leurs▁papape▁dans▁le▁définictions▁ils▁ne▁parles▁pas▁français▁ils▁partait▁jamais▁en▁français▁n'as▁pas▁penser▁à▁la▁mais▁ons▁alors▁nous▁nous▁espayants▁d'aprendre▁le▁français▁ou▁maments▁pourqu'eux▁elles▁prenent▁le▁français▁à▁leurs▁enfants▁le▁chose▁que▁je▁ne▁faites▁aucun▁organisme▁d'états▁car▁les▁ons▁linguismes▁qui▁sont▁donnés▁et▁immigrés▁et▁'arrivs▁ne▁sont▁pas▁faites▁pour▁les▁gens▁qui▁ne▁parlent▁pas▁de▁tout▁niis▁qu'ils▁n'ontait▁jamais▁à▁l'écles▁parce▁que▁'a▁perse▁en▁veut▁parce▁que▁vous▁dire▁qu'apprendre▁d'à▁lire▁à▁écrire▁à▁trente▁sans▁qu'à▁'amaise▁tenus▁un▁crayants▁ças▁fais▁pas▁en▁de▁sans▁heurs▁oui▁n'enons▁nons▁je▁pense▁que▁le▁problèmes▁je▁dis▁pas▁tous▁les▁problèmes▁'allâmes▁mais▁beaucoup▁de▁problèmes▁comment▁voulez▁vous▁apprendre▁à▁lire▁enfant▁qui▁parttontants▁parles▁définictions▁cette▁enfant▁à▁perpants▁'pés▁et▁puise▁voils▁alors▁bien▁sûrs▁qu'un▁n'ak▁s'en▁sorttes▁toujours▁déligense▁que▁parts▁par▁les▁gense▁et▁ons▁ormaux▁mais▁globalement▁c'est▁très▁difficiles▁dés▁'enfraise▁et▁mélanges▁d'uns▁çons▁réussants▁de▁gape▁qui▁parles▁ça▁s'ins▁alles▁très▁bien▁sûrs▁et▁le▁français▁lanants▁étrgers▁dans▁le▁milieu▁et▁çons▁de▁la▁clas▁'enger▁s'és▁beaucour▁bas▁fas▁et▁'és▁avec▁tout▁bien▁sûrs▁rs▁je▁parlers▁de▁mélanges▁oui▁mais▁je▁pense▁que▁vous▁avez▁res▁çà▁pense▁que▁vous▁avez▁raise▁mais▁si▁vous▁voulez▁le▁mélange▁qui▁est▁difficiles▁quand▁ils▁sont▁pos▁parce▁quand▁ils▁sont▁poésies▁et▁'aye▁ouichers▁c'est▁pas▁tellement▁bons▁d'embleiner▁un▁petits▁enfants▁de▁prs▁qu'un▁ans▁voyez▁en▁beus▁le▁loins▁déj'as▁c'est▁quand▁même▁pour▁eux▁d'allerer▁à▁l'écles▁soit▁évident▁et▁puis▁faudrai▁que▁les▁parents▁l'acceptent▁à▁ce▁que▁les▁mamants▁afrtins▁elles▁ons▁becoup▁de▁mal▁à▁séper▁de▁n'ons▁pas▁très▁volontiers▁tants▁siants▁plus▁sans▁leursets▁que▁çavais▁dire▁l'enfants▁teras▁déje▁et▁là▁bas▁les▁mamants▁afrs▁et▁veulent▁pas▁veulent▁pas

2026-01-27 10:40:34,099 | INFO | File: Rhap-D0002.wav | WER=60.782347 | S=335 D=249 I=22
2026-01-27 10:40:34,099 | INFO | ------------------------------
2026-01-27 10:40:34,100 | INFO |  File duration: 284.962125 seconds
2026-01-27 10:40:46,682 | INFO | speech length: 4021440
2026-01-27 10:40:47,301 | INFO | decoder input length: 6283
2026-01-27 10:40:47,301 | INFO | max output length: 6283
2026-01-27 10:40:47,301 | INFO | min output length: 628
