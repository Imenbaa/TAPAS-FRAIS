2026-01-23 10:19:12,519 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-23 10:19:12,551 | INFO | Vocabulary size: 350
2026-01-23 10:19:13,325 | INFO | Gradient checkpoint layers: []
2026-01-23 10:19:15,275 | INFO | BatchBeamSearch implementation is selected.
2026-01-23 10:19:15,279 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-23 10:19:15,279 | INFO | Decoding device=cuda, dtype=float32
2026-01-23 10:19:15,280 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-23 10:19:15,280 | INFO | /vol/corpora/TAPAS_FRAIS/Data_partagees_ParisTypaloc-TapasFrais/12-CTRL/AEX-CAB000-02_L01.wav
2026-01-23 10:19:15,280 | INFO |  File duration: 68.39650793650793 seconds
2026-01-23 10:19:15,289 | WARNING | This version of torchaudio is old. SpeechBrain no longer tries using the torchaudio global backend mechanism in recipes, so if you encounter issues, update torchaudio to >=2.1.0.
2026-01-23 10:19:15,292 | DEBUG | Registered checkpoint save hook for _speechbrain_save
2026-01-23 10:19:15,292 | DEBUG | Registered checkpoint load hook for _speechbrain_load
2026-01-23 10:19:15,292 | DEBUG | Registered checkpoint save hook for save
2026-01-23 10:19:15,292 | DEBUG | Registered checkpoint load hook for load
2026-01-23 10:19:15,501 | INFO | Applied quirks (see `speechbrain.utils.quirks`): [allow_tf32, disable_jit_profiling]
2026-01-23 10:19:15,501 | INFO | Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
2026-01-23 10:19:15,502 | DEBUG | Registered checkpoint save hook for _save
2026-01-23 10:19:15,502 | DEBUG | Registered checkpoint load hook for _recover
2026-01-23 10:19:15,515 | INFO | File: AEX-CAB000-02_L01.wav | WER=9.604520 | S=15 D=2 I=0
2026-01-23 10:19:15,515 | INFO | ------------------------------
2026-01-23 10:19:15,515 | INFO | /vol/corpora/TAPAS_FRAIS/Data_partagees_ParisTypaloc-TapasFrais/12-CTRL/AEX-CAB000-02_L01_pr_analyse.wav
2026-01-23 10:19:15,515 | INFO | /vol/corpora/TAPAS_FRAIS/Data_partagees_ParisTypaloc-TapasFrais/12-CTRL/AEX-CAC000-02_L01.wav
2026-01-23 10:19:15,515 | INFO |  File duration: 71.22968253968254 seconds
2026-01-23 10:19:15,532 | INFO | File: AEX-CAC000-02_L01.wav | WER=13.636364 | S=9 D=1 I=14
2026-01-23 10:19:15,532 | INFO | ------------------------------
2026-01-23 10:19:15,532 | INFO | /vol/corpora/TAPAS_FRAIS/Data_partagees_ParisTypaloc-TapasFrais/12-CTRL/AEX-CAC000-02_L01_pr_analyse.wav
2026-01-23 10:19:15,532 | INFO | /vol/corpora/TAPAS_FRAIS/Data_partagees_ParisTypaloc-TapasFrais/12-CTRL/AEX-CAG000-01_L01.wav
2026-01-23 10:19:15,533 | INFO |  File duration: 70.09696145124717 seconds
2026-01-23 10:19:15,548 | INFO | File: AEX-CAG000-01_L01.wav | WER=11.363636 | S=14 D=1 I=5
2026-01-23 10:19:15,548 | INFO | ------------------------------
2026-01-23 10:19:15,548 | INFO | /vol/corpora/TAPAS_FRAIS/Data_partagees_ParisTypaloc-TapasFrais/12-CTRL/AEX-CAG000-01_L01_pr_analyse.wav
2026-01-23 10:19:15,548 | INFO | /vol/corpora/TAPAS_FRAIS/Data_partagees_ParisTypaloc-TapasFrais/12-CTRL/AEX-CLJ000-02_L01.wav
2026-01-23 10:19:15,549 | INFO |  File duration: 75.73018140589569 seconds
2026-01-23 10:19:15,565 | INFO | File: AEX-CLJ000-02_L01.wav | WER=26.704545 | S=17 D=5 I=25
2026-01-23 10:19:15,566 | INFO | ------------------------------
2026-01-23 10:19:15,566 | INFO | /vol/corpora/TAPAS_FRAIS/Data_partagees_ParisTypaloc-TapasFrais/12-CTRL/AEX-CLJ000-02_L01_pr_analyse.wav
2026-01-23 10:19:15,566 | INFO | /vol/corpora/TAPAS_FRAIS/Data_partagees_ParisTypaloc-TapasFrais/12-CTRL/AEX-CML000-01_L01.wav
2026-01-23 10:19:15,566 | INFO |  File duration: 58.55369614512472 seconds
2026-01-23 10:19:15,581 | INFO | File: AEX-CML000-01_L01.wav | WER=8.522727 | S=10 D=2 I=3
2026-01-23 10:19:15,581 | INFO | ------------------------------
2026-01-23 10:19:15,581 | INFO | /vol/corpora/TAPAS_FRAIS/Data_partagees_ParisTypaloc-TapasFrais/12-CTRL/AEX-CML000-01_L01_pr_analyse.wav
2026-01-23 10:19:15,581 | INFO | /vol/corpora/TAPAS_FRAIS/Data_partagees_ParisTypaloc-TapasFrais/12-CTRL/AEX-CSR000-01_L01.wav
2026-01-23 10:19:15,582 | INFO |  File duration: 76.21907029478459 seconds
2026-01-23 10:19:15,596 | INFO | File: AEX-CSR000-01_L01.wav | WER=14.689266 | S=18 D=6 I=2
2026-01-23 10:19:15,597 | INFO | ------------------------------
2026-01-23 10:19:15,597 | INFO | /vol/corpora/TAPAS_FRAIS/Data_partagees_ParisTypaloc-TapasFrais/12-CTRL/AEX-CSR000-01_L01_pr_analyse.wav
2026-01-23 10:19:15,597 | INFO | /vol/corpora/TAPAS_FRAIS/Data_partagees_ParisTypaloc-TapasFrais/12-CTRL/BEX-CDB000-01_L01.wav
2026-01-23 10:19:15,597 | INFO |  File duration: 86.34117913832199 seconds
2026-01-23 10:19:15,612 | INFO | File: BEX-CDB000-01_L01.wav | WER=9.604520 | S=14 D=1 I=2
2026-01-23 10:19:15,612 | INFO | ------------------------------
2026-01-23 10:19:15,613 | INFO | /vol/corpora/TAPAS_FRAIS/Data_partagees_ParisTypaloc-TapasFrais/12-CTRL/BEX-CDB000-01_L01_pr_analyse.wav
2026-01-23 10:19:15,613 | INFO | /vol/corpora/TAPAS_FRAIS/Data_partagees_ParisTypaloc-TapasFrais/12-CTRL/BEX-CEB000-01_L01.wav
2026-01-23 10:19:15,613 | INFO |  File duration: 72.99619047619048 seconds
2026-01-23 10:19:15,628 | INFO | File: BEX-CEB000-01_L01.wav | WER=19.886364 | S=26 D=3 I=6
2026-01-23 10:19:15,629 | INFO | ------------------------------
2026-01-23 10:19:15,629 | INFO | /vol/corpora/TAPAS_FRAIS/Data_partagees_ParisTypaloc-TapasFrais/12-CTRL/BEX-CEB000-01_L01_pr_analyse.wav
2026-01-23 10:19:15,629 | INFO | /vol/corpora/TAPAS_FRAIS/Data_partagees_ParisTypaloc-TapasFrais/12-CTRL/BEX-CHE000-01_L01.wav
2026-01-23 10:19:15,629 | INFO |  File duration: 60.74340136054422 seconds
2026-01-23 10:19:15,643 | INFO | File: BEX-CHE000-01_L01.wav | WER=15.819209 | S=19 D=8 I=1
2026-01-23 10:19:15,643 | INFO | ------------------------------
2026-01-23 10:19:15,643 | INFO | /vol/corpora/TAPAS_FRAIS/Data_partagees_ParisTypaloc-TapasFrais/12-CTRL/BEX-CHE000-01_L01_pr_analyse.wav
2026-01-23 10:19:15,643 | INFO | /vol/corpora/TAPAS_FRAIS/Data_partagees_ParisTypaloc-TapasFrais/12-CTRL/BEX-CKN000-01_L01.wav
2026-01-23 10:19:15,643 | INFO |  File duration: 77.60358276643991 seconds
2026-01-23 10:19:15,659 | INFO | File: BEX-CKN000-01_L01.wav | WER=10.734463 | S=15 D=2 I=2
2026-01-23 10:19:15,659 | INFO | ------------------------------
2026-01-23 10:19:15,659 | INFO | /vol/corpora/TAPAS_FRAIS/Data_partagees_ParisTypaloc-TapasFrais/12-CTRL/BEX-CKN000-01_L01_pr_analyse.wav
2026-01-23 10:19:15,659 | INFO | /vol/corpora/TAPAS_FRAIS/Data_partagees_ParisTypaloc-TapasFrais/12-CTRL/BEX-CMB000-01_L01.wav
2026-01-23 10:19:15,659 | INFO |  File duration: 80.38402083333334 seconds
2026-01-23 10:19:15,681 | INFO | (2, 3858433)
2026-01-23 10:19:15,736 | INFO | (3858433,)
2026-01-23 10:19:20,957 | INFO | speech length: 92481
2026-01-23 10:19:22,802 | INFO | speech length: 90241
2026-01-23 10:19:24,380 | INFO | speech length: 79041
2026-01-23 10:19:25,915 | INFO | speech length: 76481
2026-01-23 10:19:27,631 | INFO | speech length: 83521
2026-01-23 10:19:29,198 | INFO | speech length: 42561
2026-01-23 10:19:30,214 | INFO | speech length: 59521
2026-01-23 10:19:31,431 | INFO | speech length: 73281
2026-01-23 10:19:33,022 | INFO | speech length: 51201
2026-01-23 10:19:34,022 | INFO | speech length: 70401
2026-01-23 10:19:35,496 | INFO | speech length: 76801
2026-01-23 10:19:36,996 | INFO | speech length: 33921
2026-01-23 10:19:37,790 | INFO | speech length: 37761
2026-01-23 10:19:38,232 | INFO | speech length: 53120
2026-01-23 10:19:39,242 | INFO | speech length: 41601
2026-01-23 10:19:40,188 | INFO | speech length: 39681
2026-01-23 10:19:40,796 | INFO | speech length: 37121
2026-01-23 10:19:41,499 | INFO | speech length: 79361
2026-01-23 10:19:42,926 | INFO | File: BEX-CMB000-01_L01.wav | WER=19.209040 | S=20 D=9 I=5
2026-01-23 10:19:42,926 | INFO | ------------------------------
2026-01-23 10:19:42,926 | INFO | /vol/corpora/TAPAS_FRAIS/Data_partagees_ParisTypaloc-TapasFrais/12-CTRL/BEX-CMB000-01_L01_pr_analyse.wav
2026-01-23 10:19:42,926 | INFO | /vol/corpora/TAPAS_FRAIS/Data_partagees_ParisTypaloc-TapasFrais/12-CTRL/BEX-CNKN00-01_L01.wav
2026-01-23 10:19:42,927 | INFO |  File duration: 77.83328798185941 seconds
2026-01-23 10:19:43,005 | INFO | (3432448,)
2026-01-23 10:19:43,005 | INFO | (3432448,)
2026-01-23 10:19:46,378 | INFO | speech length: 68801
2026-01-23 10:19:47,281 | INFO | speech length: 47681
2026-01-23 10:19:48,248 | INFO | speech length: 62081
2026-01-23 10:19:49,551 | INFO | speech length: 42241
2026-01-23 10:19:50,489 | INFO | speech length: 72641
2026-01-23 10:19:52,161 | INFO | speech length: 33921
2026-01-23 10:19:52,799 | INFO | speech length: 38401
2026-01-23 10:19:53,559 | INFO | speech length: 42561
2026-01-23 10:19:54,326 | INFO | speech length: 65601
2026-01-23 10:19:55,776 | INFO | speech length: 73601
2026-01-23 10:19:57,255 | INFO | speech length: 108161
2026-01-23 10:19:59,957 | INFO | speech length: 56961
2026-01-23 10:20:01,549 | INFO | speech length: 44801
2026-01-23 10:20:02,264 | INFO | speech length: 49921
2026-01-23 10:20:03,190 | INFO | speech length: 42561
2026-01-23 10:20:03,935 | INFO | speech length: 52801
2026-01-23 10:20:04,905 | INFO | speech length: 58241
2026-01-23 10:20:06,004 | INFO | speech length: 54401
2026-01-23 10:20:06,995 | INFO | File: BEX-CNKN00-01_L01.wav | WER=20.338983 | S=21 D=8 I=7
2026-01-23 10:20:06,995 | INFO | ------------------------------
2026-01-23 10:20:06,995 | INFO | /vol/corpora/TAPAS_FRAIS/Data_partagees_ParisTypaloc-TapasFrais/12-CTRL/BEX-CNKN00-01_L01_pr_analyse.wav
2026-01-23 10:20:06,995 | INFO | ===================== GLOBAL WER ======================
2026-01-23 10:20:06,996 | INFO | The number of files is 12
2026-01-23 10:20:06,996 | INFO | WER: 15.01%
2026-01-23 10:20:06,996 | INFO | =============================================================
