2026-01-23 10:35:35,961 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-23 10:35:35,980 | INFO | Vocabulary size: 47
2026-01-23 10:35:36,566 | INFO | Gradient checkpoint layers: []
2026-01-23 10:35:38,543 | INFO | BatchBeamSearch implementation is selected.
2026-01-23 10:35:38,548 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-23 10:35:38,548 | INFO | Decoding device=cuda, dtype=float32
2026-01-23 10:35:38,549 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-23 10:35:38,549 | INFO |  File duration: 121.48680272108844 seconds
2026-01-23 10:35:38,562 | WARNING | This version of torchaudio is old. SpeechBrain no longer tries using the torchaudio global backend mechanism in recipes, so if you encounter issues, update torchaudio to >=2.1.0.
2026-01-23 10:35:38,564 | DEBUG | Registered checkpoint save hook for _speechbrain_save
2026-01-23 10:35:38,564 | DEBUG | Registered checkpoint load hook for _speechbrain_load
2026-01-23 10:35:38,565 | DEBUG | Registered checkpoint save hook for save
2026-01-23 10:35:38,565 | DEBUG | Registered checkpoint load hook for load
2026-01-23 10:35:38,832 | INFO | Applied quirks (see `speechbrain.utils.quirks`): [disable_jit_profiling, allow_tf32]
2026-01-23 10:35:38,832 | INFO | Excluded quirks specified by the `SB_DISABLE_QUIRKS` environment (comma-separated list): []
2026-01-23 10:35:38,833 | DEBUG | Registered checkpoint save hook for _save
2026-01-23 10:35:38,833 | DEBUG | Registered checkpoint load hook for _recover
2026-01-23 10:35:38,867 | INFO | File: CCM-002710-01_L01.wav | WER=84.571429 | S=100 D=15 I=33
2026-01-23 10:35:38,867 | INFO | ------------------------------
2026-01-23 10:35:38,867 | INFO |  File duration: 127.0595918367347 seconds
2026-01-23 10:35:38,887 | INFO | File: CCM-003094-01_L01.wav | WER=38.285714 | S=42 D=9 I=16
2026-01-23 10:35:38,888 | INFO | ------------------------------
2026-01-23 10:35:38,888 | INFO |  File duration: 91.39374149659864 seconds
2026-01-23 10:35:38,907 | INFO | File: CCM-003110-01_L01.wav | WER=32.558140 | S=33 D=2 I=21
2026-01-23 10:35:38,907 | INFO | ------------------------------
2026-01-23 10:35:38,907 | INFO |  File duration: 122.32272108843537 seconds
2026-01-23 10:35:38,928 | INFO | File: CCM-003493-01_L01.wav | WER=37.078652 | S=43 D=2 I=21
2026-01-23 10:35:38,928 | INFO | ------------------------------
2026-01-23 10:35:38,929 | INFO |  File duration: 142.47764172335602 seconds
2026-01-23 10:35:38,951 | INFO | File: CCM-003998-01_L01.wav | WER=57.954545 | S=64 D=7 I=31
2026-01-23 10:35:38,951 | INFO | ------------------------------
2026-01-23 10:35:38,951 | INFO |  File duration: 94.87673469387755 seconds
2026-01-23 10:35:38,970 | INFO | File: CCM-004523-01_L01.wav | WER=36.842105 | S=35 D=8 I=20
2026-01-23 10:35:38,970 | INFO | ------------------------------
2026-01-23 10:35:38,970 | INFO |  File duration: 74.86113378684807 seconds
2026-01-23 10:35:38,989 | INFO | File: CCM-004538-01_L01.wav | WER=52.247191 | S=60 D=13 I=20
2026-01-23 10:35:38,989 | INFO | ------------------------------
2026-01-23 10:35:38,989 | INFO | ===================== GLOBAL WER ======================
2026-01-23 10:35:38,989 | INFO | The number of files is 7
2026-01-23 10:35:38,989 | INFO | WER: 48.51%
2026-01-23 10:35:38,989 | INFO | =============================================================
