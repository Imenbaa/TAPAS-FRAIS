2026-01-26 19:25:12,289 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-26 19:25:12,323 | INFO | Vocabulary size: 350
2026-01-26 19:25:12,932 | INFO | Gradient checkpoint layers: []
2026-01-26 19:25:18,565 | INFO | BatchBeamSearch implementation is selected.
2026-01-26 19:25:18,572 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-26 19:25:18,572 | INFO | Decoding device=cuda, dtype=float32
2026-01-26 19:25:18,574 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-26 19:25:18,575 | WARNING | The file /vol/corpora/Rhapsodie/wav16k_corrected/Rhap-D0001.wav does not exist
2026-01-26 19:25:18,576 | INFO |  File duration: 290.2293125 seconds
2026-01-26 19:25:23,999 | INFO | speech length: 4156096
2026-01-26 19:25:26,479 | INFO | decoder input length: 6493
2026-01-26 19:25:26,479 | INFO | max output length: 6493
2026-01-26 19:25:26,479 | INFO | min output length: 649
2026-01-26 19:53:21,314 | INFO | end detected at 2098
2026-01-26 19:53:21,315 | INFO | -6965.52 * 0.5 = -3482.76 for decoder
2026-01-26 19:53:21,316 | INFO | -3195.31 * 0.5 = -1597.66 for ctc
2026-01-26 19:53:21,316 | INFO | total log probability: -5080.42
2026-01-26 19:53:21,316 | INFO | normalized log probability: -2.43
2026-01-26 19:53:21,316 | INFO | total number of ended hypotheses: 177
2026-01-26 19:53:21,338 | INFO | best hypo: ▁ils▁avais▁j'avais▁la▁chance▁d'avoir▁des▁enfants▁qui▁travaillent▁je▁dirai▁ce▁que▁ils▁préféraient▁rigoler▁que▁de▁travailler▁et▁ils▁étaient▁tout▁à▁faits▁normaux▁mais▁je▁vous▁dise▁sans▁parlers▁de▁normalement▁et▁c'est▁vrai▁que▁bons▁abitants▁d'enensemble▁de▁paris▁que▁les▁écoles▁et▁de▁très▁bons▁niveaux▁mais▁je▁ne▁dire▁avants▁aucun▁problèmes▁mais▁j'ayeuss▁aucuns▁pourblèmes▁scolaires▁pour▁mes▁enfants▁et▁j'ai▁j'aiz▁jamais▁très▁bien▁compris▁les▁gens▁qui▁vous▁que▁je▁ne▁connais▁siais▁les▁gens▁dans▁ce▁qui▁mettaient▁leurs▁enfants▁d'abords▁dans▁le▁publics▁puis▁dans▁le▁privés▁pour▁qu'ils▁puissent▁pas▁cer▁à▁éventuellements▁avec▁soient▁pas▁forts▁et▁c'a▁ils▁étaient▁très▁étonnés▁parce▁qu'ils▁quand▁ils▁voulais▁entrerer▁en▁prépants▁et▁prômes▁les▁gens▁qu'ils▁n'ai▁publes▁vous▁apparaîts▁ormalements▁je▁dire▁les▁lyecés▁je▁pense▁à▁l'intreurs▁de▁paris▁eux▁globalements▁les▁écoles▁sont▁d'un▁bons▁niveau▁là▁pense▁à▁moigeois▁dans▁leurs▁j'aiz▁à▁des▁enfants▁qui▁n'és▁et▁vraiblements▁leurs▁si▁voultraires▁est▁un▁bons▁licés▁et▁'és▁bons▁établisants▁et▁n'un▁des▁moins▁bons▁le▁problèmes▁si▁vous▁voulez▁qu'ilespe▁à▁paris▁que▁jeuies▁d'en▁en▁matents▁c'est▁le▁problème▁des▁écoles▁maternelles▁et▁praires▁dans▁lesquels▁quand▁je▁vous▁avez▁e▁fectitivements▁est▁un▁certain▁nombre▁d'enfants▁qui▁ne▁sont▁pas▁et▁qui▁ne▁parles▁pas▁amment▁en▁français▁pour▁les▁enseignants▁et▁c'est▁pas▁facles▁et▁moi▁je▁vois▁et▁les▁mamants▁je▁ne▁fais▁parles▁dans▁le▁matents▁ils▁faudraient▁mais▁si▁j'étais▁fices▁et▁d'édctions▁nationales▁je▁mettaient▁beaucoupe▁plus▁d'enseignants▁dans▁les▁maternelles▁pour▁qu'un▁enfants▁et▁sortent▁de▁maternels▁parles▁oibéligatoirement▁en▁français▁ce▁que▁tous▁mes▁enfants▁qu'arrivent▁ils▁vont▁à▁maternels▁dont▁c'est▁pas▁de▁norbles▁qu'ils▁rivents▁en▁c'pés▁ne▁parlants▁pas▁français▁mais▁ils▁rivents▁et▁ne▁parlants▁pas▁français▁parce▁que▁comme▁ils▁ons▁beaucoup▁trop▁abre▁en▁maternels▁on▁ne▁peuts▁pas▁leurs▁apprendre▁le▁français▁pour▁régles▁les▁mamants▁et▁de▁mélangets▁pourraient▁imagner▁de▁pucins▁oui▁oui▁et▁d'aborde▁et▁j'ai▁puk▁ça▁commençés▁à▁se▁faires▁à▁ajeuste▁dire▁ças▁'és▁c'és▁c'est▁qu'une▁des▁meshure▁de▁plans▁baneu▁et▁le▁boye▁mais▁je▁pense▁surtouts▁que▁dans▁le▁sièmes▁ronnons▁et▁maternels▁et▁'as▁upismements▁d'enseignants▁mais▁que▁mais▁que▁dans▁leurs▁ils▁faudraient▁qui▁est▁qu'ons▁s'és▁pas▁à▁équ'ils▁et▁de▁cas▁de▁fais▁pour▁les▁enfants▁qui▁parlent▁pas▁français▁c'est▁pas▁compliqués▁quand▁mêmes▁c'est▁pas▁très▁difficiles▁d'aprendre▁le▁français▁les▁pu▁enfants▁s'achas▁'as▁'és▁facilements▁dons▁les▁mamants▁ne▁parles▁pas▁français▁qu'ils▁ne▁voient▁pas▁survents▁leurs▁papas▁parles▁définictions▁ils▁ne▁parles▁pas▁français▁ni▁partaient▁jamais▁en▁français▁et▁parts▁pas▁penser▁à▁la▁mais▁ons▁alors▁nous▁nous▁espayants▁d'aprendre▁le▁français▁à▁moments▁pour▁que▁s'apprenents▁le▁français▁à▁leurs▁enfants▁ose▁que▁je▁ne▁faites▁aucun▁organisme▁d'états▁car▁les▁ons▁linguismes▁qui▁sont▁donnés▁aux▁immigrés▁qui▁arrivents▁ne▁sont▁parfaites▁pour▁les▁gens▁qui▁ne▁parles▁pas▁de▁tout▁ni▁qu'ils▁n'ontaient▁jamaisés▁à▁l'ékles▁parce▁que▁ças▁pers▁en▁veut▁parce▁que▁vous▁dire▁qu'apprendre▁d'à▁lire▁à▁écrire▁quels▁qu'uns▁de▁trente▁et▁ons▁qu'à▁amaise▁tenus▁un▁crayants▁ças▁fais▁pas▁en▁de▁sans▁hures▁oui▁n'à▁donc▁nons▁je▁pense▁que▁le▁problèmes▁je▁dis▁pas▁que▁tous▁les▁problèmes▁et▁allâmes▁mais▁beaucoup▁de▁problèmes▁comment▁en▁voulez▁vous▁apprendre▁à▁lire▁enfant▁qui▁parletaient▁français▁nons▁parfiniants▁'ts▁enfant▁à▁perpiers▁en▁c'épe▁et▁puis▁voils▁alors▁bien▁sûrs▁qu'uns▁n'uns▁s'en▁sorts▁et▁toujours▁les▁gens▁très▁intelligents▁tels▁parts▁et▁m'as▁dits▁les▁gens▁la▁pluparts▁gens▁et▁ils▁sont▁ormaux▁mais▁globalements▁c'est▁très▁difficiles▁quant▁ons▁prents▁les▁enfants▁d'immigrés▁qu'on▁les▁mélanges▁et▁n'uns▁çons▁et▁réussants▁de▁gamains▁qui▁parles▁les▁français▁ça▁s'ins▁alles▁très▁bien▁sûrs▁mais▁et▁le▁français▁lanants▁étrangers▁dans▁le▁milieu▁ouicts▁de▁la▁clas▁est▁un▁ons▁étranger▁siz▁beaucoup▁de▁malins▁fas▁bien▁aller▁avec▁tout▁bien▁sûrours▁je▁parles▁de▁mélanges▁oui▁mais▁je▁pense▁que▁vous▁avez▁reçons▁je▁pense▁que▁vous▁avez▁raise▁mais▁si▁vous▁voulez▁le▁mélange▁et▁difficiles▁quand▁ils▁sont▁pos▁parce▁que▁eux▁quand▁ils▁sont▁poliés▁et▁'aye▁ouiches▁c'est▁pas▁tellement▁bons▁d'erammener▁un▁petits▁enfants▁de▁prents▁voyez▁en▁beus▁un▁peu▁de▁loins▁déj'à▁c'est▁quand▁même▁pour▁eux▁d'allerer▁à▁l'écles▁c'est▁pas▁évidents▁et▁puis▁faudrais▁que▁les▁parents▁l'acceptes▁et▁ce▁que▁les▁mamants▁afrins▁elles▁ontourourents▁mal▁à▁s'éper▁de▁leurs▁parts▁elles▁ne▁fons▁pas▁très▁volontiers▁d'oi▁en▁plus▁ons▁leurs▁apiques▁que▁çavents▁que▁l'enfants▁teras▁déjeus▁et▁là▁bas▁les▁mamants▁afrentaines▁et▁veulent▁pas▁veulent▁pas

2026-01-26 19:53:21,733 | INFO | File: Rhap-D0002.wav | WER=56.168506 | S=358 D=163 I=39
2026-01-26 19:53:21,734 | INFO | ------------------------------
2026-01-26 19:53:21,734 | INFO |  File duration: 284.962125 seconds
2026-01-26 19:53:26,028 | INFO | speech length: 4295938
2026-01-26 19:53:27,047 | INFO | decoder input length: 6711
2026-01-26 19:53:27,047 | INFO | max output length: 6711
2026-01-26 19:53:27,047 | INFO | min output length: 671
2026-01-26 20:20:45,548 | INFO | end detected at 2113
2026-01-26 20:20:45,550 | INFO | -6634.69 * 0.5 = -3317.35 for decoder
2026-01-26 20:20:45,550 | INFO | -3405.20 * 0.5 = -1702.60 for ctc
2026-01-26 20:20:45,550 | INFO | total log probability: -5019.95
2026-01-26 20:20:45,550 | INFO | normalized log probability: -2.38
2026-01-26 20:20:45,550 | INFO | total number of ended hypotheses: 183
2026-01-26 20:20:45,574 | INFO | best hypo: ▁vous▁êtes▁venu▁à▁paris▁à▁quel▁âge▁eh▁bien▁voilà▁mes▁parents▁tous▁les▁deux▁on▁vécut▁à▁paris▁et▁jeunes▁et▁puces▁mariés▁ils▁sont▁partis▁vivre▁en▁provinces▁et▁peu▁à▁paris▁ils▁ont▁essayés▁d'avoir▁dans▁quelque▁chos▁à▁paris▁studis▁qu'pletants▁pour▁revenir▁et▁voir▁leurs▁amis▁puible▁et▁ils▁étaient▁très▁mélomes▁à▁l'argents▁'a▁envie▁de▁'entemble▁de▁la▁musie▁sur▁leurs▁carants▁et▁pouvez▁faiser▁çà▁ce▁qui▁faits▁au▁moments▁de▁la▁guerentre▁nous▁étions▁toujours▁en▁bretagnes▁et▁puis▁nous▁avons▁a▁été▁'avaient▁cettendroits▁à▁paris▁où▁nous▁pouveeux▁arriers▁y▁avaient▁pièces▁soyés▁mais▁eux▁eux▁finalements▁quand▁j'avons▁étaient▁ces▁très▁gros▁bombwardements▁américains▁puis▁anglais▁et▁nous▁sommes▁le▁bombletement▁gaimes▁ons▁pères▁avants▁ils▁fauts▁partirants▁pour▁leurs▁tes▁et▁lui▁ils▁avais▁pass▁les▁tes▁nuits▁ou▁de▁nuits▁à▁repés▁dans▁les▁dégomes▁pour▁aider▁les▁gens▁à▁mourir▁paris▁qu'enon▁ne▁pouvait▁pas▁sors▁les▁gens▁là▁'étaient▁imposibles▁ons▁s'est▁retrouvés▁sans▁os▁les▁gas▁et▁téléphons▁'élicticités▁et▁les▁pampiers▁c'était▁hommes▁la▁villes▁est▁assez▁utes▁et▁là▁la▁loires▁étants▁et▁les▁mal'heureux▁allait▁puiser▁de▁l'eau▁dans▁la▁là▁et▁remons▁pour▁arrer▁et▁oyez▁'és▁c'étaient▁les▁prents▁c'était▁assez▁assez▁terrible▁et▁les▁mais▁ons▁brêlés▁et▁il▁avaient▁les▁gens▁là▁lames▁qui▁étaient▁pris▁onniers▁on▁ne▁pouvaient▁pas▁à▁mon▁pères▁mais▁pass▁et▁'és▁nuits▁à▁faires▁ommes▁et▁puis▁quand▁ils▁avs▁entendu▁les▁bombwardements▁anglais▁d'a▁deriers▁soi▁la▁guits▁ons▁les▁anglais▁parces▁qu'ils▁avaient▁été▁pilotes▁d'ess▁et▁ons▁là▁ballants▁les▁dis▁et▁juites▁à▁la▁n'as▁bien▁reconnu▁et▁'ains▁ons▁les▁anglais▁et▁ils▁vouls▁meuire▁qu'ils▁fautaient▁dévoyers▁et▁réc▁et▁puis▁là▁ider▁de▁partirurs▁nous▁ne▁sommes▁partis▁le▁lendeins▁nous▁avons▁attendu▁qu'ains▁tes▁ons▁heurs▁dans▁la▁gare▁parces▁que▁tous▁les▁officiers▁alements▁tout▁l'états▁jor▁alement▁renents▁à▁paris▁leurs▁les▁fins▁et▁ons▁plus▁'as▁'és▁'ils▁l'habitudes▁d'aquererre▁quelque▁chos▁à▁ce▁pass▁et▁comme▁ça▁c'étaient▁pas▁à▁norble▁et▁finalement▁nous▁sommes▁entrés▁à▁paris▁et▁nous▁sommes▁ins▁allés▁dans▁un▁petits▁appartements▁et▁ne▁sommes▁les▁plus▁jamais▁partis▁de▁paris▁ons▁toujours▁res▁tés▁à▁paris▁en▁allants▁fidèlement▁en▁bretagnes▁ouvent▁et▁l'és▁mais▁nous▁vivons▁à▁paris▁puits▁vous▁tes▁que▁vraiment▁paris▁ons▁ons▁ez▁pas▁oui▁oui▁mais▁comme▁les▁parents▁tous▁les▁deux▁on▁vés▁toutes▁leurs▁jeunes▁à▁paris▁puis▁nous▁d'autres▁hommes▁la▁familles▁aussi▁avants▁ouifins▁n'est▁attachés▁à▁paris▁mais▁jeunes▁je▁me▁rends▁tes▁que▁je▁suis▁attachés▁à▁paris▁malingrés▁les▁malingrés▁la▁vies▁très▁différentes▁maintenants▁mais▁on▁était▁arrivés▁et▁votres▁à▁arrivés▁à▁paris▁et▁donc▁depuisque▁vous▁avais▁toujours▁avités▁à▁paris▁et▁toujours▁à▁paris▁et▁donc▁elles▁dans▁quels▁quartiers▁ons▁que▁nous▁allons▁ons▁eus▁ontinuits▁et▁hbiters▁leurs▁leurs▁là▁où▁nous▁étions▁et▁tés▁ons▁qu'artiers▁et▁extrêmements▁sythaythiques▁mais▁tes▁tes▁et▁peup▁j'as▁j'allés▁à▁l'égle▁là▁ons▁mes▁parents▁onnaisait▁avec▁les▁beautoupe▁d'ains▁lors▁'a▁là▁ça▁c'é▁un▁quartier▁très▁agréables▁et▁rès▁nous▁avons▁les▁habés▁dans▁le▁siz▁pas▁très▁loin▁d'ici▁et▁à▁la▁fin▁de▁leurs▁ie▁mes▁parents▁ontaient▁vivras▁ils▁et▁times▁à▁nouveau▁dans▁un▁petit▁appartements▁ils▁étaient▁de▁j'étaient▁très▁contants▁là▁d'appartements▁facles▁à▁vivre▁en▁soleillés▁ons▁agréables▁et▁j'étaient▁très▁contents▁ons▁se▁plaisés▁becoup▁mais▁vous▁et▁votres▁i▁et▁votres▁familles▁avec▁leurs▁enfants▁vous▁'êtes▁toujours▁res▁à▁paris▁'ads▁oui▁et▁nous▁sommes▁res▁tés▁à▁paris▁et▁ons▁fois▁de▁propositions▁nous▁s'en▁aller▁finalement▁mais▁çà▁ne▁s'est▁pas▁'avés▁d'ins▁tants▁là▁si▁ons▁anger▁de▁situations▁ons▁avaient▁plus▁propositions▁et▁ons▁apitements▁allons▁ons▁finissez▁ce▁qu'ons▁voulait▁comme▁'as▁à▁pass▁et▁ons▁étaient▁siz▁ces▁sans▁étonnants▁à▁dire▁maintenants▁ouibleiss▁comme▁ças▁pass▁et▁ons▁ies▁achanger▁à▁plusieurs▁fois▁de▁siituations▁pas▁beautoup▁a'ailleurs▁les▁gens▁qui▁anges▁beautoupeus▁changer▁de▁fois▁mais▁les▁jours▁avants▁ils▁avaient▁plus▁propositions▁et▁nous▁presques▁sans▁alement▁becoupous▁lui▁mêmes▁en▁fins▁ons▁et▁comme▁'aix▁à▁pass▁et▁nons▁nous▁sommes▁res▁tés▁à▁paris▁mais▁ons▁et▁nés▁tout▁près▁de▁fins▁mais▁ils▁n'avés▁toute▁de▁sa▁vies▁pas▁loins▁d'ici▁et▁vites▁et▁'és▁il▁était▁attachés▁à▁paris▁et▁oigs▁nous▁serions▁partis▁et▁volotiers▁ouibley▁avés▁les▁projets▁de▁partireux▁vers▁l'étrangeus▁et▁ons▁et▁eux▁nons▁à▁l'étranger▁et▁finalements▁bre▁ons▁'ois▁de▁res▁et▁on▁nons▁pour▁votre▁hommes

2026-01-26 20:20:45,946 | INFO | File: Rhap-D0003.wav | WER=62.512980 | S=400 D=168 I=34
2026-01-26 20:20:45,946 | INFO | ------------------------------
2026-01-26 20:20:45,946 | INFO |  File duration: 292.681125 seconds
2026-01-26 20:20:51,125 | INFO | speech length: 4499826
