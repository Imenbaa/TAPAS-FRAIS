2026-01-29 16:16:51,526 | INFO | Fetch hyperparams.yaml: Using file found at '/vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-wav2vec2-commonvoice-fr/hyperparams.yaml'
2026-01-29 16:16:51,527 | DEBUG | Fetch: Source and destination '/vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-wav2vec2-commonvoice-fr/custom.py' are identical, returning assuming this is intended
2026-01-29 16:16:51,560 | WARNING | This version of torchaudio is old. SpeechBrain no longer tries using the torchaudio global backend mechanism in recipes, so if you encounter issues, update torchaudio to >=2.1.0.
2026-01-29 16:16:52,448 | WARNING | speechbrain.lobes.models.huggingface_transformers.huggingface - Wav2Vec2Model is frozen.
2026-01-29 16:16:52,460 | DEBUG | Collecting files (or symlinks) for pretraining in /vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-wav2vec2-commonvoice-fr.
2026-01-29 16:16:52,460 | INFO | Fetch wav2vec2.ckpt: Using file found at '/vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-wav2vec2-commonvoice-fr/wav2vec2.ckpt'
2026-01-29 16:16:52,460 | DEBUG | Set local path in self.paths["wav2vec2"] = /vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-wav2vec2-commonvoice-fr/wav2vec2.ckpt
2026-01-29 16:16:52,460 | INFO | Fetch asr.ckpt: Using file found at '/vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-wav2vec2-commonvoice-fr/asr.ckpt'
2026-01-29 16:16:52,460 | DEBUG | Set local path in self.paths["asr"] = /vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-wav2vec2-commonvoice-fr/asr.ckpt
2026-01-29 16:16:52,460 | INFO | Fetch tokenizer.ckpt: Using file found at '/vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-wav2vec2-commonvoice-fr/tokenizer.ckpt'
2026-01-29 16:16:52,460 | DEBUG | Set local path in self.paths["tokenizer"] = /vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-wav2vec2-commonvoice-fr/tokenizer.ckpt
2026-01-29 16:16:52,460 | INFO | Loading pretrained files for: wav2vec2, asr, tokenizer
2026-01-29 16:16:52,460 | DEBUG | Redirecting (loading from local path): wav2vec2 -> /vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-wav2vec2-commonvoice-fr/wav2vec2.ckpt
2026-01-29 16:16:52,460 | DEBUG | Redirecting (loading from local path): asr -> /vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-wav2vec2-commonvoice-fr/asr.ckpt
2026-01-29 16:16:52,460 | DEBUG | Redirecting (loading from local path): tokenizer -> /vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-wav2vec2-commonvoice-fr/tokenizer.ckpt
2026-01-29 16:16:55,264 | INFO | Fetch hyperparams.yaml: Using file found at '/vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-whisper-medium-commonvoice-fr/hyperparams.yaml'
2026-01-29 16:16:55,265 | DEBUG | Fetch: Source and destination '/vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-whisper-medium-commonvoice-fr/custom.py' are identical, returning assuming this is intended
2026-01-29 16:16:56,691 | WARNING | speechbrain.lobes.models.huggingface_transformers.huggingface - WhisperModel is frozen.
2026-01-29 16:16:56,691 | WARNING | speechbrain.lobes.models.huggingface_transformers.whisper - whisper encoder-decoder is frozen.
2026-01-29 16:16:56,808 | DEBUG | Collecting files (or symlinks) for pretraining in /vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-whisper-medium-commonvoice-fr.
2026-01-29 16:16:56,808 | INFO | Fetch whisper.ckpt: Using file found at '/vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-whisper-medium-commonvoice-fr/whisper.ckpt'
2026-01-29 16:16:56,808 | DEBUG | Set local path in self.paths["whisper"] = /vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-whisper-medium-commonvoice-fr/whisper.ckpt
2026-01-29 16:16:56,808 | INFO | Loading pretrained files for: whisper
2026-01-29 16:16:56,808 | DEBUG | Redirecting (loading from local path): whisper -> /vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-whisper-medium-commonvoice-fr/whisper.ckpt
2026-01-29 16:16:59,727 | INFO | Fetch hyperparams.yaml: Using file found at '/vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-whisper-large-v2-commonvoice-fr/hyperparams.yaml'
2026-01-29 16:16:59,727 | DEBUG | Fetch: Source and destination '/vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-whisper-large-v2-commonvoice-fr/custom.py' are identical, returning assuming this is intended
2026-01-29 16:17:01,310 | WARNING | speechbrain.lobes.models.huggingface_transformers.huggingface - WhisperModel is frozen.
2026-01-29 16:17:01,311 | WARNING | speechbrain.lobes.models.huggingface_transformers.whisper - whisper encoder-decoder is frozen.
2026-01-29 16:17:01,832 | DEBUG | Collecting files (or symlinks) for pretraining in /vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-whisper-large-v2-commonvoice-fr.
2026-01-29 16:17:01,832 | INFO | Fetch whisper.ckpt: Using file found at '/vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-whisper-large-v2-commonvoice-fr/whisper.ckpt'
2026-01-29 16:17:01,832 | DEBUG | Set local path in self.paths["whisper"] = /vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-whisper-large-v2-commonvoice-fr/whisper.ckpt
2026-01-29 16:17:01,832 | INFO | Loading pretrained files for: whisper
2026-01-29 16:17:01,832 | DEBUG | Redirecting (loading from local path): whisper -> /vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-whisper-large-v2-commonvoice-fr/whisper.ckpt
2026-01-29 16:17:08,078 | INFO | ==================================Rhap-M0009.wav=========================================
2026-01-29 16:17:08,084 | INFO | Using rVAD model
2026-01-29 16:17:12,330 | INFO | Created a temporary directory at /tmp/tmpun2uzfeo
2026-01-29 16:17:12,331 | INFO | Writing /tmp/tmpun2uzfeo/_remote_module_non_scriptable.py
2026-01-29 16:17:12,472 | DEBUG | Registered checkpoint save hook for save
2026-01-29 16:17:12,472 | DEBUG | Registered checkpoint load hook for load_if_possible
2026-01-29 16:17:12,600 | INFO | Chunk: 0 | WER=27.551020 | S=11 D=15 I=1
2026-01-29 16:17:12,601 | INFO | Chunk: 1 | WER=50.000000 | S=1 D=4 I=3
2026-01-29 16:17:12,603 | INFO | Chunk: 2 | WER=38.805970 | S=10 D=16 I=0
2026-01-29 16:17:12,604 | INFO | Chunk: 3 | WER=65.217391 | S=10 D=3 I=2
2026-01-29 16:17:12,620 | INFO | File: Rhap-M0009.wav | WER=31.746032 | S=31 D=23 I=6
2026-01-29 16:17:12,620 | INFO | ------------------------------
2026-01-29 16:17:12,620 | INFO | w2vec vad chunk Done!
2026-01-29 16:17:18,958 | INFO | Chunk: 0 | WER=72.448980 | S=4 D=67 I=0
2026-01-29 16:17:18,959 | INFO | Chunk: 1 | WER=25.000000 | S=0 D=4 I=0
2026-01-29 16:17:18,960 | INFO | Chunk: 2 | WER=65.671642 | S=2 D=42 I=0
2026-01-29 16:17:18,961 | INFO | Chunk: 3 | WER=56.521739 | S=8 D=5 I=0
2026-01-29 16:17:18,971 | INFO | File: Rhap-M0009.wav | WER=61.904762 | S=14 D=103 I=0
2026-01-29 16:17:18,971 | INFO | ------------------------------
2026-01-29 16:17:18,971 | INFO | whisper med Done!
2026-01-29 16:17:28,861 | INFO | Chunk: 0 | WER=67.346939 | S=5 D=61 I=0
2026-01-29 16:17:28,862 | INFO | Chunk: 1 | WER=43.750000 | S=0 D=4 I=3
2026-01-29 16:17:28,864 | INFO | Chunk: 2 | WER=49.253731 | S=5 D=25 I=3
2026-01-29 16:17:28,864 | INFO | Chunk: 3 | WER=65.217391 | S=11 D=3 I=1
2026-01-29 16:17:28,876 | INFO | File: Rhap-M0009.wav | WER=55.026455 | S=21 D=77 I=6
2026-01-29 16:17:28,876 | INFO | ------------------------------
2026-01-29 16:17:28,876 | INFO | whisper large Done!
2026-01-29 16:17:29,113 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-29 16:17:29,144 | INFO | Vocabulary size: 350
2026-01-29 16:17:29,710 | INFO | Gradient checkpoint layers: []
2026-01-29 16:17:30,299 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 16:17:30,302 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 16:17:30,302 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 16:17:30,303 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-29 16:17:30,304 | INFO | speech length: 427680
2026-01-29 16:17:30,351 | INFO | decoder input length: 667
2026-01-29 16:17:30,351 | INFO | max output length: 667
2026-01-29 16:17:30,351 | INFO | min output length: 66
2026-01-29 16:17:50,150 | INFO | end detected at 197
2026-01-29 16:17:50,152 | INFO | -436.76 * 0.5 = -218.38 for decoder
2026-01-29 16:17:50,152 | INFO | -142.52 * 0.5 = -71.26 for ctc
2026-01-29 16:17:50,152 | INFO | total log probability: -289.64
2026-01-29 16:17:50,152 | INFO | normalized log probability: -1.51
2026-01-29 16:17:50,152 | INFO | total number of ended hypotheses: 155
2026-01-29 16:17:50,154 | INFO | best hypo: ▁alors▁là▁tu▁vois▁donc▁tu▁continues▁tout▁droit▁ou▁droite▁tu▁traverses▁toute▁arrivant▁un▁rond▁point▁au▁rond▁point▁ces▁tout▁droits▁directions▁saint▁jean▁de▁mauriennes▁dans▁toute▁toujours▁st▁jean▁de▁maurienne▁tups▁au▁rond▁point▁donc▁toujours▁tout▁droit▁avec▁tu▁monte▁une▁grande▁grande▁ligne▁droite▁tu▁et▁passe▁devant▁la▁pisincine▁et▁un▁stades▁côté▁et▁arrives▁à▁un▁rond▁point▁platurant▁dans▁le▁centre▁t'arrivant▁un▁rond▁point▁si▁donc▁à▁gauche

2026-01-29 16:17:50,159 | INFO | speech length: 104960
2026-01-29 16:17:50,220 | INFO | decoder input length: 163
2026-01-29 16:17:50,220 | INFO | max output length: 163
2026-01-29 16:17:50,220 | INFO | min output length: 16
2026-01-29 16:17:52,036 | INFO | end detected at 37
2026-01-29 16:17:52,037 | INFO | -14.66 * 0.5 =  -7.33 for decoder
2026-01-29 16:17:52,037 | INFO | -19.95 * 0.5 =  -9.97 for ctc
2026-01-29 16:17:52,037 | INFO | total log probability: -17.30
2026-01-29 16:17:52,037 | INFO | normalized log probability: -0.56
2026-01-29 16:17:52,037 | INFO | total number of ended hypotheses: 175
2026-01-29 16:17:52,038 | INFO | best hypo: ▁ensuite▁donc▁go▁schlash▁rox▁se▁sera▁indiqué▁déjà▁gare▁mais▁bon▁tu

2026-01-29 16:17:52,039 | INFO | speech length: 334080
2026-01-29 16:17:52,074 | INFO | decoder input length: 521
2026-01-29 16:17:52,074 | INFO | max output length: 521
2026-01-29 16:17:52,074 | INFO | min output length: 52
2026-01-29 16:18:02,057 | INFO | end detected at 119
2026-01-29 16:18:02,058 | INFO | -219.36 * 0.5 = -109.68 for decoder
2026-01-29 16:18:02,058 | INFO | -103.21 * 0.5 = -51.60 for ctc
2026-01-29 16:18:02,058 | INFO | total log probability: -161.28
2026-01-29 16:18:02,059 | INFO | normalized log probability: -1.41
2026-01-29 16:18:02,059 | INFO | total number of ended hypotheses: 155
2026-01-29 16:18:02,060 | INFO | best hypo: ▁il▁traverse▁le▁champ▁de▁foie▁t'arrive▁un▁autre▁rond▁point▁placé▁encore▁à▁gauche▁et▁tu▁descends▁avec▁une▁route▁qui▁descends▁tu▁passe▁devant▁les▁pompiers▁et▁ensuite▁'▁tubavant▁la▁bastille▁scie'▁et▁ensuite▁c'est▁la▁première▁'▁deuxième▁ou▁troisième▁ou▁troisième▁lignembre▁à▁droite

2026-01-29 16:18:02,062 | INFO | speech length: 89600
2026-01-29 16:18:02,099 | INFO | decoder input length: 139
2026-01-29 16:18:02,099 | INFO | max output length: 139
2026-01-29 16:18:02,099 | INFO | min output length: 13
2026-01-29 16:18:04,437 | INFO | end detected at 53
2026-01-29 16:18:04,439 | INFO | -12.25 * 0.5 =  -6.13 for decoder
2026-01-29 16:18:04,439 | INFO | -13.68 * 0.5 =  -6.84 for ctc
2026-01-29 16:18:04,439 | INFO | total log probability: -12.96
2026-01-29 16:18:04,439 | INFO | normalized log probability: -0.29
2026-01-29 16:18:04,439 | INFO | total number of ended hypotheses: 201
2026-01-29 16:18:04,440 | INFO | best hypo: ▁et▁tu▁montes▁et▁voilà▁t▁arrive▁devant▁la▁gare▁après▁la▁sin▁indiqué▁cabinet▁téléphone▁et▁voilà

2026-01-29 16:18:04,449 | INFO | Chunk: 0 | WER=44.897959 | S=23 D=19 I=2
2026-01-29 16:18:04,450 | INFO | Chunk: 1 | WER=81.250000 | S=4 D=6 I=3
2026-01-29 16:18:04,452 | INFO | Chunk: 2 | WER=50.746269 | S=14 D=19 I=1
2026-01-29 16:18:04,452 | INFO | Chunk: 3 | WER=52.173913 | S=7 D=5 I=0
2026-01-29 16:18:04,466 | INFO | File: Rhap-M0009.wav | WER=44.973545 | S=47 D=33 I=5
2026-01-29 16:18:04,466 | INFO | ------------------------------
2026-01-29 16:18:04,466 | INFO | Conf cv Done!
2026-01-29 16:18:04,666 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-29 16:18:04,684 | INFO | Vocabulary size: 47
2026-01-29 16:18:05,193 | INFO | Gradient checkpoint layers: []
2026-01-29 16:18:05,872 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 16:18:05,875 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 16:18:05,875 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 16:18:05,876 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-29 16:18:05,878 | INFO | speech length: 427680
2026-01-29 16:18:05,911 | INFO | decoder input length: 667
2026-01-29 16:18:05,912 | INFO | max output length: 667
2026-01-29 16:18:05,912 | INFO | min output length: 66
2026-01-29 16:18:45,148 | INFO | end detected at 502
2026-01-29 16:18:45,149 | INFO | -452.39 * 0.5 = -226.19 for decoder
2026-01-29 16:18:45,150 | INFO | -132.70 * 0.5 = -66.35 for ctc
2026-01-29 16:18:45,150 | INFO | total log probability: -292.55
2026-01-29 16:18:45,150 | INFO | normalized log probability: -0.59
2026-01-29 16:18:45,150 | INFO | total number of ended hypotheses: 130
2026-01-29 16:18:45,155 | INFO | best hypo: alors<space>là<space>tu<space>vois<space>donc<space>tu<space>continues<space>te<space>fous<space>droite<space>ou<space>droite<space>tu<space>traverses<space>tout<space>arrive<space>à<space>un<space>gros<space>point<space>oh<space>on<space>prend<space>c'est<space>tout<space>droit<space>direction<space>saint<space>jean<space>de<space>marienne<space>pour<space>cent<space>hesitation<space>donc<space>tout<space>toujours<space>son<space>genre<space>de<space>marienne<space>tu<space>passes<space>un<space>autre<space>on<space>point<space>donc<space>toujours<space>tout<space>droit<space>après<space>tje<space>monte<space>une<space>grande<space>grande<space>ligne<space>droite<space>tu<space>passes<space>devant<space>la<space>piscine<space>il<space>y<space>a<space>un<space>stade<space>aussi<space>à<space>côté<space>est<space>arrive<space>à<space>un<space>rone<space>poine<space>done<space>gaus<space>te<space>rontre<space>dans<space>le<space>centre<space>ville<space>tarrive<space>à<space>ue<space>ron<space>poine<space>c'est<space>donc<space>e<space>gauche

2026-01-29 16:18:45,158 | INFO | speech length: 104960
2026-01-29 16:18:45,194 | INFO | decoder input length: 163
2026-01-29 16:18:45,194 | INFO | max output length: 163
2026-01-29 16:18:45,194 | INFO | min output length: 16
2026-01-29 16:18:49,632 | INFO | end detected at 108
2026-01-29 16:18:49,634 | INFO |  -8.71 * 0.5 =  -4.36 for decoder
2026-01-29 16:18:49,635 | INFO | -22.98 * 0.5 = -11.49 for ctc
2026-01-29 16:18:49,635 | INFO | total log probability: -15.85
2026-01-29 16:18:49,635 | INFO | normalized log probability: -0.19
2026-01-29 16:18:49,635 | INFO | total number of ended hypotheses: 218
2026-01-29 16:18:49,636 | INFO | best hypo: euh<space>ensuite<space>donc<space>gauche<space>là<space>je<space>crois<space>que<space>ça<space>sera<space>indiqué<space>déjà<space>gare<space>euh<space>mais<space>bon<space>tu

2026-01-29 16:18:49,638 | INFO | speech length: 334080
2026-01-29 16:18:49,666 | INFO | decoder input length: 521
2026-01-29 16:18:49,666 | INFO | max output length: 521
2026-01-29 16:18:49,666 | INFO | min output length: 52
2026-01-29 16:19:13,386 | INFO | end detected at 333
2026-01-29 16:19:13,388 | INFO | -109.09 * 0.5 = -54.55 for decoder
2026-01-29 16:19:13,388 | INFO | -40.55 * 0.5 = -20.28 for ctc
2026-01-29 16:19:13,388 | INFO | total log probability: -74.82
2026-01-29 16:19:13,388 | INFO | normalized log probability: -0.23
2026-01-29 16:19:13,388 | INFO | total number of ended hypotheses: 176
2026-01-29 16:19:13,392 | INFO | best hypo: je<space>traverse<space>le<space>champ<space>de<space>foire<space>ça<space>arrive<space>un<space>autre<space>point<space>là<space>c'est<space>encore<space>à<space>gauche<space>pour<space>tu<space>descend<space>en<space>fait<space>c'est<space>une<space>route<space>qui<space>descend<space>je<space>passe<space>devant<space>les<space>pompiers<space>et<space>pour<space>cent<space>hesitation<space>ensuite<space>premier<space>tu<space>passes<space>dans<space>la<space>bastille<space>aussi<space>et<space>ensuite<space>c'est<space>la<space>première<space>ou<space>fin<space>euh<space>deuxième<space>ou<space>troisième<space>troisième<space>il<space>temps<space>à<space>droite

2026-01-29 16:19:13,395 | INFO | speech length: 89600
2026-01-29 16:19:13,423 | INFO | decoder input length: 139
2026-01-29 16:19:13,424 | INFO | max output length: 139
2026-01-29 16:19:13,424 | INFO | min output length: 13
2026-01-29 16:19:17,941 | INFO | end detected at 115
2026-01-29 16:19:17,943 | INFO | -28.53 * 0.5 = -14.27 for decoder
2026-01-29 16:19:17,943 | INFO | -16.09 * 0.5 =  -8.05 for ctc
2026-01-29 16:19:17,943 | INFO | total log probability: -22.31
2026-01-29 16:19:17,943 | INFO | normalized log probability: -0.21
2026-01-29 16:19:17,943 | INFO | total number of ended hypotheses: 210
2026-01-29 16:19:17,944 | INFO | best hypo: et<space>tu<space>montes<space>et<space>voilà<space>ça<space>arrive<space>devant<space>là<space>y<space>a<space>connaissant<space>diké<space>il<space>y<space>a<space>a<space>une<space>cabine<space>de<space>téléphone<space>et<space>voilà

2026-01-29 16:19:17,953 | INFO | Chunk: 0 | WER=40.816327 | S=33 D=4 I=3
2026-01-29 16:19:17,954 | INFO | Chunk: 1 | WER=43.750000 | S=0 D=3 I=4
2026-01-29 16:19:17,956 | INFO | Chunk: 2 | WER=35.820896 | S=15 D=7 I=2
2026-01-29 16:19:17,957 | INFO | Chunk: 3 | WER=73.913043 | S=7 D=5 I=5
2026-01-29 16:19:17,974 | INFO | File: Rhap-M0009.wav | WER=40.211640 | S=50 D=8 I=18
2026-01-29 16:19:17,974 | INFO | ------------------------------
2026-01-29 16:19:17,974 | INFO | Conf ester Done!
2026-01-29 16:20:05,423 | INFO | Chunk: 0 | WER=43.877551 | S=22 D=18 I=3
2026-01-29 16:20:05,424 | INFO | Chunk: 1 | WER=37.500000 | S=1 D=4 I=1
2026-01-29 16:20:05,428 | INFO | Chunk: 2 | WER=50.746269 | S=18 D=14 I=2
2026-01-29 16:20:05,428 | INFO | Chunk: 3 | WER=86.956522 | S=15 D=4 I=1
2026-01-29 16:20:05,444 | INFO | File: Rhap-M0009.wav | WER=45.502646 | S=56 D=24 I=6
2026-01-29 16:20:05,444 | INFO | ------------------------------
2026-01-29 16:20:05,444 | INFO | hmm_tdnn Done!
2026-01-29 16:20:05,658 | INFO | ==================================Rhap-M0010.wav=========================================
2026-01-29 16:20:05,673 | INFO | Using rVAD model
2026-01-29 16:20:06,473 | INFO | Chunk: 0 | WER=27.586207 | S=4 D=4 I=0
2026-01-29 16:20:06,474 | INFO | Chunk: 1 | WER=24.242424 | S=4 D=3 I=1
2026-01-29 16:20:06,477 | INFO | File: Rhap-M0010.wav | WER=25.806452 | S=8 D=7 I=1
2026-01-29 16:20:06,477 | INFO | ------------------------------
2026-01-29 16:20:06,477 | INFO | w2vec vad chunk Done!
2026-01-29 16:20:09,397 | INFO | Chunk: 0 | WER=24.137931 | S=3 D=4 I=0
2026-01-29 16:20:09,398 | INFO | Chunk: 1 | WER=21.212121 | S=3 D=3 I=1
2026-01-29 16:20:09,402 | INFO | File: Rhap-M0010.wav | WER=22.580645 | S=6 D=7 I=1
2026-01-29 16:20:09,402 | INFO | ------------------------------
2026-01-29 16:20:09,402 | INFO | whisper med Done!
2026-01-29 16:20:13,258 | INFO | Chunk: 0 | WER=13.793103 | S=1 D=3 I=0
2026-01-29 16:20:13,259 | INFO | Chunk: 1 | WER=30.303030 | S=4 D=5 I=1
2026-01-29 16:20:13,262 | INFO | File: Rhap-M0010.wav | WER=22.580645 | S=5 D=8 I=1
2026-01-29 16:20:13,262 | INFO | ------------------------------
2026-01-29 16:20:13,262 | INFO | whisper large Done!
2026-01-29 16:20:13,458 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-29 16:20:13,489 | INFO | Vocabulary size: 350
2026-01-29 16:20:14,199 | INFO | Gradient checkpoint layers: []
2026-01-29 16:20:14,871 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 16:20:14,875 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 16:20:14,875 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 16:20:14,875 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-29 16:20:14,876 | INFO | speech length: 108320
2026-01-29 16:20:14,914 | INFO | decoder input length: 168
2026-01-29 16:20:14,914 | INFO | max output length: 168
2026-01-29 16:20:14,914 | INFO | min output length: 16
2026-01-29 16:20:18,020 | INFO | end detected at 66
2026-01-29 16:20:18,022 | INFO | -10.96 * 0.5 =  -5.48 for decoder
2026-01-29 16:20:18,022 | INFO | -11.52 * 0.5 =  -5.76 for ctc
2026-01-29 16:20:18,022 | INFO | total log probability: -11.24
2026-01-29 16:20:18,022 | INFO | normalized log probability: -0.19
2026-01-29 16:20:18,022 | INFO | total number of ended hypotheses: 186
2026-01-29 16:20:18,023 | INFO | best hypo: ▁alors▁tu▁vas▁sur▁forum▁là▁ensuite▁des▁escaliers▁et▁descends▁les▁escaliers▁tu▁vas▁tout▁droit▁tu▁vas▁passer▁dans▁un▁magasin▁de▁pantalon

2026-01-29 16:20:18,026 | INFO | speech length: 144160
2026-01-29 16:20:18,057 | INFO | decoder input length: 224
2026-01-29 16:20:18,057 | INFO | max output length: 224
2026-01-29 16:20:18,057 | INFO | min output length: 22
2026-01-29 16:20:22,432 | INFO | end detected at 83
2026-01-29 16:20:22,434 | INFO | -11.50 * 0.5 =  -5.75 for decoder
2026-01-29 16:20:22,434 | INFO | -19.80 * 0.5 =  -9.90 for ctc
2026-01-29 16:20:22,434 | INFO | total log probability: -15.65
2026-01-29 16:20:22,434 | INFO | normalized log probability: -0.21
2026-01-29 16:20:22,434 | INFO | total number of ended hypotheses: 213
2026-01-29 16:20:22,435 | INFO | best hypo: ▁ensuite▁tu▁vas▁te▁retrouver▁vers▁le▁cinéma▁star▁puis▁descends▁toute▁la▁pente▁ensuite▁vers▁un▁rond▁point▁puis▁tourne▁à▁gauche▁et▁en▁face▁néanmoins▁de▁ravoir▁la▁guerre

2026-01-29 16:20:22,441 | INFO | Chunk: 0 | WER=24.137931 | S=3 D=4 I=0
2026-01-29 16:20:22,442 | INFO | Chunk: 1 | WER=33.333333 | S=7 D=3 I=1
2026-01-29 16:20:22,445 | INFO | File: Rhap-M0010.wav | WER=29.032258 | S=10 D=7 I=1
2026-01-29 16:20:22,445 | INFO | ------------------------------
2026-01-29 16:20:22,445 | INFO | Conf cv Done!
2026-01-29 16:20:22,645 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-29 16:20:22,663 | INFO | Vocabulary size: 47
2026-01-29 16:20:23,163 | INFO | Gradient checkpoint layers: []
2026-01-29 16:20:23,810 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 16:20:23,814 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 16:20:23,814 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 16:20:23,815 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-29 16:20:23,818 | INFO | speech length: 108320
2026-01-29 16:20:23,848 | INFO | decoder input length: 168
2026-01-29 16:20:23,848 | INFO | max output length: 168
2026-01-29 16:20:23,848 | INFO | min output length: 16
2026-01-29 16:20:30,305 | INFO | end detected at 149
2026-01-29 16:20:30,308 | INFO | -26.53 * 0.5 = -13.27 for decoder
2026-01-29 16:20:30,308 | INFO | -36.43 * 0.5 = -18.22 for ctc
2026-01-29 16:20:30,308 | INFO | total log probability: -31.48
2026-01-29 16:20:30,308 | INFO | normalized log probability: -0.23
2026-01-29 16:20:30,308 | INFO | total number of ended hypotheses: 223
2026-01-29 16:20:30,310 | INFO | best hypo: alors<space>tu<space>vas<space>sur<space>le<space>forum<space>là<space>ensuite<space>c'est<space>des<space>descaliers<space>il<space>descend<space>les<space>escaliers<space>tu<space>bats<space>tou<space>droit<space>tu<space>vas<space>passer<space>mon<space>magasin<space>de<space>patamon

2026-01-29 16:20:30,313 | INFO | speech length: 144160
2026-01-29 16:20:30,350 | INFO | decoder input length: 224
2026-01-29 16:20:30,350 | INFO | max output length: 224
2026-01-29 16:20:30,350 | INFO | min output length: 22
2026-01-29 16:20:39,756 | INFO | end detected at 160
2026-01-29 16:20:39,759 | INFO | -33.75 * 0.5 = -16.88 for decoder
2026-01-29 16:20:39,760 | INFO | -32.82 * 0.5 = -16.41 for ctc
2026-01-29 16:20:39,760 | INFO | total log probability: -33.28
2026-01-29 16:20:39,760 | INFO | normalized log probability: -0.22
2026-01-29 16:20:39,760 | INFO | total number of ended hypotheses: 185
2026-01-29 16:20:39,762 | INFO | best hypo: ensuite<space>tu<space>bas<space>tu<space>retrouves<space>vers<space>le<space>cinéma<space>star<space>qui<space>descend<space>toute<space>la<space>pente<space>ensuite<space>tu<space>berras<space>hon<space>point<space>son<space>la<space>gauche<space>et<space>en<space>face<space>normal<space>devra<space>voilà<space>y<space>a

2026-01-29 16:20:39,768 | INFO | Chunk: 0 | WER=37.931034 | S=9 D=2 I=0
2026-01-29 16:20:39,769 | INFO | Chunk: 1 | WER=48.484848 | S=13 D=3 I=0
2026-01-29 16:20:39,773 | INFO | File: Rhap-M0010.wav | WER=43.548387 | S=22 D=5 I=0
2026-01-29 16:20:39,773 | INFO | ------------------------------
2026-01-29 16:20:39,773 | INFO | Conf ester Done!
2026-01-29 16:20:58,396 | INFO | Chunk: 0 | WER=65.517241 | S=8 D=11 I=0
2026-01-29 16:20:58,399 | INFO | Chunk: 1 | WER=54.545455 | S=11 D=7 I=0
2026-01-29 16:20:58,405 | INFO | File: Rhap-M0010.wav | WER=59.677419 | S=19 D=18 I=0
2026-01-29 16:20:58,405 | INFO | ------------------------------
2026-01-29 16:20:58,405 | INFO | hmm_tdnn Done!
2026-01-29 16:20:58,550 | INFO | ==================================Rhap-M0011.wav=========================================
2026-01-29 16:20:58,556 | INFO | Using rVAD model
2026-01-29 16:21:00,970 | INFO | Chunk: 0 | WER=30.769231 | S=3 D=1 I=0
2026-01-29 16:21:00,971 | INFO | Chunk: 1 | WER=19.444444 | S=4 D=2 I=1
2026-01-29 16:21:00,971 | INFO | Chunk: 2 | WER=11.111111 | S=2 D=0 I=0
2026-01-29 16:21:00,972 | INFO | Chunk: 3 | WER=21.875000 | S=3 D=3 I=1
2026-01-29 16:21:00,973 | INFO | Chunk: 4 | WER=76.470588 | S=10 D=3 I=0
2026-01-29 16:21:00,973 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 16:21:00,974 | INFO | Chunk: 6 | WER=13.888889 | S=3 D=2 I=0
2026-01-29 16:21:00,985 | INFO | File: Rhap-M0011.wav | WER=24.203822 | S=25 D=11 I=2
2026-01-29 16:21:00,985 | INFO | ------------------------------
2026-01-29 16:21:00,985 | INFO | w2vec vad chunk Done!
2026-01-29 16:21:06,247 | INFO | Chunk: 0 | WER=23.076923 | S=1 D=2 I=0
2026-01-29 16:21:06,248 | INFO | Chunk: 1 | WER=44.444444 | S=0 D=16 I=0
2026-01-29 16:21:06,248 | INFO | Chunk: 2 | WER=5.555556 | S=1 D=0 I=0
2026-01-29 16:21:06,249 | INFO | Chunk: 3 | WER=65.625000 | S=4 D=17 I=0
2026-01-29 16:21:06,249 | INFO | Chunk: 4 | WER=52.941176 | S=4 D=5 I=0
2026-01-29 16:21:06,250 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 16:21:06,250 | INFO | Chunk: 6 | WER=22.222222 | S=2 D=5 I=1
2026-01-29 16:21:06,261 | INFO | File: Rhap-M0011.wav | WER=36.942675 | S=12 D=45 I=1
2026-01-29 16:21:06,261 | INFO | ------------------------------
2026-01-29 16:21:06,261 | INFO | whisper med Done!
2026-01-29 16:21:14,138 | INFO | Chunk: 0 | WER=7.692308 | S=1 D=0 I=0
2026-01-29 16:21:14,139 | INFO | Chunk: 1 | WER=50.000000 | S=2 D=16 I=0
2026-01-29 16:21:14,139 | INFO | Chunk: 2 | WER=11.111111 | S=2 D=0 I=0
2026-01-29 16:21:14,140 | INFO | Chunk: 3 | WER=28.125000 | S=5 D=4 I=0
2026-01-29 16:21:14,140 | INFO | Chunk: 4 | WER=70.588235 | S=5 D=7 I=0
2026-01-29 16:21:14,141 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 16:21:14,141 | INFO | Chunk: 6 | WER=41.666667 | S=7 D=8 I=0
2026-01-29 16:21:14,151 | INFO | File: Rhap-M0011.wav | WER=36.305732 | S=22 D=35 I=0
2026-01-29 16:21:14,151 | INFO | ------------------------------
2026-01-29 16:21:14,151 | INFO | whisper large Done!
2026-01-29 16:21:14,281 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-29 16:21:14,311 | INFO | Vocabulary size: 350
2026-01-29 16:21:14,865 | INFO | Gradient checkpoint layers: []
2026-01-29 16:21:15,561 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 16:21:15,565 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 16:21:15,565 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 16:21:15,566 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-29 16:21:15,566 | INFO | speech length: 66560
2026-01-29 16:21:15,603 | INFO | decoder input length: 103
2026-01-29 16:21:15,603 | INFO | max output length: 103
2026-01-29 16:21:15,603 | INFO | min output length: 10
2026-01-29 16:21:16,857 | INFO | end detected at 31
2026-01-29 16:21:16,858 | INFO |  -7.63 * 0.5 =  -3.82 for decoder
2026-01-29 16:21:16,858 | INFO |  -6.00 * 0.5 =  -3.00 for ctc
2026-01-29 16:21:16,858 | INFO | total log probability: -6.82
2026-01-29 16:21:16,858 | INFO | normalized log probability: -0.27
2026-01-29 16:21:16,858 | INFO | total number of ended hypotheses: 187
2026-01-29 16:21:16,858 | INFO | best hypo: ▁alors▁ton▁pois▁les▁deux▁la▁place▁notre▁dame▁à▁la▁nef▁chavant

2026-01-29 16:21:16,861 | INFO | speech length: 152640
2026-01-29 16:21:16,894 | INFO | decoder input length: 238
2026-01-29 16:21:16,894 | INFO | max output length: 238
2026-01-29 16:21:16,894 | INFO | min output length: 23
2026-01-29 16:21:21,299 | INFO | end detected at 81
2026-01-29 16:21:21,301 | INFO | -18.28 * 0.5 =  -9.14 for decoder
2026-01-29 16:21:21,301 | INFO | -12.44 * 0.5 =  -6.22 for ctc
2026-01-29 16:21:21,301 | INFO | total log probability: -15.36
2026-01-29 16:21:21,301 | INFO | normalized log probability: -0.21
2026-01-29 16:21:21,301 | INFO | total number of ended hypotheses: 183
2026-01-29 16:21:21,302 | INFO | best hypo: ▁je▁prolonge▁la▁place▁notre▁dame▁par▁la▁place▁sainte▁claire▁je▁continue▁tout▁droit▁à▁évoluer▁stendhal▁et▁je▁tourne▁dans▁la▁rue▁du▁nom▁d'un▁géographe▁je▁me▁souviens▁plus▁lequel

2026-01-29 16:21:21,304 | INFO | speech length: 79840
2026-01-29 16:21:21,342 | INFO | decoder input length: 124
2026-01-29 16:21:21,342 | INFO | max output length: 124
2026-01-29 16:21:21,342 | INFO | min output length: 12
2026-01-29 16:21:23,224 | INFO | end detected at 46
2026-01-29 16:21:23,225 | INFO |  -6.72 * 0.5 =  -3.36 for decoder
2026-01-29 16:21:23,225 | INFO |  -2.76 * 0.5 =  -1.38 for ctc
2026-01-29 16:21:23,225 | INFO | total log probability: -4.74
2026-01-29 16:21:23,225 | INFO | normalized log probability: -0.12
2026-01-29 16:21:23,225 | INFO | total number of ended hypotheses: 169
2026-01-29 16:21:23,226 | INFO | best hypo: ▁et▁je▁passe▁sous▁le▁lycée▁stendhal▁un▁petit▁passage▁qui▁me▁mène▁vers▁le▁square▁des▁pots

2026-01-29 16:21:23,227 | INFO | speech length: 165440
2026-01-29 16:21:23,260 | INFO | decoder input length: 258
2026-01-29 16:21:23,260 | INFO | max output length: 258
2026-01-29 16:21:23,260 | INFO | min output length: 25
2026-01-29 16:21:28,206 | INFO | end detected at 86
2026-01-29 16:21:28,207 | INFO | -29.68 * 0.5 = -14.84 for decoder
2026-01-29 16:21:28,207 | INFO |  -9.21 * 0.5 =  -4.61 for ctc
2026-01-29 16:21:28,207 | INFO | total log probability: -19.44
2026-01-29 16:21:28,207 | INFO | normalized log probability: -0.25
2026-01-29 16:21:28,207 | INFO | total number of ended hypotheses: 208
2026-01-29 16:21:28,208 | INFO | best hypo: ▁le▁square▁des▁postes▁tu▁peux▁rejoindre▁la▁place▁de▁l'étoile▁et▁là▁i▁deux▁itinéraires▁je▁crois▁ou▁bien▁je▁prends▁la▁rue▁lesdiguières▁et▁le▁boulevard▁edouard▁re

2026-01-29 16:21:28,210 | INFO | speech length: 72960
2026-01-29 16:21:28,263 | INFO | decoder input length: 113
2026-01-29 16:21:28,263 | INFO | max output length: 113
2026-01-29 16:21:28,263 | INFO | min output length: 11
2026-01-29 16:21:29,730 | INFO | end detected at 35
2026-01-29 16:21:29,731 | INFO | -12.17 * 0.5 =  -6.08 for decoder
2026-01-29 16:21:29,731 | INFO | -12.41 * 0.5 =  -6.21 for ctc
2026-01-29 16:21:29,731 | INFO | total log probability: -12.29
2026-01-29 16:21:29,731 | INFO | normalized log probability: -0.42
2026-01-29 16:21:29,731 | INFO | total number of ended hypotheses: 178
2026-01-29 16:21:29,732 | INFO | best hypo: ▁et▁non▁sait▁plus▁elle▁doit▁resser▁et▁mene▁c'est▁figer▁la▁charge

2026-01-29 16:21:29,734 | INFO | speech length: 10400
2026-01-29 16:21:29,777 | INFO | decoder input length: 15
2026-01-29 16:21:29,777 | INFO | max output length: 15
2026-01-29 16:21:29,777 | INFO | min output length: 1
2026-01-29 16:21:30,233 | INFO | end detected at 13
2026-01-29 16:21:30,234 | INFO |  -2.23 * 0.5 =  -1.11 for decoder
2026-01-29 16:21:30,234 | INFO |  -0.68 * 0.5 =  -0.34 for ctc
2026-01-29 16:21:30,234 | INFO | total log probability: -1.45
2026-01-29 16:21:30,234 | INFO | normalized log probability: -0.18
2026-01-29 16:21:30,234 | INFO | total number of ended hypotheses: 154
2026-01-29 16:21:30,234 | INFO | best hypo: ▁non▁c'est▁même

2026-01-29 16:21:30,236 | INFO | speech length: 142720
2026-01-29 16:21:30,271 | INFO | decoder input length: 222
2026-01-29 16:21:30,271 | INFO | max output length: 222
2026-01-29 16:21:30,271 | INFO | min output length: 22
2026-01-29 16:21:33,671 | INFO | end detected at 66
2026-01-29 16:21:33,672 | INFO | -22.00 * 0.5 = -11.00 for decoder
2026-01-29 16:21:33,672 | INFO | -27.70 * 0.5 = -13.85 for ctc
2026-01-29 16:21:33,672 | INFO | total log probability: -24.85
2026-01-29 16:21:33,672 | INFO | normalized log probability: -0.42
2026-01-29 16:21:33,672 | INFO | total number of ended hypotheses: 188
2026-01-29 16:21:33,673 | INFO | best hypo: ▁je▁m'en▁bouge▁ou▁à▁l'arrière▁petite▁rue▁mais▁dans▁je▁ne▁sais▁pas▁le▁non▁une▁petite▁rue▁en▁qui▁tourne▁un▁peu▁et▁je▁j'arrive▁je▁tombe▁sur▁la▁nef▁chavan

2026-01-29 16:21:33,678 | INFO | Chunk: 0 | WER=30.769231 | S=4 D=0 I=0
2026-01-29 16:21:33,679 | INFO | Chunk: 1 | WER=13.888889 | S=3 D=2 I=0
2026-01-29 16:21:33,680 | INFO | Chunk: 2 | WER=16.666667 | S=3 D=0 I=0
2026-01-29 16:21:33,680 | INFO | Chunk: 3 | WER=18.750000 | S=5 D=1 I=0
2026-01-29 16:21:33,681 | INFO | Chunk: 4 | WER=64.705882 | S=8 D=3 I=0
2026-01-29 16:21:33,681 | INFO | Chunk: 5 | WER=20.000000 | S=0 D=1 I=0
2026-01-29 16:21:33,682 | INFO | Chunk: 6 | WER=30.555556 | S=5 D=3 I=3
2026-01-29 16:21:33,694 | INFO | File: Rhap-M0011.wav | WER=26.114650 | S=28 D=10 I=3
2026-01-29 16:21:33,694 | INFO | ------------------------------
2026-01-29 16:21:33,694 | INFO | Conf cv Done!
2026-01-29 16:21:33,892 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-29 16:21:33,915 | INFO | Vocabulary size: 47
2026-01-29 16:21:34,413 | INFO | Gradient checkpoint layers: []
2026-01-29 16:21:35,080 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 16:21:35,083 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 16:21:35,084 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 16:21:35,084 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-29 16:21:35,086 | INFO | speech length: 66560
2026-01-29 16:21:35,117 | INFO | decoder input length: 103
2026-01-29 16:21:35,117 | INFO | max output length: 103
2026-01-29 16:21:35,117 | INFO | min output length: 10
2026-01-29 16:21:37,792 | INFO | end detected at 73
2026-01-29 16:21:37,793 | INFO |  -6.76 * 0.5 =  -3.38 for decoder
2026-01-29 16:21:37,794 | INFO |  -5.17 * 0.5 =  -2.58 for ctc
2026-01-29 16:21:37,794 | INFO | total log probability: -5.96
2026-01-29 16:21:37,794 | INFO | normalized log probability: -0.09
2026-01-29 16:21:37,794 | INFO | total number of ended hypotheses: 200
2026-01-29 16:21:37,795 | INFO | best hypo: alors<space>quand<space>pour<space>aller<space>deux<space>la<space>place<space>notre<space>dame<space>à<space>la<space>nef<space>chavant

2026-01-29 16:21:37,797 | INFO | speech length: 152640
2026-01-29 16:21:37,839 | INFO | decoder input length: 238
2026-01-29 16:21:37,839 | INFO | max output length: 238
2026-01-29 16:21:37,839 | INFO | min output length: 23
2026-01-29 16:21:46,137 | INFO | end detected at 193
2026-01-29 16:21:46,138 | INFO | -25.03 * 0.5 = -12.51 for decoder
2026-01-29 16:21:46,138 | INFO | -12.51 * 0.5 =  -6.25 for ctc
2026-01-29 16:21:46,138 | INFO | total log probability: -18.77
2026-01-29 16:21:46,138 | INFO | normalized log probability: -0.10
2026-01-29 16:21:46,138 | INFO | total number of ended hypotheses: 194
2026-01-29 16:21:46,141 | INFO | best hypo: je<space>prolonge<space>la<space>place<space>notre<space>dame<space>par<space>la<space>place<space>sainte<space>claire<space>je<space>continue<space>le<space>droit<space>j'arrivolise<space>standale<space>mais<space>je<space>tourne<space>dans<space>la<space>rue<space>du<space>nom<space>d'un<space>géographe<space>je<space>me<space>souviens<space>plus<space>lequel

2026-01-29 16:21:46,143 | INFO | speech length: 79840
2026-01-29 16:21:46,172 | INFO | decoder input length: 124
2026-01-29 16:21:46,173 | INFO | max output length: 124
2026-01-29 16:21:46,173 | INFO | min output length: 12
2026-01-29 16:21:49,594 | INFO | end detected at 95
2026-01-29 16:21:49,595 | INFO | -11.43 * 0.5 =  -5.71 for decoder
2026-01-29 16:21:49,595 | INFO |  -5.24 * 0.5 =  -2.62 for ctc
2026-01-29 16:21:49,595 | INFO | total log probability: -8.33
2026-01-29 16:21:49,595 | INFO | normalized log probability: -0.09
2026-01-29 16:21:49,596 | INFO | total number of ended hypotheses: 162
2026-01-29 16:21:49,597 | INFO | best hypo: et<space>je<space>passe<space>sous<space>l'élycée<space>standald<space>un<space>petit<space>passage<space>qui<space>me<space>mène<space>vers<space>le<space>square<space>des<space>postes

2026-01-29 16:21:49,599 | INFO | speech length: 165440
2026-01-29 16:21:49,626 | INFO | decoder input length: 258
2026-01-29 16:21:49,626 | INFO | max output length: 258
2026-01-29 16:21:49,626 | INFO | min output length: 25
2026-01-29 16:21:57,703 | INFO | end detected at 169
2026-01-29 16:21:57,704 | INFO | -17.91 * 0.5 =  -8.96 for decoder
2026-01-29 16:21:57,704 | INFO | -12.37 * 0.5 =  -6.18 for ctc
2026-01-29 16:21:57,704 | INFO | total log probability: -15.14
2026-01-29 16:21:57,704 | INFO | normalized log probability: -0.09
2026-01-29 16:21:57,704 | INFO | total number of ended hypotheses: 212
2026-01-29 16:21:57,706 | INFO | best hypo: je<space>cors<space>des<space>postes<space>je<space>peux<space>rejoindre<space>la<space>place<space>de<space>l'étoile<space>et<space>là<space>j'ai<space>deux<space>itinéraires<space>je<space>crois<space>ou<space>bien<space>je<space>prends<space>la<space>rue<space>le<space>diguière<space>et<space>le<space>boulevard<space>edouard<space>khey

2026-01-29 16:21:57,708 | INFO | speech length: 72960
2026-01-29 16:21:57,737 | INFO | decoder input length: 113
2026-01-29 16:21:57,737 | INFO | max output length: 113
2026-01-29 16:21:57,737 | INFO | min output length: 11
2026-01-29 16:22:00,238 | INFO | end detected at 71
2026-01-29 16:22:00,239 | INFO | -15.39 * 0.5 =  -7.69 for decoder
2026-01-29 16:22:00,239 | INFO | -11.43 * 0.5 =  -5.71 for ctc
2026-01-29 16:22:00,239 | INFO | total log probability: -13.41
2026-01-29 16:22:00,240 | INFO | normalized log probability: -0.22
2026-01-29 16:22:00,240 | INFO | total number of ended hypotheses: 183
2026-01-29 16:22:00,240 | INFO | best hypo: et<space>non<space>c'est<space>plus<space>doire<space>c'est<space>et<space>me<space>c'est<space>filiser<space>la<space>charge

2026-01-29 16:22:00,242 | INFO | speech length: 10400
2026-01-29 16:22:00,272 | INFO | decoder input length: 15
2026-01-29 16:22:00,272 | INFO | max output length: 15
2026-01-29 16:22:00,272 | INFO | min output length: 1
2026-01-29 16:22:00,707 | INFO | adding <eos> in the last position in the loop
2026-01-29 16:22:00,713 | INFO | no hypothesis. Finish decoding.
2026-01-29 16:22:00,714 | INFO |  -1.26 * 0.5 =  -0.63 for decoder
2026-01-29 16:22:00,714 | INFO |  -0.34 * 0.5 =  -0.17 for ctc
2026-01-29 16:22:00,714 | INFO | total log probability: -0.80
2026-01-29 16:22:00,714 | INFO | normalized log probability: -0.05
2026-01-29 16:22:00,714 | INFO | total number of ended hypotheses: 55
2026-01-29 16:22:00,714 | INFO | best hypo: non<space>c'est<space>même<sos/eos>

2026-01-29 16:22:00,714 | WARNING | best hypo length: 15 == max output length: 15
2026-01-29 16:22:00,714 | WARNING | decoding may be stopped by the max output length limitation, please consider to increase the maxlenratio.
2026-01-29 16:22:00,715 | INFO | speech length: 142720
2026-01-29 16:22:00,744 | INFO | decoder input length: 222
2026-01-29 16:22:00,744 | INFO | max output length: 222
2026-01-29 16:22:00,744 | INFO | min output length: 22
2026-01-29 16:22:08,126 | INFO | end detected at 163
2026-01-29 16:22:08,128 | INFO | -18.82 * 0.5 =  -9.41 for decoder
2026-01-29 16:22:08,128 | INFO | -10.35 * 0.5 =  -5.18 for ctc
2026-01-29 16:22:08,128 | INFO | total log probability: -14.59
2026-01-29 16:22:08,128 | INFO | normalized log probability: -0.09
2026-01-29 16:22:08,128 | INFO | total number of ended hypotheses: 205
2026-01-29 16:22:08,130 | INFO | best hypo: je<space>m'en<space>bouge<space>ou<space>alors<space>y<space>a<space>une<space>petite<space>rue<space>mais<space>dont<space>je<space>ne<space>sais<space>pas<space>le<space>nom<space>une<space>petite<space>rue<space>en<space>qui<space>tourne<space>un<space>peu<space>et<space>je<space>j'arrive<space>je<space>tombe<space>sur<space>la<space>neuf<space>chavant

2026-01-29 16:22:08,134 | INFO | Chunk: 0 | WER=15.384615 | S=2 D=0 I=0
2026-01-29 16:22:08,135 | INFO | Chunk: 1 | WER=16.666667 | S=4 D=2 I=0
2026-01-29 16:22:08,136 | INFO | Chunk: 2 | WER=27.777778 | S=5 D=0 I=0
2026-01-29 16:22:08,137 | INFO | Chunk: 3 | WER=15.625000 | S=4 D=0 I=1
2026-01-29 16:22:08,137 | INFO | Chunk: 4 | WER=47.058824 | S=4 D=3 I=1
2026-01-29 16:22:08,137 | INFO | Chunk: 5 | WER=40.000000 | S=1 D=0 I=1
2026-01-29 16:22:08,138 | INFO | Chunk: 6 | WER=13.888889 | S=2 D=1 I=2
2026-01-29 16:22:08,150 | INFO | File: Rhap-M0011.wav | WER=21.019108 | S=22 D=6 I=5
2026-01-29 16:22:08,150 | INFO | ------------------------------
2026-01-29 16:22:08,150 | INFO | Conf ester Done!
2026-01-29 16:22:50,436 | INFO | Chunk: 0 | WER=23.076923 | S=3 D=0 I=0
2026-01-29 16:22:50,439 | INFO | Chunk: 1 | WER=13.888889 | S=4 D=0 I=1
2026-01-29 16:22:50,440 | INFO | Chunk: 2 | WER=27.777778 | S=4 D=1 I=0
2026-01-29 16:22:50,442 | INFO | Chunk: 3 | WER=31.250000 | S=6 D=2 I=2
2026-01-29 16:22:50,443 | INFO | Chunk: 4 | WER=52.941176 | S=7 D=2 I=0
2026-01-29 16:22:50,443 | INFO | Chunk: 5 | WER=20.000000 | S=1 D=0 I=0
2026-01-29 16:22:50,444 | INFO | Chunk: 6 | WER=13.888889 | S=2 D=1 I=2
2026-01-29 16:22:50,458 | INFO | File: Rhap-M0011.wav | WER=24.203822 | S=27 D=6 I=5
2026-01-29 16:22:50,458 | INFO | ------------------------------
2026-01-29 16:22:50,458 | INFO | hmm_tdnn Done!
2026-01-29 16:22:50,608 | INFO | ==================================Rhap-M0012.wav=========================================
2026-01-29 16:22:50,612 | INFO | Using rVAD model
2026-01-29 16:22:51,485 | INFO | Chunk: 0 | WER=30.000000 | S=7 D=6 I=2
2026-01-29 16:22:51,488 | INFO | File: Rhap-M0012.wav | WER=22.222222 | S=7 D=1 I=2
2026-01-29 16:22:51,488 | INFO | ------------------------------
2026-01-29 16:22:51,488 | INFO | w2vec vad chunk Done!
2026-01-29 16:22:53,472 | INFO | Chunk: 0 | WER=54.000000 | S=7 D=18 I=2
2026-01-29 16:22:53,474 | INFO | File: Rhap-M0012.wav | WER=48.888889 | S=7 D=13 I=2
2026-01-29 16:22:53,474 | INFO | ------------------------------
2026-01-29 16:22:53,475 | INFO | whisper med Done!
2026-01-29 16:22:57,112 | INFO | Chunk: 0 | WER=48.000000 | S=14 D=6 I=4
2026-01-29 16:22:57,115 | INFO | File: Rhap-M0012.wav | WER=44.444444 | S=13 D=2 I=5
2026-01-29 16:22:57,115 | INFO | ------------------------------
2026-01-29 16:22:57,115 | INFO | whisper large Done!
2026-01-29 16:22:57,242 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-29 16:22:57,273 | INFO | Vocabulary size: 350
2026-01-29 16:22:57,956 | INFO | Gradient checkpoint layers: []
2026-01-29 16:22:58,712 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 16:22:58,715 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 16:22:58,715 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 16:22:58,716 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-29 16:22:58,716 | INFO | speech length: 271840
2026-01-29 16:22:58,755 | INFO | decoder input length: 424
2026-01-29 16:22:58,755 | INFO | max output length: 424
2026-01-29 16:22:58,755 | INFO | min output length: 42
2026-01-29 16:23:07,955 | INFO | end detected at 129
2026-01-29 16:23:07,956 | INFO | -158.25 * 0.5 = -79.12 for decoder
2026-01-29 16:23:07,957 | INFO | -34.93 * 0.5 = -17.47 for ctc
2026-01-29 16:23:07,957 | INFO | total log probability: -96.59
2026-01-29 16:23:07,957 | INFO | normalized log probability: -0.79
2026-01-29 16:23:07,957 | INFO | total number of ended hypotheses: 158
2026-01-29 16:23:07,958 | INFO | best hypo: ▁non▁que▁j'irai▁à▁pied▁en▁longeant▁là▁donc▁jusqu'au▁cinéma▁neufchavan▁ensuite▁je▁continuerai▁jusqu'à▁la▁préfecture▁je▁couperais▁par▁la▁place▁grenette▁donc▁après▁je▁longerai▁le▁bouvard▁gambettache▁traverse▁donc▁le▁cours▁jean▁jaurès▁et▁j'arrive▁alse▁lorraine

2026-01-29 16:23:07,965 | INFO | Chunk: 0 | WER=30.000000 | S=10 D=5 I=0
2026-01-29 16:23:07,968 | INFO | File: Rhap-M0012.wav | WER=26.666667 | S=8 D=2 I=2
2026-01-29 16:23:07,968 | INFO | ------------------------------
2026-01-29 16:23:07,968 | INFO | Conf cv Done!
2026-01-29 16:23:08,136 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-29 16:23:08,154 | INFO | Vocabulary size: 47
2026-01-29 16:23:08,674 | INFO | Gradient checkpoint layers: []
2026-01-29 16:23:09,389 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 16:23:09,392 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 16:23:09,392 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 16:23:09,393 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-29 16:23:09,395 | INFO | speech length: 271840
2026-01-29 16:23:09,427 | INFO | decoder input length: 424
2026-01-29 16:23:09,427 | INFO | max output length: 424
2026-01-29 16:23:09,427 | INFO | min output length: 42
2026-01-29 16:23:27,194 | INFO | end detected at 294
2026-01-29 16:23:27,196 | INFO | -36.26 * 0.5 = -18.13 for decoder
2026-01-29 16:23:27,196 | INFO | -19.21 * 0.5 =  -9.61 for ctc
2026-01-29 16:23:27,196 | INFO | total log probability: -27.74
2026-01-29 16:23:27,196 | INFO | normalized log probability: -0.10
2026-01-29 16:23:27,196 | INFO | total number of ended hypotheses: 204
2026-01-29 16:23:27,199 | INFO | best hypo: donc<space>euh<space>j'irais<space>à<space>pied<space>euh<space>en<space>longeant<space>là<space>donc<space>euh<space>jusqu'au<space>cinéma<space>neuf<space>chavant<space>ensuite<space>euh<space>je<space>continuerai<space>jusqu'à<space>la<space>préfecture<space>je<space>couperais<space>par<space>la<space>place<space>grenette<space>donc<space>après<space>je<space>longerais<space>le<space>boulevard<space>euh<space>gambéta<space>je<space>traverse<space>donc<space>le<space>cours<space>jean<space>jaurès<space>et<space>j'arrive<space>euh<space>à<space>alsace<space>lorenne

2026-01-29 16:23:27,206 | INFO | Chunk: 0 | WER=12.000000 | S=3 D=0 I=3
2026-01-29 16:23:27,208 | INFO | File: Rhap-M0012.wav | WER=24.444444 | S=3 D=0 I=8
2026-01-29 16:23:27,208 | INFO | ------------------------------
2026-01-29 16:23:27,208 | INFO | Conf ester Done!
2026-01-29 16:23:37,604 | INFO | Chunk: 0 | WER=36.000000 | S=7 D=8 I=3
2026-01-29 16:23:37,609 | INFO | File: Rhap-M0012.wav | WER=28.888889 | S=7 D=3 I=3
2026-01-29 16:23:37,609 | INFO | ------------------------------
2026-01-29 16:23:37,610 | INFO | hmm_tdnn Done!
2026-01-29 16:23:37,752 | INFO | ==================================Rhap-M0013.wav=========================================
2026-01-29 16:23:37,787 | INFO | Using rVAD model
2026-01-29 16:23:40,470 | INFO | Chunk: 0 | WER=22.105263 | S=5 D=16 I=0
2026-01-29 16:23:40,473 | INFO | Chunk: 1 | WER=19.753086 | S=4 D=12 I=0
2026-01-29 16:23:40,485 | INFO | File: Rhap-M0013.wav | WER=12.738854 | S=9 D=10 I=1
2026-01-29 16:23:40,485 | INFO | ------------------------------
2026-01-29 16:23:40,485 | INFO | w2vec vad chunk Done!
2026-01-29 16:23:44,044 | INFO | Chunk: 0 | WER=75.789474 | S=2 D=70 I=0
2026-01-29 16:23:44,047 | INFO | Chunk: 1 | WER=49.382716 | S=2 D=37 I=1
2026-01-29 16:23:44,053 | INFO | File: Rhap-M0013.wav | WER=59.235669 | S=4 D=88 I=1
2026-01-29 16:23:44,053 | INFO | ------------------------------
2026-01-29 16:23:44,053 | INFO | whisper med Done!
2026-01-29 16:23:50,110 | INFO | Chunk: 0 | WER=54.736842 | S=5 D=47 I=0
2026-01-29 16:23:50,113 | INFO | Chunk: 1 | WER=45.679012 | S=3 D=33 I=1
2026-01-29 16:23:50,121 | INFO | File: Rhap-M0013.wav | WER=44.585987 | S=8 D=61 I=1
2026-01-29 16:23:50,121 | INFO | ------------------------------
2026-01-29 16:23:50,121 | INFO | whisper large Done!
2026-01-29 16:23:50,247 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-29 16:23:50,277 | INFO | Vocabulary size: 350
2026-01-29 16:23:50,839 | INFO | Gradient checkpoint layers: []
2026-01-29 16:23:51,513 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 16:23:51,516 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 16:23:51,516 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 16:23:51,517 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-29 16:23:51,517 | INFO | speech length: 441600
2026-01-29 16:23:51,561 | INFO | decoder input length: 689
2026-01-29 16:23:51,561 | INFO | max output length: 689
2026-01-29 16:23:51,561 | INFO | min output length: 68
2026-01-29 16:24:08,586 | INFO | end detected at 169
2026-01-29 16:24:08,587 | INFO | -418.92 * 0.5 = -209.46 for decoder
2026-01-29 16:24:08,587 | INFO | -88.67 * 0.5 = -44.34 for ctc
2026-01-29 16:24:08,587 | INFO | total log probability: -253.80
2026-01-29 16:24:08,587 | INFO | normalized log probability: -1.56
2026-01-29 16:24:08,588 | INFO | total number of ended hypotheses: 134
2026-01-29 16:24:08,589 | INFO | best hypo: ▁me▁leure▁à▁pied▁même▁pour▁aller▁à▁la▁gare▁vous▁allez▁vous▁éloigner▁du▁tram▁en▁longeant▁le▁cinéma▁et▁donc▁vous▁suivez▁cette▁avenue▁tout▁le▁long▁elle▁fait▁un▁petit▁coup▁d'un▁moment▁verrez▁et▁donc▁que▁vous▁passe▁à▁côté▁de▁la▁poste▁etce▁et▁au▁bout▁d'un▁emple▁mais▁elle▁est▁arrivé▁sur▁une▁grande▁place▁avec▁une▁fontame▁aux▁milieu▁c'est▁la▁place▁victor▁hgo▁etcque▁vous▁rencontrez▁à▁son▁moment▁le▁trame

2026-01-29 16:24:08,592 | INFO | speech length: 381280
2026-01-29 16:24:08,627 | INFO | decoder input length: 595
2026-01-29 16:24:08,627 | INFO | max output length: 595
2026-01-29 16:24:08,627 | INFO | min output length: 59
2026-01-29 16:24:22,855 | INFO | end detected at 159
2026-01-29 16:24:22,855 | INFO | -443.24 * 0.5 = -221.62 for decoder
2026-01-29 16:24:22,856 | INFO | -117.81 * 0.5 = -58.91 for ctc
2026-01-29 16:24:22,856 | INFO | total log probability: -280.53
2026-01-29 16:24:22,856 | INFO | normalized log probability: -1.81
2026-01-29 16:24:22,856 | INFO | total number of ended hypotheses: 137
2026-01-29 16:24:22,858 | INFO | best hypo: ▁que▁vous▁pouvez▁longer▁que▁vous▁pouvez▁suivre▁en▁prenant▁à▁gauche▁et▁à▁ce▁moment▁là▁si▁vous▁si▁vous▁suivez▁les▁rails▁vous▁allez▁arriver▁à▁peu▁près▁je▁ne▁saisz▁pas▁combien▁de▁temps▁peu▁près▁d'êt▁être▁cinq▁cents▁être▁moi▁à▁petit▁kilomètres▁plus▁loin▁vous▁allez▁arriver▁à▁la▁gare▁c'esti▁un▁peu▁bâtiment▁vous▁verrez▁ce▁sans▁po▁problème▁vous▁renaissez▁déjà▁un▁peu

2026-01-29 16:24:22,866 | INFO | Chunk: 0 | WER=37.894737 | S=17 D=16 I=3
2026-01-29 16:24:22,869 | INFO | Chunk: 1 | WER=27.160494 | S=10 D=10 I=2
2026-01-29 16:24:22,881 | INFO | File: Rhap-M0013.wav | WER=26.114650 | S=25 D=9 I=7
2026-01-29 16:24:22,882 | INFO | ------------------------------
2026-01-29 16:24:22,882 | INFO | Conf cv Done!
2026-01-29 16:24:23,073 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-29 16:24:23,097 | INFO | Vocabulary size: 47
2026-01-29 16:24:23,618 | INFO | Gradient checkpoint layers: []
2026-01-29 16:24:24,269 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 16:24:24,273 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 16:24:24,273 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 16:24:24,273 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-29 16:24:24,276 | INFO | speech length: 441600
2026-01-29 16:24:24,310 | INFO | decoder input length: 689
2026-01-29 16:24:24,310 | INFO | max output length: 689
2026-01-29 16:24:24,310 | INFO | min output length: 68
2026-01-29 16:25:04,215 | INFO | end detected at 490
2026-01-29 16:25:04,216 | INFO | -427.90 * 0.5 = -213.95 for decoder
2026-01-29 16:25:04,216 | INFO | -76.80 * 0.5 = -38.40 for ctc
2026-01-29 16:25:04,216 | INFO | total log probability: -252.35
2026-01-29 16:25:04,216 | INFO | normalized log probability: -0.52
2026-01-29 16:25:04,216 | INFO | total number of ended hypotheses: 182
2026-01-29 16:25:04,222 | INFO | best hypo: alors<space>à<space>pied<space>ben<space>pour<space>euh<space>aller<space>à<space>la<space>gare<space>vous<space>allez<space>euh<space>vous<space>éloigner<space>du<space>drame<space>en<space>en<space>logeant<space>euh<space>la<space>le<space>cinéma<space>et<space>pour<space>cent<space>hesitation<space>donc<space>vous<space>suivez<space>cet<space>avenue<space>pour<space>cent<space>hesitation<space>tout<space>long<space>elle<space>fait<space>un<space>petit<space>coup<space>d'un<space>moment<space>vous<space>verrez<space>et<space>pour<space>cent<space>hesitation<space>donc<space>vous<space>passez<space>à<space>côté<space>de<space>la<space>poste<space>etc<space>et<space>au<space>bout<space>d'attendre<space>vous<space>allez<space>arriver<space>sur<space>une<space>grande<space>place<space>avec<space>une<space>fontaine<space>au<space>milieu<space>c'est<space>la<space>place<space>victor<space>heugo<space>et<space>euh<space>donc<space>eun<space>vous<space>roncontrez<space>à<space>ce<space>ment<space>là<space>le<space>le<space>drame

2026-01-29 16:25:04,224 | INFO | speech length: 381280
2026-01-29 16:25:04,252 | INFO | decoder input length: 595
2026-01-29 16:25:04,253 | INFO | max output length: 595
2026-01-29 16:25:04,253 | INFO | min output length: 59
2026-01-29 16:25:35,815 | INFO | end detected at 441
2026-01-29 16:25:35,816 | INFO | -185.05 * 0.5 = -92.52 for decoder
2026-01-29 16:25:35,816 | INFO | -33.30 * 0.5 = -16.65 for ctc
2026-01-29 16:25:35,816 | INFO | total log probability: -109.17
2026-01-29 16:25:35,816 | INFO | normalized log probability: -0.25
2026-01-29 16:25:35,816 | INFO | total number of ended hypotheses: 169
2026-01-29 16:25:35,821 | INFO | best hypo: que<space>vous<space>pouvez<space>longer<space>que<space>vous<space>pouvez<space>suivre<space>euh<space>en<space>prenant<space>à<space>gauche<space>et<space>à<space>ce<space>moment<space>là<space>si<space>vous<space>le<space>si<space>vous<space>suivez<space>les<space>rails<space>vous<space>allez<space>arriver<space>pour<space>cent<space>hesitation<space>à<space>peu<space>près<space>pour<space>cent<space>hesitation<space>je<space>sais<space>pas<space>combien<space>de<space>temps<space>à<space>peu<space>près<space>pour<space>ent<space>hesitation<space>d'être<space>cinq<space>cent<space>peut<space>être<space>moins<space>un<space>petit<space>kilomètre<space>plus<space>loin<space>vous<space>allez<space>arriver<space>à<space>la<space>gare<space>c'est<space>un<space>grand<space>bâtiment<space>vous<space>le<space>verrez<space>sans<space>problème<space>vous<space>connaissez<space>déjà<space>un<space>peu<space>ou

2026-01-29 16:25:35,830 | INFO | Chunk: 0 | WER=28.421053 | S=21 D=2 I=4
2026-01-29 16:25:35,833 | INFO | Chunk: 1 | WER=17.283951 | S=7 D=2 I=5
2026-01-29 16:25:35,847 | INFO | File: Rhap-M0013.wav | WER=29.299363 | S=18 D=2 I=26
2026-01-29 16:25:35,847 | INFO | ------------------------------
2026-01-29 16:25:35,847 | INFO | Conf ester Done!
2026-01-29 16:26:04,038 | INFO | Chunk: 0 | WER=22.105263 | S=11 D=10 I=0
2026-01-29 16:26:04,043 | INFO | Chunk: 1 | WER=29.629630 | S=14 D=8 I=2
2026-01-29 16:26:04,056 | INFO | File: Rhap-M0013.wav | WER=24.840764 | S=26 D=5 I=8
2026-01-29 16:26:04,056 | INFO | ------------------------------
2026-01-29 16:26:04,056 | INFO | hmm_tdnn Done!
2026-01-29 16:26:04,280 | INFO | ==================================Rhap-M0014.wav=========================================
2026-01-29 16:26:04,285 | INFO | Using rVAD model
2026-01-29 16:26:05,629 | INFO | Chunk: 0 | WER=29.032258 | S=1 D=4 I=4
2026-01-29 16:26:05,630 | INFO | Chunk: 1 | WER=43.478261 | S=8 D=8 I=4
2026-01-29 16:26:05,635 | INFO | File: Rhap-M0014.wav | WER=28.000000 | S=9 D=7 I=5
2026-01-29 16:26:05,635 | INFO | ------------------------------
2026-01-29 16:26:05,635 | INFO | w2vec vad chunk Done!
2026-01-29 16:26:08,538 | INFO | Chunk: 0 | WER=16.129032 | S=1 D=0 I=4
2026-01-29 16:26:08,539 | INFO | Chunk: 1 | WER=54.347826 | S=1 D=24 I=0
2026-01-29 16:26:08,543 | INFO | File: Rhap-M0014.wav | WER=29.333333 | S=2 D=19 I=1
2026-01-29 16:26:08,543 | INFO | ------------------------------
2026-01-29 16:26:08,543 | INFO | whisper med Done!
2026-01-29 16:26:13,658 | INFO | Chunk: 0 | WER=32.258065 | S=3 D=2 I=5
2026-01-29 16:26:13,660 | INFO | Chunk: 1 | WER=41.304348 | S=3 D=8 I=8
2026-01-29 16:26:13,665 | INFO | File: Rhap-M0014.wav | WER=28.000000 | S=6 D=5 I=10
2026-01-29 16:26:13,665 | INFO | ------------------------------
2026-01-29 16:26:13,665 | INFO | whisper large Done!
2026-01-29 16:26:13,812 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-29 16:26:13,849 | INFO | Vocabulary size: 350
2026-01-29 16:26:14,521 | INFO | Gradient checkpoint layers: []
2026-01-29 16:26:15,266 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 16:26:15,270 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 16:26:15,271 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 16:26:15,271 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-29 16:26:15,271 | INFO | speech length: 185760
2026-01-29 16:26:15,321 | INFO | decoder input length: 289
2026-01-29 16:26:15,321 | INFO | max output length: 289
2026-01-29 16:26:15,321 | INFO | min output length: 28
2026-01-29 16:26:20,088 | INFO | end detected at 81
2026-01-29 16:26:20,089 | INFO | -53.94 * 0.5 = -26.97 for decoder
2026-01-29 16:26:20,089 | INFO | -25.29 * 0.5 = -12.65 for ctc
2026-01-29 16:26:20,089 | INFO | total log probability: -39.62
2026-01-29 16:26:20,089 | INFO | normalized log probability: -0.53
2026-01-29 16:26:20,089 | INFO | total number of ended hypotheses: 160
2026-01-29 16:26:20,090 | INFO | best hypo: ▁alors▁en▁partant▁de▁la▁place▁paul▁vally▁pour▁la▁place▁notre▁dame▁alors▁champrunte▁la▁rue▁de▁strasbourg▁je▁passe▁par▁la▁place▁vaucançon▁je▁prends▁en▁direction▁de▁maison▁du▁tourisme

2026-01-29 16:26:20,093 | INFO | speech length: 221600
2026-01-29 16:26:20,136 | INFO | decoder input length: 345
2026-01-29 16:26:20,136 | INFO | max output length: 345
2026-01-29 16:26:20,136 | INFO | min output length: 34
2026-01-29 16:26:28,412 | INFO | end detected at 99
2026-01-29 16:26:28,415 | INFO | -92.67 * 0.5 = -46.33 for decoder
2026-01-29 16:26:28,415 | INFO | -40.50 * 0.5 = -20.25 for ctc
2026-01-29 16:26:28,415 | INFO | total log probability: -66.58
2026-01-29 16:26:28,415 | INFO | normalized log probability: -0.71
2026-01-29 16:26:28,415 | INFO | total number of ended hypotheses: 164
2026-01-29 16:26:28,416 | INFO | best hypo: ▁à▁la▁maison▁du▁tourisme▁je▁compte▁tourne▁laval▁je▁prends▁la▁rue▁de▁la▁république▁en▁remontant▁la▁rue▁de▁la▁république▁je▁tombe▁sur▁la▁place▁sainte▁clerc▁on▁va▁à▁dire▁la▁rue▁à▁la▁halle▁et▁j'arriver▁à▁la▁place▁notre▁dame

2026-01-29 16:26:28,422 | INFO | Chunk: 0 | WER=35.483871 | S=3 D=3 I=5
2026-01-29 16:26:28,424 | INFO | Chunk: 1 | WER=43.478261 | S=7 D=6 I=7
2026-01-29 16:26:28,428 | INFO | File: Rhap-M0014.wav | WER=30.666667 | S=10 D=4 I=9
2026-01-29 16:26:28,428 | INFO | ------------------------------
2026-01-29 16:26:28,428 | INFO | Conf cv Done!
2026-01-29 16:26:28,663 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-29 16:26:28,681 | INFO | Vocabulary size: 47
2026-01-29 16:26:29,233 | INFO | Gradient checkpoint layers: []
2026-01-29 16:26:29,942 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 16:26:29,946 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 16:26:29,946 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 16:26:29,946 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-29 16:26:29,949 | INFO | speech length: 185760
2026-01-29 16:26:29,989 | INFO | decoder input length: 289
2026-01-29 16:26:29,990 | INFO | max output length: 289
2026-01-29 16:26:29,990 | INFO | min output length: 28
2026-01-29 16:26:39,971 | INFO | end detected at 191
2026-01-29 16:26:39,974 | INFO | -17.38 * 0.5 =  -8.69 for decoder
2026-01-29 16:26:39,978 | INFO | -21.53 * 0.5 = -10.77 for ctc
2026-01-29 16:26:39,978 | INFO | total log probability: -19.46
2026-01-29 16:26:39,978 | INFO | normalized log probability: -0.11
2026-01-29 16:26:39,978 | INFO | total number of ended hypotheses: 211
2026-01-29 16:26:39,980 | INFO | best hypo: alors<space>en<space>partant<space>de<space>la<space>place<space>paul<space>vailly<space>pour<space>aller<space>à<space>place<space>notre<space>dame<space>alors<space>j'emprunte<space>par<space>une<space>strasbourg<space>je<space>passe<space>par<space>la<space>place<space>vos<space>cansons<space>je<space>prends<space>direction<space>de<space>maison<space>du<space>tourisme

2026-01-29 16:26:39,984 | INFO | speech length: 221600
2026-01-29 16:26:40,026 | INFO | decoder input length: 345
2026-01-29 16:26:40,027 | INFO | max output length: 345
2026-01-29 16:26:40,027 | INFO | min output length: 34
2026-01-29 16:26:52,396 | INFO | end detected at 223
2026-01-29 16:26:52,398 | INFO | -21.65 * 0.5 = -10.82 for decoder
2026-01-29 16:26:52,398 | INFO | -13.20 * 0.5 =  -6.60 for ctc
2026-01-29 16:26:52,398 | INFO | total log probability: -17.43
2026-01-29 16:26:52,398 | INFO | normalized log probability: -0.08
2026-01-29 16:26:52,398 | INFO | total number of ended hypotheses: 193
2026-01-29 16:26:52,401 | INFO | best hypo: à<space>la<space>maison<space>du<space>tourisme<space>je<space>contourne<space>enfin<space>je<space>prends<space>la<space>rue<space>de<space>la<space>république<space>en<space>remontant<space>la<space>rue<space>de<space>la<space>république<space>je<space>tombe<space>sur<space>la<space>place<space>cinq<space>claire<space>on<space>va<space>dire<space>là<space>où<space>y<space>a<space>la<space>halle<space>et<space>j'arrivais<space>à<space>la<space>place<space>notre<space>dame

2026-01-29 16:26:52,409 | INFO | Chunk: 0 | WER=35.483871 | S=4 D=2 I=5
2026-01-29 16:26:52,410 | INFO | Chunk: 1 | WER=39.130435 | S=2 D=8 I=8
2026-01-29 16:26:52,415 | INFO | File: Rhap-M0014.wav | WER=28.000000 | S=6 D=5 I=10
2026-01-29 16:26:52,415 | INFO | ------------------------------
2026-01-29 16:26:52,416 | INFO | Conf ester Done!
2026-01-29 16:27:09,744 | INFO | Chunk: 0 | WER=48.387097 | S=6 D=4 I=5
2026-01-29 16:27:09,747 | INFO | Chunk: 1 | WER=41.304348 | S=9 D=5 I=5
2026-01-29 16:27:09,754 | INFO | File: Rhap-M0014.wav | WER=34.666667 | S=15 D=4 I=7
2026-01-29 16:27:09,755 | INFO | ------------------------------
2026-01-29 16:27:09,755 | INFO | hmm_tdnn Done!
2026-01-29 16:27:09,898 | INFO | ==================================Rhap-M0015.wav=========================================
2026-01-29 16:27:09,904 | INFO | Using rVAD model
2026-01-29 16:27:11,171 | INFO | Chunk: 0 | WER=22.950820 | S=5 D=9 I=0
2026-01-29 16:27:11,171 | INFO | Chunk: 1 | WER=27.777778 | S=2 D=3 I=0
2026-01-29 16:27:11,175 | INFO | File: Rhap-M0015.wav | WER=21.917808 | S=6 D=8 I=2
2026-01-29 16:27:11,175 | INFO | ------------------------------
2026-01-29 16:27:11,175 | INFO | w2vec vad chunk Done!
2026-01-29 16:27:14,077 | INFO | Chunk: 0 | WER=45.901639 | S=7 D=21 I=0
2026-01-29 16:27:14,077 | INFO | Chunk: 1 | WER=27.777778 | S=2 D=3 I=0
2026-01-29 16:27:14,081 | INFO | File: Rhap-M0015.wav | WER=36.986301 | S=9 D=18 I=0
2026-01-29 16:27:14,081 | INFO | ------------------------------
2026-01-29 16:27:14,081 | INFO | whisper med Done!
2026-01-29 16:27:18,332 | INFO | Chunk: 0 | WER=42.622951 | S=9 D=17 I=0
2026-01-29 16:27:18,333 | INFO | Chunk: 1 | WER=33.333333 | S=4 D=2 I=0
2026-01-29 16:27:18,336 | INFO | File: Rhap-M0015.wav | WER=38.356164 | S=15 D=13 I=0
2026-01-29 16:27:18,336 | INFO | ------------------------------
2026-01-29 16:27:18,336 | INFO | whisper large Done!
2026-01-29 16:27:18,465 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-29 16:27:18,495 | INFO | Vocabulary size: 350
2026-01-29 16:27:19,097 | INFO | Gradient checkpoint layers: []
2026-01-29 16:27:19,784 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 16:27:19,787 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 16:27:19,787 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 16:27:19,788 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-29 16:27:19,788 | INFO | speech length: 349920
2026-01-29 16:27:19,827 | INFO | decoder input length: 546
2026-01-29 16:27:19,827 | INFO | max output length: 546
2026-01-29 16:27:19,827 | INFO | min output length: 54
2026-01-29 16:27:32,902 | INFO | end detected at 156
2026-01-29 16:27:32,903 | INFO | -212.65 * 0.5 = -106.33 for decoder
2026-01-29 16:27:32,903 | INFO | -93.09 * 0.5 = -46.55 for ctc
2026-01-29 16:27:32,903 | INFO | total log probability: -152.87
2026-01-29 16:27:32,903 | INFO | normalized log probability: -1.01
2026-01-29 16:27:32,903 | INFO | total number of ended hypotheses: 144
2026-01-29 16:27:32,905 | INFO | best hypo: ▁heu▁je▁d'ici▁du▁cc▁j▁je▁remonte▁la▁viande▁à▁lorenne▁jusqu'à▁la▁place▁victor▁hugo▁je▁traverse▁l'avenue▁vic▁la▁place▁victor▁hugo▁en▁diagonale▁et▁jusqu'à▁arrivé▁au▁boulevard▁gute▁samba▁je▁remonte▁le▁boulevard▁à▁gute▁samba▁et▁donc▁vers▁le▁sud▁à▁tonaia▁dire▁et▁déjà▁je▁passe▁devant▁la▁grand▁poste

2026-01-29 16:27:32,908 | INFO | speech length: 51360
2026-01-29 16:27:32,950 | INFO | decoder input length: 79
2026-01-29 16:27:32,950 | INFO | max output length: 79
2026-01-29 16:27:32,950 | INFO | min output length: 7
2026-01-29 16:27:34,455 | INFO | end detected at 43
2026-01-29 16:27:34,455 | INFO | -11.45 * 0.5 =  -5.72 for decoder
2026-01-29 16:27:34,456 | INFO | -12.62 * 0.5 =  -6.31 for ctc
2026-01-29 16:27:34,456 | INFO | total log probability: -12.03
2026-01-29 16:27:34,456 | INFO | normalized log probability: -0.35
2026-01-29 16:27:34,456 | INFO | total number of ended hypotheses: 181
2026-01-29 16:27:34,456 | INFO | best hypo: ▁après▁de▁passer▁devant▁ma▁grande▁poste▁et▁bien▁j'arrive▁au▁cinéma▁à▁neuf▁chars

2026-01-29 16:27:34,462 | INFO | Chunk: 0 | WER=40.983607 | S=17 D=4 I=4
2026-01-29 16:27:34,463 | INFO | Chunk: 1 | WER=55.555556 | S=8 D=2 I=0
2026-01-29 16:27:34,467 | INFO | File: Rhap-M0015.wav | WER=43.835616 | S=22 D=3 I=7
2026-01-29 16:27:34,467 | INFO | ------------------------------
2026-01-29 16:27:34,467 | INFO | Conf cv Done!
2026-01-29 16:27:34,633 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-29 16:27:34,651 | INFO | Vocabulary size: 47
2026-01-29 16:27:35,152 | INFO | Gradient checkpoint layers: []
2026-01-29 16:27:35,837 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 16:27:35,841 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 16:27:35,841 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 16:27:35,841 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-29 16:27:35,843 | INFO | speech length: 349920
2026-01-29 16:27:35,875 | INFO | decoder input length: 546
2026-01-29 16:27:35,876 | INFO | max output length: 546
2026-01-29 16:27:35,876 | INFO | min output length: 54
2026-01-29 16:27:58,194 | INFO | end detected at 303
2026-01-29 16:27:58,196 | INFO | -63.54 * 0.5 = -31.77 for decoder
2026-01-29 16:27:58,196 | INFO | -42.05 * 0.5 = -21.02 for ctc
2026-01-29 16:27:58,196 | INFO | total log probability: -52.79
2026-01-29 16:27:58,196 | INFO | normalized log probability: -0.18
2026-01-29 16:27:58,196 | INFO | total number of ended hypotheses: 178
2026-01-29 16:27:58,200 | INFO | best hypo: euh<space>je<space>d'ici<space>du<space>csij<space>je<space>remonte<space>la<space>violence<space>forelle<space>jusqu'à<space>la<space>place<space>victor<space>hugo<space>je<space>traverse<space>la<space>venue<space>vite<space>le<space>la<space>place<space>victor<space>hugo<space>en<space>diagonal<space>jusqu'à<space>arriver<space>au<space>au<space>boulevard<space>du<space>de<space>samba<space>je<space>remonte<space>le<space>boulevard<space>agoutte<space>samba<space>donc<space>vers<space>le<space>sud<space>on<space>va<space>dire<space>et<space>j'ar<space>je<space>passe<space>devant<space>la<space>grande<space>poste

2026-01-29 16:27:58,202 | INFO | speech length: 51360
2026-01-29 16:27:58,243 | INFO | decoder input length: 79
2026-01-29 16:27:58,243 | INFO | max output length: 79
2026-01-29 16:27:58,243 | INFO | min output length: 7
2026-01-29 16:28:00,596 | INFO | adding <eos> in the last position in the loop
2026-01-29 16:28:00,603 | INFO | no hypothesis. Finish decoding.
2026-01-29 16:28:00,604 | INFO | -28.54 * 0.5 = -14.27 for decoder
2026-01-29 16:28:00,604 | INFO | -56.77 * 0.5 = -28.38 for ctc
2026-01-29 16:28:00,604 | INFO | total log probability: -42.65
2026-01-29 16:28:00,604 | INFO | normalized log probability: -0.55
2026-01-29 16:28:00,604 | INFO | total number of ended hypotheses: 85
2026-01-29 16:28:00,605 | INFO | best hypo: et<space>après<space>de<space>pser<space>devant<space>la<space>grande<space>pote<space>eh<space>bien<space>j'arrive<space>au<space>cinéma<space>la<space>necher

2026-01-29 16:28:00,610 | INFO | Chunk: 0 | WER=26.229508 | S=13 D=2 I=1
2026-01-29 16:28:00,610 | INFO | Chunk: 1 | WER=38.888889 | S=5 D=2 I=0
2026-01-29 16:28:00,614 | INFO | File: Rhap-M0015.wav | WER=31.506849 | S=16 D=2 I=5
2026-01-29 16:28:00,614 | INFO | ------------------------------
2026-01-29 16:28:00,614 | INFO | Conf ester Done!
2026-01-29 16:28:17,145 | INFO | Chunk: 0 | WER=31.147541 | S=15 D=4 I=0
2026-01-29 16:28:17,146 | INFO | Chunk: 1 | WER=27.777778 | S=3 D=2 I=0
2026-01-29 16:28:17,153 | INFO | File: Rhap-M0015.wav | WER=30.136986 | S=16 D=3 I=3
2026-01-29 16:28:17,153 | INFO | ------------------------------
2026-01-29 16:28:17,153 | INFO | hmm_tdnn Done!
2026-01-29 16:28:17,297 | INFO | ==================================Rhap-M0016.wav=========================================
2026-01-29 16:28:17,324 | INFO | Using rVAD model
2026-01-29 16:28:21,281 | INFO | Chunk: 0 | WER=30.769231 | S=5 D=13 I=2
2026-01-29 16:28:21,284 | INFO | Chunk: 1 | WER=38.095238 | S=16 D=16 I=0
2026-01-29 16:28:21,288 | INFO | Chunk: 2 | WER=19.277108 | S=8 D=6 I=2
2026-01-29 16:28:21,288 | INFO | Chunk: 3 | WER=60.000000 | S=1 D=0 I=2
2026-01-29 16:28:21,308 | INFO | File: Rhap-M0016.wav | WER=25.821596 | S=32 D=14 I=9
2026-01-29 16:28:21,308 | INFO | ------------------------------
2026-01-29 16:28:21,308 | INFO | w2vec vad chunk Done!
2026-01-29 16:28:29,652 | INFO | Chunk: 0 | WER=44.615385 | S=14 D=12 I=3
2026-01-29 16:28:29,655 | INFO | Chunk: 1 | WER=63.095238 | S=31 D=21 I=1
2026-01-29 16:28:29,657 | INFO | Chunk: 2 | WER=48.192771 | S=9 D=30 I=1
2026-01-29 16:28:29,657 | INFO | Chunk: 3 | WER=60.000000 | S=1 D=0 I=2
2026-01-29 16:28:29,675 | INFO | File: Rhap-M0016.wav | WER=49.295775 | S=57 D=40 I=8
2026-01-29 16:28:29,675 | INFO | ------------------------------
2026-01-29 16:28:29,675 | INFO | whisper med Done!
2026-01-29 16:28:37,737 | INFO | Chunk: 0 | WER=41.538462 | S=3 D=24 I=0
2026-01-29 16:28:37,739 | INFO | Chunk: 1 | WER=61.904762 | S=1 D=50 I=1
2026-01-29 16:28:37,741 | INFO | Chunk: 2 | WER=51.807229 | S=10 D=30 I=3
2026-01-29 16:28:37,742 | INFO | Chunk: 3 | WER=60.000000 | S=1 D=0 I=2
2026-01-29 16:28:37,756 | INFO | File: Rhap-M0016.wav | WER=49.295775 | S=13 D=83 I=9
2026-01-29 16:28:37,756 | INFO | ------------------------------
2026-01-29 16:28:37,756 | INFO | whisper large Done!
2026-01-29 16:28:37,897 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-29 16:28:37,928 | INFO | Vocabulary size: 350
2026-01-29 16:28:38,484 | INFO | Gradient checkpoint layers: []
2026-01-29 16:28:39,153 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 16:28:39,157 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 16:28:39,157 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 16:28:39,157 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-29 16:28:39,157 | INFO | speech length: 426880
2026-01-29 16:28:39,194 | INFO | decoder input length: 666
2026-01-29 16:28:39,194 | INFO | max output length: 666
2026-01-29 16:28:39,195 | INFO | min output length: 66
2026-01-29 16:28:51,859 | INFO | end detected at 129
2026-01-29 16:28:51,860 | INFO | -272.51 * 0.5 = -136.25 for decoder
2026-01-29 16:28:51,860 | INFO | -70.67 * 0.5 = -35.33 for ctc
2026-01-29 16:28:51,860 | INFO | total log probability: -171.59
2026-01-29 16:28:51,860 | INFO | normalized log probability: -1.38
2026-01-29 16:28:51,860 | INFO | total number of ended hypotheses: 156
2026-01-29 16:28:51,862 | INFO | best hypo: ▁mon▁pour▁aller▁du▁serd'épée▁à▁la▁gare▁de▁grenoble▁je▁me▁sors▁déjà▁du▁cr▁d'épée▁je▁remonte▁l'avenue▁général▁champon▁je▁traverse▁face▁à▁la▁mdieh▁et▁je▁je▁continue▁je▁continue▁jusque'à▁une▁place▁'▁face▁de▁la▁grande▁de▁poste▁où▁la▁chambre▁de▁commerce▁et▁d'industrie

2026-01-29 16:28:51,865 | INFO | speech length: 395840
2026-01-29 16:28:51,902 | INFO | decoder input length: 618
2026-01-29 16:28:51,902 | INFO | max output length: 618
2026-01-29 16:28:51,902 | INFO | min output length: 61
2026-01-29 16:29:06,995 | INFO | end detected at 165
2026-01-29 16:29:06,996 | INFO | -393.79 * 0.5 = -196.89 for decoder
2026-01-29 16:29:06,996 | INFO | -111.68 * 0.5 = -55.84 for ctc
2026-01-29 16:29:06,996 | INFO | total log probability: -252.73
2026-01-29 16:29:06,996 | INFO | normalized log probability: -1.57
2026-01-29 16:29:06,996 | INFO | total number of ended hypotheses: 91
2026-01-29 16:29:06,998 | INFO | best hypo: ▁et▁je▁reprends▁je▁crois▁le▁non▁c'est▁pas▁le▁boulevard▁gambetta▁je▁ne▁sais▁pas▁le▁qu'elle▁sait▁c'est▁l'autre▁et▁donc▁je▁va▁jusqu'squ'à▁la▁place▁victor▁hugo▁et▁l'âge▁me▁retrouve▁en▁effetp▁près▁du▁ras▁du▁tram▁et▁de▁çail▁du▁tram▁et▁jer▁je▁l'ai▁longer▁je▁vai▁vers▁le▁boulevard▁ga▁métas▁cette▁fois▁c'est▁le▁boulevard▁ga▁betta

2026-01-29 16:29:07,000 | INFO | speech length: 441760
2026-01-29 16:29:07,048 | INFO | decoder input length: 689
2026-01-29 16:29:07,048 | INFO | max output length: 689
2026-01-29 16:29:07,048 | INFO | min output length: 68
2026-01-29 16:29:24,265 | INFO | end detected at 173
2026-01-29 16:29:24,266 | INFO | -476.37 * 0.5 = -238.19 for decoder
2026-01-29 16:29:24,267 | INFO | -128.41 * 0.5 = -64.21 for ctc
2026-01-29 16:29:24,267 | INFO | total log probability: -302.39
2026-01-29 16:29:24,267 | INFO | normalized log probability: -1.80
2026-01-29 16:29:24,267 | INFO | total number of ended hypotheses: 160
2026-01-29 16:29:24,269 | INFO | best hypo: ▁ensuite▁je▁vais▁prendre▁je▁froxer▁l'avenue▁alsace▁lorraine▁que▁je▁vais▁remonté▁remonter▁et▁je▁vais▁traverser▁le▁coursjean▁jaurès▁si▁je▁me▁souviens▁bien▁et▁je▁vais▁je▁vais▁toujours▁g▁continuer▁cette▁avenue▁alace▁de▁lorraine▁et▁e▁valà▁j'arrive▁du▁niveau▁de▁la▁grande▁place▁et▁de▁la▁gare▁bien▁à▁tous▁les▁trames▁tous▁les▁bus▁'on▁pas▁tous▁les▁bus▁qui▁sont▁pas▁de▁ce▁côtés▁là▁voilà

2026-01-29 16:29:24,272 | INFO | speech length: 36320
2026-01-29 16:29:24,320 | INFO | decoder input length: 56
2026-01-29 16:29:24,320 | INFO | max output length: 56
2026-01-29 16:29:24,320 | INFO | min output length: 5
2026-01-29 16:29:24,987 | INFO | end detected at 19
2026-01-29 16:29:24,988 | INFO |  -1.56 * 0.5 =  -0.78 for decoder
2026-01-29 16:29:24,988 | INFO |  -0.39 * 0.5 =  -0.20 for ctc
2026-01-29 16:29:24,988 | INFO | total log probability: -0.98
2026-01-29 16:29:24,989 | INFO | normalized log probability: -0.07
2026-01-29 16:29:24,989 | INFO | total number of ended hypotheses: 149
2026-01-29 16:29:24,989 | INFO | best hypo: ▁et▁je▁suis▁arrivé▁sur▁la▁gare

2026-01-29 16:29:24,995 | INFO | Chunk: 0 | WER=38.461538 | S=7 D=14 I=4
2026-01-29 16:29:24,998 | INFO | Chunk: 1 | WER=40.476190 | S=22 D=11 I=1
2026-01-29 16:29:25,002 | INFO | Chunk: 2 | WER=33.734940 | S=13 D=11 I=4
2026-01-29 16:29:25,002 | INFO | Chunk: 3 | WER=60.000000 | S=1 D=0 I=2
2026-01-29 16:29:25,022 | INFO | File: Rhap-M0016.wav | WER=32.863850 | S=41 D=15 I=14
2026-01-29 16:29:25,022 | INFO | ------------------------------
2026-01-29 16:29:25,022 | INFO | Conf cv Done!
2026-01-29 16:29:25,191 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-29 16:29:25,209 | INFO | Vocabulary size: 47
2026-01-29 16:29:25,943 | INFO | Gradient checkpoint layers: []
2026-01-29 16:29:26,692 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 16:29:26,696 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 16:29:26,696 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 16:29:26,696 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-29 16:29:26,700 | INFO | speech length: 426880
2026-01-29 16:29:26,731 | INFO | decoder input length: 666
2026-01-29 16:29:26,731 | INFO | max output length: 666
2026-01-29 16:29:26,731 | INFO | min output length: 66
2026-01-29 16:29:52,017 | INFO | end detected at 288
2026-01-29 16:29:52,019 | INFO | -70.49 * 0.5 = -35.25 for decoder
2026-01-29 16:29:52,019 | INFO |  -8.39 * 0.5 =  -4.20 for ctc
2026-01-29 16:29:52,019 | INFO | total log probability: -39.44
2026-01-29 16:29:52,019 | INFO | normalized log probability: -0.14
2026-01-29 16:29:52,019 | INFO | total number of ended hypotheses: 198
2026-01-29 16:29:52,022 | INFO | best hypo: mon<space>pour<space>aller<space>du<space>crdp<space>à<space>la<space>gare<space>euh<space>de<space>grenoble<space>je<space>euh<space>mais<space>je<space>sors<space>déjà<space>du<space>crdt<space>je<space>remonte<space>l'avenue<space>général<space>champon<space>je<space>traverse<space>euh<space>face<space>à<space>la<space>mnde<space>et<space>je<space>euh<space>je<space>continue<space>je<space>continue<space>jusqu'à<space>une<space>place<space>qui<space>est<space>face<space>à<space>la<space>grande<space>poste<space>où<space>il<space>y<space>a<space>la<space>chambre<space>de<space>commerce<space>et<space>d'industrie

2026-01-29 16:29:52,024 | INFO | speech length: 395840
2026-01-29 16:29:52,055 | INFO | decoder input length: 618
2026-01-29 16:29:52,055 | INFO | max output length: 618
2026-01-29 16:29:52,055 | INFO | min output length: 61
2026-01-29 16:30:22,635 | INFO | end detected at 386
2026-01-29 16:30:22,637 | INFO | -152.45 * 0.5 = -76.22 for decoder
2026-01-29 16:30:22,637 | INFO | -40.07 * 0.5 = -20.03 for ctc
2026-01-29 16:30:22,637 | INFO | total log probability: -96.26
2026-01-29 16:30:22,637 | INFO | normalized log probability: -0.25
2026-01-29 16:30:22,637 | INFO | total number of ended hypotheses: 196
2026-01-29 16:30:22,641 | INFO | best hypo: et<space>je<space>reprends<space>je<space>crois<space>le<space>non<space>c'est<space>pas<space>le<space>boulevard<space>grand<space>métal<space>je<space>ne<space>sais<space>pas<space>le<space>qu'elle<space>fait<space>euh<space>c'est<space>l'autre<space>et<space>euh<space>donc<space>je<space>vais<space>jusqu'au<space>jusqu'à<space>la<space>place<space>victor<space>hugo<space>et<space>là<space>je<space>me<space>retrouve<space>en<space>effet<space>euh<space>s<space>près<space>des<space>rails<space>du<space>drame<space>et<space>euh<space>de<space>ses<space>rails<space>de<space>traves<space>eh<space>ben<space>j'ai<space>j'ai<space>l'ai<space>longé<space>je<space>l'ai<space>traversé<space>euh<space>le<space>boulevard<space>en<space>métat<space>ça<space>cette<space>fois<space>c'est<space>le<space>boulevard<space>d'en<space>métat

2026-01-29 16:30:22,645 | INFO | speech length: 441760
2026-01-29 16:30:22,690 | INFO | decoder input length: 689
2026-01-29 16:30:22,690 | INFO | max output length: 689
2026-01-29 16:30:22,691 | INFO | min output length: 68
2026-01-29 16:30:56,853 | INFO | end detected at 402
2026-01-29 16:30:56,854 | INFO | -168.79 * 0.5 = -84.40 for decoder
2026-01-29 16:30:56,854 | INFO | -33.22 * 0.5 = -16.61 for ctc
2026-01-29 16:30:56,854 | INFO | total log probability: -101.01
2026-01-29 16:30:56,854 | INFO | normalized log probability: -0.25
2026-01-29 16:30:56,854 | INFO | total number of ended hypotheses: 160
2026-01-29 16:30:56,859 | INFO | best hypo: ensuite<space>euh<space>je<space>vais<space>euh<space>prendre<space>je<space>crois<space>que<space>c'est<space>l'avenir<space>de<space>la<space>floraine<space>que<space>je<space>vais<space>remonter<space>remonter<space>je<space>vais<space>traverser<space>le<space>courant<space>jaurès<space>et<space>je<space>me<space>souviens<space>bien<space>et<space>je<space>vais<space>euh<space>je<space>vais<space>toujours<space>continuer<space>cette<space>amie<space>alzas<space>lorenne<space>et<space>euh<space>eh<space>ben<space>voilà<space>j'arrive<space>au<space>niveau<space>de<space>la<space>grande<space>place<space>de<space>hagard<space>il<space>y<space>a<space>tous<space>les<space>tramains<space>tous<space>les<space>bus<space>n'ont<space>pas<space>tous<space>les<space>bus<space>qui<space>sont<space>pas<space>de<space>ce<space>côté<space>là<space>et<space>voilà

2026-01-29 16:30:56,861 | INFO | speech length: 36320
2026-01-29 16:30:56,888 | INFO | decoder input length: 56
2026-01-29 16:30:56,889 | INFO | max output length: 56
2026-01-29 16:30:56,889 | INFO | min output length: 5
2026-01-29 16:30:58,065 | INFO | end detected at 40
2026-01-29 16:30:58,067 | INFO |  -3.48 * 0.5 =  -1.74 for decoder
2026-01-29 16:30:58,067 | INFO |  -2.00 * 0.5 =  -1.00 for ctc
2026-01-29 16:30:58,067 | INFO | total log probability: -2.74
2026-01-29 16:30:58,067 | INFO | normalized log probability: -0.08
2026-01-29 16:30:58,067 | INFO | total number of ended hypotheses: 189
2026-01-29 16:30:58,067 | INFO | best hypo: et<space>je<space>suis<space>arrivé<space>un<space>surveillard

2026-01-29 16:30:58,076 | INFO | Chunk: 0 | WER=12.307692 | S=4 D=4 I=0
2026-01-29 16:30:58,080 | INFO | Chunk: 1 | WER=33.333333 | S=19 D=2 I=7
2026-01-29 16:30:58,083 | INFO | Chunk: 2 | WER=22.891566 | S=13 D=3 I=3
2026-01-29 16:30:58,084 | INFO | Chunk: 3 | WER=60.000000 | S=2 D=0 I=1
2026-01-29 16:30:58,106 | INFO | File: Rhap-M0016.wav | WER=32.863850 | S=38 D=3 I=29
2026-01-29 16:30:58,106 | INFO | ------------------------------
2026-01-29 16:30:58,106 | INFO | Conf ester Done!
2026-01-29 16:31:45,873 | INFO | Chunk: 0 | WER=29.230769 | S=7 D=10 I=2
2026-01-29 16:31:45,879 | INFO | Chunk: 1 | WER=30.952381 | S=8 D=18 I=0
2026-01-29 16:31:45,883 | INFO | Chunk: 2 | WER=26.506024 | S=14 D=5 I=3
2026-01-29 16:31:45,883 | INFO | Chunk: 3 | WER=60.000000 | S=1 D=0 I=2
2026-01-29 16:31:45,904 | INFO | File: Rhap-M0016.wav | WER=24.882629 | S=27 D=14 I=12
2026-01-29 16:31:45,905 | INFO | ------------------------------
2026-01-29 16:31:45,905 | INFO | hmm_tdnn Done!
2026-01-29 16:31:46,121 | INFO | ==================================Rhap-M0018.wav=========================================
2026-01-29 16:31:46,135 | INFO | Using rVAD model
2026-01-29 16:31:50,519 | INFO | Chunk: 0 | WER=27.272727 | S=2 D=0 I=1
2026-01-29 16:31:50,522 | INFO | Chunk: 1 | WER=7.594937 | S=2 D=4 I=0
2026-01-29 16:31:50,523 | INFO | Chunk: 2 | WER=32.432432 | S=5 D=7 I=0
2026-01-29 16:31:50,523 | INFO | Chunk: 3 | WER=18.181818 | S=0 D=1 I=1
2026-01-29 16:31:50,525 | INFO | Chunk: 4 | WER=29.166667 | S=9 D=4 I=1
2026-01-29 16:31:50,525 | INFO | Chunk: 5 | WER=16.666667 | S=2 D=0 I=0
2026-01-29 16:31:50,526 | INFO | Chunk: 6 | WER=19.230769 | S=0 D=5 I=0
2026-01-29 16:31:50,526 | INFO | Chunk: 7 | WER=50.000000 | S=4 D=3 I=3
2026-01-29 16:31:50,526 | INFO | Chunk: 8 | WER=500.000000 | S=1 D=0 I=4
2026-01-29 16:31:50,549 | INFO | File: Rhap-M0018.wav | WER=19.480519 | S=25 D=10 I=10
2026-01-29 16:31:50,549 | INFO | ------------------------------
2026-01-29 16:31:50,549 | INFO | w2vec vad chunk Done!
2026-01-29 16:32:00,189 | INFO | Chunk: 0 | WER=36.363636 | S=3 D=0 I=1
2026-01-29 16:32:00,191 | INFO | Chunk: 1 | WER=56.962025 | S=3 D=42 I=0
2026-01-29 16:32:00,192 | INFO | Chunk: 2 | WER=29.729730 | S=4 D=7 I=0
2026-01-29 16:32:00,193 | INFO | Chunk: 3 | WER=27.272727 | S=0 D=2 I=1
2026-01-29 16:32:00,194 | INFO | Chunk: 4 | WER=12.500000 | S=1 D=4 I=1
2026-01-29 16:32:00,194 | INFO | Chunk: 5 | WER=25.000000 | S=3 D=0 I=0
2026-01-29 16:32:00,195 | INFO | Chunk: 6 | WER=19.230769 | S=1 D=4 I=0
2026-01-29 16:32:00,195 | INFO | Chunk: 7 | WER=30.000000 | S=1 D=2 I=3
2026-01-29 16:32:00,196 | INFO | Chunk: 8 | WER=500.000000 | S=1 D=0 I=4
2026-01-29 16:32:00,216 | INFO | File: Rhap-M0018.wav | WER=32.034632 | S=17 D=47 I=10
2026-01-29 16:32:00,216 | INFO | ------------------------------
2026-01-29 16:32:00,216 | INFO | whisper med Done!
2026-01-29 16:32:13,671 | INFO | Chunk: 0 | WER=27.272727 | S=2 D=0 I=1
2026-01-29 16:32:13,674 | INFO | Chunk: 1 | WER=34.177215 | S=7 D=20 I=0
2026-01-29 16:32:13,676 | INFO | Chunk: 2 | WER=18.918919 | S=2 D=5 I=0
2026-01-29 16:32:13,676 | INFO | Chunk: 3 | WER=27.272727 | S=0 D=2 I=1
2026-01-29 16:32:13,677 | INFO | Chunk: 4 | WER=35.416667 | S=4 D=13 I=0
2026-01-29 16:32:13,678 | INFO | Chunk: 5 | WER=25.000000 | S=3 D=0 I=0
2026-01-29 16:32:13,678 | INFO | Chunk: 6 | WER=19.230769 | S=0 D=5 I=0
2026-01-29 16:32:13,679 | INFO | Chunk: 7 | WER=50.000000 | S=3 D=4 I=3
2026-01-29 16:32:13,679 | INFO | Chunk: 8 | WER=100.000000 | S=1 D=0 I=0
2026-01-29 16:32:13,705 | INFO | File: Rhap-M0018.wav | WER=26.406926 | S=23 D=34 I=4
2026-01-29 16:32:13,705 | INFO | ------------------------------
2026-01-29 16:32:13,705 | INFO | whisper large Done!
2026-01-29 16:32:13,940 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-29 16:32:13,978 | INFO | Vocabulary size: 350
2026-01-29 16:32:14,667 | INFO | Gradient checkpoint layers: []
2026-01-29 16:32:15,320 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 16:32:15,324 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 16:32:15,324 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 16:32:15,324 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-29 16:32:15,325 | INFO | speech length: 60480
2026-01-29 16:32:15,369 | INFO | decoder input length: 94
2026-01-29 16:32:15,369 | INFO | max output length: 94
2026-01-29 16:32:15,369 | INFO | min output length: 9
2026-01-29 16:32:16,886 | INFO | end detected at 36
2026-01-29 16:32:16,888 | INFO |  -3.15 * 0.5 =  -1.57 for decoder
2026-01-29 16:32:16,888 | INFO |  -1.72 * 0.5 =  -0.86 for ctc
2026-01-29 16:32:16,888 | INFO | total log probability: -2.43
2026-01-29 16:32:16,888 | INFO | normalized log probability: -0.08
2026-01-29 16:32:16,888 | INFO | total number of ended hypotheses: 147
2026-01-29 16:32:16,889 | INFO | best hypo: ▁il▁s'enchaîne▁sur▁une▁seconde▁scène▁où▁on▁voit▁une▁femme

2026-01-29 16:32:16,893 | INFO | speech length: 372480
2026-01-29 16:32:16,932 | INFO | decoder input length: 581
2026-01-29 16:32:16,933 | INFO | max output length: 581
2026-01-29 16:32:16,933 | INFO | min output length: 58
2026-01-29 16:32:31,025 | INFO | end detected at 161
2026-01-29 16:32:31,027 | INFO | -310.37 * 0.5 = -155.19 for decoder
2026-01-29 16:32:31,027 | INFO | -52.52 * 0.5 = -26.26 for ctc
2026-01-29 16:32:31,027 | INFO | total log probability: -181.45
2026-01-29 16:32:31,027 | INFO | normalized log probability: -1.17
2026-01-29 16:32:31,027 | INFO | total number of ended hypotheses: 158
2026-01-29 16:32:31,029 | INFO | best hypo: ▁qui▁a▁priorié▁pauvre▁et▁qui▁passe▁à▁l'endroit▁où▁un▁boulanger▁vient▁de▁se▁stationner▁avec▁des▁baguettes▁de▁pain▁qui▁il▁prend▁du▁pain▁un▁traverse▁qu'on▁sort▁de▁sa▁voiture▁rendent▁dans▁une▁boutique▁et▁au▁moment▁où▁la▁fille▁y▁passe▁et▁qu'elle▁voit▁le▁manque▁d'attentend▁du▁boulanger▁elle▁prend▁un▁bou▁de▁pain▁elle▁se▁sauve▁en▁courant▁mais▁à▁ce▁moment▁là▁elle▁est▁vue▁par▁une▁dame

2026-01-29 16:32:31,032 | INFO | speech length: 214880
2026-01-29 16:32:31,086 | INFO | decoder input length: 335
2026-01-29 16:32:31,086 | INFO | max output length: 335
2026-01-29 16:32:31,086 | INFO | min output length: 33
2026-01-29 16:32:36,533 | INFO | end detected at 87
2026-01-29 16:32:36,535 | INFO | -88.99 * 0.5 = -44.50 for decoder
2026-01-29 16:32:36,535 | INFO | -30.03 * 0.5 = -15.02 for ctc
2026-01-29 16:32:36,535 | INFO | total log probability: -59.51
2026-01-29 16:32:36,535 | INFO | normalized log probability: -0.73
2026-01-29 16:32:36,535 | INFO | total number of ended hypotheses: 167
2026-01-29 16:32:36,536 | INFO | best hypo: ▁plutôt▁âgé▁bonne▁bourgeoise▁voilà▁tout▁le▁conflit▁social▁dans▁cette▁petite▁année▁d'astumeran▁et▁qui▁bien▁évidemment▁délate▁le▁vol▁au▁boulanger▁lors▁qu'il▁sort▁de▁la▁boutique

2026-01-29 16:32:36,538 | INFO | speech length: 94400
2026-01-29 16:32:36,571 | INFO | decoder input length: 147
2026-01-29 16:32:36,571 | INFO | max output length: 147
2026-01-29 16:32:36,571 | INFO | min output length: 14
2026-01-29 16:32:38,116 | INFO | end detected at 35
2026-01-29 16:32:38,118 | INFO |  -4.28 * 0.5 =  -2.14 for decoder
2026-01-29 16:32:38,118 | INFO |  -6.62 * 0.5 =  -3.31 for ctc
2026-01-29 16:32:38,118 | INFO | total log probability: -5.45
2026-01-29 16:32:38,118 | INFO | normalized log probability: -0.19
2026-01-29 16:32:38,118 | INFO | total number of ended hypotheses: 190
2026-01-29 16:32:38,118 | INFO | best hypo: ▁et▁ha▁la▁réaction▁du▁boulanger▁pendant▁que▁ça▁se▁passait▁ça

2026-01-29 16:32:38,120 | INFO | speech length: 238720
2026-01-29 16:32:38,157 | INFO | decoder input length: 372
2026-01-29 16:32:38,157 | INFO | max output length: 372
2026-01-29 16:32:38,157 | INFO | min output length: 37
2026-01-29 16:32:44,742 | INFO | end detected at 101
2026-01-29 16:32:44,743 | INFO | -129.69 * 0.5 = -64.84 for decoder
2026-01-29 16:32:44,743 | INFO | -51.32 * 0.5 = -25.66 for ctc
2026-01-29 16:32:44,743 | INFO | total log probability: -90.50
2026-01-29 16:32:44,744 | INFO | normalized log probability: -0.94
2026-01-29 16:32:44,744 | INFO | total number of ended hypotheses: 172
2026-01-29 16:32:44,745 | INFO | best hypo: ▁j'ai▁préfé▁dans▁l'ordre▁on▁voit▁la▁vigue▁non▁qui▁part▁avec▁son▁bout▁de▁pain▁au▁moment▁où▁charlie▁chaplin▁arrive▁sur▁le▁trottoir▁où▁il▁se▁heurte▁sauvagement▁et▁il▁tombe▁en▁terre▁dans▁les▁bras▁l'un▁de▁l'autre▁et

2026-01-29 16:32:44,747 | INFO | speech length: 47040
2026-01-29 16:32:44,782 | INFO | decoder input length: 73
2026-01-29 16:32:44,782 | INFO | max output length: 73
2026-01-29 16:32:44,782 | INFO | min output length: 7
2026-01-29 16:32:45,746 | INFO | end detected at 26
2026-01-29 16:32:45,747 | INFO |  -3.28 * 0.5 =  -1.64 for decoder
2026-01-29 16:32:45,748 | INFO |  -1.81 * 0.5 =  -0.90 for ctc
2026-01-29 16:32:45,748 | INFO | total log probability: -2.54
2026-01-29 16:32:45,748 | INFO | normalized log probability: -0.12
2026-01-29 16:32:45,748 | INFO | total number of ended hypotheses: 169
2026-01-29 16:32:45,748 | INFO | best hypo: ▁et▁voilà▁et▁bien▁à▁la▁fin▁je▁peux▁déconcentrer

2026-01-29 16:32:45,750 | INFO | speech length: 132000
2026-01-29 16:32:45,785 | INFO | decoder input length: 205
2026-01-29 16:32:45,785 | INFO | max output length: 205
2026-01-29 16:32:45,785 | INFO | min output length: 20
2026-01-29 16:32:48,887 | INFO | end detected at 64
2026-01-29 16:32:48,888 | INFO | -13.11 * 0.5 =  -6.56 for decoder
2026-01-29 16:32:48,888 | INFO |  -5.76 * 0.5 =  -2.88 for ctc
2026-01-29 16:32:48,888 | INFO | total log probability: -9.44
2026-01-29 16:32:48,888 | INFO | normalized log probability: -0.16
2026-01-29 16:32:48,888 | INFO | total number of ended hypotheses: 182
2026-01-29 16:32:48,889 | INFO | best hypo: ▁mais▁ça▁se▁termine▁un▁peu▁en▁coupure▁où▁le▁boulanger▁la▁vieille▁dame▁rejoint▁la▁bourgeoise▁rejoint▁le▁boulanger

2026-01-29 16:32:48,891 | INFO | speech length: 102400
2026-01-29 16:32:48,924 | INFO | decoder input length: 159
2026-01-29 16:32:48,924 | INFO | max output length: 159
2026-01-29 16:32:48,924 | INFO | min output length: 15
2026-01-29 16:32:51,567 | INFO | end detected at 61
2026-01-29 16:32:51,569 | INFO | -20.09 * 0.5 = -10.05 for decoder
2026-01-29 16:32:51,569 | INFO | -14.14 * 0.5 =  -7.07 for ctc
2026-01-29 16:32:51,569 | INFO | total log probability: -17.12
2026-01-29 16:32:51,569 | INFO | normalized log probability: -0.34
2026-01-29 16:32:51,569 | INFO | total number of ended hypotheses: 215
2026-01-29 16:32:51,570 | INFO | best hypo: ▁après▁que▁la▁fille▁soit▁par▁terre▁et▁charlie▁nous▁gère▁l'échapune▁soit▁par▁terre▁et▁la▁fille▁soit▁partie▁je▁vois▁pue

2026-01-29 16:32:51,572 | INFO | speech length: 14400
2026-01-29 16:32:51,603 | INFO | decoder input length: 22
2026-01-29 16:32:51,603 | INFO | max output length: 22
2026-01-29 16:32:51,603 | INFO | min output length: 2
2026-01-29 16:32:52,116 | INFO | end detected at 16
2026-01-29 16:32:52,118 | INFO |  -3.27 * 0.5 =  -1.64 for decoder
2026-01-29 16:32:52,118 | INFO | -12.76 * 0.5 =  -6.38 for ctc
2026-01-29 16:32:52,118 | INFO | total log probability: -8.02
2026-01-29 16:32:52,118 | INFO | normalized log probability: -1.34
2026-01-29 16:32:52,118 | INFO | total number of ended hypotheses: 200
2026-01-29 16:32:52,118 | INFO | best hypo: ▁mon▁grand▁set

2026-01-29 16:32:52,126 | INFO | Chunk: 0 | WER=27.272727 | S=2 D=0 I=1
2026-01-29 16:32:52,130 | INFO | Chunk: 1 | WER=16.455696 | S=7 D=3 I=3
2026-01-29 16:32:52,131 | INFO | Chunk: 2 | WER=40.540541 | S=7 D=7 I=1
2026-01-29 16:32:52,131 | INFO | Chunk: 3 | WER=27.272727 | S=2 D=0 I=1
2026-01-29 16:32:52,133 | INFO | Chunk: 4 | WER=27.083333 | S=9 D=3 I=1
2026-01-29 16:32:52,133 | INFO | Chunk: 5 | WER=41.666667 | S=3 D=2 I=0
2026-01-29 16:32:52,134 | INFO | Chunk: 6 | WER=23.076923 | S=0 D=6 I=0
2026-01-29 16:32:52,134 | INFO | Chunk: 7 | WER=35.000000 | S=3 D=0 I=4
2026-01-29 16:32:52,134 | INFO | Chunk: 8 | WER=300.000000 | S=1 D=0 I=2
2026-01-29 16:32:52,159 | INFO | File: Rhap-M0018.wav | WER=24.242424 | S=32 D=9 I=15
2026-01-29 16:32:52,159 | INFO | ------------------------------
2026-01-29 16:32:52,159 | INFO | Conf cv Done!
2026-01-29 16:32:52,402 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-29 16:32:52,420 | INFO | Vocabulary size: 47
2026-01-29 16:32:52,932 | INFO | Gradient checkpoint layers: []
2026-01-29 16:32:53,521 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 16:32:53,525 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 16:32:53,525 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 16:32:53,525 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-29 16:32:53,528 | INFO | speech length: 60480
2026-01-29 16:32:53,557 | INFO | decoder input length: 94
2026-01-29 16:32:53,557 | INFO | max output length: 94
2026-01-29 16:32:53,557 | INFO | min output length: 9
2026-01-29 16:32:55,832 | INFO | end detected at 67
2026-01-29 16:32:55,833 | INFO |  -6.12 * 0.5 =  -3.06 for decoder
2026-01-29 16:32:55,834 | INFO |  -4.51 * 0.5 =  -2.25 for ctc
2026-01-29 16:32:55,834 | INFO | total log probability: -5.31
2026-01-29 16:32:55,834 | INFO | normalized log probability: -0.09
2026-01-29 16:32:55,834 | INFO | total number of ended hypotheses: 200
2026-01-29 16:32:55,835 | INFO | best hypo: et<space>s'enchaîne<space>sur<space>une<space>seconde<space>scène<space>où<space>on<space>voit<space>une<space>femme

2026-01-29 16:32:55,836 | INFO | speech length: 372480
2026-01-29 16:32:55,864 | INFO | decoder input length: 581
2026-01-29 16:32:55,864 | INFO | max output length: 581
2026-01-29 16:32:55,864 | INFO | min output length: 58
2026-01-29 16:33:23,694 | INFO | end detected at 386
2026-01-29 16:33:23,696 | INFO | -211.45 * 0.5 = -105.72 for decoder
2026-01-29 16:33:23,696 | INFO | -35.53 * 0.5 = -17.77 for ctc
2026-01-29 16:33:23,696 | INFO | total log probability: -123.49
2026-01-29 16:33:23,696 | INFO | normalized log probability: -0.33
2026-01-29 16:33:23,696 | INFO | total number of ended hypotheses: 182
2026-01-29 16:33:23,700 | INFO | best hypo: qui<space>a<space>priori<space>est<space>pauvre<space>euh<space>et<space>qui<space>passe<space>à<space>l'endroit<space>où<space>un<space>boulanger<space>vient<space>de<space>se<space>stationner<space>avec<space>des<space>baillettes<space>de<space>pain<space>qui<space>prend<space>du<space>pain<space>traverse<space>son<space>sort<space>de<space>sa<space>voiture<space>rentre<space>dans<space>une<space>boutique<space>et<space>au<space>moment<space>où<space>la<space>fille<space>passe<space>et<space>qu'elle<space>voit<space>le<space>manque<space>d'attention<space>du<space>boulanger<space>elle<space>prend<space>en<space>bout<space>le<space>pain<space>et<space>elle<space>st<space>sauve<space>aun<space>courant<space>mais<space>à<space>ce<space>moment<space>là<space>elle<space>est<space>vu<space>par<space>une<space>dame

2026-01-29 16:33:23,702 | INFO | speech length: 214880
2026-01-29 16:33:23,732 | INFO | decoder input length: 335
2026-01-29 16:33:23,732 | INFO | max output length: 335
2026-01-29 16:33:23,732 | INFO | min output length: 33
2026-01-29 16:33:33,983 | INFO | end detected at 188
2026-01-29 16:33:33,985 | INFO | -21.17 * 0.5 = -10.59 for decoder
2026-01-29 16:33:33,985 | INFO | -14.74 * 0.5 =  -7.37 for ctc
2026-01-29 16:33:33,985 | INFO | total log probability: -17.96
2026-01-29 16:33:33,985 | INFO | normalized log probability: -0.10
2026-01-29 16:33:33,985 | INFO | total number of ended hypotheses: 181
2026-01-29 16:33:33,987 | INFO | best hypo: plutôt<space>agée<space>bonne<space>bourgeoise<space>euh<space>voilà<space>tout<space>le<space>conflit<space>social<space>dans<space>cette<space>petite<space>anecdotement<space>et<space>euh<space>qui<space>bien<space>évidemment<space>délate<space>le<space>vol<space>au<space>boulanger<space>lorsqu'il<space>ressort<space>de<space>la<space>boutique

2026-01-29 16:33:33,990 | INFO | speech length: 94400
2026-01-29 16:33:34,018 | INFO | decoder input length: 147
2026-01-29 16:33:34,019 | INFO | max output length: 147
2026-01-29 16:33:34,019 | INFO | min output length: 14
2026-01-29 16:33:37,405 | INFO | end detected at 87
2026-01-29 16:33:37,407 | INFO |  -7.07 * 0.5 =  -3.54 for decoder
2026-01-29 16:33:37,407 | INFO |  -4.48 * 0.5 =  -2.24 for ctc
2026-01-29 16:33:37,407 | INFO | total log probability: -5.78
2026-01-29 16:33:37,407 | INFO | normalized log probability: -0.07
2026-01-29 16:33:37,407 | INFO | total number of ended hypotheses: 183
2026-01-29 16:33:37,408 | INFO | best hypo: et<space>pour<space>cent<space>hesitation<space>voilà<space>réaction<space>du<space>boulanger<space>pendant<space>que<space>ça<space>se<space>passait<space>ça

2026-01-29 16:33:37,411 | INFO | speech length: 238720
2026-01-29 16:33:37,441 | INFO | decoder input length: 372
2026-01-29 16:33:37,441 | INFO | max output length: 372
2026-01-29 16:33:37,441 | INFO | min output length: 37
2026-01-29 16:33:51,370 | INFO | end detected at 246
2026-01-29 16:33:51,373 | INFO | -34.87 * 0.5 = -17.44 for decoder
2026-01-29 16:33:51,373 | INFO | -38.23 * 0.5 = -19.11 for ctc
2026-01-29 16:33:51,373 | INFO | total log probability: -36.55
2026-01-29 16:33:51,373 | INFO | normalized log probability: -0.15
2026-01-29 16:33:51,373 | INFO | total number of ended hypotheses: 209
2026-01-29 16:33:51,376 | INFO | best hypo: j'ai<space>pas<space>fait<space>dans<space>l'ordre<space>on<space>voit<space>la<space>vigue<space>donc<space>qui<space>part<space>euh<space>avec<space>son<space>boupin<space>au<space>moment<space>où<space>charlie<space>chaklin<space>arrive<space>sur<space>le<space>trottoir<space>où<space>il<space>se<space>heurte<space>euh<space>sauvagement<space>et<space>tombent<space>par<space>terre<space>dans<space>les<space>bralins<space>de<space>l'autre<space>et<space>pour<space>cent<space>hesitation

2026-01-29 16:33:51,378 | INFO | speech length: 47040
2026-01-29 16:33:51,413 | INFO | decoder input length: 73
2026-01-29 16:33:51,413 | INFO | max output length: 73
2026-01-29 16:33:51,414 | INFO | min output length: 7
2026-01-29 16:33:53,304 | INFO | end detected at 60
2026-01-29 16:33:53,306 | INFO |  -4.62 * 0.5 =  -2.31 for decoder
2026-01-29 16:33:53,307 | INFO |  -9.16 * 0.5 =  -4.58 for ctc
2026-01-29 16:33:53,307 | INFO | total log probability: -6.89
2026-01-29 16:33:53,307 | INFO | normalized log probability: -0.14
2026-01-29 16:33:53,307 | INFO | total number of ended hypotheses: 199
2026-01-29 16:33:53,307 | INFO | best hypo: et<space>voilà<space>et<space>puis<space>à<space>la<space>fin<space>je<space>peux<space>déconcentrer

2026-01-29 16:33:53,309 | INFO | speech length: 132000
2026-01-29 16:33:53,335 | INFO | decoder input length: 205
2026-01-29 16:33:53,335 | INFO | max output length: 205
2026-01-29 16:33:53,335 | INFO | min output length: 20
2026-01-29 16:34:00,571 | INFO | end detected at 150
2026-01-29 16:34:00,574 | INFO | -11.86 * 0.5 =  -5.93 for decoder
2026-01-29 16:34:00,574 | INFO |  -2.86 * 0.5 =  -1.43 for ctc
2026-01-29 16:34:00,574 | INFO | total log probability: -7.36
2026-01-29 16:34:00,575 | INFO | normalized log probability: -0.05
2026-01-29 16:34:00,575 | INFO | total number of ended hypotheses: 247
2026-01-29 16:34:00,577 | INFO | best hypo: mais<space>euh<space>et<space>ça<space>se<space>termine<space>un<space>peu<space>en<space>coupure<space>ou<space>euh<space>le<space>boulanger<space>fin<space>la<space>la<space>vieille<space>dame<space>rejoint<space>euh<space>la<space>bourgeoise<space>rejoint<space>le<space>boulanger<space>euh

2026-01-29 16:34:00,579 | INFO | speech length: 102400
2026-01-29 16:34:00,616 | INFO | decoder input length: 159
2026-01-29 16:34:00,616 | INFO | max output length: 159
2026-01-29 16:34:00,616 | INFO | min output length: 15
2026-01-29 16:34:05,728 | INFO | end detected at 136
2026-01-29 16:34:05,729 | INFO | -17.82 * 0.5 =  -8.91 for decoder
2026-01-29 16:34:05,729 | INFO | -12.62 * 0.5 =  -6.31 for ctc
2026-01-29 16:34:05,729 | INFO | total log probability: -15.22
2026-01-29 16:34:05,729 | INFO | normalized log probability: -0.12
2026-01-29 16:34:05,729 | INFO | total number of ended hypotheses: 185
2026-01-29 16:34:05,731 | INFO | best hypo: après<space>que<space>euh<space>la<space>fille<space>soit<space>par<space>terre<space>et<space>charlie<space>mouchère<space>les<space>chefs<space>qui<space>ne<space>soient<space>par<space>terre<space>à<space>la<space>fille<space>soit<space>partie<space>je<space>vois<space>plus

2026-01-29 16:34:05,733 | INFO | speech length: 14400
2026-01-29 16:34:05,762 | INFO | decoder input length: 22
2026-01-29 16:34:05,762 | INFO | max output length: 22
2026-01-29 16:34:05,762 | INFO | min output length: 2
2026-01-29 16:34:06,350 | INFO | adding <eos> in the last position in the loop
2026-01-29 16:34:06,358 | INFO | no hypothesis. Finish decoding.
2026-01-29 16:34:06,359 | INFO |  -3.76 * 0.5 =  -1.88 for decoder
2026-01-29 16:34:06,359 | INFO |  -6.96 * 0.5 =  -3.48 for ctc
2026-01-29 16:34:06,359 | INFO | total log probability: -5.36
2026-01-29 16:34:06,359 | INFO | normalized log probability: -0.28
2026-01-29 16:34:06,359 | INFO | total number of ended hypotheses: 128
2026-01-29 16:34:06,360 | INFO | best hypo: bon<space>en<space>gros<space>c'est

2026-01-29 16:34:06,366 | INFO | Chunk: 0 | WER=18.181818 | S=1 D=0 I=1
2026-01-29 16:34:06,369 | INFO | Chunk: 1 | WER=11.392405 | S=8 D=1 I=0
2026-01-29 16:34:06,370 | INFO | Chunk: 2 | WER=24.324324 | S=2 D=7 I=0
2026-01-29 16:34:06,371 | INFO | Chunk: 3 | WER=36.363636 | S=1 D=0 I=3
2026-01-29 16:34:06,372 | INFO | Chunk: 4 | WER=35.416667 | S=8 D=5 I=4
2026-01-29 16:34:06,372 | INFO | Chunk: 5 | WER=33.333333 | S=2 D=2 I=0
2026-01-29 16:34:06,373 | INFO | Chunk: 6 | WER=7.692308 | S=1 D=0 I=1
2026-01-29 16:34:06,374 | INFO | Chunk: 7 | WER=55.000000 | S=5 D=0 I=6
2026-01-29 16:34:06,374 | INFO | Chunk: 8 | WER=400.000000 | S=0 D=0 I=4
2026-01-29 16:34:06,401 | INFO | File: Rhap-M0018.wav | WER=28.571429 | S=26 D=11 I=29
2026-01-29 16:34:06,401 | INFO | ------------------------------
2026-01-29 16:34:06,401 | INFO | Conf ester Done!
2026-01-29 16:35:10,042 | INFO | Chunk: 0 | WER=27.272727 | S=2 D=0 I=1
2026-01-29 16:35:10,049 | INFO | Chunk: 1 | WER=21.518987 | S=10 D=7 I=0
2026-01-29 16:35:10,051 | INFO | Chunk: 2 | WER=27.027027 | S=3 D=7 I=0
2026-01-29 16:35:10,051 | INFO | Chunk: 3 | WER=36.363636 | S=3 D=0 I=1
2026-01-29 16:35:10,052 | INFO | Chunk: 4 | WER=20.833333 | S=8 D=2 I=0
2026-01-29 16:35:10,052 | INFO | Chunk: 5 | WER=58.333333 | S=3 D=4 I=0
2026-01-29 16:35:10,053 | INFO | Chunk: 6 | WER=30.769231 | S=1 D=6 I=1
2026-01-29 16:35:10,054 | INFO | Chunk: 7 | WER=40.000000 | S=3 D=2 I=3
2026-01-29 16:35:10,054 | INFO | Chunk: 8 | WER=300.000000 | S=1 D=0 I=2
2026-01-29 16:35:10,077 | INFO | File: Rhap-M0018.wav | WER=25.541126 | S=31 D=17 I=11
2026-01-29 16:35:10,077 | INFO | ------------------------------
2026-01-29 16:35:10,078 | INFO | hmm_tdnn Done!
2026-01-29 16:35:10,308 | INFO | ==================================Rhap-M0019.wav=========================================
2026-01-29 16:35:10,335 | INFO | Using rVAD model
2026-01-29 16:35:13,031 | INFO | Chunk: 0 | WER=25.287356 | S=4 D=18 I=0
2026-01-29 16:35:13,032 | INFO | Chunk: 1 | WER=16.666667 | S=1 D=4 I=1
2026-01-29 16:35:13,034 | INFO | Chunk: 2 | WER=46.774194 | S=9 D=18 I=2
2026-01-29 16:35:13,034 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 16:35:13,046 | INFO | File: Rhap-M0019.wav | WER=21.951220 | S=13 D=18 I=5
2026-01-29 16:35:13,046 | INFO | ------------------------------
2026-01-29 16:35:13,046 | INFO | w2vec vad chunk Done!
2026-01-29 16:35:19,759 | INFO | Chunk: 0 | WER=57.471264 | S=21 D=28 I=1
2026-01-29 16:35:19,761 | INFO | Chunk: 1 | WER=33.333333 | S=5 D=5 I=2
2026-01-29 16:35:19,762 | INFO | Chunk: 2 | WER=61.290323 | S=3 D=32 I=3
2026-01-29 16:35:19,762 | INFO | Chunk: 3 | WER=66.666667 | S=1 D=0 I=1
2026-01-29 16:35:19,773 | INFO | File: Rhap-M0019.wav | WER=46.341463 | S=30 D=40 I=6
2026-01-29 16:35:19,773 | INFO | ------------------------------
2026-01-29 16:35:19,773 | INFO | whisper med Done!
2026-01-29 16:35:27,641 | INFO | Chunk: 0 | WER=49.425287 | S=4 D=39 I=0
2026-01-29 16:35:27,643 | INFO | Chunk: 1 | WER=22.222222 | S=2 D=5 I=1
2026-01-29 16:35:27,644 | INFO | Chunk: 2 | WER=74.193548 | S=3 D=43 I=0
2026-01-29 16:35:27,644 | INFO | Chunk: 3 | WER=66.666667 | S=1 D=1 I=0
2026-01-29 16:35:27,652 | INFO | File: Rhap-M0019.wav | WER=45.121951 | S=9 D=64 I=1
2026-01-29 16:35:27,653 | INFO | ------------------------------
2026-01-29 16:35:27,653 | INFO | whisper large Done!
2026-01-29 16:35:27,853 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-29 16:35:27,906 | INFO | Vocabulary size: 350
2026-01-29 16:35:28,478 | INFO | Gradient checkpoint layers: []
2026-01-29 16:35:29,082 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 16:35:29,086 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 16:35:29,086 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 16:35:29,086 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-29 16:35:29,087 | INFO | speech length: 427840
2026-01-29 16:35:29,124 | INFO | decoder input length: 668
2026-01-29 16:35:29,124 | INFO | max output length: 668
2026-01-29 16:35:29,124 | INFO | min output length: 66
2026-01-29 16:35:43,625 | INFO | end detected at 147
2026-01-29 16:35:43,627 | INFO | -313.80 * 0.5 = -156.90 for decoder
2026-01-29 16:35:43,627 | INFO | -85.82 * 0.5 = -42.91 for ctc
2026-01-29 16:35:43,627 | INFO | total log probability: -199.81
2026-01-29 16:35:43,627 | INFO | normalized log probability: -1.42
2026-01-29 16:35:43,627 | INFO | total number of ended hypotheses: 150
2026-01-29 16:35:43,629 | INFO | best hypo: ▁ensuite▁on▁change▁de▁séquence▁et▁on▁se▁retrouve▁dans▁la▁rue▁avec▁une▁jeune▁fille▁qui▁visiblement▁regarde▁des▁vitrines▁avec▁chaussures▁en▁train▁de▁formolage▁à▁l'air▁triste▁à▁côté▁d'allier▁une▁voiture▁avec▁une▁camionnette▁avec▁un▁monsieur▁en▁train▁'charger▁de▁saler▁d'être▁un▁iteur▁en▁train'écharger▁des▁aliment▁et▁elle▁vole▁une▁baguette▁de▁pain

2026-01-29 16:35:43,632 | INFO | speech length: 164000
2026-01-29 16:35:43,672 | INFO | decoder input length: 255
2026-01-29 16:35:43,672 | INFO | max output length: 255
2026-01-29 16:35:43,672 | INFO | min output length: 25
2026-01-29 16:35:47,706 | INFO | end detected at 75
2026-01-29 16:35:47,707 | INFO | -12.81 * 0.5 =  -6.40 for decoder
2026-01-29 16:35:47,707 | INFO | -11.32 * 0.5 =  -5.66 for ctc
2026-01-29 16:35:47,707 | INFO | total log probability: -12.06
2026-01-29 16:35:47,707 | INFO | normalized log probability: -0.17
2026-01-29 16:35:47,707 | INFO | total number of ended hypotheses: 177
2026-01-29 16:35:47,708 | INFO | best hypo: ▁elle▁est▁prise▁en▁flagrant▁délit▁par▁une▁autre▁dame▁qui▁à▁côté▁et▁elle▁s'apprête▁à▁courir▁avec▁sa▁baguette▁et▁elle▁rentre▁dans▁chary▁chaplin▁ensuite▁le▁traiteur▁revient

2026-01-29 16:35:47,710 | INFO | speech length: 273600
2026-01-29 16:35:47,757 | INFO | decoder input length: 427
2026-01-29 16:35:47,758 | INFO | max output length: 427
2026-01-29 16:35:47,758 | INFO | min output length: 42
2026-01-29 16:35:55,442 | INFO | end detected at 107
2026-01-29 16:35:55,443 | INFO | -222.21 * 0.5 = -111.11 for decoder
2026-01-29 16:35:55,443 | INFO | -133.48 * 0.5 = -66.74 for ctc
2026-01-29 16:35:55,443 | INFO | total log probability: -177.84
2026-01-29 16:35:55,443 | INFO | normalized log probability: -1.76
2026-01-29 16:35:55,443 | INFO | total number of ended hypotheses: 165
2026-01-29 16:35:55,445 | INFO | best hypo: ▁ah▁et▁le▁on▁voit▁expliquer▁un▁mot▁après▁macho▁j'ai▁pas▁compris▁ce▁qui▁est▁écrit▁je▁ne▁le▁disais▁pas▁et▁on▁le▁le▁voit▁madame▁expliquer▁qu'il▁voulé▁à▁la▁baguette▁de▁pin▁et▁à▁une▁explication▁qu'il▁rence▁et▁la▁foup▁qui▁sa▁trompée

2026-01-29 16:35:55,447 | INFO | speech length: 12480
2026-01-29 16:35:55,483 | INFO | decoder input length: 19
2026-01-29 16:35:55,483 | INFO | max output length: 19
2026-01-29 16:35:55,483 | INFO | min output length: 1
2026-01-29 16:35:55,888 | INFO | end detected at 13
2026-01-29 16:35:55,889 | INFO |  -0.97 * 0.5 =  -0.48 for decoder
2026-01-29 16:35:55,889 | INFO |  -2.40 * 0.5 =  -1.20 for ctc
2026-01-29 16:35:55,889 | INFO | total log probability: -1.68
2026-01-29 16:35:55,889 | INFO | normalized log probability: -0.21
2026-01-29 16:35:55,889 | INFO | total number of ended hypotheses: 166
2026-01-29 16:35:55,889 | INFO | best hypo: ▁des▁policiers

2026-01-29 16:35:55,897 | INFO | Chunk: 0 | WER=34.482759 | S=7 D=23 I=0
2026-01-29 16:35:55,898 | INFO | Chunk: 1 | WER=19.444444 | S=1 D=5 I=1
2026-01-29 16:35:55,900 | INFO | Chunk: 2 | WER=54.838710 | S=19 D=13 I=2
2026-01-29 16:35:55,900 | INFO | Chunk: 3 | WER=100.000000 | S=2 D=1 I=0
2026-01-29 16:35:55,912 | INFO | File: Rhap-M0019.wav | WER=32.926829 | S=25 D=22 I=7
2026-01-29 16:35:55,912 | INFO | ------------------------------
2026-01-29 16:35:55,912 | INFO | Conf cv Done!
2026-01-29 16:35:56,172 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-29 16:35:56,191 | INFO | Vocabulary size: 47
2026-01-29 16:35:56,706 | INFO | Gradient checkpoint layers: []
2026-01-29 16:35:57,426 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 16:35:57,429 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 16:35:57,429 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 16:35:57,430 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-29 16:35:57,433 | INFO | speech length: 427840
2026-01-29 16:35:57,468 | INFO | decoder input length: 668
2026-01-29 16:35:57,468 | INFO | max output length: 668
2026-01-29 16:35:57,468 | INFO | min output length: 66
2026-01-29 16:36:34,173 | INFO | end detected at 474
2026-01-29 16:36:34,174 | INFO | -316.50 * 0.5 = -158.25 for decoder
2026-01-29 16:36:34,174 | INFO | -46.86 * 0.5 = -23.43 for ctc
2026-01-29 16:36:34,174 | INFO | total log probability: -181.68
2026-01-29 16:36:34,174 | INFO | normalized log probability: -0.39
2026-01-29 16:36:34,174 | INFO | total number of ended hypotheses: 179
2026-01-29 16:36:34,179 | INFO | best hypo: juste<space>on<space>change<space>de<space>séquence<space>et<space>on<space>se<space>retrouve<space>dans<space>la<space>rue<space>avec<space>une<space>jeune<space>fille<space>pour<space>cent<space>hesitation<space>qui<space>visiblement<space>regarde<space>des<space>vitrines<space>pour<space>cent<space>hesitation<space>avec<space>des<space>chaussures<space>en<space>train<space>de<space>pour<space>cent<space>hesitation<space>enfin<space>voilà<space>la<space>l'air<space>triste<space>pour<space>cent<space>hesitation<space>à<space>côté<space>d'elle<space>il<space>y<space>a<space>une<space>voiture<space>avec<space>une<space>camionnette<space>en<space>avec<space>euh<space>une<space>je<space>suis<space>en<space>train<space>de<space>de<space>décharger<space>euh<space>de<space>salaire<space>d'être<space>un<space>traiteur<space>en<space>train<space>de<space>cherger<space>des<space>alimant<space>e<space>et<space>elle<space>vol<space>e<space>une<space>baguette<space>de<space>pant

2026-01-29 16:36:34,182 | INFO | speech length: 164000
2026-01-29 16:36:34,211 | INFO | decoder input length: 255
2026-01-29 16:36:34,211 | INFO | max output length: 255
2026-01-29 16:36:34,211 | INFO | min output length: 25
2026-01-29 16:36:42,883 | INFO | end detected at 197
2026-01-29 16:36:42,885 | INFO | -19.43 * 0.5 =  -9.71 for decoder
2026-01-29 16:36:42,885 | INFO | -13.75 * 0.5 =  -6.88 for ctc
2026-01-29 16:36:42,885 | INFO | total log probability: -16.59
2026-01-29 16:36:42,885 | INFO | normalized log probability: -0.09
2026-01-29 16:36:42,885 | INFO | total number of ended hypotheses: 193
2026-01-29 16:36:42,887 | INFO | best hypo: elle<space>est<space>euh<space>prise<space>en<space>flagrant<space>délit<space>par<space>une<space>une<space>autre<space>dame<space>euh<space>qui<space>est<space>à<space>côté<space>et<space>elle<space>elle<space>s'apprête<space>à<space>courir<space>avec<space>sa<space>baguette<space>et<space>elle<space>rentre<space>dans<space>chahi<space>chapi<space>ensuite<space>le<space>traiteur<space>revient

2026-01-29 16:36:42,889 | INFO | speech length: 273600
2026-01-29 16:36:42,920 | INFO | decoder input length: 427
2026-01-29 16:36:42,921 | INFO | max output length: 427
2026-01-29 16:36:42,921 | INFO | min output length: 42
2026-01-29 16:36:59,331 | INFO | end detected at 273
2026-01-29 16:36:59,334 | INFO | -39.32 * 0.5 = -19.66 for decoder
2026-01-29 16:36:59,334 | INFO | -23.02 * 0.5 = -11.51 for ctc
2026-01-29 16:36:59,334 | INFO | total log probability: -31.17
2026-01-29 16:36:59,334 | INFO | normalized log probability: -0.12
2026-01-29 16:36:59,334 | INFO | total number of ended hypotheses: 244
2026-01-29 16:36:59,337 | INFO | best hypo: euh<space>et<space>le<space>le<space>on<space>voit<space>expliquer<space>euh<space>euh<space>bon<space>après<space>moi<space>je<space>j'ai<space>pas<space>compris<space>c'est<space>ce<space>qui<space>était<space>écrit<space>euh<space>je<space>le<space>disais<space>pas<space>euh<space>et<space>on<space>le<space>le<space>voit<space>la<space>dame<space>a<space>expliqué<space>qu'à<space>la<space>volait<space>la<space>vaguette<space>de<space>pain<space>et<space>une<space>explication<space>qui<space>commence<space>avec<space>euh<space>la<space>foule<space>qui<space>s'a<space>troupée

2026-01-29 16:36:59,340 | INFO | speech length: 12480
2026-01-29 16:36:59,367 | INFO | decoder input length: 19
2026-01-29 16:36:59,368 | INFO | max output length: 19
2026-01-29 16:36:59,368 | INFO | min output length: 1
2026-01-29 16:36:59,848 | INFO | adding <eos> in the last position in the loop
2026-01-29 16:36:59,855 | INFO | no hypothesis. Finish decoding.
2026-01-29 16:36:59,856 | INFO |  -1.41 * 0.5 =  -0.70 for decoder
2026-01-29 16:36:59,856 | INFO |  -1.24 * 0.5 =  -0.62 for ctc
2026-01-29 16:36:59,856 | INFO | total log probability: -1.32
2026-01-29 16:36:59,856 | INFO | normalized log probability: -0.08
2026-01-29 16:36:59,856 | INFO | total number of ended hypotheses: 130
2026-01-29 16:36:59,856 | INFO | best hypo: et<space>le<space>policier

2026-01-29 16:36:59,864 | INFO | Chunk: 0 | WER=34.482759 | S=15 D=6 I=9
2026-01-29 16:36:59,865 | INFO | Chunk: 1 | WER=13.888889 | S=2 D=1 I=2
2026-01-29 16:36:59,867 | INFO | Chunk: 2 | WER=25.806452 | S=7 D=6 I=3
2026-01-29 16:36:59,868 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 16:36:59,882 | INFO | File: Rhap-M0019.wav | WER=35.975610 | S=22 D=6 I=31
2026-01-29 16:36:59,882 | INFO | ------------------------------
2026-01-29 16:36:59,882 | INFO | Conf ester Done!
2026-01-29 16:37:37,015 | INFO | Chunk: 0 | WER=25.287356 | S=6 D=16 I=0
2026-01-29 16:37:37,016 | INFO | Chunk: 1 | WER=16.666667 | S=1 D=5 I=0
2026-01-29 16:37:37,019 | INFO | Chunk: 2 | WER=37.096774 | S=6 D=14 I=3
2026-01-29 16:37:37,019 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 16:37:37,033 | INFO | File: Rhap-M0019.wav | WER=20.121951 | S=13 D=14 I=6
2026-01-29 16:37:37,033 | INFO | ------------------------------
2026-01-29 16:37:37,033 | INFO | hmm_tdnn Done!
2026-01-29 16:37:37,182 | INFO | ==================================Rhap-M0021.wav=========================================
2026-01-29 16:37:37,199 | INFO | Using rVAD model
2026-01-29 16:37:41,803 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 16:37:41,803 | INFO | Chunk: 1 | WER=12.500000 | S=1 D=0 I=0
2026-01-29 16:37:41,804 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 16:37:41,804 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 16:37:41,804 | INFO | Chunk: 4 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 16:37:41,804 | INFO | Chunk: 5 | WER=27.272727 | S=1 D=1 I=1
2026-01-29 16:37:41,804 | INFO | Chunk: 6 | WER=47.058824 | S=4 D=2 I=2
2026-01-29 16:37:41,805 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 16:37:41,805 | INFO | Chunk: 8 | WER=20.000000 | S=0 D=1 I=0
2026-01-29 16:37:41,805 | INFO | Chunk: 9 | WER=15.789474 | S=2 D=0 I=1
2026-01-29 16:37:41,806 | INFO | Chunk: 10 | WER=35.294118 | S=0 D=2 I=4
2026-01-29 16:37:41,806 | INFO | Chunk: 11 | WER=8.695652 | S=0 D=2 I=0
2026-01-29 16:37:41,807 | INFO | Chunk: 12 | WER=26.190476 | S=8 D=2 I=1
2026-01-29 16:37:41,808 | INFO | Chunk: 13 | WER=41.666667 | S=3 D=1 I=1
2026-01-29 16:37:41,821 | INFO | File: Rhap-M0021.wav | WER=20.370370 | S=21 D=3 I=9
2026-01-29 16:37:41,821 | INFO | ------------------------------
2026-01-29 16:37:41,821 | INFO | w2vec vad chunk Done!
2026-01-29 16:37:50,883 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 16:37:50,883 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 16:37:50,883 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 16:37:50,883 | INFO | Chunk: 3 | WER=500.000000 | S=1 D=0 I=4
2026-01-29 16:37:50,883 | INFO | Chunk: 4 | WER=40.000000 | S=0 D=2 I=0
2026-01-29 16:37:50,883 | INFO | Chunk: 5 | WER=18.181818 | S=0 D=1 I=1
2026-01-29 16:37:50,884 | INFO | Chunk: 6 | WER=41.176471 | S=4 D=3 I=0
2026-01-29 16:37:50,884 | INFO | Chunk: 7 | WER=40.000000 | S=2 D=0 I=0
2026-01-29 16:37:50,884 | INFO | Chunk: 8 | WER=40.000000 | S=0 D=2 I=0
2026-01-29 16:37:50,885 | INFO | Chunk: 9 | WER=31.578947 | S=1 D=4 I=1
2026-01-29 16:37:50,885 | INFO | Chunk: 10 | WER=35.294118 | S=1 D=2 I=3
2026-01-29 16:37:50,886 | INFO | Chunk: 11 | WER=8.695652 | S=0 D=2 I=0
2026-01-29 16:37:50,887 | INFO | Chunk: 12 | WER=33.333333 | S=7 D=5 I=2
2026-01-29 16:37:50,887 | INFO | Chunk: 13 | WER=8.333333 | S=0 D=0 I=1
2026-01-29 16:37:50,899 | INFO | File: Rhap-M0021.wav | WER=25.925926 | S=20 D=12 I=10
2026-01-29 16:37:50,899 | INFO | ------------------------------
2026-01-29 16:37:50,899 | INFO | whisper med Done!
2026-01-29 16:38:02,628 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 16:38:02,628 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 16:38:02,628 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 16:38:02,628 | INFO | Chunk: 3 | WER=200.000000 | S=1 D=0 I=1
2026-01-29 16:38:02,628 | INFO | Chunk: 4 | WER=40.000000 | S=0 D=2 I=0
2026-01-29 16:38:02,629 | INFO | Chunk: 5 | WER=18.181818 | S=0 D=1 I=1
2026-01-29 16:38:02,629 | INFO | Chunk: 6 | WER=47.058824 | S=4 D=3 I=1
2026-01-29 16:38:02,629 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 16:38:02,629 | INFO | Chunk: 8 | WER=40.000000 | S=0 D=2 I=0
2026-01-29 16:38:02,630 | INFO | Chunk: 9 | WER=21.052632 | S=2 D=1 I=1
2026-01-29 16:38:02,630 | INFO | Chunk: 10 | WER=35.294118 | S=1 D=2 I=3
2026-01-29 16:38:02,631 | INFO | Chunk: 11 | WER=8.695652 | S=0 D=2 I=0
2026-01-29 16:38:02,632 | INFO | Chunk: 12 | WER=19.047619 | S=1 D=6 I=1
2026-01-29 16:38:02,632 | INFO | Chunk: 13 | WER=25.000000 | S=2 D=0 I=1
2026-01-29 16:38:02,645 | INFO | File: Rhap-M0021.wav | WER=19.753086 | S=11 D=12 I=9
2026-01-29 16:38:02,645 | INFO | ------------------------------
2026-01-29 16:38:02,645 | INFO | whisper large Done!
2026-01-29 16:38:02,799 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-29 16:38:02,830 | INFO | Vocabulary size: 350
2026-01-29 16:38:03,507 | INFO | Gradient checkpoint layers: []
2026-01-29 16:38:04,178 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 16:38:04,181 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 16:38:04,181 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 16:38:04,182 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-29 16:38:04,182 | INFO | speech length: 18240
2026-01-29 16:38:04,221 | INFO | decoder input length: 28
2026-01-29 16:38:04,221 | INFO | max output length: 28
2026-01-29 16:38:04,221 | INFO | min output length: 2
2026-01-29 16:38:04,656 | INFO | end detected at 13
2026-01-29 16:38:04,657 | INFO |  -0.76 * 0.5 =  -0.38 for decoder
2026-01-29 16:38:04,657 | INFO |  -0.04 * 0.5 =  -0.02 for ctc
2026-01-29 16:38:04,657 | INFO | total log probability: -0.40
2026-01-29 16:38:04,657 | INFO | normalized log probability: -0.04
2026-01-29 16:38:04,657 | INFO | total number of ended hypotheses: 137
2026-01-29 16:38:04,657 | INFO | best hypo: ▁la▁deuxième▁scène

2026-01-29 16:38:04,660 | INFO | speech length: 49280
2026-01-29 16:38:04,692 | INFO | decoder input length: 76
2026-01-29 16:38:04,692 | INFO | max output length: 76
2026-01-29 16:38:04,692 | INFO | min output length: 7
2026-01-29 16:38:05,530 | INFO | end detected at 22
2026-01-29 16:38:05,531 | INFO |  -1.28 * 0.5 =  -0.64 for decoder
2026-01-29 16:38:05,531 | INFO |  -0.04 * 0.5 =  -0.02 for ctc
2026-01-29 16:38:05,531 | INFO | total log probability: -0.66
2026-01-29 16:38:05,531 | INFO | normalized log probability: -0.04
2026-01-29 16:38:05,531 | INFO | total number of ended hypotheses: 143
2026-01-29 16:38:05,531 | INFO | best hypo: ▁c'est▁une▁jeune▁fille▁pauvre▁et▁affamée

2026-01-29 16:38:05,533 | INFO | speech length: 12320
2026-01-29 16:38:05,565 | INFO | decoder input length: 18
2026-01-29 16:38:05,565 | INFO | max output length: 18
2026-01-29 16:38:05,565 | INFO | min output length: 1
2026-01-29 16:38:05,807 | INFO | end detected at 7
2026-01-29 16:38:05,807 | INFO |  -0.14 * 0.5 =  -0.07 for decoder
2026-01-29 16:38:05,808 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-29 16:38:05,808 | INFO | total log probability: -0.08
2026-01-29 16:38:05,808 | INFO | normalized log probability: -0.03
2026-01-29 16:38:05,808 | INFO | total number of ended hypotheses: 125
2026-01-29 16:38:05,808 | INFO | best hypo: ▁qui

2026-01-29 16:38:05,809 | INFO | speech length: 28640
2026-01-29 16:38:05,841 | INFO | decoder input length: 44
2026-01-29 16:38:05,841 | INFO | max output length: 44
2026-01-29 16:38:05,841 | INFO | min output length: 4
2026-01-29 16:38:06,274 | INFO | end detected at 12
2026-01-29 16:38:06,275 | INFO |  -0.56 * 0.5 =  -0.28 for decoder
2026-01-29 16:38:06,275 | INFO |  -1.81 * 0.5 =  -0.91 for ctc
2026-01-29 16:38:06,275 | INFO | total log probability: -1.19
2026-01-29 16:38:06,275 | INFO | normalized log probability: -0.17
2026-01-29 16:38:06,275 | INFO | total number of ended hypotheses: 160
2026-01-29 16:38:06,275 | INFO | best hypo: ▁l'ornière

2026-01-29 16:38:06,277 | INFO | speech length: 38560
2026-01-29 16:38:06,308 | INFO | decoder input length: 59
2026-01-29 16:38:06,308 | INFO | max output length: 59
2026-01-29 16:38:06,308 | INFO | min output length: 5
2026-01-29 16:38:07,041 | INFO | end detected at 20
2026-01-29 16:38:07,042 | INFO |  -2.72 * 0.5 =  -1.36 for decoder
2026-01-29 16:38:07,043 | INFO |  -2.93 * 0.5 =  -1.47 for ctc
2026-01-29 16:38:07,043 | INFO | total log probability: -2.83
2026-01-29 16:38:07,043 | INFO | normalized log probability: -0.20
2026-01-29 16:38:07,043 | INFO | total number of ended hypotheses: 164
2026-01-29 16:38:07,043 | INFO | best hypo: ▁la▁vitrine▁a▁pâtissier

2026-01-29 16:38:07,044 | INFO | speech length: 92800
2026-01-29 16:38:07,077 | INFO | decoder input length: 144
2026-01-29 16:38:07,077 | INFO | max output length: 144
2026-01-29 16:38:07,077 | INFO | min output length: 14
2026-01-29 16:38:08,917 | INFO | end detected at 42
2026-01-29 16:38:08,920 | INFO |  -4.50 * 0.5 =  -2.25 for decoder
2026-01-29 16:38:08,920 | INFO |  -4.95 * 0.5 =  -2.47 for ctc
2026-01-29 16:38:08,920 | INFO | total log probability: -4.73
2026-01-29 16:38:08,920 | INFO | normalized log probability: -0.14
2026-01-29 16:38:08,920 | INFO | total number of ended hypotheses: 195
2026-01-29 16:38:08,920 | INFO | best hypo: ▁elle▁voit▁le▁garçon▁pâtissier▁qui▁transporte▁des▁plateaux▁chargés▁d'eux

2026-01-29 16:38:08,922 | INFO | speech length: 172800
2026-01-29 16:38:08,955 | INFO | decoder input length: 269
2026-01-29 16:38:08,955 | INFO | max output length: 269
2026-01-29 16:38:08,955 | INFO | min output length: 26
2026-01-29 16:38:11,582 | INFO | end detected at 46
2026-01-29 16:38:11,585 | INFO |  -8.61 * 0.5 =  -4.30 for decoder
2026-01-29 16:38:11,585 | INFO | -18.96 * 0.5 =  -9.48 for ctc
2026-01-29 16:38:11,585 | INFO | total log probability: -13.78
2026-01-29 16:38:11,585 | INFO | normalized log probability: -0.37
2026-01-29 16:38:11,585 | INFO | total number of ended hypotheses: 212
2026-01-29 16:38:11,585 | INFO | best hypo: ▁friandise▁gâteau▁etc▁et▁donc▁elle▁vole▁elle▁emprunte▁dans▁le▁camion▁de▁livraison▁en▁une

2026-01-29 16:38:11,587 | INFO | speech length: 32000
2026-01-29 16:38:11,620 | INFO | decoder input length: 49
2026-01-29 16:38:11,620 | INFO | max output length: 49
2026-01-29 16:38:11,620 | INFO | min output length: 4
2026-01-29 16:38:12,219 | INFO | end detected at 18
2026-01-29 16:38:12,220 | INFO |  -1.08 * 0.5 =  -0.54 for decoder
2026-01-29 16:38:12,220 | INFO |  -1.31 * 0.5 =  -0.66 for ctc
2026-01-29 16:38:12,220 | INFO | total log probability: -1.20
2026-01-29 16:38:12,220 | INFO | normalized log probability: -0.09
2026-01-29 16:38:12,220 | INFO | total number of ended hypotheses: 145
2026-01-29 16:38:12,221 | INFO | best hypo: ▁un▁sandwich▁ou▁une▁baguette

2026-01-29 16:38:12,222 | INFO | speech length: 50240
2026-01-29 16:38:12,254 | INFO | decoder input length: 78
2026-01-29 16:38:12,254 | INFO | max output length: 78
2026-01-29 16:38:12,254 | INFO | min output length: 7
2026-01-29 16:38:12,827 | INFO | end detected at 15
2026-01-29 16:38:12,828 | INFO |  -0.73 * 0.5 =  -0.37 for decoder
2026-01-29 16:38:12,828 | INFO |  -0.11 * 0.5 =  -0.05 for ctc
2026-01-29 16:38:12,828 | INFO | total log probability: -0.42
2026-01-29 16:38:12,828 | INFO | normalized log probability: -0.04
2026-01-29 16:38:12,828 | INFO | total number of ended hypotheses: 161
2026-01-29 16:38:12,828 | INFO | best hypo: ▁elle▁elle▁s'enfuit

2026-01-29 16:38:12,830 | INFO | speech length: 116480
2026-01-29 16:38:12,862 | INFO | decoder input length: 181
2026-01-29 16:38:12,862 | INFO | max output length: 181
2026-01-29 16:38:12,862 | INFO | min output length: 18
2026-01-29 16:38:15,453 | INFO | end detected at 56
2026-01-29 16:38:15,454 | INFO |  -5.45 * 0.5 =  -2.73 for decoder
2026-01-29 16:38:15,454 | INFO |  -2.09 * 0.5 =  -1.04 for ctc
2026-01-29 16:38:15,454 | INFO | total log probability: -3.77
2026-01-29 16:38:15,454 | INFO | normalized log probability: -0.07
2026-01-29 16:38:15,454 | INFO | total number of ended hypotheses: 163
2026-01-29 16:38:15,455 | INFO | best hypo: ▁mais▁malheureusement▁heureusement▁il▁se▁heurte▁à▁personnage▁de▁charlot▁qui▁apparaît▁au▁coin▁de▁la▁rue▁il▁tombe

2026-01-29 16:38:15,457 | INFO | speech length: 130720
2026-01-29 16:38:15,489 | INFO | decoder input length: 203
2026-01-29 16:38:15,489 | INFO | max output length: 203
2026-01-29 16:38:15,489 | INFO | min output length: 20
2026-01-29 16:38:18,082 | INFO | end detected at 53
2026-01-29 16:38:18,083 | INFO |  -5.08 * 0.5 =  -2.54 for decoder
2026-01-29 16:38:18,083 | INFO |  -2.81 * 0.5 =  -1.41 for ctc
2026-01-29 16:38:18,083 | INFO | total log probability: -3.95
2026-01-29 16:38:18,083 | INFO | normalized log probability: -0.08
2026-01-29 16:38:18,083 | INFO | total number of ended hypotheses: 171
2026-01-29 16:38:18,084 | INFO | best hypo: ▁une▁dame▁vertueuse▁et▁bien▁pensante▁de▁bonne▁bourgeoisie▁dénonce▁le▁larsin▁au▁solide▁gaillard▁qui▁est▁le

2026-01-29 16:38:18,086 | INFO | speech length: 128960
2026-01-29 16:38:18,119 | INFO | decoder input length: 201
2026-01-29 16:38:18,119 | INFO | max output length: 201
2026-01-29 16:38:18,119 | INFO | min output length: 20
2026-01-29 16:38:20,607 | INFO | end detected at 51
2026-01-29 16:38:20,608 | INFO |  -3.54 * 0.5 =  -1.77 for decoder
2026-01-29 16:38:20,608 | INFO |  -0.27 * 0.5 =  -0.13 for ctc
2026-01-29 16:38:20,608 | INFO | total log probability: -1.90
2026-01-29 16:38:20,608 | INFO | normalized log probability: -0.04
2026-01-29 16:38:20,608 | INFO | total number of ended hypotheses: 140
2026-01-29 16:38:20,609 | INFO | best hypo: ▁qui▁est▁donc▁l'employé▁de▁la▁pâtisserie▁qui▁se▁précipite▁pour▁rétablir▁l'ordre▁au▁même▁moment▁arrive▁un▁policier

2026-01-29 16:38:20,611 | INFO | speech length: 230560
2026-01-29 16:38:20,643 | INFO | decoder input length: 359
2026-01-29 16:38:20,643 | INFO | max output length: 359
2026-01-29 16:38:20,643 | INFO | min output length: 35
2026-01-29 16:38:26,994 | INFO | end detected at 98
2026-01-29 16:38:26,995 | INFO | -165.41 * 0.5 = -82.71 for decoder
2026-01-29 16:38:26,995 | INFO | -55.82 * 0.5 = -27.91 for ctc
2026-01-29 16:38:26,995 | INFO | total log probability: -110.62
2026-01-29 16:38:26,995 | INFO | normalized log probability: -1.19
2026-01-29 16:38:26,995 | INFO | total number of ended hypotheses: 160
2026-01-29 16:38:26,996 | INFO | best hypo: ▁et▁après▁quelques▁explications▁on▁comprend▁que▁charlot▁prend▁la▁faute▁ou▁le▁l'arcin▁sur▁lui▁et▁s'en▁met▁et▁est▁emmené▁par▁le▁policier▁et▁de▁la▁jeune▁fille▁elle▁est▁toute▁ébahis▁et▁se▁retrouvent▁seul▁dans▁le▁trottoir▁mais▁libre

2026-01-29 16:38:26,998 | INFO | speech length: 42400
2026-01-29 16:38:27,030 | INFO | decoder input length: 65
2026-01-29 16:38:27,030 | INFO | max output length: 65
2026-01-29 16:38:27,030 | INFO | min output length: 6
2026-01-29 16:38:28,090 | INFO | end detected at 31
2026-01-29 16:38:28,091 | INFO |  -3.09 * 0.5 =  -1.54 for decoder
2026-01-29 16:38:28,091 | INFO |  -7.77 * 0.5 =  -3.89 for ctc
2026-01-29 16:38:28,091 | INFO | total log probability: -5.43
2026-01-29 16:38:28,091 | INFO | normalized log probability: -0.22
2026-01-29 16:38:28,091 | INFO | total number of ended hypotheses: 159
2026-01-29 16:38:28,092 | INFO | best hypo: ▁je▁crois▁que▁c'est▁à▁peu▁près▁tout▁cette▁scène▁là

2026-01-29 16:38:28,095 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 16:38:28,096 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 16:38:28,096 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 16:38:28,096 | INFO | Chunk: 3 | WER=200.000000 | S=1 D=0 I=1
2026-01-29 16:38:28,096 | INFO | Chunk: 4 | WER=40.000000 | S=1 D=1 I=0
2026-01-29 16:38:28,096 | INFO | Chunk: 5 | WER=27.272727 | S=0 D=1 I=2
2026-01-29 16:38:28,097 | INFO | Chunk: 6 | WER=41.176471 | S=2 D=3 I=2
2026-01-29 16:38:28,097 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 16:38:28,097 | INFO | Chunk: 8 | WER=20.000000 | S=0 D=1 I=0
2026-01-29 16:38:28,098 | INFO | Chunk: 9 | WER=26.315789 | S=3 D=1 I=1
2026-01-29 16:38:28,098 | INFO | Chunk: 10 | WER=35.294118 | S=1 D=2 I=3
2026-01-29 16:38:28,099 | INFO | Chunk: 11 | WER=8.695652 | S=0 D=2 I=0
2026-01-29 16:38:28,100 | INFO | Chunk: 12 | WER=35.714286 | S=8 D=2 I=5
2026-01-29 16:38:28,100 | INFO | Chunk: 13 | WER=16.666667 | S=0 D=1 I=1
2026-01-29 16:38:28,113 | INFO | File: Rhap-M0021.wav | WER=23.456790 | S=18 D=6 I=14
2026-01-29 16:38:28,113 | INFO | ------------------------------
2026-01-29 16:38:28,113 | INFO | Conf cv Done!
2026-01-29 16:38:28,253 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-29 16:38:28,271 | INFO | Vocabulary size: 47
2026-01-29 16:38:28,740 | INFO | Gradient checkpoint layers: []
2026-01-29 16:38:29,363 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 16:38:29,366 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 16:38:29,366 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 16:38:29,367 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-29 16:38:29,369 | INFO | speech length: 18240
2026-01-29 16:38:29,402 | INFO | decoder input length: 28
2026-01-29 16:38:29,402 | INFO | max output length: 28
2026-01-29 16:38:29,402 | INFO | min output length: 2
2026-01-29 16:38:30,111 | INFO | end detected at 24
2026-01-29 16:38:30,112 | INFO |  -1.42 * 0.5 =  -0.71 for decoder
2026-01-29 16:38:30,112 | INFO |  -0.24 * 0.5 =  -0.12 for ctc
2026-01-29 16:38:30,112 | INFO | total log probability: -0.83
2026-01-29 16:38:30,112 | INFO | normalized log probability: -0.04
2026-01-29 16:38:30,112 | INFO | total number of ended hypotheses: 167
2026-01-29 16:38:30,112 | INFO | best hypo: la<space>deuxième<space>scène

2026-01-29 16:38:30,114 | INFO | speech length: 49280
2026-01-29 16:38:30,141 | INFO | decoder input length: 76
2026-01-29 16:38:30,141 | INFO | max output length: 76
2026-01-29 16:38:30,141 | INFO | min output length: 7
2026-01-29 16:38:31,664 | INFO | end detected at 46
2026-01-29 16:38:31,666 | INFO |  -3.32 * 0.5 =  -1.66 for decoder
2026-01-29 16:38:31,666 | INFO |  -2.16 * 0.5 =  -1.08 for ctc
2026-01-29 16:38:31,666 | INFO | total log probability: -2.74
2026-01-29 16:38:31,666 | INFO | normalized log probability: -0.07
2026-01-29 16:38:31,666 | INFO | total number of ended hypotheses: 169
2026-01-29 16:38:31,667 | INFO | best hypo: c'est<space>une<space>jeune<space>fille<space>pauvre<space>et<space>affamée

2026-01-29 16:38:31,668 | INFO | speech length: 12320
2026-01-29 16:38:31,694 | INFO | decoder input length: 18
2026-01-29 16:38:31,695 | INFO | max output length: 18
2026-01-29 16:38:31,695 | INFO | min output length: 1
2026-01-29 16:38:31,975 | INFO | end detected at 9
2026-01-29 16:38:31,976 | INFO |  -0.32 * 0.5 =  -0.16 for decoder
2026-01-29 16:38:31,976 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 16:38:31,976 | INFO | total log probability: -0.16
2026-01-29 16:38:31,976 | INFO | normalized log probability: -0.03
2026-01-29 16:38:31,976 | INFO | total number of ended hypotheses: 126
2026-01-29 16:38:31,976 | INFO | best hypo: qui

2026-01-29 16:38:31,977 | INFO | speech length: 28640
2026-01-29 16:38:32,004 | INFO | decoder input length: 44
2026-01-29 16:38:32,004 | INFO | max output length: 44
2026-01-29 16:38:32,004 | INFO | min output length: 4
2026-01-29 16:38:33,016 | INFO | end detected at 33
2026-01-29 16:38:33,018 | INFO |  -3.06 * 0.5 =  -1.53 for decoder
2026-01-29 16:38:33,018 | INFO | -11.11 * 0.5 =  -5.55 for ctc
2026-01-29 16:38:33,018 | INFO | total log probability: -7.08
2026-01-29 16:38:33,018 | INFO | normalized log probability: -0.54
2026-01-29 16:38:33,018 | INFO | total number of ended hypotheses: 173
2026-01-29 16:38:33,018 | INFO | best hypo: l'or<space>ni<space>euh

2026-01-29 16:38:33,021 | INFO | speech length: 38560
2026-01-29 16:38:33,048 | INFO | decoder input length: 59
2026-01-29 16:38:33,048 | INFO | max output length: 59
2026-01-29 16:38:33,048 | INFO | min output length: 5
2026-01-29 16:38:34,117 | INFO | end detected at 33
2026-01-29 16:38:34,118 | INFO |  -2.17 * 0.5 =  -1.08 for decoder
2026-01-29 16:38:34,119 | INFO |  -0.57 * 0.5 =  -0.29 for ctc
2026-01-29 16:38:34,119 | INFO | total log probability: -1.37
2026-01-29 16:38:34,119 | INFO | normalized log probability: -0.05
2026-01-29 16:38:34,119 | INFO | total number of ended hypotheses: 177
2026-01-29 16:38:34,119 | INFO | best hypo: la<space>vitrine<space>d'un<space>pâtissier

2026-01-29 16:38:34,121 | INFO | speech length: 92800
2026-01-29 16:38:34,148 | INFO | decoder input length: 144
2026-01-29 16:38:34,148 | INFO | max output length: 144
2026-01-29 16:38:34,148 | INFO | min output length: 14
2026-01-29 16:38:37,167 | INFO | end detected at 79
2026-01-29 16:38:37,168 | INFO |  -5.87 * 0.5 =  -2.94 for decoder
2026-01-29 16:38:37,168 | INFO |  -2.37 * 0.5 =  -1.18 for ctc
2026-01-29 16:38:37,168 | INFO | total log probability: -4.12
2026-01-29 16:38:37,168 | INFO | normalized log probability: -0.06
2026-01-29 16:38:37,168 | INFO | total number of ended hypotheses: 177
2026-01-29 16:38:37,169 | INFO | best hypo: elle<space>voit<space>le<space>garçon<space>pâtissier<space>euh<space>qui<space>transporte<space>des<space>plateaux<space>chargés<space>de

2026-01-29 16:38:37,171 | INFO | speech length: 172800
2026-01-29 16:38:37,200 | INFO | decoder input length: 269
2026-01-29 16:38:37,200 | INFO | max output length: 269
2026-01-29 16:38:37,200 | INFO | min output length: 26
2026-01-29 16:38:44,140 | INFO | end detected at 143
2026-01-29 16:38:44,141 | INFO | -27.03 * 0.5 = -13.51 for decoder
2026-01-29 16:38:44,141 | INFO | -14.51 * 0.5 =  -7.26 for ctc
2026-01-29 16:38:44,141 | INFO | total log probability: -20.77
2026-01-29 16:38:44,141 | INFO | normalized log probability: -0.15
2026-01-29 16:38:44,141 | INFO | total number of ended hypotheses: 150
2026-01-29 16:38:44,143 | INFO | best hypo: friandises<space>gatto<space>etc<space>et<space>donc<space>elles<space>pour<space>cent<space>hesitation<space>elles<space>volent<space>elle<space>empreinte<space>dans<space>le<space>camion<space>de<space>livraisons<space>une<space>pou<space>cent<space>hesitation

2026-01-29 16:38:44,145 | INFO | speech length: 32000
2026-01-29 16:38:44,173 | INFO | decoder input length: 49
2026-01-29 16:38:44,174 | INFO | max output length: 49
2026-01-29 16:38:44,174 | INFO | min output length: 4
2026-01-29 16:38:45,387 | INFO | end detected at 41
2026-01-29 16:38:45,389 | INFO |  -6.67 * 0.5 =  -3.34 for decoder
2026-01-29 16:38:45,389 | INFO | -10.34 * 0.5 =  -5.17 for ctc
2026-01-29 16:38:45,389 | INFO | total log probability: -8.51
2026-01-29 16:38:45,389 | INFO | normalized log probability: -0.27
2026-01-29 16:38:45,389 | INFO | total number of ended hypotheses: 212
2026-01-29 16:38:45,390 | INFO | best hypo: un<space>sandomi<space>chouer<space>une<space>baguette

2026-01-29 16:38:45,392 | INFO | speech length: 50240
2026-01-29 16:38:45,419 | INFO | decoder input length: 78
2026-01-29 16:38:45,419 | INFO | max output length: 78
2026-01-29 16:38:45,419 | INFO | min output length: 7
2026-01-29 16:38:46,993 | INFO | end detected at 47
2026-01-29 16:38:46,995 | INFO |  -4.39 * 0.5 =  -2.20 for decoder
2026-01-29 16:38:46,995 | INFO | -10.60 * 0.5 =  -5.30 for ctc
2026-01-29 16:38:46,995 | INFO | total log probability: -7.50
2026-01-29 16:38:46,995 | INFO | normalized log probability: -0.18
2026-01-29 16:38:46,995 | INFO | total number of ended hypotheses: 233
2026-01-29 16:38:46,996 | INFO | best hypo: elle<space>pour<space>cent<space>hesitation<space>elle<space>s'enfuie

2026-01-29 16:38:46,998 | INFO | speech length: 116480
2026-01-29 16:38:47,024 | INFO | decoder input length: 181
2026-01-29 16:38:47,024 | INFO | max output length: 181
2026-01-29 16:38:47,024 | INFO | min output length: 18
2026-01-29 16:38:52,013 | INFO | end detected at 126
2026-01-29 16:38:52,015 | INFO | -11.55 * 0.5 =  -5.78 for decoder
2026-01-29 16:38:52,015 | INFO |  -3.80 * 0.5 =  -1.90 for ctc
2026-01-29 16:38:52,015 | INFO | total log probability: -7.68
2026-01-29 16:38:52,015 | INFO | normalized log probability: -0.07
2026-01-29 16:38:52,015 | INFO | total number of ended hypotheses: 244
2026-01-29 16:38:52,017 | INFO | best hypo: et<space>malheureusement<space>ou<space>heureusement<space>elle<space>se<space>heurte<space>à<space>un<space>personnage<space>de<space>charlot<space>qui<space>apparaît<space>au<space>coin<space>de<space>la<space>rue<space>il<space>tombe

2026-01-29 16:38:52,019 | INFO | speech length: 130720
2026-01-29 16:38:52,047 | INFO | decoder input length: 203
2026-01-29 16:38:52,047 | INFO | max output length: 203
2026-01-29 16:38:52,047 | INFO | min output length: 20
2026-01-29 16:38:57,056 | INFO | end detected at 118
2026-01-29 16:38:57,058 | INFO | -11.84 * 0.5 =  -5.92 for decoder
2026-01-29 16:38:57,058 | INFO |  -4.12 * 0.5 =  -2.06 for ctc
2026-01-29 16:38:57,058 | INFO | total log probability: -7.98
2026-01-29 16:38:57,058 | INFO | normalized log probability: -0.07
2026-01-29 16:38:57,058 | INFO | total number of ended hypotheses: 197
2026-01-29 16:38:57,060 | INFO | best hypo: une<space>dame<space>vertueuse<space>et<space>bien<space>pensante<space>de<space>bonne<space>bourgeoisie<space>dénonce<space>le<space>larsin<space>euh<space>au<space>solide<space>gaillard<space>qui<space>est<space>le

2026-01-29 16:38:57,062 | INFO | speech length: 128960
2026-01-29 16:38:57,089 | INFO | decoder input length: 201
2026-01-29 16:38:57,089 | INFO | max output length: 201
2026-01-29 16:38:57,089 | INFO | min output length: 20
2026-01-29 16:39:02,472 | INFO | end detected at 124
2026-01-29 16:39:02,473 | INFO |  -9.94 * 0.5 =  -4.97 for decoder
2026-01-29 16:39:02,473 | INFO |  -0.94 * 0.5 =  -0.47 for ctc
2026-01-29 16:39:02,473 | INFO | total log probability: -5.44
2026-01-29 16:39:02,473 | INFO | normalized log probability: -0.05
2026-01-29 16:39:02,473 | INFO | total number of ended hypotheses: 179
2026-01-29 16:39:02,475 | INFO | best hypo: qui<space>est<space>donc<space>le<space>l'employé<space>de<space>la<space>pâtisserie<space>qui<space>se<space>précipite<space>pour<space>rétablir<space>l'ordre<space>au<space>même<space>moment<space>arrive<space>à<space>un<space>policier

2026-01-29 16:39:02,477 | INFO | speech length: 230560
2026-01-29 16:39:02,505 | INFO | decoder input length: 359
2026-01-29 16:39:02,505 | INFO | max output length: 359
2026-01-29 16:39:02,505 | INFO | min output length: 35
2026-01-29 16:39:16,356 | INFO | end detected at 267
2026-01-29 16:39:16,357 | INFO | -40.59 * 0.5 = -20.30 for decoder
2026-01-29 16:39:16,358 | INFO | -25.50 * 0.5 = -12.75 for ctc
2026-01-29 16:39:16,358 | INFO | total log probability: -33.05
2026-01-29 16:39:16,358 | INFO | normalized log probability: -0.13
2026-01-29 16:39:16,358 | INFO | total number of ended hypotheses: 228
2026-01-29 16:39:16,361 | INFO | best hypo: et<space>pour<space>cent<space>hesitation<space>après<space>quelques<space>explications<space>on<space>comprend<space>que<space>charlot<space>prend<space>la<space>la<space>faute<space>ou<space>le<space>l'arsain<space>sur<space>lui<space>et<space>sans<space>met<space>et<space>est<space>emmené<space>par<space>le<space>policier<space>euh<space>et<space>que<space>la<space>jeune<space>fille<space>euh<space>tout<space>est<space>tout<space>est<space>bailli<space>se<space>retrouve<space>seul<space>sur<space>le<space>trottoir<space>mais<space>libre

2026-01-29 16:39:16,363 | INFO | speech length: 42400
2026-01-29 16:39:16,390 | INFO | decoder input length: 65
2026-01-29 16:39:16,390 | INFO | max output length: 65
2026-01-29 16:39:16,390 | INFO | min output length: 6
2026-01-29 16:39:18,197 | INFO | end detected at 61
2026-01-29 16:39:18,199 | INFO |  -6.31 * 0.5 =  -3.16 for decoder
2026-01-29 16:39:18,199 | INFO |  -8.59 * 0.5 =  -4.29 for ctc
2026-01-29 16:39:18,199 | INFO | total log probability: -7.45
2026-01-29 16:39:18,199 | INFO | normalized log probability: -0.16
2026-01-29 16:39:18,199 | INFO | total number of ended hypotheses: 211
2026-01-29 16:39:18,200 | INFO | best hypo: je<space>crois<space>que<space>c'est<space>à<space>peu<space>près<space>et<space>tout<space>c'est<space>là

2026-01-29 16:39:18,204 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 16:39:18,204 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 16:39:18,204 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 16:39:18,205 | INFO | Chunk: 3 | WER=400.000000 | S=1 D=0 I=3
2026-01-29 16:39:18,205 | INFO | Chunk: 4 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 16:39:18,205 | INFO | Chunk: 5 | WER=9.090909 | S=0 D=0 I=1
2026-01-29 16:39:18,205 | INFO | Chunk: 6 | WER=94.117647 | S=9 D=1 I=6
2026-01-29 16:39:18,206 | INFO | Chunk: 7 | WER=40.000000 | S=2 D=0 I=0
2026-01-29 16:39:18,206 | INFO | Chunk: 8 | WER=80.000000 | S=2 D=0 I=2
2026-01-29 16:39:18,206 | INFO | Chunk: 9 | WER=15.789474 | S=1 D=0 I=2
2026-01-29 16:39:18,207 | INFO | Chunk: 10 | WER=29.411765 | S=1 D=1 I=3
2026-01-29 16:39:18,207 | INFO | Chunk: 11 | WER=4.347826 | S=1 D=0 I=0
2026-01-29 16:39:18,209 | INFO | Chunk: 12 | WER=40.476190 | S=9 D=0 I=8
2026-01-29 16:39:18,209 | INFO | Chunk: 13 | WER=33.333333 | S=3 D=0 I=1
2026-01-29 16:39:18,224 | INFO | File: Rhap-M0021.wav | WER=35.185185 | S=26 D=0 I=31
2026-01-29 16:39:18,224 | INFO | ------------------------------
2026-01-29 16:39:18,224 | INFO | Conf ester Done!
2026-01-29 16:40:27,956 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 16:40:27,956 | INFO | Chunk: 1 | WER=12.500000 | S=1 D=0 I=0
2026-01-29 16:40:27,957 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 16:40:27,957 | INFO | Chunk: 3 | WER=200.000000 | S=1 D=0 I=1
2026-01-29 16:40:27,957 | INFO | Chunk: 4 | WER=40.000000 | S=0 D=2 I=0
2026-01-29 16:40:27,958 | INFO | Chunk: 5 | WER=27.272727 | S=1 D=1 I=1
2026-01-29 16:40:27,958 | INFO | Chunk: 6 | WER=58.823529 | S=8 D=1 I=1
2026-01-29 16:40:27,959 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 16:40:27,959 | INFO | Chunk: 8 | WER=60.000000 | S=3 D=0 I=0
2026-01-29 16:40:27,960 | INFO | Chunk: 9 | WER=31.578947 | S=4 D=0 I=2
2026-01-29 16:40:27,960 | INFO | Chunk: 10 | WER=58.823529 | S=7 D=1 I=2
2026-01-29 16:40:27,961 | INFO | Chunk: 11 | WER=8.695652 | S=1 D=1 I=0
2026-01-29 16:40:27,964 | INFO | Chunk: 12 | WER=42.857143 | S=13 D=3 I=2
2026-01-29 16:40:27,964 | INFO | Chunk: 13 | WER=25.000000 | S=0 D=2 I=1
2026-01-29 16:40:27,979 | INFO | File: Rhap-M0021.wav | WER=34.567901 | S=36 D=7 I=13
2026-01-29 16:40:27,979 | INFO | ------------------------------
2026-01-29 16:40:27,979 | INFO | hmm_tdnn Done!
2026-01-29 16:40:28,191 | INFO | ==================================Rhap-M0022.wav=========================================
2026-01-29 16:40:28,220 | INFO | Using rVAD model
2026-01-29 16:40:32,983 | INFO | Chunk: 0 | WER=100.000000 | S=2 D=1 I=0
2026-01-29 16:40:32,986 | INFO | Chunk: 1 | WER=27.586207 | S=5 D=9 I=2
2026-01-29 16:40:32,986 | INFO | Chunk: 2 | WER=75.000000 | S=1 D=2 I=0
2026-01-29 16:40:32,986 | INFO | Chunk: 3 | WER=20.000000 | S=1 D=1 I=1
2026-01-29 16:40:32,987 | INFO | Chunk: 4 | WER=29.411765 | S=1 D=4 I=0
2026-01-29 16:40:32,988 | INFO | Chunk: 5 | WER=30.909091 | S=3 D=14 I=0
2026-01-29 16:40:32,990 | INFO | Chunk: 6 | WER=8.955224 | S=1 D=5 I=0
2026-01-29 16:40:32,991 | INFO | Chunk: 7 | WER=60.000000 | S=2 D=1 I=0
2026-01-29 16:40:33,009 | INFO | File: Rhap-M0022.wav | WER=16.831683 | S=14 D=16 I=4
2026-01-29 16:40:33,009 | INFO | ------------------------------
2026-01-29 16:40:33,009 | INFO | w2vec vad chunk Done!
2026-01-29 16:40:41,188 | INFO | Chunk: 0 | WER=66.666667 | S=1 D=1 I=0
2026-01-29 16:40:41,189 | INFO | Chunk: 1 | WER=41.379310 | S=2 D=22 I=0
2026-01-29 16:40:41,189 | INFO | Chunk: 2 | WER=75.000000 | S=0 D=3 I=0
2026-01-29 16:40:41,190 | INFO | Chunk: 3 | WER=6.666667 | S=0 D=1 I=0
2026-01-29 16:40:41,190 | INFO | Chunk: 4 | WER=47.058824 | S=4 D=4 I=0
2026-01-29 16:40:41,191 | INFO | Chunk: 5 | WER=43.636364 | S=6 D=18 I=0
2026-01-29 16:40:41,193 | INFO | Chunk: 6 | WER=32.835821 | S=2 D=20 I=0
2026-01-29 16:40:41,194 | INFO | Chunk: 7 | WER=60.000000 | S=2 D=1 I=0
2026-01-29 16:40:41,208 | INFO | File: Rhap-M0022.wav | WER=32.673267 | S=16 D=49 I=1
2026-01-29 16:40:41,209 | INFO | ------------------------------
2026-01-29 16:40:41,209 | INFO | whisper med Done!
2026-01-29 16:40:52,293 | INFO | Chunk: 0 | WER=100.000000 | S=1 D=2 I=0
2026-01-29 16:40:52,295 | INFO | Chunk: 1 | WER=39.655172 | S=1 D=22 I=0
2026-01-29 16:40:52,295 | INFO | Chunk: 2 | WER=100.000000 | S=2 D=2 I=0
2026-01-29 16:40:52,295 | INFO | Chunk: 3 | WER=20.000000 | S=1 D=2 I=0
2026-01-29 16:40:52,296 | INFO | Chunk: 4 | WER=29.411765 | S=1 D=4 I=0
2026-01-29 16:40:52,297 | INFO | Chunk: 5 | WER=32.727273 | S=2 D=15 I=1
2026-01-29 16:40:52,299 | INFO | Chunk: 6 | WER=34.328358 | S=7 D=15 I=1
2026-01-29 16:40:52,299 | INFO | Chunk: 7 | WER=60.000000 | S=2 D=0 I=1
2026-01-29 16:40:52,315 | INFO | File: Rhap-M0022.wav | WER=29.702970 | S=17 D=40 I=3
2026-01-29 16:40:52,315 | INFO | ------------------------------
2026-01-29 16:40:52,315 | INFO | whisper large Done!
2026-01-29 16:40:52,542 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-29 16:40:52,572 | INFO | Vocabulary size: 350
2026-01-29 16:40:53,264 | INFO | Gradient checkpoint layers: []
2026-01-29 16:40:53,923 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 16:40:53,927 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 16:40:53,927 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 16:40:53,927 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-29 16:40:53,927 | INFO | speech length: 34240
2026-01-29 16:40:53,973 | INFO | decoder input length: 53
2026-01-29 16:40:53,973 | INFO | max output length: 53
2026-01-29 16:40:53,973 | INFO | min output length: 5
2026-01-29 16:40:54,367 | INFO | end detected at 11
2026-01-29 16:40:54,368 | INFO |  -3.71 * 0.5 =  -1.85 for decoder
2026-01-29 16:40:54,368 | INFO |  -6.53 * 0.5 =  -3.27 for ctc
2026-01-29 16:40:54,368 | INFO | total log probability: -5.12
2026-01-29 16:40:54,368 | INFO | normalized log probability: -0.73
2026-01-29 16:40:54,368 | INFO | total number of ended hypotheses: 130
2026-01-29 16:40:54,368 | INFO | best hypo: ▁ah▁doncques

2026-01-29 16:40:54,371 | INFO | speech length: 462720
2026-01-29 16:40:54,403 | INFO | decoder input length: 722
2026-01-29 16:40:54,403 | INFO | max output length: 722
2026-01-29 16:40:54,403 | INFO | min output length: 72
2026-01-29 16:41:07,302 | INFO | end detected at 122
2026-01-29 16:41:07,305 | INFO | -185.37 * 0.5 = -92.69 for decoder
2026-01-29 16:41:07,305 | INFO | -50.11 * 0.5 = -25.06 for ctc
2026-01-29 16:41:07,305 | INFO | total log probability: -117.74
2026-01-29 16:41:07,305 | INFO | normalized log probability: -1.02
2026-01-29 16:41:07,305 | INFO | total number of ended hypotheses: 188
2026-01-29 16:41:07,306 | INFO | best hypo: ▁on▁voit▁chaplin▁s'habiller▁pour▁sortir▁son▁usine▁ensuite▁on▁voit▁une▁jeune▁fille▁habillée▁en▁noir▁qui▁regarde▁une▁devanture▁de▁magasins▁qui▁profitent▁d'un▁aller▁retour▁de▁boulanger▁pour▁piquer▁une▁baguette▁sur▁ce▁une▁vieille▁femme▁le▁voit▁le▁vol▁vient▁boulanger▁la▁jeune

2026-01-29 16:41:07,309 | INFO | speech length: 29600
2026-01-29 16:41:07,355 | INFO | decoder input length: 45
2026-01-29 16:41:07,355 | INFO | max output length: 45
2026-01-29 16:41:07,355 | INFO | min output length: 4
2026-01-29 16:41:07,741 | INFO | end detected at 11
2026-01-29 16:41:07,742 | INFO |  -5.00 * 0.5 =  -2.50 for decoder
2026-01-29 16:41:07,742 | INFO | -10.75 * 0.5 =  -5.38 for ctc
2026-01-29 16:41:07,742 | INFO | total log probability: -7.88
2026-01-29 16:41:07,742 | INFO | normalized log probability: -1.31
2026-01-29 16:41:07,742 | INFO | total number of ended hypotheses: 160
2026-01-29 16:41:07,742 | INFO | best hypo: ▁fi▁hey

2026-01-29 16:41:07,744 | INFO | speech length: 138400
2026-01-29 16:41:07,777 | INFO | decoder input length: 215
2026-01-29 16:41:07,777 | INFO | max output length: 215
2026-01-29 16:41:07,777 | INFO | min output length: 21
2026-01-29 16:41:09,900 | INFO | end detected at 41
2026-01-29 16:41:09,901 | INFO |  -3.28 * 0.5 =  -1.64 for decoder
2026-01-29 16:41:09,902 | INFO |  -3.19 * 0.5 =  -1.60 for ctc
2026-01-29 16:41:09,902 | INFO | total log probability: -3.24
2026-01-29 16:41:09,902 | INFO | normalized log probability: -0.09
2026-01-29 16:41:09,902 | INFO | total number of ended hypotheses: 164
2026-01-29 16:41:09,902 | INFO | best hypo: ▁il▁se▁retrouve▁par▁terre▁avec▁chaplin▁au▁sol▁le▁boulanger▁arrive▁à▁la▁rattraper

2026-01-29 16:41:09,904 | INFO | speech length: 144480
2026-01-29 16:41:09,939 | INFO | decoder input length: 225
2026-01-29 16:41:09,939 | INFO | max output length: 225
2026-01-29 16:41:09,939 | INFO | min output length: 22
2026-01-29 16:41:12,175 | INFO | end detected at 41
2026-01-29 16:41:12,177 | INFO |  -9.58 * 0.5 =  -4.79 for decoder
2026-01-29 16:41:12,177 | INFO |  -6.49 * 0.5 =  -3.25 for ctc
2026-01-29 16:41:12,177 | INFO | total log probability: -8.03
2026-01-29 16:41:12,177 | INFO | normalized log probability: -0.24
2026-01-29 16:41:12,177 | INFO | total number of ended hypotheses: 199
2026-01-29 16:41:12,178 | INFO | best hypo: ▁à▁travers▁la▁foule▁le▁policier▁arrive▁demande▁des▁explications▁de▁ce▁broat

2026-01-29 16:41:12,182 | INFO | speech length: 308320
2026-01-29 16:41:12,229 | INFO | decoder input length: 481
2026-01-29 16:41:12,229 | INFO | max output length: 481
2026-01-29 16:41:12,229 | INFO | min output length: 48
2026-01-29 16:41:20,322 | INFO | end detected at 101
2026-01-29 16:41:20,324 | INFO | -229.25 * 0.5 = -114.62 for decoder
2026-01-29 16:41:20,324 | INFO | -81.45 * 0.5 = -40.73 for ctc
2026-01-29 16:41:20,324 | INFO | total log probability: -155.35
2026-01-29 16:41:20,324 | INFO | normalized log probability: -1.65
2026-01-29 16:41:20,324 | INFO | total number of ended hypotheses: 179
2026-01-29 16:41:20,325 | INFO | best hypo: ▁le▁boulanger▁est▁très▁bien▁très▁énervé▁contre▁la▁fille▁explique▁que▁le▁vol▁le▁chapelet▁grand▁seigneur▁explique▁que▁séduit▁qu'à▁la▁baguette▁est▁donc▁la▁pâtte▁de▁vol▁uf▁que▁pour▁policier▁ses▁colon▁du▁vol▁dont▁il▁embarque

2026-01-29 16:41:20,328 | INFO | speech length: 343680
2026-01-29 16:41:20,365 | INFO | decoder input length: 536
2026-01-29 16:41:20,365 | INFO | max output length: 536
2026-01-29 16:41:20,365 | INFO | min output length: 53
2026-01-29 16:41:32,007 | INFO | end detected at 140
2026-01-29 16:41:32,009 | INFO | -391.76 * 0.5 = -195.88 for decoder
2026-01-29 16:41:32,009 | INFO | -111.47 * 0.5 = -55.73 for ctc
2026-01-29 16:41:32,009 | INFO | total log probability: -251.61
2026-01-29 16:41:32,009 | INFO | normalized log probability: -1.86
2026-01-29 16:41:32,009 | INFO | total number of ended hypotheses: 151
2026-01-29 16:41:32,011 | INFO | best hypo: ▁le▁chapeline▁pour▁en▁suite▁on▁se▁retrouve▁donc▁tout▁le▁monde▁s'en▁va▁y▁compris▁la▁foule▁puisque▁la▁foulée▁intéressée▁par▁les▁flics▁on▁se▁retrouve▁b▁avec▁le▁boulanger▁d▁la▁jeune▁fille▁au▁sols▁et▁la▁vieille▁dame▁s▁qui▁revient▁vers▁boulanger▁le▁pour▁lui▁dire'▁non▁'est▁pas▁le▁monsieur▁mais▁c'est▁la▁jeune▁fille▁effectivement

2026-01-29 16:41:32,013 | INFO | speech length: 31520
2026-01-29 16:41:32,047 | INFO | decoder input length: 48
2026-01-29 16:41:32,047 | INFO | max output length: 48
2026-01-29 16:41:32,047 | INFO | min output length: 4
2026-01-29 16:41:32,527 | INFO | end detected at 14
2026-01-29 16:41:32,528 | INFO |  -1.92 * 0.5 =  -0.96 for decoder
2026-01-29 16:41:32,528 | INFO |  -6.63 * 0.5 =  -3.31 for ctc
2026-01-29 16:41:32,528 | INFO | total log probability: -4.27
2026-01-29 16:41:32,528 | INFO | normalized log probability: -0.53
2026-01-29 16:41:32,528 | INFO | total number of ended hypotheses: 177
2026-01-29 16:41:32,528 | INFO | best hypo: ▁et▁il▁était▁revenu

2026-01-29 16:41:32,533 | INFO | Chunk: 0 | WER=100.000000 | S=2 D=1 I=0
2026-01-29 16:41:32,535 | INFO | Chunk: 1 | WER=29.310345 | S=6 D=10 I=1
2026-01-29 16:41:32,535 | INFO | Chunk: 2 | WER=100.000000 | S=2 D=2 I=0
2026-01-29 16:41:32,536 | INFO | Chunk: 3 | WER=13.333333 | S=0 D=1 I=1
2026-01-29 16:41:32,536 | INFO | Chunk: 4 | WER=29.411765 | S=1 D=4 I=0
2026-01-29 16:41:32,538 | INFO | Chunk: 5 | WER=50.909091 | S=11 D=15 I=2
2026-01-29 16:41:32,540 | INFO | Chunk: 6 | WER=22.388060 | S=6 D=6 I=3
2026-01-29 16:41:32,540 | INFO | Chunk: 7 | WER=80.000000 | S=3 D=1 I=0
2026-01-29 16:41:32,558 | INFO | File: Rhap-M0022.wav | WER=28.712871 | S=27 D=21 I=10
2026-01-29 16:41:32,558 | INFO | ------------------------------
2026-01-29 16:41:32,558 | INFO | Conf cv Done!
2026-01-29 16:41:32,796 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-29 16:41:32,814 | INFO | Vocabulary size: 47
2026-01-29 16:41:33,331 | INFO | Gradient checkpoint layers: []
2026-01-29 16:41:34,002 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 16:41:34,005 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 16:41:34,005 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 16:41:34,006 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-29 16:41:34,009 | INFO | speech length: 34240
2026-01-29 16:41:34,038 | INFO | decoder input length: 53
2026-01-29 16:41:34,038 | INFO | max output length: 53
2026-01-29 16:41:34,038 | INFO | min output length: 5
2026-01-29 16:41:34,811 | INFO | end detected at 23
2026-01-29 16:41:34,813 | INFO |  -5.68 * 0.5 =  -2.84 for decoder
2026-01-29 16:41:34,813 | INFO |  -5.22 * 0.5 =  -2.61 for ctc
2026-01-29 16:41:34,813 | INFO | total log probability: -5.45
2026-01-29 16:41:34,813 | INFO | normalized log probability: -0.30
2026-01-29 16:41:34,813 | INFO | total number of ended hypotheses: 208
2026-01-29 16:41:34,813 | INFO | best hypo: euh<space>donc<space>qui<space>euh

2026-01-29 16:41:34,816 | INFO | speech length: 462720
2026-01-29 16:41:34,843 | INFO | decoder input length: 722
2026-01-29 16:41:34,843 | INFO | max output length: 722
2026-01-29 16:41:34,843 | INFO | min output length: 72
2026-01-29 16:42:04,915 | INFO | end detected at 327
2026-01-29 16:42:04,917 | INFO | -182.95 * 0.5 = -91.47 for decoder
2026-01-29 16:42:04,917 | INFO | -18.09 * 0.5 =  -9.04 for ctc
2026-01-29 16:42:04,917 | INFO | total log probability: -100.52
2026-01-29 16:42:04,917 | INFO | normalized log probability: -0.31
2026-01-29 16:42:04,917 | INFO | total number of ended hypotheses: 177
2026-01-29 16:42:04,921 | INFO | best hypo: on<space>voit<space>chaqueline<space>euh<space>s'habiller<space>pour<space>sortir<space>de<space>son<space>usine<space>ensuite<space>euh<space>on<space>voit<space>une<space>jeune<space>fille<space>habillée<space>en<space>noir<space>qui<space>regarde<space>une<space>devanture<space>de<space>magasin<space>qui<space>profite<space>d'un<space>aller<space>retour<space>du<space>boulanger<space>on<space>va<space>dire<space>pour<space>piquer<space>une<space>baquette<space>euh<space>sur<space>ce<space>une<space>vieille<space>femme<space>pour<space>cent<space>hesitation<space>voit<space>le<space>vol<space>très<space>vient<space>boulanger<space>la<space>jeune

2026-01-29 16:42:04,924 | INFO | speech length: 29600
2026-01-29 16:42:04,978 | INFO | decoder input length: 45
2026-01-29 16:42:04,978 | INFO | max output length: 45
2026-01-29 16:42:04,978 | INFO | min output length: 4
2026-01-29 16:42:05,477 | INFO | end detected at 16
2026-01-29 16:42:05,478 | INFO |  -1.07 * 0.5 =  -0.54 for decoder
2026-01-29 16:42:05,478 | INFO |  -0.37 * 0.5 =  -0.19 for ctc
2026-01-29 16:42:05,478 | INFO | total log probability: -0.72
2026-01-29 16:42:05,478 | INFO | normalized log probability: -0.07
2026-01-29 16:42:05,478 | INFO | total number of ended hypotheses: 167
2026-01-29 16:42:05,478 | INFO | best hypo: fille<space>euh

2026-01-29 16:42:05,480 | INFO | speech length: 138400
2026-01-29 16:42:05,507 | INFO | decoder input length: 215
2026-01-29 16:42:05,507 | INFO | max output length: 215
2026-01-29 16:42:05,507 | INFO | min output length: 21
2026-01-29 16:42:09,583 | INFO | end detected at 89
2026-01-29 16:42:09,585 | INFO |  -9.63 * 0.5 =  -4.82 for decoder
2026-01-29 16:42:09,585 | INFO | -11.76 * 0.5 =  -5.88 for ctc
2026-01-29 16:42:09,585 | INFO | total log probability: -10.70
2026-01-29 16:42:09,585 | INFO | normalized log probability: -0.13
2026-01-29 16:42:09,585 | INFO | total number of ended hypotheses: 175
2026-01-29 16:42:09,586 | INFO | best hypo: se<space>retrouve<space>par<space>terre<space>avec<space>sa<space>pline<space>euh<space>au<space>sol<space>le<space>boulanger<space>arrive<space>à<space>la<space>rattraper

2026-01-29 16:42:09,588 | INFO | speech length: 144480
2026-01-29 16:42:09,614 | INFO | decoder input length: 225
2026-01-29 16:42:09,614 | INFO | max output length: 225
2026-01-29 16:42:09,614 | INFO | min output length: 22
2026-01-29 16:42:13,739 | INFO | end detected at 87
2026-01-29 16:42:13,741 | INFO |  -7.35 * 0.5 =  -3.67 for decoder
2026-01-29 16:42:13,741 | INFO |  -6.50 * 0.5 =  -3.25 for ctc
2026-01-29 16:42:13,741 | INFO | total log probability: -6.92
2026-01-29 16:42:13,741 | INFO | normalized log probability: -0.09
2026-01-29 16:42:13,741 | INFO | total number of ended hypotheses: 204
2026-01-29 16:42:13,742 | INFO | best hypo: euh<space>à<space>travers<space>la<space>foule<space>le<space>policier<space>arrive<space>demande<space>des<space>explications<space>de<space>ce<space>proie

2026-01-29 16:42:13,744 | INFO | speech length: 308320
2026-01-29 16:42:13,774 | INFO | decoder input length: 481
2026-01-29 16:42:13,775 | INFO | max output length: 481
2026-01-29 16:42:13,775 | INFO | min output length: 48
2026-01-29 16:42:32,315 | INFO | end detected at 270
2026-01-29 16:42:32,317 | INFO | -47.44 * 0.5 = -23.72 for decoder
2026-01-29 16:42:32,317 | INFO | -35.94 * 0.5 = -17.97 for ctc
2026-01-29 16:42:32,317 | INFO | total log probability: -41.69
2026-01-29 16:42:32,317 | INFO | normalized log probability: -0.16
2026-01-29 16:42:32,317 | INFO | total number of ended hypotheses: 229
2026-01-29 16:42:32,320 | INFO | best hypo: euh<space>le<space>boulanger<space>très<space>très<space>énervé<space>contre<space>la<space>fille<space>euh<space>explique<space>le<space>vol<space>chapely<space>grand<space>seigneur<space>explique<space>que<space>euh<space>euh<space>c'est<space>duit<space>à<space>la<space>baguette<space>et<space>que<space>donc<space>y<space>a<space>pas<space>de<space>vol<space>euh<space>sauf<space>que<space>pour<space>le<space>policier<space>c'est<space>quand<space>même<space>du<space>vol<space>donc<space>il<space>embarque<space>pour<space>cent<space>hesitation

2026-01-29 16:42:32,322 | INFO | speech length: 343680
2026-01-29 16:42:32,351 | INFO | decoder input length: 536
2026-01-29 16:42:32,351 | INFO | max output length: 536
2026-01-29 16:42:32,351 | INFO | min output length: 53
2026-01-29 16:42:55,730 | INFO | end detected at 337
2026-01-29 16:42:55,731 | INFO | -132.09 * 0.5 = -66.04 for decoder
2026-01-29 16:42:55,731 | INFO |  -5.20 * 0.5 =  -2.60 for ctc
2026-01-29 16:42:55,731 | INFO | total log probability: -68.64
2026-01-29 16:42:55,731 | INFO | normalized log probability: -0.21
2026-01-29 16:42:55,731 | INFO | total number of ended hypotheses: 191
2026-01-29 16:42:55,735 | INFO | best hypo: le<space>chaqueline<space>pour<space>en<space>suite<space>euh<space>on<space>se<space>retrouve<space>donc<space>tout<space>le<space>monde<space>s'en<space>va<space>y<space>compris<space>la<space>foule<space>puisque<space>la<space>foulée<space>intéressée<space>par<space>les<space>flics<space>euh<space>on<space>se<space>retrouve<space>donc<space>avec<space>le<space>boulanger<space>la<space>jeune<space>fille<space>au<space>sol<space>et<space>la<space>vieille<space>dame<space>qui<space>revient<space>vers<space>boulanger<space>pour<space>lui<space>dire<space>non<space>ce<space>n'est<space>pas<space>le<space>monsieur<space>mais<space>c'est<space>la<space>jeune<space>fille<space>effectivement

2026-01-29 16:42:55,738 | INFO | speech length: 31520
2026-01-29 16:42:55,767 | INFO | decoder input length: 48
2026-01-29 16:42:55,767 | INFO | max output length: 48
2026-01-29 16:42:55,767 | INFO | min output length: 4
2026-01-29 16:42:56,726 | INFO | end detected at 31
2026-01-29 16:42:56,729 | INFO |  -6.40 * 0.5 =  -3.20 for decoder
2026-01-29 16:42:56,729 | INFO |  -5.69 * 0.5 =  -2.85 for ctc
2026-01-29 16:42:56,729 | INFO | total log probability: -6.05
2026-01-29 16:42:56,729 | INFO | normalized log probability: -0.27
2026-01-29 16:42:56,729 | INFO | total number of ended hypotheses: 226
2026-01-29 16:42:56,730 | INFO | best hypo: et<space>là<space>te<space>est<space>revenue

2026-01-29 16:42:56,735 | INFO | Chunk: 0 | WER=66.666667 | S=1 D=0 I=1
2026-01-29 16:42:56,737 | INFO | Chunk: 1 | WER=18.965517 | S=6 D=2 I=3
2026-01-29 16:42:56,737 | INFO | Chunk: 2 | WER=50.000000 | S=0 D=2 I=0
2026-01-29 16:42:56,738 | INFO | Chunk: 3 | WER=13.333333 | S=1 D=0 I=1
2026-01-29 16:42:56,738 | INFO | Chunk: 4 | WER=23.529412 | S=1 D=3 I=0
2026-01-29 16:42:56,740 | INFO | Chunk: 5 | WER=20.000000 | S=4 D=5 I=2
2026-01-29 16:42:56,742 | INFO | Chunk: 6 | WER=10.447761 | S=3 D=3 I=1
2026-01-29 16:42:56,742 | INFO | Chunk: 7 | WER=40.000000 | S=2 D=0 I=0
2026-01-29 16:42:56,761 | INFO | File: Rhap-M0022.wav | WER=23.267327 | S=14 D=9 I=24
2026-01-29 16:42:56,762 | INFO | ------------------------------
2026-01-29 16:42:56,762 | INFO | Conf ester Done!
2026-01-29 16:43:57,701 | INFO | Chunk: 0 | WER=100.000000 | S=3 D=0 I=0
2026-01-29 16:43:57,706 | INFO | Chunk: 1 | WER=24.137931 | S=3 D=10 I=1
2026-01-29 16:43:57,706 | INFO | Chunk: 2 | WER=100.000000 | S=2 D=2 I=0
2026-01-29 16:43:57,707 | INFO | Chunk: 3 | WER=53.333333 | S=4 D=2 I=2
2026-01-29 16:43:57,707 | INFO | Chunk: 4 | WER=47.058824 | S=3 D=4 I=1
2026-01-29 16:43:57,710 | INFO | Chunk: 5 | WER=41.818182 | S=7 D=16 I=0
2026-01-29 16:43:57,714 | INFO | Chunk: 6 | WER=10.447761 | S=3 D=4 I=0
2026-01-29 16:43:57,714 | INFO | Chunk: 7 | WER=100.000000 | S=3 D=2 I=0
2026-01-29 16:43:57,732 | INFO | File: Rhap-M0022.wav | WER=26.237624 | S=23 D=22 I=8
2026-01-29 16:43:57,732 | INFO | ------------------------------
2026-01-29 16:43:57,732 | INFO | hmm_tdnn Done!
2026-01-29 16:43:57,872 | INFO | ==================================Rhap-M0023.wav=========================================
2026-01-29 16:43:57,888 | INFO | Using rVAD model
2026-01-29 16:44:02,063 | INFO | Chunk: 0 | WER=44.086022 | S=11 D=30 I=0
2026-01-29 16:44:02,064 | INFO | Chunk: 1 | WER=29.545455 | S=4 D=8 I=1
2026-01-29 16:44:02,065 | INFO | Chunk: 2 | WER=63.461538 | S=8 D=24 I=1
2026-01-29 16:44:02,067 | INFO | Chunk: 3 | WER=24.324324 | S=4 D=14 I=0
2026-01-29 16:44:02,068 | INFO | Chunk: 4 | WER=22.727273 | S=1 D=9 I=0
2026-01-29 16:44:02,069 | INFO | Chunk: 5 | WER=15.384615 | S=2 D=0 I=0
2026-01-29 16:44:02,098 | INFO | File: Rhap-M0023.wav | WER=31.186441 | S=30 D=60 I=2
2026-01-29 16:44:02,098 | INFO | ------------------------------
2026-01-29 16:44:02,098 | INFO | w2vec vad chunk Done!
2026-01-29 16:44:10,883 | INFO | Chunk: 0 | WER=65.591398 | S=11 D=50 I=0
2026-01-29 16:44:10,885 | INFO | Chunk: 1 | WER=40.909091 | S=7 D=10 I=1
2026-01-29 16:44:10,886 | INFO | Chunk: 2 | WER=53.846154 | S=4 D=23 I=1
2026-01-29 16:44:10,887 | INFO | Chunk: 3 | WER=56.756757 | S=8 D=34 I=0
2026-01-29 16:44:10,888 | INFO | Chunk: 4 | WER=29.545455 | S=3 D=10 I=0
2026-01-29 16:44:10,889 | INFO | Chunk: 5 | WER=7.692308 | S=1 D=0 I=0
2026-01-29 16:44:10,913 | INFO | File: Rhap-M0023.wav | WER=46.779661 | S=34 D=102 I=2
2026-01-29 16:44:10,913 | INFO | ------------------------------
2026-01-29 16:44:10,913 | INFO | whisper med Done!
2026-01-29 16:44:21,598 | INFO | Chunk: 0 | WER=77.419355 | S=6 D=66 I=0
2026-01-29 16:44:21,600 | INFO | Chunk: 1 | WER=34.090909 | S=4 D=11 I=0
2026-01-29 16:44:21,600 | INFO | Chunk: 2 | WER=90.384615 | S=2 D=45 I=0
2026-01-29 16:44:21,603 | INFO | Chunk: 3 | WER=47.297297 | S=12 D=16 I=7
2026-01-29 16:44:21,604 | INFO | Chunk: 4 | WER=27.272727 | S=2 D=10 I=0
2026-01-29 16:44:21,604 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 16:44:21,627 | INFO | File: Rhap-M0023.wav | WER=52.542373 | S=23 D=124 I=8
2026-01-29 16:44:21,627 | INFO | ------------------------------
2026-01-29 16:44:21,627 | INFO | whisper large Done!
2026-01-29 16:44:21,776 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-29 16:44:21,807 | INFO | Vocabulary size: 350
2026-01-29 16:44:22,453 | INFO | Gradient checkpoint layers: []
2026-01-29 16:44:23,179 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 16:44:23,183 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 16:44:23,183 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 16:44:23,183 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-29 16:44:23,183 | INFO | speech length: 395520
2026-01-29 16:44:23,222 | INFO | decoder input length: 617
2026-01-29 16:44:23,222 | INFO | max output length: 617
2026-01-29 16:44:23,222 | INFO | min output length: 61
2026-01-29 16:44:35,987 | INFO | end detected at 139
2026-01-29 16:44:35,989 | INFO | -359.82 * 0.5 = -179.91 for decoder
2026-01-29 16:44:35,989 | INFO | -149.47 * 0.5 = -74.74 for ctc
2026-01-29 16:44:35,989 | INFO | total log probability: -254.64
2026-01-29 16:44:35,989 | INFO | normalized log probability: -1.91
2026-01-29 16:44:35,989 | INFO | total number of ended hypotheses: 186
2026-01-29 16:44:35,991 | INFO | best hypo: ▁alors▁il▁y▁a▁une▁jeune▁fille▁qui▁en▁aborde▁j'ai▁commencé▁est▁d'abord▁une▁voiture▁qui▁à▁une▁voiture▁avec▁des▁marins▁des▁marchandises▁qui▁arrive▁et▁en▁fait▁c'est▁pour▁un▁boulanger▁ou▁comme▁on▁les▁voi▁sortir▁et▁du▁pain▁avec▁des▁brioges▁trueux▁comme▁ça▁mais▁en▁fait▁'▁marioyage▁la▁jeune▁fille▁la▁jeune▁fille▁qui▁à▁l'air▁pauvre▁elle▁a▁juste

2026-01-29 16:44:35,994 | INFO | speech length: 228960
2026-01-29 16:44:36,027 | INFO | decoder input length: 357
2026-01-29 16:44:36,027 | INFO | max output length: 357
2026-01-29 16:44:36,027 | INFO | min output length: 35
2026-01-29 16:44:41,473 | INFO | end detected at 85
2026-01-29 16:44:41,474 | INFO | -72.17 * 0.5 = -36.08 for decoder
2026-01-29 16:44:41,474 | INFO | -45.68 * 0.5 = -22.84 for ctc
2026-01-29 16:44:41,474 | INFO | total log probability: -58.93
2026-01-29 16:44:41,474 | INFO | normalized log probability: -0.74
2026-01-29 16:44:41,474 | INFO | total number of ended hypotheses: 158
2026-01-29 16:44:41,475 | INFO | best hypo: ▁justin▁un▁morceau▁de▁tissu▁comme▁habit▁et▁qui▁wa▁qui▁voit▁sa▁passée▁est▁donc▁elle▁se▁dire▁et▁peut▁récupérer▁et▁pourra▁récupérer▁quelque▁chose▁non▁qu'elle▁va▁voler▁en▁fait▁un▁une▁baguette

2026-01-29 16:44:41,477 | INFO | speech length: 177120
2026-01-29 16:44:41,510 | INFO | decoder input length: 276
2026-01-29 16:44:41,510 | INFO | max output length: 276
2026-01-29 16:44:41,510 | INFO | min output length: 27
2026-01-29 16:44:45,002 | INFO | end detected at 62
2026-01-29 16:44:45,003 | INFO | -39.35 * 0.5 = -19.68 for decoder
2026-01-29 16:44:45,003 | INFO | -20.49 * 0.5 = -10.24 for ctc
2026-01-29 16:44:45,003 | INFO | total log probability: -29.92
2026-01-29 16:44:45,003 | INFO | normalized log probability: -0.53
2026-01-29 16:44:45,003 | INFO | total number of ended hypotheses: 170
2026-01-29 16:44:45,004 | INFO | best hypo: ▁et▁donc▁elle▁va▁courir▁va▁courir▁et▁fatyach▁et▁charlotte▁arrive▁à▁ce▁moment▁là▁et▁sa▁rencontre▁et▁une▁enquête▁marie▁un▁accident▁coaisseur

2026-01-29 16:44:45,006 | INFO | speech length: 327680
2026-01-29 16:44:45,042 | INFO | decoder input length: 511
2026-01-29 16:44:45,042 | INFO | max output length: 511
2026-01-29 16:44:45,042 | INFO | min output length: 51
2026-01-29 16:44:56,475 | INFO | end detected at 146
2026-01-29 16:44:56,476 | INFO | -246.38 * 0.5 = -123.19 for decoder
2026-01-29 16:44:56,476 | INFO | -118.05 * 0.5 = -59.03 for ctc
2026-01-29 16:44:56,476 | INFO | total log probability: -182.22
2026-01-29 16:44:56,476 | INFO | normalized log probability: -1.29
2026-01-29 16:44:56,476 | INFO | total number of ended hypotheses: 160
2026-01-29 16:44:56,478 | INFO | best hypo: ▁ils▁tombent▁tous▁les▁deux▁et▁c'est▁à▁ce▁moment▁là▁que▁le▁blonger▁voit▁que▁la▁baguette▁a▁disparu▁et▁se▁dit▁que▁ronge▁pense▁qu'un▁faite▁ils▁voient▁d'abord▁la▁jeune▁fille▁partir▁et▁ont▁elles▁l'appelle▁la▁police▁et▁en▁faite▁'ins▁est▁forcément▁chars▁▁ils▁se▁rencontaient▁tous▁les▁deux▁carlot▁et▁récupéré▁la▁la▁baguette

2026-01-29 16:44:56,480 | INFO | speech length: 164160
2026-01-29 16:44:56,516 | INFO | decoder input length: 256
2026-01-29 16:44:56,516 | INFO | max output length: 256
2026-01-29 16:44:56,516 | INFO | min output length: 25
2026-01-29 16:45:00,157 | INFO | end detected at 68
2026-01-29 16:45:00,159 | INFO | -25.20 * 0.5 = -12.60 for decoder
2026-01-29 16:45:00,159 | INFO | -35.18 * 0.5 = -17.59 for ctc
2026-01-29 16:45:00,159 | INFO | total log probability: -30.19
2026-01-29 16:45:00,159 | INFO | normalized log probability: -0.51
2026-01-29 16:45:00,159 | INFO | total number of ended hypotheses: 194
2026-01-29 16:45:00,160 | INFO | best hypo: ▁et▁le▁policier▁va▁voir▁le▁boulanger▁dit▁au▁policier▁qu'en▁fait▁c'est▁la▁fille▁mais▁en▁fait▁que▁charlot▁va▁dire▁en▁fait▁que▁c'est▁lui▁qu'il▁a▁pris

2026-01-29 16:45:00,162 | INFO | speech length: 43520
2026-01-29 16:45:00,194 | INFO | decoder input length: 67
2026-01-29 16:45:00,194 | INFO | max output length: 67
2026-01-29 16:45:00,194 | INFO | min output length: 6
2026-01-29 16:45:01,367 | INFO | end detected at 34
2026-01-29 16:45:01,367 | INFO |  -2.22 * 0.5 =  -1.11 for decoder
2026-01-29 16:45:01,367 | INFO |  -9.04 * 0.5 =  -4.52 for ctc
2026-01-29 16:45:01,368 | INFO | total log probability: -5.63
2026-01-29 16:45:01,368 | INFO | normalized log probability: -0.19
2026-01-29 16:45:01,368 | INFO | total number of ended hypotheses: 143
2026-01-29 16:45:01,368 | INFO | best hypo: ▁ce▁qu'il▁a▁dans▁la▁main▁et▁finalement▁les▁policiers▁vont▁l'arrêter

2026-01-29 16:45:01,375 | INFO | Chunk: 0 | WER=47.311828 | S=17 D=25 I=2
2026-01-29 16:45:01,377 | INFO | Chunk: 1 | WER=38.636364 | S=8 D=8 I=1
2026-01-29 16:45:01,378 | INFO | Chunk: 2 | WER=67.307692 | S=9 D=26 I=0
2026-01-29 16:45:01,380 | INFO | Chunk: 3 | WER=36.486486 | S=15 D=11 I=1
2026-01-29 16:45:01,381 | INFO | Chunk: 4 | WER=31.818182 | S=4 D=10 I=0
2026-01-29 16:45:01,382 | INFO | Chunk: 5 | WER=15.384615 | S=1 D=0 I=1
2026-01-29 16:45:01,412 | INFO | File: Rhap-M0023.wav | WER=39.661017 | S=53 D=57 I=7
2026-01-29 16:45:01,412 | INFO | ------------------------------
2026-01-29 16:45:01,412 | INFO | Conf cv Done!
2026-01-29 16:45:01,573 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-29 16:45:01,591 | INFO | Vocabulary size: 47
2026-01-29 16:45:02,160 | INFO | Gradient checkpoint layers: []
2026-01-29 16:45:02,870 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 16:45:02,873 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 16:45:02,873 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 16:45:02,874 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-29 16:45:02,876 | INFO | speech length: 395520
2026-01-29 16:45:02,908 | INFO | decoder input length: 617
2026-01-29 16:45:02,908 | INFO | max output length: 617
2026-01-29 16:45:02,908 | INFO | min output length: 61
2026-01-29 16:45:33,287 | INFO | end detected at 407
2026-01-29 16:45:33,289 | INFO | -158.20 * 0.5 = -79.10 for decoder
2026-01-29 16:45:33,289 | INFO | -75.07 * 0.5 = -37.53 for ctc
2026-01-29 16:45:33,289 | INFO | total log probability: -116.63
2026-01-29 16:45:33,289 | INFO | normalized log probability: -0.29
2026-01-29 16:45:33,289 | INFO | total number of ended hypotheses: 183
2026-01-29 16:45:33,294 | INFO | best hypo: alors<space>euh<space>y<space>a<space>une<space>jeune<space>fille<space>pour<space>cent<space>hesitation<space>qui<space>en<space>abord<space>j'ai<space>commencé<space>et<space>d'abord<space>une<space>voiture<space>qui<space>est<space>ouais<space>c'est<space>un<space>une<space>voiture<space>avec<space>des<space>ma<space>des<space>marchandises<space>qui<space>arrivent<space>et<space>en<space>fait<space>c'est<space>pour<space>un<space>boulanger<space>donc<space>on<space>vont<space>on<space>les<space>voit<space>sortir<space>pour<space>cent<space>hesitation<space>du<space>pain<space>des<space>brillons<space>des<space>trucs<space>comme<space>ça<space>et<space>en<space>fait<space>moi<space>il<space>y<space>a<space>jeune<space>fille<space>y<space>une<space>jeune<space>fille<space>qui<space>est<space>qui<space>est<space>l'art<space>pauvre<space>qui<space>a<space>juste

2026-01-29 16:45:33,296 | INFO | speech length: 228960
2026-01-29 16:45:33,326 | INFO | decoder input length: 357
2026-01-29 16:45:33,326 | INFO | max output length: 357
2026-01-29 16:45:33,326 | INFO | min output length: 35
2026-01-29 16:45:46,766 | INFO | end detected at 246
2026-01-29 16:45:46,768 | INFO | -46.98 * 0.5 = -23.49 for decoder
2026-01-29 16:45:46,768 | INFO | -42.27 * 0.5 = -21.13 for ctc
2026-01-29 16:45:46,768 | INFO | total log probability: -44.63
2026-01-29 16:45:46,768 | INFO | normalized log probability: -0.19
2026-01-29 16:45:46,768 | INFO | total number of ended hypotheses: 192
2026-01-29 16:45:46,771 | INFO | best hypo: juste<space>un<space>un<space>morceau<space>de<space>tissu<space>comme<space>un<space>bi<space>et<space>qui<space>pour<space>cent<space>hesitation<space>qui<space>oit<space>pour<space>cent<space>hesitation<space>qui<space>voit<space>s'attasser<space>et<space>donc<space>elle<space>se<space>dit<space>ben<space>elle<space>peut<space>récupérer<space>et<space>pour<space>récupérer<space>quelque<space>chose<space>donc<space>elle<space>va<space>voler<space>en<space>fait<space>un<space>une<space>baguette

2026-01-29 16:45:46,773 | INFO | speech length: 177120
2026-01-29 16:45:46,804 | INFO | decoder input length: 276
2026-01-29 16:45:46,804 | INFO | max output length: 276
2026-01-29 16:45:46,804 | INFO | min output length: 27
2026-01-29 16:45:56,185 | INFO | end detected at 194
2026-01-29 16:45:56,187 | INFO | -28.92 * 0.5 = -14.46 for decoder
2026-01-29 16:45:56,187 | INFO | -31.42 * 0.5 = -15.71 for ctc
2026-01-29 16:45:56,187 | INFO | total log probability: -30.17
2026-01-29 16:45:56,187 | INFO | normalized log probability: -0.17
2026-01-29 16:45:56,187 | INFO | total number of ended hypotheses: 202
2026-01-29 16:45:56,189 | INFO | best hypo: et<space>donc<space>elle<space>va<space>courir<space>euh<space>va<space>courir<space>et<space>euh<space>en<space>fait<space>il<space>y<space>a<space>charlot<space>qui<space>arrive<space>à<space>ce<space>moment<space>là<space>et<space>euh<space>ils<space>se<space>se<space>rencontrent<space>et<space>donc<space>elles<space>tous<space>euh<space>moi<space>il<space>y<space>a<space>un<space>accident<space>quoi<space>il<space>se

2026-01-29 16:45:56,191 | INFO | speech length: 327680
2026-01-29 16:45:56,227 | INFO | decoder input length: 511
2026-01-29 16:45:56,227 | INFO | max output length: 511
2026-01-29 16:45:56,227 | INFO | min output length: 51
2026-01-29 16:46:19,385 | INFO | end detected at 353
2026-01-29 16:46:19,387 | INFO | -54.93 * 0.5 = -27.46 for decoder
2026-01-29 16:46:19,387 | INFO | -38.21 * 0.5 = -19.10 for ctc
2026-01-29 16:46:19,387 | INFO | total log probability: -46.57
2026-01-29 16:46:19,387 | INFO | normalized log probability: -0.14
2026-01-29 16:46:19,387 | INFO | total number of ended hypotheses: 206
2026-01-29 16:46:19,391 | INFO | best hypo: ils<space>tombent<space>tous<space>les<space>deux<space>et<space>c'est<space>à<space>ce<space>moment<space>là<space>que<space>le<space>le<space>blonger<space>voit<space>que<space>la<space>baquette<space>disparue<space>et<space>il<space>se<space>dit<space>que<space>bon<space>donc<space>je<space>pense<space>qu'en<space>fait<space>il<space>voit<space>d'abord<space>la<space>la<space>jeune<space>fille<space>partir<space>donc<space>il<space>appelle<space>la<space>police<space>et<space>pour<space>cent<space>hesitation<space>en<space>fait<space>euh<space>bon<space>forcément<space>charles<space>comme<space>si<space>on<space>rencontrait<space>tous<space>les<space>deux<space>charlot<space>a<space>récupéré<space>la<space>la<space>baguette

2026-01-29 16:46:19,394 | INFO | speech length: 164160
2026-01-29 16:46:19,424 | INFO | decoder input length: 256
2026-01-29 16:46:19,424 | INFO | max output length: 256
2026-01-29 16:46:19,424 | INFO | min output length: 25
2026-01-29 16:46:27,993 | INFO | end detected at 191
2026-01-29 16:46:27,995 | INFO | -30.29 * 0.5 = -15.15 for decoder
2026-01-29 16:46:27,996 | INFO | -59.74 * 0.5 = -29.87 for ctc
2026-01-29 16:46:27,996 | INFO | total log probability: -45.02
2026-01-29 16:46:27,996 | INFO | normalized log probability: -0.25
2026-01-29 16:46:27,996 | INFO | total number of ended hypotheses: 266
2026-01-29 16:46:27,998 | INFO | best hypo: et<space>euh<space>le<space>policier<space>euh<space>va<space>voir<space>le<space>le<space>boulanger<space>pour<space>cent<space>hesitation<space>au<space>policier<space>qu'en<space>fait<space>c'est<space>la<space>fille<space>mais<space>en<space>fait<space>charlot<space>va<space>dire<space>en<space>fait<space>que<space>c'est<space>lui<space>qu'il<space>qu'il<space>a<space>pris

2026-01-29 16:46:28,001 | INFO | speech length: 43520
2026-01-29 16:46:28,028 | INFO | decoder input length: 67
2026-01-29 16:46:28,028 | INFO | max output length: 67
2026-01-29 16:46:28,028 | INFO | min output length: 6
2026-01-29 16:46:29,999 | INFO | adding <eos> in the last position in the loop
2026-01-29 16:46:30,007 | INFO | no hypothesis. Finish decoding.
2026-01-29 16:46:30,008 | INFO | -11.66 * 0.5 =  -5.83 for decoder
2026-01-29 16:46:30,008 | INFO | -17.52 * 0.5 =  -8.76 for ctc
2026-01-29 16:46:30,008 | INFO | total log probability: -14.59
2026-01-29 16:46:30,008 | INFO | normalized log probability: -0.22
2026-01-29 16:46:30,008 | INFO | total number of ended hypotheses: 82
2026-01-29 16:46:30,009 | INFO | best hypo: ce<space>qu'il<space>a<space>dans<space>lamain<space>et<space>finalement<space>les<space>policiers<space>vont<space>l'arrêter

2026-01-29 16:46:30,020 | INFO | Chunk: 0 | WER=32.258065 | S=12 D=13 I=5
2026-01-29 16:46:30,021 | INFO | Chunk: 1 | WER=36.363636 | S=10 D=2 I=4
2026-01-29 16:46:30,023 | INFO | Chunk: 2 | WER=30.769231 | S=6 D=10 I=0
2026-01-29 16:46:30,025 | INFO | Chunk: 3 | WER=24.324324 | S=10 D=6 I=2
2026-01-29 16:46:30,027 | INFO | Chunk: 4 | WER=29.545455 | S=7 D=5 I=1
2026-01-29 16:46:30,027 | INFO | Chunk: 5 | WER=30.769231 | S=2 D=1 I=1
2026-01-29 16:46:30,064 | INFO | File: Rhap-M0023.wav | WER=30.847458 | S=46 D=22 I=23
2026-01-29 16:46:30,064 | INFO | ------------------------------
2026-01-29 16:46:30,064 | INFO | Conf ester Done!
2026-01-29 16:47:34,350 | INFO | Chunk: 0 | WER=47.311828 | S=18 D=24 I=2
2026-01-29 16:47:34,352 | INFO | Chunk: 1 | WER=36.363636 | S=8 D=8 I=0
2026-01-29 16:47:34,354 | INFO | Chunk: 2 | WER=59.615385 | S=9 D=22 I=0
2026-01-29 16:47:34,357 | INFO | Chunk: 3 | WER=32.432432 | S=13 D=11 I=0
2026-01-29 16:47:34,358 | INFO | Chunk: 4 | WER=36.363636 | S=6 D=10 I=0
2026-01-29 16:47:34,358 | INFO | Chunk: 5 | WER=15.384615 | S=1 D=0 I=1
2026-01-29 16:47:34,390 | INFO | File: Rhap-M0023.wav | WER=37.966102 | S=53 D=53 I=6
2026-01-29 16:47:34,390 | INFO | ------------------------------
2026-01-29 16:47:34,390 | INFO | hmm_tdnn Done!
2026-01-29 16:47:34,599 | INFO | ==================================Rhap-M0024.wav=========================================
2026-01-29 16:47:34,618 | INFO | Using rVAD model
2026-01-29 16:47:36,690 | INFO | Chunk: 0 | WER=21.917808 | S=7 D=9 I=0
2026-01-29 16:47:36,692 | INFO | Chunk: 1 | WER=25.862069 | S=4 D=10 I=1
2026-01-29 16:47:36,700 | INFO | File: Rhap-M0024.wav | WER=16.806723 | S=10 D=8 I=2
2026-01-29 16:47:36,700 | INFO | ------------------------------
2026-01-29 16:47:36,700 | INFO | w2vec vad chunk Done!
2026-01-29 16:47:40,737 | INFO | Chunk: 0 | WER=49.315068 | S=1 D=35 I=0
2026-01-29 16:47:40,739 | INFO | Chunk: 1 | WER=44.827586 | S=1 D=24 I=1
2026-01-29 16:47:40,744 | INFO | File: Rhap-M0024.wav | WER=42.016807 | S=2 D=47 I=1
2026-01-29 16:47:40,745 | INFO | ------------------------------
2026-01-29 16:47:40,745 | INFO | whisper med Done!
2026-01-29 16:47:46,682 | INFO | Chunk: 0 | WER=41.095890 | S=21 D=9 I=0
2026-01-29 16:47:46,683 | INFO | Chunk: 1 | WER=58.620690 | S=0 D=34 I=0
2026-01-29 16:47:46,690 | INFO | File: Rhap-M0024.wav | WER=43.697479 | S=21 D=31 I=0
2026-01-29 16:47:46,690 | INFO | ------------------------------
2026-01-29 16:47:46,690 | INFO | whisper large Done!
2026-01-29 16:47:46,921 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-29 16:47:46,951 | INFO | Vocabulary size: 350
2026-01-29 16:47:47,506 | INFO | Gradient checkpoint layers: []
2026-01-29 16:47:48,135 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 16:47:48,138 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 16:47:48,139 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 16:47:48,139 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-29 16:47:48,139 | INFO | speech length: 425600
2026-01-29 16:47:48,186 | INFO | decoder input length: 664
2026-01-29 16:47:48,186 | INFO | max output length: 664
2026-01-29 16:47:48,186 | INFO | min output length: 66
2026-01-29 16:48:06,013 | INFO | end detected at 152
2026-01-29 16:48:06,014 | INFO | -514.56 * 0.5 = -257.28 for decoder
2026-01-29 16:48:06,014 | INFO | -150.69 * 0.5 = -75.35 for ctc
2026-01-29 16:48:06,014 | INFO | total log probability: -332.63
2026-01-29 16:48:06,014 | INFO | normalized log probability: -2.25
2026-01-29 16:48:06,014 | INFO | total number of ended hypotheses: 143
2026-01-29 16:48:06,016 | INFO | best hypo: ▁il▁y▁avait▁donc▁une▁jeune▁fille▁qui▁regardait▁dans▁une▁boutique▁apparemment▁une▁pâtisserie▁qui▁semblait▁avoir▁faim▁qui▁il▁a▁profité▁de▁ce▁que▁leur▁livreur▁s'éloigne▁y▁volerait▁à▁une▁baguette▁rencontrer▁dans▁charlot▁à▁sa▁moment▁là▁à▁lui▁est▁rentrée▁de▁'an▁dans▁la▁infusion▁d'une▁f▁passante▁à▁dénoncer▁la▁jeune▁fille▁de▁livreur▁qui▁accourue▁après▁la▁jeune▁fille

2026-01-29 16:48:06,020 | INFO | speech length: 277120
2026-01-29 16:48:06,062 | INFO | decoder input length: 432
2026-01-29 16:48:06,062 | INFO | max output length: 432
2026-01-29 16:48:06,062 | INFO | min output length: 43
2026-01-29 16:48:18,077 | INFO | end detected at 124
2026-01-29 16:48:18,079 | INFO | -176.78 * 0.5 = -88.39 for decoder
2026-01-29 16:48:18,079 | INFO | -48.61 * 0.5 = -24.31 for ctc
2026-01-29 16:48:18,079 | INFO | total log probability: -112.70
2026-01-29 16:48:18,079 | INFO | normalized log probability: -0.96
2026-01-29 16:48:18,079 | INFO | total number of ended hypotheses: 172
2026-01-29 16:48:18,081 | INFO | best hypo: ▁les▁policiers▁sont▁arrivés▁en▁raison▁du▁duvakhamj▁pense▁et▁charlot▁s'est▁accusé▁plutôt▁que▁de▁laisser▁la▁jeune▁fille▁s'accuser▁etie▁ça▁s'arrête▁lorsque▁lapsante▁indique▁aux▁livreur▁que▁c'était▁bien▁la▁jeune▁fille▁qui▁avait▁volé▁le▁pain▁et▁pas▁le▁jeune

2026-01-29 16:48:18,090 | INFO | Chunk: 0 | WER=34.246575 | S=14 D=8 I=3
2026-01-29 16:48:18,092 | INFO | Chunk: 1 | WER=24.137931 | S=4 D=10 I=0
2026-01-29 16:48:18,099 | INFO | File: Rhap-M0024.wav | WER=23.529412 | S=17 D=7 I=4
2026-01-29 16:48:18,099 | INFO | ------------------------------
2026-01-29 16:48:18,099 | INFO | Conf cv Done!
2026-01-29 16:48:18,289 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-29 16:48:18,308 | INFO | Vocabulary size: 47
2026-01-29 16:48:18,820 | INFO | Gradient checkpoint layers: []
2026-01-29 16:48:19,481 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 16:48:19,484 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 16:48:19,485 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 16:48:19,485 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-29 16:48:19,488 | INFO | speech length: 425600
2026-01-29 16:48:19,530 | INFO | decoder input length: 664
2026-01-29 16:48:19,531 | INFO | max output length: 664
2026-01-29 16:48:19,531 | INFO | min output length: 66
2026-01-29 16:49:01,786 | INFO | end detected at 434
2026-01-29 16:49:01,788 | INFO | -333.95 * 0.5 = -166.97 for decoder
2026-01-29 16:49:01,788 | INFO | -68.87 * 0.5 = -34.44 for ctc
2026-01-29 16:49:01,788 | INFO | total log probability: -201.41
2026-01-29 16:49:01,788 | INFO | normalized log probability: -0.47
2026-01-29 16:49:01,788 | INFO | total number of ended hypotheses: 170
2026-01-29 16:49:01,793 | INFO | best hypo: euh<space>il<space>y<space>avait<space>donc<space>une<space>jeune<space>fille<space>qui<space>regardait<space>dans<space>une<space>boutique<space>apparemment<space>une<space>pâtisserie<space>qui<space>semblait<space>avoir<space>faim<space>qui<space>a<space>profité<space>de<space>ce<space>que<space>le<space>livreur<space>s'éloigne<space>pour<space>voler<space>un<space>une<space>baguette<space>pour<space>cent<space>hesitation<space>a<space>rencontré<space>donc<space>charlot<space>à<space>ce<space>moment<space>là<space>lui<space>est<space>rentré<space>de<space>dans<space>pour<space>cent<space>hesitation<space>dans<space>la<space>confusion<space>'<space>qu'une<space>pour<space>cent<space>hesi<space>ation<space>une<space>euh<space>assant<space>à<space>dénonce<space>la<space>jeune<space>fille<space>a<space>livreur<space>qui<space>acourue<space>après<space>la<space>jeune<space>fille

2026-01-29 16:49:01,795 | INFO | speech length: 277120
2026-01-29 16:49:01,824 | INFO | decoder input length: 432
2026-01-29 16:49:01,824 | INFO | max output length: 432
2026-01-29 16:49:01,824 | INFO | min output length: 43
2026-01-29 16:49:20,480 | INFO | end detected at 320
2026-01-29 16:49:20,481 | INFO | -38.64 * 0.5 = -19.32 for decoder
2026-01-29 16:49:20,481 | INFO | -33.61 * 0.5 = -16.80 for ctc
2026-01-29 16:49:20,481 | INFO | total log probability: -36.12
2026-01-29 16:49:20,481 | INFO | normalized log probability: -0.12
2026-01-29 16:49:20,481 | INFO | total number of ended hypotheses: 171
2026-01-29 16:49:20,485 | INFO | best hypo: pour<space>cent<space>hesitation<space>les<space>policiers<space>sont<space>arrivés<space>en<space>raison<space>du<space>du<space>du<space>vacame<space>je<space>pense<space>et<space>euh<space>charlot<space>c'est<space>accusé<space>plutôt<space>que<space>de<space>laisser<space>la<space>jeune<space>fille<space>s'accuser<space>et<space>pour<space>cent<space>hesitation<space>ça<space>s'arrête<space>lorsque<space>la<space>passante<space>euh<space>indique<space>aux<space>livreurs<space>que<space>c'était<space>bien<space>la<space>jeune<space>fille<space>qui<space>avait<space>volé<space>le<space>pain<space>et<space>pas<space>le<space>jeune

2026-01-29 16:49:20,495 | INFO | Chunk: 0 | WER=30.136986 | S=10 D=3 I=9
2026-01-29 16:49:20,497 | INFO | Chunk: 1 | WER=20.689655 | S=6 D=2 I=4
2026-01-29 16:49:20,506 | INFO | File: Rhap-M0024.wav | WER=29.411765 | S=13 D=1 I=21
2026-01-29 16:49:20,506 | INFO | ------------------------------
2026-01-29 16:49:20,506 | INFO | Conf ester Done!
2026-01-29 16:49:43,090 | INFO | Chunk: 0 | WER=19.178082 | S=7 D=6 I=1
2026-01-29 16:49:43,094 | INFO | Chunk: 1 | WER=17.241379 | S=3 D=7 I=0
2026-01-29 16:49:43,103 | INFO | File: Rhap-M0024.wav | WER=13.445378 | S=12 D=2 I=2
2026-01-29 16:49:43,103 | INFO | ------------------------------
2026-01-29 16:49:43,103 | INFO | hmm_tdnn Done!
2026-01-29 16:49:43,260 | INFO | ==================================Rhap-M1001.wav=========================================
2026-01-29 16:49:43,277 | INFO | Using rVAD model
2026-01-29 16:49:48,320 | INFO | Chunk: 0 | WER=31.884058 | S=5 D=17 I=0
2026-01-29 16:49:48,321 | INFO | Chunk: 1 | WER=25.000000 | S=1 D=4 I=1
2026-01-29 16:49:48,323 | INFO | Chunk: 2 | WER=39.062500 | S=13 D=12 I=0
2026-01-29 16:49:48,325 | INFO | Chunk: 3 | WER=18.309859 | S=3 D=10 I=0
2026-01-29 16:49:48,329 | INFO | Chunk: 4 | WER=29.032258 | S=9 D=17 I=1
2026-01-29 16:49:48,331 | INFO | Chunk: 5 | WER=35.483871 | S=9 D=13 I=0
2026-01-29 16:49:48,377 | INFO | File: Rhap-M1001.wav | WER=23.782235 | S=38 D=41 I=4
2026-01-29 16:49:48,377 | INFO | ------------------------------
2026-01-29 16:49:48,377 | INFO | w2vec vad chunk Done!
2026-01-29 16:49:59,031 | INFO | Chunk: 0 | WER=40.579710 | S=4 D=23 I=1
2026-01-29 16:49:59,032 | INFO | Chunk: 1 | WER=29.166667 | S=4 D=3 I=0
2026-01-29 16:49:59,033 | INFO | Chunk: 2 | WER=64.062500 | S=5 D=36 I=0
2026-01-29 16:49:59,035 | INFO | Chunk: 3 | WER=40.845070 | S=1 D=28 I=0
2026-01-29 16:49:59,037 | INFO | Chunk: 4 | WER=84.946237 | S=11 D=67 I=1
2026-01-29 16:49:59,038 | INFO | Chunk: 5 | WER=40.322581 | S=9 D=16 I=0
2026-01-29 16:49:59,070 | INFO | File: Rhap-M1001.wav | WER=49.570201 | S=34 D=138 I=1
2026-01-29 16:49:59,071 | INFO | ------------------------------
2026-01-29 16:49:59,071 | INFO | whisper med Done!
2026-01-29 16:50:15,820 | INFO | Chunk: 0 | WER=40.579710 | S=1 D=27 I=0
2026-01-29 16:50:15,821 | INFO | Chunk: 1 | WER=29.166667 | S=4 D=3 I=0
2026-01-29 16:50:15,823 | INFO | Chunk: 2 | WER=31.250000 | S=5 D=13 I=2
2026-01-29 16:50:15,825 | INFO | Chunk: 3 | WER=45.070423 | S=1 D=31 I=0
2026-01-29 16:50:15,828 | INFO | Chunk: 4 | WER=48.387097 | S=12 D=31 I=2
2026-01-29 16:50:15,829 | INFO | Chunk: 5 | WER=33.870968 | S=4 D=17 I=0
2026-01-29 16:50:15,868 | INFO | File: Rhap-M1001.wav | WER=33.810888 | S=26 D=88 I=4
2026-01-29 16:50:15,868 | INFO | ------------------------------
2026-01-29 16:50:15,868 | INFO | whisper large Done!
2026-01-29 16:50:16,099 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-29 16:50:16,129 | INFO | Vocabulary size: 350
2026-01-29 16:50:16,727 | INFO | Gradient checkpoint layers: []
2026-01-29 16:50:17,376 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 16:50:17,379 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 16:50:17,379 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 16:50:17,379 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-29 16:50:17,380 | INFO | speech length: 259200
2026-01-29 16:50:17,420 | INFO | decoder input length: 404
2026-01-29 16:50:17,420 | INFO | max output length: 404
2026-01-29 16:50:17,420 | INFO | min output length: 40
2026-01-29 16:50:27,184 | INFO | end detected at 145
2026-01-29 16:50:27,185 | INFO | -240.51 * 0.5 = -120.25 for decoder
2026-01-29 16:50:27,185 | INFO | -104.09 * 0.5 = -52.05 for ctc
2026-01-29 16:50:27,185 | INFO | total log probability: -172.30
2026-01-29 16:50:27,185 | INFO | normalized log probability: -1.26
2026-01-29 16:50:27,185 | INFO | total number of ended hypotheses: 180
2026-01-29 16:50:27,187 | INFO | best hypo: ▁elle▁a▁leur▁rangement▁avec▁clarage▁et▁dix▁neuf▁ans▁j'ai▁eu▁l'obtention▁d'un▁bac▁glad▁emerner▁simbeck▁et▁sa▁messe▁donc▁technologique▁ses▁sciences▁médico▁sociales▁s'anarrêt▁d'avoir▁avec▁eux▁la▁littérature▁et▁pres▁qu'en▁faiteugène▁la▁biologie▁et▁plus▁se▁ver▁la▁biologie▁le▁sociale

2026-01-29 16:50:27,190 | INFO | speech length: 89600
2026-01-29 16:50:27,216 | INFO | decoder input length: 139
2026-01-29 16:50:27,216 | INFO | max output length: 139
2026-01-29 16:50:27,216 | INFO | min output length: 13
2026-01-29 16:50:29,518 | INFO | end detected at 56
2026-01-29 16:50:29,519 | INFO | -13.78 * 0.5 =  -6.89 for decoder
2026-01-29 16:50:29,519 | INFO | -17.12 * 0.5 =  -8.56 for ctc
2026-01-29 16:50:29,519 | INFO | total log probability: -15.45
2026-01-29 16:50:29,519 | INFO | normalized log probability: -0.32
2026-01-29 16:50:29,519 | INFO | total number of ended hypotheses: 185
2026-01-29 16:50:29,520 | INFO | best hypo: ▁le▁sens▁médico▁social▁est▁donc▁en▁fait▁ji▁choisi▁italien▁en▁deuxième▁chambre▁au▁premier▁choix▁cette▁psychologie

2026-01-29 16:50:29,522 | INFO | speech length: 194880
2026-01-29 16:50:29,556 | INFO | decoder input length: 304
2026-01-29 16:50:29,556 | INFO | max output length: 304
2026-01-29 16:50:29,556 | INFO | min output length: 30
2026-01-29 16:50:36,643 | INFO | end detected at 125
2026-01-29 16:50:36,644 | INFO | -137.50 * 0.5 = -68.75 for decoder
2026-01-29 16:50:36,644 | INFO | -81.54 * 0.5 = -40.77 for ctc
2026-01-29 16:50:36,644 | INFO | total log probability: -109.52
2026-01-29 16:50:36,644 | INFO | normalized log probability: -0.92
2026-01-29 16:50:36,644 | INFO | total number of ended hypotheses: 169
2026-01-29 16:50:36,646 | INFO | best hypo: ▁et▁en▁fait▁j'ai▁a▁été▁accepté▁pascal▁avant▁d'entretien▁aurai▁je▁le▁savais▁bain▁ou▁que▁fetterola▁c'est▁cette▁province▁stupide▁dont▁jej'ai▁pris▁tali▁un▁deuxième▁choi▁à▁ce▁que▁aussi▁j'ai▁dépai▁italien▁je▁suis▁bilin▁je▁parle▁l'italien▁je▁le▁comprends

2026-01-29 16:50:36,648 | INFO | speech length: 275360
2026-01-29 16:50:36,700 | INFO | decoder input length: 429
2026-01-29 16:50:36,700 | INFO | max output length: 429
2026-01-29 16:50:36,700 | INFO | min output length: 42
2026-01-29 16:50:46,234 | INFO | end detected at 129
2026-01-29 16:50:46,235 | INFO | -149.11 * 0.5 = -74.56 for decoder
2026-01-29 16:50:46,236 | INFO | -47.88 * 0.5 = -23.94 for ctc
2026-01-29 16:50:46,236 | INFO | total log probability: -98.50
2026-01-29 16:50:46,236 | INFO | normalized log probability: -0.80
2026-01-29 16:50:46,236 | INFO | total number of ended hypotheses: 155
2026-01-29 16:50:46,237 | INFO | best hypo: ▁et▁donc▁en▁fait▁bah▁j'aime▁beaucoup▁l'italien▁là▁et▁j'apprécie▁vraiment▁ce▁que▁je▁finis▁m'en▁faite▁je▁pense▁que▁je▁vais▁être▁me▁réorienter▁si▁à▁la▁fin▁d'année▁si▁je▁vais▁bien▁finir▁l'année▁là▁un▁an▁et▁si▁je▁réussi▁pas▁je▁pense▁que▁je▁vais▁en▁psychologie

2026-01-29 16:50:46,241 | INFO | speech length: 298560
2026-01-29 16:50:46,291 | INFO | decoder input length: 466
2026-01-29 16:50:46,291 | INFO | max output length: 466
2026-01-29 16:50:46,291 | INFO | min output length: 46
2026-01-29 16:50:59,903 | INFO | end detected at 190
2026-01-29 16:50:59,905 | INFO | -353.32 * 0.5 = -176.66 for decoder
2026-01-29 16:50:59,905 | INFO | -158.46 * 0.5 = -79.23 for ctc
2026-01-29 16:50:59,905 | INFO | total log probability: -255.89
2026-01-29 16:50:59,906 | INFO | normalized log probability: -1.39
2026-01-29 16:50:59,906 | INFO | total number of ended hypotheses: 165
2026-01-29 16:50:59,908 | INFO | best hypo: ▁est▁ce▁que▁c'est▁plus▁que▁je▁veux▁faire▁mapsco▁et▁si▁je▁passe▁l'année▁par▁constance▁ce▁que▁j'ai▁fini▁à▁ma▁licence▁si▁je▁ne▁passe▁passerai▁un▁deuxième▁annéeier▁en▁troisième▁année▁chiniam▁finirai▁ma▁discence▁pour▁avoir▁ma▁licence▁mais▁si▁j'apase▁pas▁fini▁je▁n'ai▁j'ai▁j'à▁priseaucou▁de▁chos▁et▁très▁intéressantes▁mais▁je▁pense▁ce▁que▁je▁me▁renier▁entre▁en▁psychologique▁parce▁que▁ça▁m'intéresse▁plus

2026-01-29 16:50:59,912 | INFO | speech length: 216160
2026-01-29 16:50:59,970 | INFO | decoder input length: 337
2026-01-29 16:50:59,970 | INFO | max output length: 337
2026-01-29 16:50:59,970 | INFO | min output length: 33
2026-01-29 16:51:07,081 | INFO | end detected at 117
2026-01-29 16:51:07,082 | INFO | -140.63 * 0.5 = -70.31 for decoder
2026-01-29 16:51:07,082 | INFO | -37.61 * 0.5 = -18.81 for ctc
2026-01-29 16:51:07,082 | INFO | total log probability: -89.12
2026-01-29 16:51:07,082 | INFO | normalized log probability: -0.80
2026-01-29 16:51:07,082 | INFO | total number of ended hypotheses: 169
2026-01-29 16:51:07,084 | INFO | best hypo: ▁je▁suis▁plus▁vent▁on▁dit▁que▁tu▁à▁l'écoute▁des▁personnes▁tu▁comprends▁aussi▁et▁que▁ventement▁me▁dit▁que▁je▁suis▁plus▁ce▁faite▁pour▁eux▁pour▁ça▁mettez▁là▁ces▁études▁là▁que▁pour▁l'italien▁tout▁que▁je▁pense▁que▁je▁vais▁je▁vais▁surprendre▁rientée▁en▁psychologie

2026-01-29 16:51:07,092 | INFO | Chunk: 0 | WER=63.768116 | S=26 D=17 I=1
2026-01-29 16:51:07,092 | INFO | Chunk: 1 | WER=54.166667 | S=8 D=5 I=0
2026-01-29 16:51:07,094 | INFO | Chunk: 2 | WER=54.687500 | S=20 D=14 I=1
2026-01-29 16:51:07,096 | INFO | Chunk: 3 | WER=29.577465 | S=6 D=14 I=1
2026-01-29 16:51:07,100 | INFO | Chunk: 4 | WER=48.387097 | S=22 D=17 I=6
2026-01-29 16:51:07,102 | INFO | Chunk: 5 | WER=37.096774 | S=14 D=9 I=0
2026-01-29 16:51:07,160 | INFO | File: Rhap-M1001.wav | WER=43.839542 | S=90 D=48 I=15
2026-01-29 16:51:07,161 | INFO | ------------------------------
2026-01-29 16:51:07,161 | INFO | Conf cv Done!
2026-01-29 16:51:07,427 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-29 16:51:07,446 | INFO | Vocabulary size: 47
2026-01-29 16:51:08,089 | INFO | Gradient checkpoint layers: []
2026-01-29 16:51:08,768 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 16:51:08,772 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 16:51:08,772 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 16:51:08,773 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-29 16:51:08,776 | INFO | speech length: 259200
2026-01-29 16:51:08,806 | INFO | decoder input length: 404
2026-01-29 16:51:08,806 | INFO | max output length: 404
2026-01-29 16:51:08,806 | INFO | min output length: 40
2026-01-29 16:51:25,747 | INFO | end detected at 302
2026-01-29 16:51:25,750 | INFO | -49.22 * 0.5 = -24.61 for decoder
2026-01-29 16:51:25,750 | INFO | -54.40 * 0.5 = -27.20 for ctc
2026-01-29 16:51:25,750 | INFO | total log probability: -51.81
2026-01-29 16:51:25,750 | INFO | normalized log probability: -0.18
2026-01-29 16:51:25,750 | INFO | total number of ended hypotheses: 209
2026-01-29 16:51:25,753 | INFO | best hypo: alors<space>je<space>m'appelle<space>éclarage<space>de<space>dix<space>neuf<space>ans<space>j'ai<space>eu<space>le<space>petit<space>sous<space>mon<space>bac<space>l'année<space>dernière<space>son<space>bac<space>euh<space>sms<space>donc<space>technologie<space>et<space>sciences<space>médicaux<space>sociales<space>ça<space>n'a<space>rien<space>à<space>voir<space>avec<space>la<space>littérature<space>parce<space>qu'en<space>fait<space>euh<space>j'aime<space>la<space>biologie<space>et<space>je<space>plus<space>euh<space>par<space>la<space>biologie<space>et<space>euh<space>le<space>social<space>donc<space>euh

2026-01-29 16:51:25,757 | INFO | speech length: 89600
2026-01-29 16:51:25,793 | INFO | decoder input length: 139
2026-01-29 16:51:25,794 | INFO | max output length: 139
2026-01-29 16:51:25,794 | INFO | min output length: 13
2026-01-29 16:51:30,340 | INFO | end detected at 131
2026-01-29 16:51:30,342 | INFO | -14.90 * 0.5 =  -7.45 for decoder
2026-01-29 16:51:30,342 | INFO | -15.97 * 0.5 =  -7.98 for ctc
2026-01-29 16:51:30,342 | INFO | total log probability: -15.44
2026-01-29 16:51:30,342 | INFO | normalized log probability: -0.13
2026-01-29 16:51:30,342 | INFO | total number of ended hypotheses: 196
2026-01-29 16:51:30,343 | INFO | best hypo: ouais<space>sens<space>médico<space>social<space>et<space>donc<space>en<space>fait<space>euh<space>j'ai<space>choisi<space>talie<space>en<space>deuxième<space>fois<space>mon<space>premier<space>choix<space>c'était<space>euh<space>psychologie

2026-01-29 16:51:30,346 | INFO | speech length: 194880
2026-01-29 16:51:30,378 | INFO | decoder input length: 304
2026-01-29 16:51:30,378 | INFO | max output length: 304
2026-01-29 16:51:30,378 | INFO | min output length: 30
2026-01-29 16:51:43,427 | INFO | end detected at 291
2026-01-29 16:51:43,429 | INFO | -49.03 * 0.5 = -24.52 for decoder
2026-01-29 16:51:43,429 | INFO | -99.64 * 0.5 = -49.82 for ctc
2026-01-29 16:51:43,429 | INFO | total log probability: -74.34
2026-01-29 16:51:43,429 | INFO | normalized log probability: -0.26
2026-01-29 16:51:43,429 | INFO | total number of ended hypotheses: 214
2026-01-29 16:51:43,432 | INFO | best hypo: et<space>en<space>fait<space>euh<space>j'ai<space>pas<space>été<space>accepté<space>parce<space>qu'<space>y<space>avait<space>un<space>entretien<space>oral<space>et<space>je<space>le<space>savais<space>pas<space>donc<space>euh<space>fait<space>voilà<space>c'est<space>c'est<space>trop<space>vince<space>je<space>suis<space>donc<space>j'ai<space>pris<space>palé<space>en<space>deuxième<space>fois<space>parce<space>que<space>aussi<space>euh<space>je<space>dis<space>pas<space>ce<space>lit<space>en<space>italien<space>je<space>je<space>suis<space>non<space>je<space>parle<space>euh<space>italien<space>je<space>le<space>comprends

2026-01-29 16:51:43,435 | INFO | speech length: 275360
2026-01-29 16:51:43,466 | INFO | decoder input length: 429
2026-01-29 16:51:43,466 | INFO | max output length: 429
2026-01-29 16:51:43,466 | INFO | min output length: 42
2026-01-29 16:52:02,041 | INFO | end detected at 316
2026-01-29 16:52:02,044 | INFO | -40.35 * 0.5 = -20.18 for decoder
2026-01-29 16:52:02,044 | INFO | -12.34 * 0.5 =  -6.17 for ctc
2026-01-29 16:52:02,044 | INFO | total log probability: -26.35
2026-01-29 16:52:02,044 | INFO | normalized log probability: -0.09
2026-01-29 16:52:02,044 | INFO | total number of ended hypotheses: 225
2026-01-29 16:52:02,047 | INFO | best hypo: et<space>donc<space>euh<space>en<space>fait<space>euh<space>ben<space>j'aime<space>beaucoup<space>l'italien<space>là<space>euh<space>et<space>j'apprécie<space>vraiment<space>ce<space>que<space>je<space>fais<space>mais<space>en<space>fait<space>euh<space>je<space>pense<space>que<space>je<space>vais<space>peut<space>être<space>me<space>réorienter<space>euh<space>si<space>à<space>la<space>fin<space>de<space>l'année<space>si<space>euh<space>fin<space>je<space>vais<space>finir<space>l'année<space>là<space>euh<space>en<space>un<space>an<space>et<space>euh<space>si<space>euh<space>je<space>réussis<space>pas<space>ben<space>je<space>pense<space>que<space>je<space>vais<space>en<space>psychologie

2026-01-29 16:52:02,051 | INFO | speech length: 298560
2026-01-29 16:52:02,081 | INFO | decoder input length: 466
2026-01-29 16:52:02,081 | INFO | max output length: 466
2026-01-29 16:52:02,081 | INFO | min output length: 46
2026-01-29 16:52:26,532 | INFO | end detected at 428
2026-01-29 16:52:26,533 | INFO | -138.45 * 0.5 = -69.23 for decoder
2026-01-29 16:52:26,534 | INFO | -141.03 * 0.5 = -70.51 for ctc
2026-01-29 16:52:26,534 | INFO | total log probability: -139.74
2026-01-29 16:52:26,534 | INFO | normalized log probability: -0.33
2026-01-29 16:52:26,534 | INFO | total number of ended hypotheses: 188
2026-01-29 16:52:26,538 | INFO | best hypo: parce<space>que<space>c'est<space>plus<space>euh<space>ce<space>que<space>je<space>veux<space>faire<space>ma<space>psycho<space>et<space>euh<space>si<space>je<space>si<space>je<space>passe<space>'année<space>par<space>contre<space>je<space>pense<space>que<space>je<space>fais<space>finir<space>ma<space>lescence<space>je<space>passe<space>je<space>passe<space>en<space>deuxième<space>et<space>demi<space>en<space>troisième<space>année<space>je<space>finis<space>à<space>finirais<space>ma<space>licence<space>pour<space>avoir<space>mal<space>sens<space>mais<space>si<space>je<space>passe<space>pas<space>fin<space>je<space>c'est<space>j'ai<space>beau<space>j'appris<space>beaucop<space>de<space>chose<space>et<space>euh<space>c'est<space>très<space>intéressant<space>mais<space>je<space>pense<space>que<space>je<space>me<space>rendse<space>en<space>psychologie<space>parce<space>que<space>ça<space>m'intéresse<space>plus

2026-01-29 16:52:26,541 | INFO | speech length: 216160
2026-01-29 16:52:26,572 | INFO | decoder input length: 337
2026-01-29 16:52:26,572 | INFO | max output length: 337
2026-01-29 16:52:26,572 | INFO | min output length: 33
2026-01-29 16:52:41,273 | INFO | end detected at 293
2026-01-29 16:52:41,276 | INFO | -35.64 * 0.5 = -17.82 for decoder
2026-01-29 16:52:41,276 | INFO | -26.13 * 0.5 = -13.07 for ctc
2026-01-29 16:52:41,276 | INFO | total log probability: -30.89
2026-01-29 16:52:41,276 | INFO | normalized log probability: -0.11
2026-01-29 16:52:41,276 | INFO | total number of ended hypotheses: 233
2026-01-29 16:52:41,280 | INFO | best hypo: je<space>suis<space>plus<space>euh<space>fin<space>on<space>dit<space>que<space>je<space>suis<space>à<space>l'écoute<space>des<space>personnes<space>je<space>comprends<space>aussi<space>et<space>que<space>quand<space>tout<space>le<space>monde<space>me<space>dit<space>que<space>je<space>suis<space>plus<space>euh<space>faite<space>pour<space>euh<space>pour<space>ce<space>métier<space>là<space>ces<space>études<space>là<space>que<space>pour<space>l'italien<space>donc<space>euh<space>je<space>pense<space>que<space>j'ai<space>euh<space>je<space>vais<space>sûrement<space>rereurter<space>en<space>psychologie

2026-01-29 16:52:41,291 | INFO | Chunk: 0 | WER=28.985507 | S=10 D=9 I=1
2026-01-29 16:52:41,291 | INFO | Chunk: 1 | WER=25.000000 | S=5 D=1 I=0
2026-01-29 16:52:41,294 | INFO | Chunk: 2 | WER=26.562500 | S=12 D=3 I=2
2026-01-29 16:52:41,296 | INFO | Chunk: 3 | WER=1.408451 | S=1 D=0 I=0
2026-01-29 16:52:41,301 | INFO | Chunk: 4 | WER=22.580645 | S=14 D=5 I=2
2026-01-29 16:52:41,303 | INFO | Chunk: 5 | WER=16.129032 | S=7 D=3 I=0
2026-01-29 16:52:41,356 | INFO | File: Rhap-M1001.wav | WER=27.507163 | S=50 D=14 I=32
2026-01-29 16:52:41,356 | INFO | ------------------------------
2026-01-29 16:52:41,356 | INFO | Conf ester Done!
2026-01-29 16:54:04,300 | INFO | Chunk: 0 | WER=47.826087 | S=8 D=25 I=0
2026-01-29 16:54:04,301 | INFO | Chunk: 1 | WER=41.666667 | S=5 D=5 I=0
2026-01-29 16:54:04,305 | INFO | Chunk: 2 | WER=48.437500 | S=16 D=14 I=1
2026-01-29 16:54:04,308 | INFO | Chunk: 3 | WER=28.169014 | S=7 D=13 I=0
2026-01-29 16:54:04,312 | INFO | Chunk: 4 | WER=43.010753 | S=19 D=19 I=2
2026-01-29 16:54:04,314 | INFO | Chunk: 5 | WER=38.709677 | S=16 D=8 I=0
2026-01-29 16:54:04,360 | INFO | File: Rhap-M1001.wav | WER=36.676218 | S=69 D=53 I=6
2026-01-29 16:54:04,360 | INFO | ------------------------------
2026-01-29 16:54:04,361 | INFO | hmm_tdnn Done!
2026-01-29 16:54:04,563 | INFO | ==================================Rhap-M1003.wav=========================================
2026-01-29 16:54:04,656 | INFO | Using rVAD model
2026-01-29 16:54:16,389 | INFO | Chunk: 0 | WER=16.000000 | S=5 D=11 I=0
2026-01-29 16:54:16,399 | INFO | Chunk: 1 | WER=17.241379 | S=5 D=10 I=0
2026-01-29 16:54:16,405 | INFO | Chunk: 2 | WER=24.324324 | S=7 D=10 I=1
2026-01-29 16:54:16,406 | INFO | Chunk: 3 | WER=21.739130 | S=1 D=4 I=0
2026-01-29 16:54:16,409 | INFO | Chunk: 4 | WER=13.333333 | S=1 D=5 I=0
2026-01-29 16:54:16,410 | INFO | Chunk: 5 | WER=100.000000 | S=1 D=0 I=0
2026-01-29 16:54:16,411 | INFO | Chunk: 6 | WER=20.689655 | S=3 D=3 I=0
2026-01-29 16:54:16,412 | INFO | Chunk: 7 | WER=20.000000 | S=1 D=0 I=0
2026-01-29 16:54:16,419 | INFO | Chunk: 8 | WER=30.487805 | S=7 D=18 I=0
2026-01-29 16:54:16,420 | INFO | Chunk: 9 | WER=41.666667 | S=1 D=9 I=0
2026-01-29 16:54:16,424 | INFO | Chunk: 10 | WER=8.928571 | S=1 D=4 I=0
2026-01-29 16:54:16,426 | INFO | Chunk: 11 | WER=13.793103 | S=2 D=2 I=0
2026-01-29 16:54:16,432 | INFO | Chunk: 12 | WER=25.000000 | S=6 D=13 I=0
2026-01-29 16:54:16,435 | INFO | Chunk: 13 | WER=2.564103 | S=0 D=1 I=0
2026-01-29 16:54:16,439 | INFO | Chunk: 14 | WER=33.750000 | S=7 D=20 I=0
2026-01-29 16:54:16,441 | INFO | Chunk: 15 | WER=16.071429 | S=5 D=4 I=0
2026-01-29 16:54:16,663 | INFO | File: Rhap-M1003.wav | WER=14.170040 | S=53 D=50 I=2
2026-01-29 16:54:16,663 | INFO | ------------------------------
2026-01-29 16:54:16,663 | INFO | w2vec vad chunk Done!
2026-01-29 16:54:43,010 | INFO | Chunk: 0 | WER=70.000000 | S=2 D=68 I=0
2026-01-29 16:54:43,012 | INFO | Chunk: 1 | WER=78.160920 | S=0 D=68 I=0
2026-01-29 16:54:43,014 | INFO | Chunk: 2 | WER=51.351351 | S=3 D=35 I=0
2026-01-29 16:54:43,014 | INFO | Chunk: 3 | WER=30.434783 | S=2 D=5 I=0
2026-01-29 16:54:43,015 | INFO | Chunk: 4 | WER=24.444444 | S=3 D=8 I=0
2026-01-29 16:54:43,015 | INFO | Chunk: 5 | WER=100.000000 | S=1 D=0 I=0
2026-01-29 16:54:43,016 | INFO | Chunk: 6 | WER=13.793103 | S=0 D=4 I=0
2026-01-29 16:54:43,016 | INFO | Chunk: 7 | WER=40.000000 | S=1 D=1 I=0
2026-01-29 16:54:43,018 | INFO | Chunk: 8 | WER=51.219512 | S=5 D=37 I=0
2026-01-29 16:54:43,019 | INFO | Chunk: 9 | WER=37.500000 | S=2 D=7 I=0
2026-01-29 16:54:43,020 | INFO | Chunk: 10 | WER=21.428571 | S=0 D=12 I=0
2026-01-29 16:54:43,021 | INFO | Chunk: 11 | WER=17.241379 | S=2 D=3 I=0
2026-01-29 16:54:43,023 | INFO | Chunk: 12 | WER=63.157895 | S=0 D=48 I=0
2026-01-29 16:54:43,024 | INFO | Chunk: 13 | WER=10.256410 | S=1 D=1 I=2
2026-01-29 16:54:43,026 | INFO | Chunk: 14 | WER=61.250000 | S=22 D=26 I=1
2026-01-29 16:54:43,028 | INFO | Chunk: 15 | WER=17.857143 | S=5 D=3 I=2
2026-01-29 16:54:43,185 | INFO | File: Rhap-M1003.wav | WER=42.780027 | S=47 D=263 I=7
2026-01-29 16:54:43,185 | INFO | ------------------------------
2026-01-29 16:54:43,185 | INFO | whisper med Done!
2026-01-29 16:55:22,403 | INFO | Chunk: 0 | WER=68.000000 | S=0 D=68 I=0
2026-01-29 16:55:22,406 | INFO | Chunk: 1 | WER=44.827586 | S=6 D=33 I=0
2026-01-29 16:55:22,408 | INFO | Chunk: 2 | WER=52.702703 | S=0 D=39 I=0
2026-01-29 16:55:22,408 | INFO | Chunk: 3 | WER=30.434783 | S=2 D=5 I=0
2026-01-29 16:55:22,409 | INFO | Chunk: 4 | WER=15.555556 | S=1 D=6 I=0
2026-01-29 16:55:22,410 | INFO | Chunk: 5 | WER=100.000000 | S=1 D=0 I=0
2026-01-29 16:55:22,410 | INFO | Chunk: 6 | WER=6.896552 | S=1 D=1 I=0
2026-01-29 16:55:22,410 | INFO | Chunk: 7 | WER=100.000000 | S=2 D=3 I=0
2026-01-29 16:55:22,413 | INFO | Chunk: 8 | WER=47.560976 | S=7 D=31 I=1
2026-01-29 16:55:22,413 | INFO | Chunk: 9 | WER=37.500000 | S=1 D=8 I=0
2026-01-29 16:55:22,415 | INFO | Chunk: 10 | WER=33.928571 | S=2 D=17 I=0
2026-01-29 16:55:22,415 | INFO | Chunk: 11 | WER=17.241379 | S=3 D=2 I=0
2026-01-29 16:55:22,418 | INFO | Chunk: 12 | WER=36.842105 | S=11 D=17 I=0
2026-01-29 16:55:22,419 | INFO | Chunk: 13 | WER=7.692308 | S=2 D=1 I=0
2026-01-29 16:55:22,422 | INFO | Chunk: 14 | WER=42.500000 | S=14 D=19 I=1
2026-01-29 16:55:22,423 | INFO | Chunk: 15 | WER=12.500000 | S=3 D=4 I=0
2026-01-29 16:55:22,602 | INFO | File: Rhap-M1003.wav | WER=33.603239 | S=54 D=191 I=4
2026-01-29 16:55:22,602 | INFO | ------------------------------
2026-01-29 16:55:22,602 | INFO | whisper large Done!
2026-01-29 16:55:22,760 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-29 16:55:22,790 | INFO | Vocabulary size: 350
2026-01-29 16:55:23,340 | INFO | Gradient checkpoint layers: []
2026-01-29 16:55:23,898 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 16:55:23,901 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 16:55:23,901 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 16:55:23,902 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-29 16:55:23,902 | INFO | speech length: 465280
2026-01-29 16:55:23,945 | INFO | decoder input length: 726
2026-01-29 16:55:23,945 | INFO | max output length: 726
2026-01-29 16:55:23,945 | INFO | min output length: 72
2026-01-29 16:55:46,487 | INFO | end detected at 227
2026-01-29 16:55:46,488 | INFO | -741.34 * 0.5 = -370.67 for decoder
2026-01-29 16:55:46,489 | INFO | -307.06 * 0.5 = -153.53 for ctc
2026-01-29 16:55:46,489 | INFO | total log probability: -524.20
2026-01-29 16:55:46,489 | INFO | normalized log probability: -2.37
2026-01-29 16:55:46,489 | INFO | total number of ended hypotheses: 163
2026-01-29 16:55:46,491 | INFO | best hypo: ▁alors▁un▁bonjour▁à▁tous▁donc▁je▁me▁présente▁de▁façon▁brève▁je▁m'appelle▁angeline▁âge▁et▁dix▁huit▁ans▁qui▁a▁obtenu▁montacle▁l'année▁dernière▁donc▁section▁économique▁et▁sociale▁lorsque▁je▁le▁réfèree▁de▁ma▁vis▁à▁devenir▁professieur▁d'italien▁à▁savoir▁certser▁donc▁enseigner▁le▁collège▁et▁ainsi▁qu'au▁lychée▁donc▁l'enseignement▁c'est▁le▁fait▁de▁le▁tranchesmettres▁et▁ses▁compéttes▁et▁ces▁elle▁acquise▁les▁chosée▁qu'onn▁acquisme▁tout▁au▁lonogre▁de▁mes▁années▁d'étude▁également▁de▁façonersonnelle

2026-01-29 16:55:46,495 | INFO | speech length: 438720
2026-01-29 16:55:46,544 | INFO | decoder input length: 685
2026-01-29 16:55:46,544 | INFO | max output length: 685
2026-01-29 16:55:46,544 | INFO | min output length: 68
2026-01-29 16:56:05,362 | INFO | end detected at 197
2026-01-29 16:56:05,364 | INFO | -441.19 * 0.5 = -220.59 for decoder
2026-01-29 16:56:05,365 | INFO | -111.68 * 0.5 = -55.84 for ctc
2026-01-29 16:56:05,365 | INFO | total log probability: -276.43
2026-01-29 16:56:05,365 | INFO | normalized log probability: -1.45
2026-01-29 16:56:05,365 | INFO | total number of ended hypotheses: 152
2026-01-29 16:56:05,367 | INFO | best hypo: ▁et▁afin▁d'offrir▁les▁clés▁et▁les▁outils▁nécessaires▁à▁la▁réussite▁à▁une▁classe▁à▁un▁groupe▁d'élèves▁par▁ailleurs▁ça▁demande▁également▁d'être▁à▁l'écoute▁un▁car▁les▁élèves▁sont▁souvent▁complexes▁et▁également▁fragiles▁il▁faut▁également▁pour▁proposer▁des▁méthodes▁pédagogiques▁avec▁qui▁visent▁à▁la▁intéresster▁dont▁on▁auditor▁dans▁sa▁classe▁pour▁parvenir▁à▁de▁très▁bons▁rés▁seultats▁et▁également▁c'est▁le▁fait▁'ain▁de▁corger▁mais▁également▁de▁réaliser▁des▁devoirs

2026-01-29 16:56:05,370 | INFO | speech length: 296800
2026-01-29 16:56:05,426 | INFO | decoder input length: 463
2026-01-29 16:56:05,427 | INFO | max output length: 463
2026-01-29 16:56:05,427 | INFO | min output length: 46
2026-01-29 16:56:17,300 | INFO | end detected at 166
2026-01-29 16:56:17,301 | INFO | -459.00 * 0.5 = -229.50 for decoder
2026-01-29 16:56:17,301 | INFO | -167.04 * 0.5 = -83.52 for ctc
2026-01-29 16:56:17,301 | INFO | total log probability: -313.02
2026-01-29 16:56:17,301 | INFO | normalized log probability: -1.94
2026-01-29 16:56:17,301 | INFO | total number of ended hypotheses: 157
2026-01-29 16:56:17,303 | INFO | best hypo: ▁donc▁par▁ailleurs▁ce▁métier▁que▁nous▁emmenies▁nous▁amener▁également▁à▁nous▁déplacer▁donc▁lors▁de▁sorties▁qui▁peuvent▁être▁plus▁ou▁moins▁importantes▁nous▁nous▁appons▁être▁p▁la▁région▁ou▁comme▁à▁l'étnranger▁en▁particulier▁avec▁lorsqu'onceigne▁une▁langued▁ce▁métier▁s'exerce'▁ou▁d'un▁privé▁ou▁dans▁un▁public▁de▁ce▁qui▁ne▁se▁concernen▁jamais▁reneigné▁dans▁un▁établissement▁public

2026-01-29 16:56:17,305 | INFO | speech length: 130400
2026-01-29 16:56:17,335 | INFO | decoder input length: 203
2026-01-29 16:56:17,335 | INFO | max output length: 203
2026-01-29 16:56:17,335 | INFO | min output length: 20
2026-01-29 16:56:19,691 | INFO | end detected at 48
2026-01-29 16:56:19,692 | INFO |  -5.22 * 0.5 =  -2.61 for decoder
2026-01-29 16:56:19,693 | INFO |  -3.83 * 0.5 =  -1.92 for ctc
2026-01-29 16:56:19,693 | INFO | total log probability: -4.53
2026-01-29 16:56:19,693 | INFO | normalized log probability: -0.10
2026-01-29 16:56:19,693 | INFO | total number of ended hypotheses: 165
2026-01-29 16:56:19,693 | INFO | best hypo: ▁donc▁c'est▁à▁peu▁près▁donc▁un▁professeur▁certifié▁à▁de▁travail▁dix▁huit▁heures▁par▁semaine▁ou▁hebdomadaire

2026-01-29 16:56:19,695 | INFO | speech length: 204000
2026-01-29 16:56:19,728 | INFO | decoder input length: 318
2026-01-29 16:56:19,729 | INFO | max output length: 318
2026-01-29 16:56:19,729 | INFO | min output length: 31
2026-01-29 16:56:25,787 | INFO | end detected at 103
2026-01-29 16:56:25,790 | INFO | -82.36 * 0.5 = -41.18 for decoder
2026-01-29 16:56:25,790 | INFO |  -9.48 * 0.5 =  -4.74 for ctc
2026-01-29 16:56:25,790 | INFO | total log probability: -45.92
2026-01-29 16:56:25,790 | INFO | normalized log probability: -0.48
2026-01-29 16:56:25,790 | INFO | total number of ended hypotheses: 213
2026-01-29 16:56:25,791 | INFO | best hypo: ▁vous▁généralisez▁une▁enquête▁métier▁qui▁a▁mis▁fortement▁en▁valeur▁le▁fait▁que▁cette▁profession▁est▁véritablement▁réservée▁aux▁personnes▁non▁seulement▁passionnées▁par▁la▁matière▁qu'ils▁enseignent▁les▁également▁par▁l'enseignement▁parce▁qu'on▁remarque

2026-01-29 16:56:25,793 | INFO | speech length: 8480
2026-01-29 16:56:25,821 | INFO | decoder input length: 12
2026-01-29 16:56:25,821 | INFO | max output length: 12
2026-01-29 16:56:25,821 | INFO | min output length: 1
2026-01-29 16:56:26,076 | INFO | end detected at 8
2026-01-29 16:56:26,077 | INFO |  -1.44 * 0.5 =  -0.72 for decoder
2026-01-29 16:56:26,077 | INFO |  -1.98 * 0.5 =  -0.99 for ctc
2026-01-29 16:56:26,077 | INFO | total log probability: -1.71
2026-01-29 16:56:26,077 | INFO | normalized log probability: -0.57
2026-01-29 16:56:26,077 | INFO | total number of ended hypotheses: 154
2026-01-29 16:56:26,077 | INFO | best hypo: ▁un

2026-01-29 16:56:26,078 | INFO | speech length: 137920
2026-01-29 16:56:26,108 | INFO | decoder input length: 215
2026-01-29 16:56:26,108 | INFO | max output length: 215
2026-01-29 16:56:26,108 | INFO | min output length: 21
2026-01-29 16:56:29,569 | INFO | end detected at 65
2026-01-29 16:56:29,570 | INFO |  -8.30 * 0.5 =  -4.15 for decoder
2026-01-29 16:56:29,570 | INFO |  -4.21 * 0.5 =  -2.11 for ctc
2026-01-29 16:56:29,570 | INFO | total log probability: -6.26
2026-01-29 16:56:29,570 | INFO | normalized log probability: -0.10
2026-01-29 16:56:29,570 | INFO | total number of ended hypotheses: 137
2026-01-29 16:56:29,571 | INFO | best hypo: ▁on▁remarque▁bien▁que▁le▁salaire▁n'est▁ni▁en▁adéquation▁avec▁le▁nombre▁d'années▁avec▁le▁nombre▁d'études▁suivies▁ni▁avec▁le▁travail▁a▁fourni

2026-01-29 16:56:29,572 | INFO | speech length: 19840
2026-01-29 16:56:29,605 | INFO | decoder input length: 30
2026-01-29 16:56:29,605 | INFO | max output length: 30
2026-01-29 16:56:29,605 | INFO | min output length: 3
2026-01-29 16:56:30,277 | INFO | end detected at 21
2026-01-29 16:56:30,278 | INFO |  -1.14 * 0.5 =  -0.57 for decoder
2026-01-29 16:56:30,278 | INFO |  -0.93 * 0.5 =  -0.46 for ctc
2026-01-29 16:56:30,278 | INFO | total log probability: -1.04
2026-01-29 16:56:30,278 | INFO | normalized log probability: -0.06
2026-01-29 16:56:30,278 | INFO | total number of ended hypotheses: 166
2026-01-29 16:56:30,278 | INFO | best hypo: ▁personnel▁que▁l'on▁fournit

2026-01-29 16:56:30,280 | INFO | speech length: 417600
2026-01-29 16:56:30,313 | INFO | decoder input length: 652
2026-01-29 16:56:30,313 | INFO | max output length: 652
2026-01-29 16:56:30,313 | INFO | min output length: 65
2026-01-29 16:56:48,742 | INFO | end detected at 199
2026-01-29 16:56:48,744 | INFO | -706.64 * 0.5 = -353.32 for decoder
2026-01-29 16:56:48,744 | INFO | -297.36 * 0.5 = -148.68 for ctc
2026-01-29 16:56:48,744 | INFO | total log probability: -502.00
2026-01-29 16:56:48,744 | INFO | normalized log probability: -2.61
2026-01-29 16:56:48,744 | INFO | total number of ended hypotheses: 194
2026-01-29 16:56:48,747 | INFO | best hypo: ▁peuh▁d'autre▁part▁j'ai▁pu▁relever▁en▁faisant▁tous▁ces▁tests▁au▁cours▁de▁donc▁du▁semestre▁dans▁cette▁matière▁mais▁également▁j▁grâce▁à▁mon▁dossier▁hommemallah▁à▁l'éboration▁de▁mon▁projet▁comme▁je▁compreux▁no▁train▁je▁pouvais▁noter▁un▁certains▁comme▁atoupt▁indis▁pensables▁pas▁dans▁ce▁métier▁à▁avoir▁donc▁que▁je▁suis▁dynramique▁contsencieuse▁percevérante▁objectif▁et▁j▁justes▁j'aimement▁énorrmment▁m'indresct▁fe▁un▁auditoire▁et

2026-01-29 16:56:48,750 | INFO | speech length: 90240
2026-01-29 16:56:48,796 | INFO | decoder input length: 140
2026-01-29 16:56:48,796 | INFO | max output length: 140
2026-01-29 16:56:48,796 | INFO | min output length: 14
2026-01-29 16:56:50,589 | INFO | end detected at 43
2026-01-29 16:56:50,591 | INFO |  -5.61 * 0.5 =  -2.80 for decoder
2026-01-29 16:56:50,591 | INFO |  -8.19 * 0.5 =  -4.09 for ctc
2026-01-29 16:56:50,591 | INFO | total log probability: -6.90
2026-01-29 16:56:50,591 | INFO | normalized log probability: -0.18
2026-01-29 16:56:50,591 | INFO | total number of ended hypotheses: 175
2026-01-29 16:56:50,591 | INFO | best hypo: ▁et▁je▁jugeais▁je▁venger▁le▁sentiment▁de▁m'accomplir▁en▁formant▁et▁en▁aidant▁les▁autres

2026-01-29 16:56:50,593 | INFO | speech length: 254560
2026-01-29 16:56:50,627 | INFO | decoder input length: 397
2026-01-29 16:56:50,627 | INFO | max output length: 397
2026-01-29 16:56:50,627 | INFO | min output length: 39
2026-01-29 16:56:58,700 | INFO | end detected at 118
2026-01-29 16:56:58,702 | INFO | -198.29 * 0.5 = -99.14 for decoder
2026-01-29 16:56:58,702 | INFO | -90.53 * 0.5 = -45.27 for ctc
2026-01-29 16:56:58,702 | INFO | total log probability: -144.41
2026-01-29 16:56:58,702 | INFO | normalized log probability: -1.29
2026-01-29 16:56:58,702 | INFO | total number of ended hypotheses: 198
2026-01-29 16:56:58,704 | INFO | best hypo: ▁et▁donc▁on▁note▁j'ai▁le▁souci▁également▁de▁bien▁faire▁dont▁qu'on▁peut▁noter▁donc▁par▁rapport▁à▁mon▁profil▁que▁je▁mêle▁le▁côté▁conventionnel▁avec▁ce▁souci▁de▁bien▁père▁le▁fait▁d'ête▁conscienciencieuse▁et▁également▁le▁côté▁du▁soucial▁avec▁ce▁besoing▁performent▁d'changer▁avec▁autru

2026-01-29 16:56:58,707 | INFO | speech length: 130400
2026-01-29 16:56:58,762 | INFO | decoder input length: 203
2026-01-29 16:56:58,762 | INFO | max output length: 203
2026-01-29 16:56:58,762 | INFO | min output length: 20
2026-01-29 16:57:02,434 | INFO | end detected at 77
2026-01-29 16:57:02,436 | INFO |  -7.94 * 0.5 =  -3.97 for decoder
2026-01-29 16:57:02,436 | INFO |  -8.24 * 0.5 =  -4.12 for ctc
2026-01-29 16:57:02,436 | INFO | total log probability: -8.09
2026-01-29 16:57:02,436 | INFO | normalized log probability: -0.12
2026-01-29 16:57:02,436 | INFO | total number of ended hypotheses: 176
2026-01-29 16:57:02,437 | INFO | best hypo: ▁par▁ailleurs▁les▁avantages▁dans▁ce▁métier▁sont▁les▁possibilités▁d'évolution▁puisque▁on▁peut▁après▁passer▁le▁concours▁de▁lagré▁pour▁enseigner▁à▁l'université

2026-01-29 16:57:02,439 | INFO | speech length: 351680
2026-01-29 16:57:02,480 | INFO | decoder input length: 549
2026-01-29 16:57:02,480 | INFO | max output length: 549
2026-01-29 16:57:02,480 | INFO | min output length: 54
2026-01-29 16:57:15,658 | INFO | end detected at 159
2026-01-29 16:57:15,660 | INFO | -321.51 * 0.5 = -160.75 for decoder
2026-01-29 16:57:15,660 | INFO | -67.71 * 0.5 = -33.85 for ctc
2026-01-29 16:57:15,660 | INFO | total log probability: -194.61
2026-01-29 16:57:15,660 | INFO | normalized log probability: -1.27
2026-01-29 16:57:15,660 | INFO | total number of ended hypotheses: 146
2026-01-29 16:57:15,662 | INFO | best hypo: ▁d'autre▁part▁il▁ne▁faut▁pas▁se▁mentir▁les▁vacances▁sont▁nombreuses▁ce▁qui▁permet▁d'avoir▁une▁vie▁privée▁agréable▁et▁de▁construire▁également▁une▁famille▁ou▁les▁horaires▁sont▁également▁satisfaisants▁puisques▁auteures▁par▁semaine▁ce▁n'est▁pas▁le▁but▁atroces▁et▁c'▁que▁même▁si▁on▁a▁un▁programme▁suivre▁on▁les▁programmes▁sont▁établis▁ont▁ont▁ui▁d'une▁certaine▁liberté

2026-01-29 16:57:15,664 | INFO | speech length: 195680
2026-01-29 16:57:15,701 | INFO | decoder input length: 305
2026-01-29 16:57:15,701 | INFO | max output length: 305
2026-01-29 16:57:15,701 | INFO | min output length: 30
2026-01-29 16:57:21,425 | INFO | end detected at 98
2026-01-29 16:57:21,427 | INFO | -52.22 * 0.5 = -26.11 for decoder
2026-01-29 16:57:21,427 | INFO |  -7.43 * 0.5 =  -3.72 for ctc
2026-01-29 16:57:21,427 | INFO | total log probability: -29.83
2026-01-29 16:57:21,428 | INFO | normalized log probability: -0.32
2026-01-29 16:57:21,428 | INFO | total number of ended hypotheses: 185
2026-01-29 16:57:21,429 | INFO | best hypo: ▁les▁inconvénients▁sont▁par▁ailleurs▁en▁premier▁lieu▁le▁salaire▁évidemment▁qui▁n'est▁pas▁vraiment▁puisque▁un▁professeur▁certifié▁a▁commencé▁à▁mille▁quatre▁cents▁euros▁par▁mois▁et▁finira▁aux▁alentours▁de▁trois▁mille▁euros

2026-01-29 16:57:21,433 | INFO | speech length: 387040
2026-01-29 16:57:21,488 | INFO | decoder input length: 604
2026-01-29 16:57:21,488 | INFO | max output length: 604
2026-01-29 16:57:21,488 | INFO | min output length: 60
2026-01-29 16:57:36,813 | INFO | end detected at 172
2026-01-29 16:57:36,814 | INFO | -423.79 * 0.5 = -211.90 for decoder
2026-01-29 16:57:36,814 | INFO | -119.10 * 0.5 = -59.55 for ctc
2026-01-29 16:57:36,815 | INFO | total log probability: -271.44
2026-01-29 16:57:36,815 | INFO | normalized log probability: -1.65
2026-01-29 16:57:36,815 | INFO | total number of ended hypotheses: 176
2026-01-29 16:57:36,817 | INFO | best hypo: ▁en▁d'autre▁part▁le▁comportement▁des▁élèves▁semble▁au▁fil▁des▁années▁se▁dégrader▁également▁beaucoup▁moins▁concentré▁à▁mains▁de▁respect▁pour▁le▁professeur▁ce▁qui▁débalkaya▁plusieurs▁années▁le▁nombre▁de▁posttes▁diminue▁est▁également▁ce▁qui▁est▁regrettable▁d'autant▁que▁j'ai▁j'aimerais▁encoenseigner▁d'▁en▁corthe▁ou▁de▁l'amendroit▁où▁je▁suis▁né▁les▁pos▁encore▁moins▁nombreux

2026-01-29 16:57:36,819 | INFO | speech length: 194400
2026-01-29 16:57:36,853 | INFO | decoder input length: 303
2026-01-29 16:57:36,854 | INFO | max output length: 303
2026-01-29 16:57:36,854 | INFO | min output length: 30
2026-01-29 16:57:43,878 | INFO | end detected at 118
2026-01-29 16:57:43,879 | INFO | -115.25 * 0.5 = -57.62 for decoder
2026-01-29 16:57:43,880 | INFO | -26.47 * 0.5 = -13.24 for ctc
2026-01-29 16:57:43,880 | INFO | total log probability: -70.86
2026-01-29 16:57:43,880 | INFO | normalized log probability: -0.63
2026-01-29 16:57:43,880 | INFO | total number of ended hypotheses: 167
2026-01-29 16:57:43,881 | INFO | best hypo: ▁hé▁c'est▁drai▁qu'on▁est▁souvent▁éloigné▁de▁son▁domicile▁puisque▁ma▁professeur▁d'italien▁par▁exemple▁de▁l'an▁dernière▁mettait▁une▁heure▁et▁demie▁avant▁d'arriver▁d'un▁à▁son▁lycée▁et▁mamerk▁est▁également▁prof▁est▁à▁trois▁quart▁d'heure▁de▁son▁lycée▁également

2026-01-29 16:57:43,891 | INFO | Chunk: 0 | WER=44.000000 | S=28 D=13 I=3
2026-01-29 16:57:43,895 | INFO | Chunk: 1 | WER=22.988506 | S=11 D=6 I=3
2026-01-29 16:57:43,898 | INFO | Chunk: 2 | WER=37.837838 | S=18 D=8 I=2
2026-01-29 16:57:43,898 | INFO | Chunk: 3 | WER=34.782609 | S=5 D=3 I=0
2026-01-29 16:57:43,899 | INFO | Chunk: 4 | WER=20.000000 | S=3 D=6 I=0
2026-01-29 16:57:43,900 | INFO | Chunk: 5 | WER=100.000000 | S=1 D=0 I=0
2026-01-29 16:57:43,900 | INFO | Chunk: 6 | WER=6.896552 | S=1 D=1 I=0
2026-01-29 16:57:43,900 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 16:57:43,904 | INFO | Chunk: 8 | WER=43.902439 | S=22 D=10 I=4
2026-01-29 16:57:43,904 | INFO | Chunk: 9 | WER=45.833333 | S=4 D=7 I=0
2026-01-29 16:57:43,906 | INFO | Chunk: 10 | WER=26.785714 | S=9 D=4 I=2
2026-01-29 16:57:43,907 | INFO | Chunk: 11 | WER=13.793103 | S=1 D=3 I=0
2026-01-29 16:57:43,909 | INFO | Chunk: 12 | WER=25.000000 | S=10 D=9 I=0
2026-01-29 16:57:43,911 | INFO | Chunk: 13 | WER=10.256410 | S=2 D=2 I=0
2026-01-29 16:57:43,913 | INFO | Chunk: 14 | WER=40.000000 | S=14 D=16 I=2
2026-01-29 16:57:43,915 | INFO | Chunk: 15 | WER=23.214286 | S=6 D=6 I=1
2026-01-29 16:57:44,157 | INFO | File: Rhap-M1003.wav | WER=26.180837 | S=122 D=42 I=30
2026-01-29 16:57:44,157 | INFO | ------------------------------
2026-01-29 16:57:44,158 | INFO | Conf cv Done!
2026-01-29 16:57:44,288 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-29 16:57:44,307 | INFO | Vocabulary size: 47
2026-01-29 16:57:44,845 | INFO | Gradient checkpoint layers: []
2026-01-29 16:57:45,426 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 16:57:45,429 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 16:57:45,429 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 16:57:45,430 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-29 16:57:45,433 | INFO | speech length: 465280
2026-01-29 16:57:45,469 | INFO | decoder input length: 726
2026-01-29 16:57:45,469 | INFO | max output length: 726
2026-01-29 16:57:45,469 | INFO | min output length: 72
2026-01-29 16:58:29,539 | INFO | end detected at 532
2026-01-29 16:58:29,540 | INFO | -469.14 * 0.5 = -234.57 for decoder
2026-01-29 16:58:29,540 | INFO | -92.41 * 0.5 = -46.21 for ctc
2026-01-29 16:58:29,540 | INFO | total log probability: -280.78
2026-01-29 16:58:29,540 | INFO | normalized log probability: -0.53
2026-01-29 16:58:29,540 | INFO | total number of ended hypotheses: 161
2026-01-29 16:58:29,546 | INFO | best hypo: alors<space>bonjour<space>à<space>tous<space>donc<space>euh<space>je<space>me<space>présente<space>de<space>façon<space>brève<space>je<space>m'appelle<space>en<space>jeune<space>nage<space>et<space>dix<space>huit<space>ans<space>qu'est<space>ce<space>qui<space>me<space>m'en<space>bat<space>que<space>l'année<space>dernière<space>euh<space>donc<space>euh<space>section<space>économique<space>et<space>sociale<space>alors<space>ce<space>que<space>j'ai<space>préfère<space>de<space>ma<space>vie<space>c'est<space>de<space>manière<space>professeur<space>d'italiens<space>à<space>savoir<space>certifier<space>donc<space>euh<space>enseigner<space>au<space>collège<space>ainsi<space>qu'au<space>lycée<space>donc<space>euh<space>l'enseignement<space>euh<space>c'est<space>le<space>fait<space>de<space>transmettre<space>euh<space>ses<space>compétences<space>t<space>ses<space>acquis<space>des<space>choses<space>qu'on<space>a<space>acquis<space>t<space>tout<space>au<space>long<space>de<space>ces<space>années<space>d'étude<space>mais<space>également<space>de<space>façon<space>personnelle

2026-01-29 16:58:29,549 | INFO | speech length: 438720
2026-01-29 16:58:29,580 | INFO | decoder input length: 685
2026-01-29 16:58:29,580 | INFO | max output length: 685
2026-01-29 16:58:29,580 | INFO | min output length: 68
2026-01-29 16:59:08,815 | INFO | end detected at 489
2026-01-29 16:59:08,817 | INFO | -447.88 * 0.5 = -223.94 for decoder
2026-01-29 16:59:08,817 | INFO | -62.20 * 0.5 = -31.10 for ctc
2026-01-29 16:59:08,817 | INFO | total log probability: -255.04
2026-01-29 16:59:08,817 | INFO | normalized log probability: -0.53
2026-01-29 16:59:08,817 | INFO | total number of ended hypotheses: 166
2026-01-29 16:59:08,822 | INFO | best hypo: et<space>euh<space>afin<space>d'offrir<space>les<space>clés<space>tous<space>les<space>les<space>outils<space>nécessaires<space>à<space>la<space>réussite<space>à<space>une<space>classe<space>donc<space>à<space>un<space>groupe<space>d'élèves<space>par<space>ailleurs<space>ça<space>demande<space>également<space>d'être<space>à<space>l'écoute<space>car<space>les<space>élèves<space>sont<space>souvent<space>complexes<space>et<space>également<space>fragiles<space>euh<space>il<space>faut<space>également<space>proposer<space>des<space>méthodes<space>pédagogiques<space>qui<space>qui<space>visent<space>à<space>intéresser<space>euh<space>donc<space>au<space>monde<space>temps<space>dans<space>sa<space>classe<space>pour<space>parvenir<space>à<space>de<space>très<space>bonus<space>résultats<space>et<space>euh<space>également<space>c'est<space>le<space>fait<space>deuh<space>ça<space>de<space>corriger<space>mais<space>dégalement<space>de<space>réaliser<space>des<space>débor

2026-01-29 16:59:08,824 | INFO | speech length: 296800
2026-01-29 16:59:08,853 | INFO | decoder input length: 463
2026-01-29 16:59:08,853 | INFO | max output length: 463
2026-01-29 16:59:08,853 | INFO | min output length: 46
2026-01-29 16:59:31,672 | INFO | end detected at 395
2026-01-29 16:59:31,673 | INFO | -61.26 * 0.5 = -30.63 for decoder
2026-01-29 16:59:31,673 | INFO | -24.22 * 0.5 = -12.11 for ctc
2026-01-29 16:59:31,673 | INFO | total log probability: -42.74
2026-01-29 16:59:31,673 | INFO | normalized log probability: -0.11
2026-01-29 16:59:31,673 | INFO | total number of ended hypotheses: 194
2026-01-29 16:59:31,678 | INFO | best hypo: donc<space>euh<space>par<space>ailleurs<space>euh<space>ce<space>métier<space>qui<space>nous<space>emmene<space>nous<space>amène<space>également<space>à<space>nous<space>déplacer<space>donc<space>leurs<space>de<space>sortie<space>qui<space>peuvent<space>être<space>plus<space>ou<space>moins<space>importantes<space>donc<space>ça<space>peut<space>être<space>dans<space>la<space>région<space>comme<space>euh<space>à<space>l'étranger<space>et<space>en<space>particulier<space>lorsqu'on<space>enseigne<space>une<space>langue<space>donc<space>euh<space>ce<space>métier<space>s'exerce<space>ou<space>dans<space>le<space>privé<space>ou<space>dans<space>le<space>public<space>en<space>ce<space>qui<space>me<space>concerne<space>jamais<space>enseigné<space>dans<space>un<space>établissement<space>public

2026-01-29 16:59:31,680 | INFO | speech length: 130400
2026-01-29 16:59:31,709 | INFO | decoder input length: 203
2026-01-29 16:59:31,709 | INFO | max output length: 203
2026-01-29 16:59:31,709 | INFO | min output length: 20
2026-01-29 16:59:37,278 | INFO | end detected at 134
2026-01-29 16:59:37,280 | INFO | -14.67 * 0.5 =  -7.34 for decoder
2026-01-29 16:59:37,280 | INFO |  -6.93 * 0.5 =  -3.46 for ctc
2026-01-29 16:59:37,280 | INFO | total log probability: -10.80
2026-01-29 16:59:37,280 | INFO | normalized log probability: -0.09
2026-01-29 16:59:37,280 | INFO | total number of ended hypotheses: 189
2026-01-29 16:59:37,282 | INFO | best hypo: donc<space>euh<space>hum<space>c'est<space>à<space>peu<space>près<space>donc<space>un<space>professeur<space>certifié<space>euh<space>donc<space>travaille<space>dix<space>huit<space>heures<space>euh<space>par<space>semaine<space>au<space>hebdomadaire

2026-01-29 16:59:37,284 | INFO | speech length: 204000
2026-01-29 16:59:37,312 | INFO | decoder input length: 318
2026-01-29 16:59:37,312 | INFO | max output length: 318
2026-01-29 16:59:37,312 | INFO | min output length: 31
2026-01-29 16:59:50,504 | INFO | end detected at 272
2026-01-29 16:59:50,506 | INFO | -21.74 * 0.5 = -10.87 for decoder
2026-01-29 16:59:50,506 | INFO |  -4.60 * 0.5 =  -2.30 for ctc
2026-01-29 16:59:50,506 | INFO | total log probability: -13.17
2026-01-29 16:59:50,506 | INFO | normalized log probability: -0.05
2026-01-29 16:59:50,507 | INFO | total number of ended hypotheses: 196
2026-01-29 16:59:50,510 | INFO | best hypo: donc<space>j'ai<space>réalisé<space>une<space>enquête<space>métier<space>qui<space>a<space>mis<space>euh<space>fortement<space>en<space>valeur<space>le<space>fait<space>que<space>cette<space>profession<space>est<space>véritablement<space>réservée<space>aux<space>personnes<space>non<space>seulement<space>passionnées<space>par<space>la<space>matière<space>qu'ils<space>enseignent<space>mais<space>également<space>par<space>l'enseignement<space>parce<space>qu'on<space>remarque<space>euh<space>à<space>ce

2026-01-29 16:59:50,512 | INFO | speech length: 8480
2026-01-29 16:59:50,533 | INFO | decoder input length: 12
2026-01-29 16:59:50,533 | INFO | max output length: 12
2026-01-29 16:59:50,533 | INFO | min output length: 1
2026-01-29 16:59:50,845 | INFO | adding <eos> in the last position in the loop
2026-01-29 16:59:50,854 | INFO | end detected at 11
2026-01-29 16:59:50,855 | INFO |  -0.51 * 0.5 =  -0.25 for decoder
2026-01-29 16:59:50,855 | INFO |  -1.12 * 0.5 =  -0.56 for ctc
2026-01-29 16:59:50,855 | INFO | total log probability: -0.82
2026-01-29 16:59:50,855 | INFO | normalized log probability: -0.16
2026-01-29 16:59:50,855 | INFO | total number of ended hypotheses: 203
2026-01-29 16:59:50,855 | INFO | best hypo: oui

2026-01-29 16:59:50,857 | INFO | speech length: 137920
2026-01-29 16:59:50,885 | INFO | decoder input length: 215
2026-01-29 16:59:50,885 | INFO | max output length: 215
2026-01-29 16:59:50,885 | INFO | min output length: 21
2026-01-29 16:59:56,926 | INFO | end detected at 142
2026-01-29 16:59:56,928 | INFO | -12.65 * 0.5 =  -6.33 for decoder
2026-01-29 16:59:56,928 | INFO |  -8.42 * 0.5 =  -4.21 for ctc
2026-01-29 16:59:56,928 | INFO | total log probability: -10.54
2026-01-29 16:59:56,928 | INFO | normalized log probability: -0.08
2026-01-29 16:59:56,928 | INFO | total number of ended hypotheses: 177
2026-01-29 16:59:56,930 | INFO | best hypo: on<space>remarque<space>bien<space>que<space>le<space>salaire<space>mis<space>en<space>adéquation<space>avec<space>le<space>nombre<space>d'années<space>avec<space>le<space>nombre<space>d'études<space>suivies<space>mis<space>avec<space>le<space>travail<space>à<space>fournir

2026-01-29 16:59:56,932 | INFO | speech length: 19840
2026-01-29 16:59:56,959 | INFO | decoder input length: 30
2026-01-29 16:59:56,959 | INFO | max output length: 30
2026-01-29 16:59:56,959 | INFO | min output length: 3
2026-01-29 16:59:57,754 | INFO | adding <eos> in the last position in the loop
2026-01-29 16:59:57,761 | INFO | no hypothesis. Finish decoding.
2026-01-29 16:59:57,762 | INFO |  -3.65 * 0.5 =  -1.82 for decoder
2026-01-29 16:59:57,762 | INFO |  -7.95 * 0.5 =  -3.98 for ctc
2026-01-29 16:59:57,762 | INFO | total log probability: -5.80
2026-01-29 16:59:57,762 | INFO | normalized log probability: -0.21
2026-01-29 16:59:57,762 | INFO | total number of ended hypotheses: 111
2026-01-29 16:59:57,763 | INFO | best hypo: personnel<space>que<space>l'on<space>fournit

2026-01-29 16:59:57,764 | INFO | speech length: 417600
2026-01-29 16:59:57,792 | INFO | decoder input length: 652
2026-01-29 16:59:57,793 | INFO | max output length: 652
2026-01-29 16:59:57,793 | INFO | min output length: 65
2026-01-29 17:00:32,365 | INFO | end detected at 445
2026-01-29 17:00:32,367 | INFO | -445.04 * 0.5 = -222.52 for decoder
2026-01-29 17:00:32,367 | INFO | -39.90 * 0.5 = -19.95 for ctc
2026-01-29 17:00:32,367 | INFO | total log probability: -242.47
2026-01-29 17:00:32,367 | INFO | normalized log probability: -0.55
2026-01-29 17:00:32,367 | INFO | total number of ended hypotheses: 190
2026-01-29 17:00:32,372 | INFO | best hypo: euh<space>d'autre<space>part<space>j'ai<space>pu<space>relever<space>en<space>faisant<space>tous<space>ces<space>tests<space>au<space>cours<space>de<space>de<space>mon<space>de<space>du<space>semestre<space>dans<space>cette<space>matière<space>mais<space>également<space>grâce<space>à<space>mon<space>dossier<space>à<space>m<space>à<space>la<space>à<space>l'élaboration<space>de<space>mon<space>projet<space>que<space>euh<space>j'ai<space>on<space>pouvait<space>mon<space>pain<space>je<space>pouvais<space>monter<space>certains<space>euh<space>atouts<space>indispensables<space>euh<space>dans<space>ce<space>métier<space>à<space>savoir<space>euh<space>donc<space>euh<space>je<space>suis<space>dynamique<space>conscientieuse<space>euh<space>persévérante<space>objective<space>et<space>juste<space>j'aime<space>énormément<space>m'adresser<space>pa<space>ça<space>un<space>auditoire<space>et<space>euh

2026-01-29 17:00:32,374 | INFO | speech length: 90240
2026-01-29 17:00:32,403 | INFO | decoder input length: 140
2026-01-29 17:00:32,403 | INFO | max output length: 140
2026-01-29 17:00:32,403 | INFO | min output length: 14
2026-01-29 17:00:35,984 | INFO | end detected at 99
2026-01-29 17:00:35,986 | INFO | -13.02 * 0.5 =  -6.51 for decoder
2026-01-29 17:00:35,986 | INFO |  -6.30 * 0.5 =  -3.15 for ctc
2026-01-29 17:00:35,986 | INFO | total log probability: -9.66
2026-01-29 17:00:35,986 | INFO | normalized log probability: -0.11
2026-01-29 17:00:35,986 | INFO | total number of ended hypotheses: 209
2026-01-29 17:00:35,987 | INFO | best hypo: et<space>je<space>j<space>j'a<space>je<space>fin<space>j'ai<space>le<space>le<space>sentiment<space>de<space>m'accomplir<space>en<space>formant<space>et<space>en<space>aidant<space>les<space>autres

2026-01-29 17:00:35,989 | INFO | speech length: 254560
2026-01-29 17:00:36,017 | INFO | decoder input length: 397
2026-01-29 17:00:36,017 | INFO | max output length: 397
2026-01-29 17:00:36,017 | INFO | min output length: 39
2026-01-29 17:00:52,333 | INFO | end detected at 289
2026-01-29 17:00:52,334 | INFO | -31.08 * 0.5 = -15.54 for decoder
2026-01-29 17:00:52,334 | INFO | -12.32 * 0.5 =  -6.16 for ctc
2026-01-29 17:00:52,334 | INFO | total log probability: -21.70
2026-01-29 17:00:52,334 | INFO | normalized log probability: -0.08
2026-01-29 17:00:52,334 | INFO | total number of ended hypotheses: 181
2026-01-29 17:00:52,337 | INFO | best hypo: euh<space>et<space>donc<space>on<space>note<space>j'ai<space>le<space>souci<space>également<space>de<space>bien<space>faire<space>donc<space>on<space>peut<space>noter<space>donc<space>par<space>rapport<space>à<space>mon<space>profil<space>que<space>je<space>mêle<space>euh<space>le<space>côté<space>conventionnel<space>avec<space>ce<space>souci<space>de<space>bien<space>père<space>euh<space>le<space>fait<space>d'être<space>consciencieuse<space>et<space>également<space>le<space>côté<space>social<space>avec<space>ce<space>nous<space>en<space>permanent<space>d'échanger<space>avec<space>autre

2026-01-29 17:00:52,339 | INFO | speech length: 130400
2026-01-29 17:00:52,368 | INFO | decoder input length: 203
2026-01-29 17:00:52,368 | INFO | max output length: 203
2026-01-29 17:00:52,368 | INFO | min output length: 20
2026-01-29 17:00:59,359 | INFO | end detected at 172
2026-01-29 17:00:59,360 | INFO | -13.64 * 0.5 =  -6.82 for decoder
2026-01-29 17:00:59,360 | INFO |  -0.82 * 0.5 =  -0.41 for ctc
2026-01-29 17:00:59,360 | INFO | total log probability: -7.23
2026-01-29 17:00:59,360 | INFO | normalized log probability: -0.04
2026-01-29 17:00:59,360 | INFO | total number of ended hypotheses: 154
2026-01-29 17:00:59,362 | INFO | best hypo: euh<space>par<space>ailleurs<space>les<space>avantages<space>dans<space>ce<space>métier<space>sont<space>euh<space>les<space>possibilités<space>d'évolution<space>puisque<space>on<space>peut<space>après<space>passer<space>le<space>concours<space>de<space>la<space>grève<space>pour<space>enseigner<space>à<space>l'université

2026-01-29 17:00:59,364 | INFO | speech length: 351680
2026-01-29 17:00:59,394 | INFO | decoder input length: 549
2026-01-29 17:00:59,394 | INFO | max output length: 549
2026-01-29 17:00:59,394 | INFO | min output length: 54
2026-01-29 17:01:26,784 | INFO | end detected at 404
2026-01-29 17:01:26,786 | INFO | -44.94 * 0.5 = -22.47 for decoder
2026-01-29 17:01:26,787 | INFO | -18.12 * 0.5 =  -9.06 for ctc
2026-01-29 17:01:26,787 | INFO | total log probability: -31.53
2026-01-29 17:01:26,787 | INFO | normalized log probability: -0.08
2026-01-29 17:01:26,787 | INFO | total number of ended hypotheses: 182
2026-01-29 17:01:26,791 | INFO | best hypo: euh<space>d'autre<space>part<space>il<space>ne<space>faut<space>pas<space>se<space>mentir<space>les<space>vacances<space>sont<space>nombreuses<space>ce<space>qui<space>permet<space>euh<space>d'avoir<space>une<space>vie<space>privée<space>agréable<space>et<space>de<space>construire<space>également<space>une<space>famille<space>euh<space>les<space>horaires<space>sont<space>également<space>satisfaisants<space>puisque<space>dix<space>heures<space>par<space>semaine<space>ce<space>n'est<space>pas<space>même<space>vu<space>atroce<space>et<space>euh<space>hum<space>c'est<space>vrai<space>que<space>même<space>si<space>on<space>a<space>un<space>programme<space>à<space>suivre<space>donc<space>les<space>programmes<space>sont<space>établis<space>euh<space>on<space>on<space>jouit<space>une<space>certaine<space>liberté

2026-01-29 17:01:26,795 | INFO | speech length: 195680
2026-01-29 17:01:26,836 | INFO | decoder input length: 305
2026-01-29 17:01:26,836 | INFO | max output length: 305
2026-01-29 17:01:26,836 | INFO | min output length: 30
2026-01-29 17:01:38,413 | INFO | end detected at 237
2026-01-29 17:01:38,414 | INFO | -18.83 * 0.5 =  -9.42 for decoder
2026-01-29 17:01:38,415 | INFO |  -5.15 * 0.5 =  -2.57 for ctc
2026-01-29 17:01:38,415 | INFO | total log probability: -11.99
2026-01-29 17:01:38,415 | INFO | normalized log probability: -0.05
2026-01-29 17:01:38,415 | INFO | total number of ended hypotheses: 188
2026-01-29 17:01:38,417 | INFO | best hypo: les<space>inconvénients<space>sont<space>en<space>par<space>ailleurs<space>en<space>premier<space>lieu<space>le<space>salaire<space>évidemment<space>qui<space>n'est<space>pas<space>vraiment<space>haut<space>puisque<space>un<space>professeur<space>certifié<space>va<space>commencer<space>à<space>mille<space>quatre<space>cents<space>euros<space>par<space>mois<space>et<space>finira<space>aux<space>alentours<space>de<space>trois<space>mille<space>euros

2026-01-29 17:01:38,420 | INFO | speech length: 387040
2026-01-29 17:01:38,450 | INFO | decoder input length: 604
2026-01-29 17:01:38,450 | INFO | max output length: 604
2026-01-29 17:01:38,450 | INFO | min output length: 60
2026-01-29 17:02:08,611 | INFO | end detected at 410
2026-01-29 17:02:08,612 | INFO | -97.37 * 0.5 = -48.69 for decoder
2026-01-29 17:02:08,612 | INFO | -24.42 * 0.5 = -12.21 for ctc
2026-01-29 17:02:08,612 | INFO | total log probability: -60.90
2026-01-29 17:02:08,612 | INFO | normalized log probability: -0.15
2026-01-29 17:02:08,612 | INFO | total number of ended hypotheses: 67
2026-01-29 17:02:08,617 | INFO | best hypo: euh<space>d'autre<space>part<space>le<space>comportement<space>des<space>élèves<space>semble<space>au<space>fil<space>des<space>années<space>se<space>dégrader<space>également<space>donc<space>beaucoup<space>moins<space>concentré<space>euh<space>mon<space>respect<space>pour<space>le<space>professeur<space>ce<space>qui<space>n'était<space>pas<space>le<space>cas<space>y<space>a<space>plusieurs<space>années<space>euh<space>le<space>nombre<space>de<space>postes<space>diminue<space>également<space>ce<space>qui<space>est<space>regrettable<space>euh<space>d'autant<space>plus<space>que<space>j'ai<space>j'aimerais<space>enseigner<space>donc<space>euh<space>en<space>corse<space>l'endroit<space>où<space>je<space>suis<space>né<space>euh<space>les<space>les<space>postes<space>donc<space>en<space>moins<space>nombreux

2026-01-29 17:02:08,619 | INFO | speech length: 194400
2026-01-29 17:02:08,648 | INFO | decoder input length: 303
2026-01-29 17:02:08,648 | INFO | max output length: 303
2026-01-29 17:02:08,648 | INFO | min output length: 30
2026-01-29 17:02:21,739 | INFO | end detected at 288
2026-01-29 17:02:21,741 | INFO | -35.24 * 0.5 = -17.62 for decoder
2026-01-29 17:02:21,741 | INFO | -42.64 * 0.5 = -21.32 for ctc
2026-01-29 17:02:21,741 | INFO | total log probability: -38.94
2026-01-29 17:02:21,742 | INFO | normalized log probability: -0.14
2026-01-29 17:02:21,742 | INFO | total number of ended hypotheses: 199
2026-01-29 17:02:21,745 | INFO | best hypo: et<space>euh<space>c'est<space>vrai<space>qu'on<space>est<space>souvent<space>et<space>l'année<space>de<space>son<space>domicile<space>puisque<space>ma<space>professeur<space>d'italien<space>par<space>exemple<space>l'année<space>dernière<space>euh<space>mettait<space>une<space>heure<space>et<space>demie<space>avant<space>l'arrivée<space>donc<space>à<space>son<space>lycée<space>et<space>ma<space>mère<space>qui<space>est<space>également<space>le<space>prof<space>est<space>à<space>trois<space>quart<space>d'heure<space>de<space>de<space>son<space>idée<space>également

2026-01-29 17:02:21,758 | INFO | Chunk: 0 | WER=27.000000 | S=19 D=1 I=7
2026-01-29 17:02:21,762 | INFO | Chunk: 1 | WER=12.643678 | S=8 D=1 I=2
2026-01-29 17:02:21,765 | INFO | Chunk: 2 | WER=12.162162 | S=7 D=2 I=0
2026-01-29 17:02:21,765 | INFO | Chunk: 3 | WER=13.043478 | S=3 D=0 I=0
2026-01-29 17:02:21,767 | INFO | Chunk: 4 | WER=6.666667 | S=1 D=1 I=1
2026-01-29 17:02:21,767 | INFO | Chunk: 5 | WER=100.000000 | S=1 D=0 I=0
2026-01-29 17:02:21,767 | INFO | Chunk: 6 | WER=17.241379 | S=2 D=3 I=0
2026-01-29 17:02:21,768 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:02:21,771 | INFO | Chunk: 8 | WER=13.414634 | S=9 D=0 I=2
2026-01-29 17:02:21,772 | INFO | Chunk: 9 | WER=25.000000 | S=4 D=2 I=0
2026-01-29 17:02:21,773 | INFO | Chunk: 10 | WER=8.928571 | S=3 D=1 I=1
2026-01-29 17:02:21,774 | INFO | Chunk: 11 | WER=6.896552 | S=2 D=0 I=0
2026-01-29 17:02:21,777 | INFO | Chunk: 12 | WER=6.578947 | S=2 D=2 I=1
2026-01-29 17:02:21,778 | INFO | Chunk: 13 | WER=2.564103 | S=1 D=0 I=0
2026-01-29 17:02:21,781 | INFO | Chunk: 14 | WER=15.000000 | S=6 D=5 I=1
2026-01-29 17:02:21,783 | INFO | Chunk: 15 | WER=17.857143 | S=5 D=2 I=3
2026-01-29 17:02:22,045 | INFO | File: Rhap-M1003.wav | WER=20.377868 | S=68 D=10 I=73
2026-01-29 17:02:22,045 | INFO | ------------------------------
2026-01-29 17:02:22,045 | INFO | Conf ester Done!
2026-01-29 17:05:04,041 | INFO | Chunk: 0 | WER=34.000000 | S=19 D=14 I=1
2026-01-29 17:05:04,044 | INFO | Chunk: 1 | WER=24.137931 | S=7 D=14 I=0
2026-01-29 17:05:04,047 | INFO | Chunk: 2 | WER=35.135135 | S=17 D=8 I=1
2026-01-29 17:05:04,047 | INFO | Chunk: 3 | WER=34.782609 | S=5 D=3 I=0
2026-01-29 17:05:04,049 | INFO | Chunk: 4 | WER=20.000000 | S=6 D=3 I=0
2026-01-29 17:05:04,049 | INFO | Chunk: 5 | WER=100.000000 | S=1 D=0 I=0
2026-01-29 17:05:04,050 | INFO | Chunk: 6 | WER=27.586207 | S=4 D=4 I=0
2026-01-29 17:05:04,050 | INFO | Chunk: 7 | WER=80.000000 | S=3 D=1 I=0
2026-01-29 17:05:04,053 | INFO | Chunk: 8 | WER=34.146341 | S=14 D=14 I=0
2026-01-29 17:05:04,053 | INFO | Chunk: 9 | WER=25.000000 | S=1 D=5 I=0
2026-01-29 17:05:04,055 | INFO | Chunk: 10 | WER=32.142857 | S=9 D=6 I=3
2026-01-29 17:05:04,056 | INFO | Chunk: 11 | WER=20.689655 | S=4 D=2 I=0
2026-01-29 17:05:04,058 | INFO | Chunk: 12 | WER=25.000000 | S=6 D=13 I=0
2026-01-29 17:05:04,059 | INFO | Chunk: 13 | WER=12.820513 | S=4 D=0 I=1
2026-01-29 17:05:04,062 | INFO | Chunk: 14 | WER=46.250000 | S=16 D=20 I=1
2026-01-29 17:05:04,064 | INFO | Chunk: 15 | WER=25.000000 | S=8 D=6 I=0
2026-01-29 17:05:04,291 | INFO | File: Rhap-M1003.wav | WER=26.180837 | S=111 D=62 I=21
2026-01-29 17:05:04,292 | INFO | ------------------------------
2026-01-29 17:05:04,292 | INFO | hmm_tdnn Done!
2026-01-29 17:05:04,503 | INFO | ==================================Rhap-M2001.wav=========================================
2026-01-29 17:05:04,529 | INFO | Using rVAD model
2026-01-29 17:05:16,857 | INFO | Chunk: 0 | WER=33.333333 | S=1 D=0 I=0
2026-01-29 17:05:16,857 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:16,858 | INFO | Chunk: 2 | WER=33.333333 | S=2 D=2 I=0
2026-01-29 17:05:16,858 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:16,858 | INFO | Chunk: 4 | WER=37.500000 | S=2 D=1 I=0
2026-01-29 17:05:16,859 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:16,859 | INFO | Chunk: 6 | WER=5.882353 | S=0 D=1 I=0
2026-01-29 17:05:16,859 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:16,860 | INFO | Chunk: 8 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:16,860 | INFO | Chunk: 9 | WER=14.285714 | S=1 D=0 I=0
2026-01-29 17:05:16,860 | INFO | Chunk: 10 | WER=16.666667 | S=1 D=0 I=0
2026-01-29 17:05:16,860 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:16,860 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:16,860 | INFO | Chunk: 13 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:16,861 | INFO | Chunk: 14 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:16,862 | INFO | Chunk: 15 | WER=5.000000 | S=0 D=0 I=1
2026-01-29 17:05:16,862 | INFO | Chunk: 16 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:16,862 | INFO | Chunk: 17 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:16,862 | INFO | Chunk: 18 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:16,863 | INFO | Chunk: 19 | WER=57.142857 | S=2 D=2 I=0
2026-01-29 17:05:16,863 | INFO | Chunk: 20 | WER=100.000000 | S=1 D=1 I=0
2026-01-29 17:05:16,863 | INFO | Chunk: 21 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:16,863 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:16,864 | INFO | Chunk: 23 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:16,864 | INFO | Chunk: 24 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:16,864 | INFO | Chunk: 25 | WER=5.263158 | S=1 D=0 I=0
2026-01-29 17:05:16,865 | INFO | Chunk: 26 | WER=4.761905 | S=0 D=0 I=1
2026-01-29 17:05:16,865 | INFO | Chunk: 27 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:16,865 | INFO | Chunk: 28 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:16,865 | INFO | Chunk: 29 | WER=16.666667 | S=1 D=0 I=0
2026-01-29 17:05:16,866 | INFO | Chunk: 30 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:16,866 | INFO | Chunk: 31 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:16,866 | INFO | Chunk: 32 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:16,867 | INFO | Chunk: 33 | WER=7.142857 | S=1 D=0 I=0
2026-01-29 17:05:16,867 | INFO | Chunk: 34 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:16,867 | INFO | Chunk: 35 | WER=33.333333 | S=0 D=0 I=1
2026-01-29 17:05:16,868 | INFO | Chunk: 36 | WER=4.000000 | S=0 D=1 I=0
2026-01-29 17:05:16,868 | INFO | Chunk: 37 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:16,868 | INFO | Chunk: 38 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:16,869 | INFO | Chunk: 39 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:16,869 | INFO | Chunk: 40 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:16,869 | INFO | Chunk: 41 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:16,870 | INFO | Chunk: 42 | WER=5.000000 | S=1 D=0 I=0
2026-01-29 17:05:16,870 | INFO | Chunk: 43 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:16,870 | INFO | Chunk: 44 | WER=20.000000 | S=0 D=1 I=0
2026-01-29 17:05:16,870 | INFO | Chunk: 45 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:16,871 | INFO | Chunk: 46 | WER=6.666667 | S=1 D=0 I=0
2026-01-29 17:05:16,871 | INFO | Chunk: 47 | WER=12.500000 | S=1 D=0 I=0
2026-01-29 17:05:16,991 | INFO | File: Rhap-M2001.wav | WER=5.056180 | S=16 D=8 I=3
2026-01-29 17:05:16,991 | INFO | ------------------------------
2026-01-29 17:05:16,991 | INFO | w2vec vad chunk Done!
2026-01-29 17:05:47,429 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:47,430 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:47,430 | INFO | Chunk: 2 | WER=33.333333 | S=2 D=1 I=1
2026-01-29 17:05:47,430 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:47,430 | INFO | Chunk: 4 | WER=25.000000 | S=2 D=0 I=0
2026-01-29 17:05:47,431 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:47,431 | INFO | Chunk: 6 | WER=5.882353 | S=0 D=1 I=0
2026-01-29 17:05:47,431 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:47,432 | INFO | Chunk: 8 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:47,432 | INFO | Chunk: 9 | WER=14.285714 | S=1 D=0 I=0
2026-01-29 17:05:47,432 | INFO | Chunk: 10 | WER=16.666667 | S=1 D=0 I=0
2026-01-29 17:05:47,432 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:47,433 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:47,433 | INFO | Chunk: 13 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:47,433 | INFO | Chunk: 14 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:47,434 | INFO | Chunk: 15 | WER=5.000000 | S=0 D=0 I=1
2026-01-29 17:05:47,434 | INFO | Chunk: 16 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:47,434 | INFO | Chunk: 17 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:47,435 | INFO | Chunk: 18 | WER=5.882353 | S=1 D=0 I=0
2026-01-29 17:05:47,435 | INFO | Chunk: 19 | WER=28.571429 | S=1 D=1 I=0
2026-01-29 17:05:47,435 | INFO | Chunk: 20 | WER=100.000000 | S=1 D=1 I=0
2026-01-29 17:05:47,435 | INFO | Chunk: 21 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:47,435 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:47,436 | INFO | Chunk: 23 | WER=7.692308 | S=1 D=0 I=0
2026-01-29 17:05:47,436 | INFO | Chunk: 24 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:47,436 | INFO | Chunk: 25 | WER=5.263158 | S=1 D=0 I=0
2026-01-29 17:05:47,437 | INFO | Chunk: 26 | WER=4.761905 | S=0 D=0 I=1
2026-01-29 17:05:47,437 | INFO | Chunk: 27 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:47,437 | INFO | Chunk: 28 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:47,437 | INFO | Chunk: 29 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:47,438 | INFO | Chunk: 30 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:47,438 | INFO | Chunk: 31 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:47,438 | INFO | Chunk: 32 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:47,439 | INFO | Chunk: 33 | WER=7.142857 | S=1 D=0 I=0
2026-01-29 17:05:47,439 | INFO | Chunk: 34 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:47,439 | INFO | Chunk: 35 | WER=33.333333 | S=0 D=0 I=1
2026-01-29 17:05:47,439 | INFO | Chunk: 36 | WER=4.000000 | S=0 D=1 I=0
2026-01-29 17:05:47,440 | INFO | Chunk: 37 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:47,440 | INFO | Chunk: 38 | WER=4.545455 | S=0 D=1 I=0
2026-01-29 17:05:47,441 | INFO | Chunk: 39 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:47,441 | INFO | Chunk: 40 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:47,441 | INFO | Chunk: 41 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:47,441 | INFO | Chunk: 42 | WER=5.000000 | S=1 D=0 I=0
2026-01-29 17:05:47,442 | INFO | Chunk: 43 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:47,442 | INFO | Chunk: 44 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:05:47,442 | INFO | Chunk: 45 | WER=6.666667 | S=0 D=0 I=1
2026-01-29 17:05:47,442 | INFO | Chunk: 46 | WER=6.666667 | S=1 D=0 I=0
2026-01-29 17:05:47,443 | INFO | Chunk: 47 | WER=50.000000 | S=4 D=0 I=0
2026-01-29 17:05:47,564 | INFO | File: Rhap-M2001.wav | WER=5.243446 | S=18 D=5 I=5
2026-01-29 17:05:47,564 | INFO | ------------------------------
2026-01-29 17:05:47,564 | INFO | whisper med Done!
2026-01-29 17:06:26,110 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:06:26,111 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:06:26,111 | INFO | Chunk: 2 | WER=33.333333 | S=2 D=2 I=0
2026-01-29 17:06:26,111 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:06:26,111 | INFO | Chunk: 4 | WER=12.500000 | S=1 D=0 I=0
2026-01-29 17:06:26,112 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:06:26,112 | INFO | Chunk: 6 | WER=5.882353 | S=0 D=1 I=0
2026-01-29 17:06:26,112 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:06:26,113 | INFO | Chunk: 8 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:06:26,113 | INFO | Chunk: 9 | WER=14.285714 | S=1 D=0 I=0
2026-01-29 17:06:26,113 | INFO | Chunk: 10 | WER=16.666667 | S=1 D=0 I=0
2026-01-29 17:06:26,113 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:06:26,113 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:06:26,114 | INFO | Chunk: 13 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:06:26,114 | INFO | Chunk: 14 | WER=4.166667 | S=1 D=0 I=0
2026-01-29 17:06:26,115 | INFO | Chunk: 15 | WER=5.000000 | S=0 D=0 I=1
2026-01-29 17:06:26,115 | INFO | Chunk: 16 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:06:26,115 | INFO | Chunk: 17 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:06:26,115 | INFO | Chunk: 18 | WER=5.882353 | S=1 D=0 I=0
2026-01-29 17:06:26,116 | INFO | Chunk: 19 | WER=28.571429 | S=1 D=1 I=0
2026-01-29 17:06:26,116 | INFO | Chunk: 20 | WER=100.000000 | S=1 D=1 I=0
2026-01-29 17:06:26,116 | INFO | Chunk: 21 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:06:26,116 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:06:26,116 | INFO | Chunk: 23 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:06:26,117 | INFO | Chunk: 24 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:06:26,117 | INFO | Chunk: 25 | WER=5.263158 | S=1 D=0 I=0
2026-01-29 17:06:26,118 | INFO | Chunk: 26 | WER=4.761905 | S=0 D=0 I=1
2026-01-29 17:06:26,118 | INFO | Chunk: 27 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:06:26,118 | INFO | Chunk: 28 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:06:26,118 | INFO | Chunk: 29 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:06:26,118 | INFO | Chunk: 30 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:06:26,119 | INFO | Chunk: 31 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:06:26,119 | INFO | Chunk: 32 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:06:26,119 | INFO | Chunk: 33 | WER=7.142857 | S=1 D=0 I=0
2026-01-29 17:06:26,120 | INFO | Chunk: 34 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:06:26,120 | INFO | Chunk: 35 | WER=33.333333 | S=0 D=0 I=1
2026-01-29 17:06:26,120 | INFO | Chunk: 36 | WER=4.000000 | S=0 D=1 I=0
2026-01-29 17:06:26,120 | INFO | Chunk: 37 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:06:26,121 | INFO | Chunk: 38 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:06:26,121 | INFO | Chunk: 39 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:06:26,122 | INFO | Chunk: 40 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:06:26,122 | INFO | Chunk: 41 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:06:26,122 | INFO | Chunk: 42 | WER=5.000000 | S=1 D=0 I=0
2026-01-29 17:06:26,122 | INFO | Chunk: 43 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:06:26,123 | INFO | Chunk: 44 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:06:26,123 | INFO | Chunk: 45 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:06:26,123 | INFO | Chunk: 46 | WER=6.666667 | S=1 D=0 I=0
2026-01-29 17:06:26,123 | INFO | Chunk: 47 | WER=25.000000 | S=2 D=0 I=0
2026-01-29 17:06:26,244 | INFO | File: Rhap-M2001.wav | WER=4.307116 | S=15 D=5 I=3
2026-01-29 17:06:26,245 | INFO | ------------------------------
2026-01-29 17:06:26,245 | INFO | whisper large Done!
2026-01-29 17:06:26,413 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-29 17:06:26,443 | INFO | Vocabulary size: 350
2026-01-29 17:06:26,999 | INFO | Gradient checkpoint layers: []
2026-01-29 17:06:27,671 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 17:06:27,674 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 17:06:27,675 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 17:06:27,675 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-29 17:06:27,675 | INFO | speech length: 14880
2026-01-29 17:06:27,709 | INFO | decoder input length: 22
2026-01-29 17:06:27,709 | INFO | max output length: 22
2026-01-29 17:06:27,709 | INFO | min output length: 2
2026-01-29 17:06:28,302 | INFO | end detected at 19
2026-01-29 17:06:28,303 | INFO |  -1.10 * 0.5 =  -0.55 for decoder
2026-01-29 17:06:28,303 | INFO |  -1.39 * 0.5 =  -0.69 for ctc
2026-01-29 17:06:28,303 | INFO | total log probability: -1.24
2026-01-29 17:06:28,303 | INFO | normalized log probability: -0.09
2026-01-29 17:06:28,304 | INFO | total number of ended hypotheses: 159
2026-01-29 17:06:28,304 | INFO | best hypo: ▁mesdames▁et▁messieurs

2026-01-29 17:06:28,306 | INFO | speech length: 53600
2026-01-29 17:06:28,337 | INFO | decoder input length: 83
2026-01-29 17:06:28,338 | INFO | max output length: 83
2026-01-29 17:06:28,338 | INFO | min output length: 8
2026-01-29 17:06:29,335 | INFO | end detected at 25
2026-01-29 17:06:29,336 | INFO |  -1.37 * 0.5 =  -0.68 for decoder
2026-01-29 17:06:29,336 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:06:29,336 | INFO | total log probability: -0.69
2026-01-29 17:06:29,336 | INFO | normalized log probability: -0.03
2026-01-29 17:06:29,336 | INFO | total number of ended hypotheses: 141
2026-01-29 17:06:29,336 | INFO | best hypo: ▁je▁suis▁heureux▁de▁me▁retrouver▁ce▁soir▁parmi▁vous

2026-01-29 17:06:29,338 | INFO | speech length: 53280
2026-01-29 17:06:29,368 | INFO | decoder input length: 82
2026-01-29 17:06:29,368 | INFO | max output length: 82
2026-01-29 17:06:29,368 | INFO | min output length: 8
2026-01-29 17:06:30,564 | INFO | end detected at 32
2026-01-29 17:06:30,565 | INFO |  -3.07 * 0.5 =  -1.53 for decoder
2026-01-29 17:06:30,565 | INFO |  -7.42 * 0.5 =  -3.71 for ctc
2026-01-29 17:06:30,565 | INFO | total log probability: -5.24
2026-01-29 17:06:30,566 | INFO | normalized log probability: -0.20
2026-01-29 17:06:30,566 | INFO | total number of ended hypotheses: 176
2026-01-29 17:06:30,566 | INFO | best hypo: ▁après▁ma▁visite▁landivisiau▁et▁à▁l'île▁longue▁ce▁matin

2026-01-29 17:06:30,568 | INFO | speech length: 37440
2026-01-29 17:06:30,599 | INFO | decoder input length: 58
2026-01-29 17:06:30,599 | INFO | max output length: 58
2026-01-29 17:06:30,599 | INFO | min output length: 5
2026-01-29 17:06:31,255 | INFO | end detected at 18
2026-01-29 17:06:31,255 | INFO |  -0.98 * 0.5 =  -0.49 for decoder
2026-01-29 17:06:31,255 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:06:31,256 | INFO | total log probability: -0.49
2026-01-29 17:06:31,256 | INFO | normalized log probability: -0.04
2026-01-29 17:06:31,256 | INFO | total number of ended hypotheses: 146
2026-01-29 17:06:31,256 | INFO | best hypo: ▁c'est▁donc▁une▁journée▁entière

2026-01-29 17:06:31,257 | INFO | speech length: 30400
2026-01-29 17:06:31,288 | INFO | decoder input length: 47
2026-01-29 17:06:31,289 | INFO | max output length: 47
2026-01-29 17:06:31,289 | INFO | min output length: 4
2026-01-29 17:06:32,150 | INFO | end detected at 25
2026-01-29 17:06:32,151 | INFO |  -2.21 * 0.5 =  -1.11 for decoder
2026-01-29 17:06:32,151 | INFO |  -0.31 * 0.5 =  -0.16 for ctc
2026-01-29 17:06:32,151 | INFO | total log probability: -1.26
2026-01-29 17:06:32,151 | INFO | normalized log probability: -0.06
2026-01-29 17:06:32,151 | INFO | total number of ended hypotheses: 145
2026-01-29 17:06:32,151 | INFO | best hypo: ▁que▁j'aurais▁passé▁avec▁nos▁forces▁armées

2026-01-29 17:06:32,153 | INFO | speech length: 88640
2026-01-29 17:06:32,189 | INFO | decoder input length: 138
2026-01-29 17:06:32,189 | INFO | max output length: 138
2026-01-29 17:06:32,189 | INFO | min output length: 13
2026-01-29 17:06:34,559 | INFO | end detected at 56
2026-01-29 17:06:34,560 | INFO |  -6.06 * 0.5 =  -3.03 for decoder
2026-01-29 17:06:34,561 | INFO |  -3.37 * 0.5 =  -1.68 for ctc
2026-01-29 17:06:34,561 | INFO | total log probability: -4.71
2026-01-29 17:06:34,561 | INFO | normalized log probability: -0.09
2026-01-29 17:06:34,561 | INFO | total number of ended hypotheses: 172
2026-01-29 17:06:34,561 | INFO | best hypo: ▁c'est▁bien▁sûr▁dans▁mon▁esprit▁la▁marque▁du▁lien▁direct▁qui▁une▁île▁président▁de▁la▁république▁chef▁des▁armées

2026-01-29 17:06:34,563 | INFO | speech length: 86880
2026-01-29 17:06:34,595 | INFO | decoder input length: 135
2026-01-29 17:06:34,595 | INFO | max output length: 135
2026-01-29 17:06:34,595 | INFO | min output length: 13
2026-01-29 17:06:36,520 | INFO | end detected at 46
2026-01-29 17:06:36,522 | INFO |  -3.55 * 0.5 =  -1.78 for decoder
2026-01-29 17:06:36,522 | INFO |  -2.07 * 0.5 =  -1.04 for ctc
2026-01-29 17:06:36,522 | INFO | total log probability: -2.81
2026-01-29 17:06:36,522 | INFO | normalized log probability: -0.07
2026-01-29 17:06:36,522 | INFO | total number of ended hypotheses: 179
2026-01-29 17:06:36,523 | INFO | best hypo: ▁avec▁toute▁celle▁et▁tous▁ceux▁qui▁ont▁la▁difficile▁mission▁de▁veiller▁sur▁nos▁intérêts

2026-01-29 17:06:36,525 | INFO | speech length: 31360
2026-01-29 17:06:36,553 | INFO | decoder input length: 48
2026-01-29 17:06:36,553 | INFO | max output length: 48
2026-01-29 17:06:36,553 | INFO | min output length: 4
2026-01-29 17:06:37,300 | INFO | end detected at 22
2026-01-29 17:06:37,301 | INFO |  -1.26 * 0.5 =  -0.63 for decoder
2026-01-29 17:06:37,301 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:06:37,301 | INFO | total log probability: -0.63
2026-01-29 17:06:37,301 | INFO | normalized log probability: -0.03
2026-01-29 17:06:37,301 | INFO | total number of ended hypotheses: 141
2026-01-29 17:06:37,301 | INFO | best hypo: ▁et▁sur▁la▁sécurité▁de▁nos▁concitoyens

2026-01-29 17:06:37,303 | INFO | speech length: 62720
2026-01-29 17:06:37,334 | INFO | decoder input length: 97
2026-01-29 17:06:37,334 | INFO | max output length: 97
2026-01-29 17:06:37,334 | INFO | min output length: 9
2026-01-29 17:06:38,799 | INFO | end detected at 38
2026-01-29 17:06:38,799 | INFO |  -2.41 * 0.5 =  -1.21 for decoder
2026-01-29 17:06:38,800 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-29 17:06:38,800 | INFO | total log probability: -1.21
2026-01-29 17:06:38,800 | INFO | normalized log probability: -0.04
2026-01-29 17:06:38,800 | INFO | total number of ended hypotheses: 149
2026-01-29 17:06:38,800 | INFO | best hypo: ▁c'est▁pour▁cela▁que▁je▁tenais▁à▁vous▁rencontrer▁la▁veille▁de▁notre▁fête▁nationale

2026-01-29 17:06:38,802 | INFO | speech length: 50080
2026-01-29 17:06:38,833 | INFO | decoder input length: 77
2026-01-29 17:06:38,833 | INFO | max output length: 77
2026-01-29 17:06:38,833 | INFO | min output length: 7
2026-01-29 17:06:39,870 | INFO | end detected at 28
2026-01-29 17:06:39,871 | INFO |  -1.87 * 0.5 =  -0.93 for decoder
2026-01-29 17:06:39,871 | INFO |  -0.51 * 0.5 =  -0.25 for ctc
2026-01-29 17:06:39,871 | INFO | total log probability: -1.19
2026-01-29 17:06:39,871 | INFO | normalized log probability: -0.05
2026-01-29 17:06:39,871 | INFO | total number of ended hypotheses: 150
2026-01-29 17:06:39,871 | INFO | best hypo: ▁le▁quatorze▁juillet▁lors▁du▁traditionnel▁défilé

2026-01-29 17:06:39,873 | INFO | speech length: 25280
2026-01-29 17:06:39,904 | INFO | decoder input length: 39
2026-01-29 17:06:39,904 | INFO | max output length: 39
2026-01-29 17:06:39,904 | INFO | min output length: 3
2026-01-29 17:06:40,454 | INFO | end detected at 16
2026-01-29 17:06:40,455 | INFO |  -0.83 * 0.5 =  -0.42 for decoder
2026-01-29 17:06:40,455 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-29 17:06:40,455 | INFO | total log probability: -0.42
2026-01-29 17:06:40,455 | INFO | normalized log probability: -0.04
2026-01-29 17:06:40,455 | INFO | total number of ended hypotheses: 140
2026-01-29 17:06:40,455 | INFO | best hypo: ▁c'est▁la▁nation▁tout▁entière

2026-01-29 17:06:40,456 | INFO | speech length: 13600
2026-01-29 17:06:40,488 | INFO | decoder input length: 20
2026-01-29 17:06:40,488 | INFO | max output length: 20
2026-01-29 17:06:40,488 | INFO | min output length: 2
2026-01-29 17:06:41,003 | INFO | end detected at 16
2026-01-29 17:06:41,003 | INFO |  -1.12 * 0.5 =  -0.56 for decoder
2026-01-29 17:06:41,004 | INFO |  -0.98 * 0.5 =  -0.49 for ctc
2026-01-29 17:06:41,004 | INFO | total log probability: -1.05
2026-01-29 17:06:41,004 | INFO | normalized log probability: -0.09
2026-01-29 17:06:41,004 | INFO | total number of ended hypotheses: 141
2026-01-29 17:06:41,004 | INFO | best hypo: ▁qui▁vous▁rend▁hommage

2026-01-29 17:06:41,005 | INFO | speech length: 21920
2026-01-29 17:06:41,036 | INFO | decoder input length: 33
2026-01-29 17:06:41,036 | INFO | max output length: 33
2026-01-29 17:06:41,036 | INFO | min output length: 3
2026-01-29 17:06:41,481 | INFO | end detected at 13
2026-01-29 17:06:41,482 | INFO |  -0.61 * 0.5 =  -0.31 for decoder
2026-01-29 17:06:41,482 | INFO |  -0.05 * 0.5 =  -0.02 for ctc
2026-01-29 17:06:41,482 | INFO | total log probability: -0.33
2026-01-29 17:06:41,482 | INFO | normalized log probability: -0.04
2026-01-29 17:06:41,482 | INFO | total number of ended hypotheses: 140
2026-01-29 17:06:41,482 | INFO | best hypo: ▁elle▁salue▁le▁courage

2026-01-29 17:06:41,484 | INFO | speech length: 48480
2026-01-29 17:06:41,514 | INFO | decoder input length: 75
2026-01-29 17:06:41,514 | INFO | max output length: 75
2026-01-29 17:06:41,514 | INFO | min output length: 7
2026-01-29 17:06:42,457 | INFO | end detected at 25
2026-01-29 17:06:42,458 | INFO |  -1.42 * 0.5 =  -0.71 for decoder
2026-01-29 17:06:42,458 | INFO |  -0.10 * 0.5 =  -0.05 for ctc
2026-01-29 17:06:42,458 | INFO | total log probability: -0.76
2026-01-29 17:06:42,458 | INFO | normalized log probability: -0.04
2026-01-29 17:06:42,458 | INFO | total number of ended hypotheses: 140
2026-01-29 17:06:42,458 | INFO | best hypo: ▁elle▁salue▁la▁loyauté▁elle▁salue▁le▁dévouement

2026-01-29 17:06:42,460 | INFO | speech length: 147840
2026-01-29 17:06:42,491 | INFO | decoder input length: 230
2026-01-29 17:06:42,492 | INFO | max output length: 230
2026-01-29 17:06:42,492 | INFO | min output length: 23
2026-01-29 17:06:45,737 | INFO | end detected at 63
2026-01-29 17:06:45,738 | INFO |  -4.72 * 0.5 =  -2.36 for decoder
2026-01-29 17:06:45,738 | INFO |  -0.38 * 0.5 =  -0.19 for ctc
2026-01-29 17:06:45,738 | INFO | total log probability: -2.55
2026-01-29 17:06:45,739 | INFO | normalized log probability: -0.04
2026-01-29 17:06:45,739 | INFO | total number of ended hypotheses: 165
2026-01-29 17:06:45,739 | INFO | best hypo: ▁elle▁salue▁l'esprit▁de▁sacrifice▁de▁ceux▁qui▁ont▁choisi▁de▁servir▁sa▁défense▁ses▁intérêts▁et▁ses▁valeurs▁à▁travers▁le▁monde

2026-01-29 17:06:45,741 | INFO | speech length: 95200
2026-01-29 17:06:45,773 | INFO | decoder input length: 148
2026-01-29 17:06:45,774 | INFO | max output length: 148
2026-01-29 17:06:45,774 | INFO | min output length: 14
2026-01-29 17:06:47,920 | INFO | end detected at 50
2026-01-29 17:06:47,922 | INFO |  -5.53 * 0.5 =  -2.76 for decoder
2026-01-29 17:06:47,922 | INFO |  -3.11 * 0.5 =  -1.56 for ctc
2026-01-29 17:06:47,922 | INFO | total log probability: -4.32
2026-01-29 17:06:47,922 | INFO | normalized log probability: -0.10
2026-01-29 17:06:47,922 | INFO | total number of ended hypotheses: 172
2026-01-29 17:06:47,922 | INFO | best hypo: ▁et▁c'est▁la▁raison▁pour▁laquelle▁au▁nom▁de▁la▁république▁j'honorerai▁certaines▁d'entre▁vous▁dans▁quelques▁uns

2026-01-29 17:06:47,924 | INFO | speech length: 69920
2026-01-29 17:06:47,957 | INFO | decoder input length: 108
2026-01-29 17:06:47,957 | INFO | max output length: 108
2026-01-29 17:06:47,957 | INFO | min output length: 10
2026-01-29 17:06:49,636 | INFO | end detected at 43
2026-01-29 17:06:49,637 | INFO |  -5.10 * 0.5 =  -2.55 for decoder
2026-01-29 17:06:49,637 | INFO |  -0.40 * 0.5 =  -0.20 for ctc
2026-01-29 17:06:49,638 | INFO | total log probability: -2.75
2026-01-29 17:06:49,638 | INFO | normalized log probability: -0.07
2026-01-29 17:06:49,638 | INFO | total number of ended hypotheses: 150
2026-01-29 17:06:49,638 | INFO | best hypo: ▁cette▁année▁j'ai▁tenue▁à▁ce▁que▁cette▁fête▁nationale▁soit▁aussi▁celle▁de▁l'europe

2026-01-29 17:06:49,640 | INFO | speech length: 12160
2026-01-29 17:06:49,668 | INFO | decoder input length: 18
2026-01-29 17:06:49,668 | INFO | max output length: 18
2026-01-29 17:06:49,668 | INFO | min output length: 1
2026-01-29 17:06:49,998 | INFO | end detected at 10
2026-01-29 17:06:49,999 | INFO |  -0.50 * 0.5 =  -0.25 for decoder
2026-01-29 17:06:50,000 | INFO |  -2.31 * 0.5 =  -1.16 for ctc
2026-01-29 17:06:50,000 | INFO | total log probability: -1.40
2026-01-29 17:06:50,000 | INFO | normalized log probability: -0.28
2026-01-29 17:06:50,000 | INFO | total number of ended hypotheses: 154
2026-01-29 17:06:50,000 | INFO | best hypo: ▁demain

2026-01-29 17:06:50,001 | INFO | speech length: 76000
2026-01-29 17:06:50,032 | INFO | decoder input length: 118
2026-01-29 17:06:50,032 | INFO | max output length: 118
2026-01-29 17:06:50,033 | INFO | min output length: 11
2026-01-29 17:06:51,993 | INFO | end detected at 48
2026-01-29 17:06:51,994 | INFO |  -3.08 * 0.5 =  -1.54 for decoder
2026-01-29 17:06:51,994 | INFO |  -0.56 * 0.5 =  -0.28 for ctc
2026-01-29 17:06:51,994 | INFO | total log probability: -1.82
2026-01-29 17:06:51,994 | INFO | normalized log probability: -0.04
2026-01-29 17:06:51,994 | INFO | total number of ended hypotheses: 150
2026-01-29 17:06:51,995 | INFO | best hypo: ▁ce▁sont▁les▁vingt▁six▁drapeaux▁de▁nos▁partenaires▁européens▁qui▁défileront▁aux▁côtés▁de▁nos▁armées

2026-01-29 17:06:51,997 | INFO | speech length: 29120
2026-01-29 17:06:52,025 | INFO | decoder input length: 45
2026-01-29 17:06:52,025 | INFO | max output length: 45
2026-01-29 17:06:52,025 | INFO | min output length: 4
2026-01-29 17:06:52,871 | INFO | end detected at 25
2026-01-29 17:06:52,871 | INFO |  -2.69 * 0.5 =  -1.35 for decoder
2026-01-29 17:06:52,871 | INFO |  -5.62 * 0.5 =  -2.81 for ctc
2026-01-29 17:06:52,872 | INFO | total log probability: -4.16
2026-01-29 17:06:52,872 | INFO | normalized log probability: -0.20
2026-01-29 17:06:52,872 | INFO | total number of ended hypotheses: 161
2026-01-29 17:06:52,872 | INFO | best hypo: ▁je▁remercie▁d'ailleurs▁chez▁hervé

2026-01-29 17:06:52,874 | INFO | speech length: 9600
2026-01-29 17:06:52,901 | INFO | decoder input length: 14
2026-01-29 17:06:52,902 | INFO | max output length: 14
2026-01-29 17:06:52,902 | INFO | min output length: 1
2026-01-29 17:06:53,295 | INFO | end detected at 12
2026-01-29 17:06:53,296 | INFO |  -0.60 * 0.5 =  -0.30 for decoder
2026-01-29 17:06:53,296 | INFO |  -1.86 * 0.5 =  -0.93 for ctc
2026-01-29 17:06:53,296 | INFO | total log probability: -1.23
2026-01-29 17:06:53,296 | INFO | normalized log probability: -0.18
2026-01-29 17:06:53,296 | INFO | total number of ended hypotheses: 164
2026-01-29 17:06:53,296 | INFO | best hypo: ▁cher▁alain

2026-01-29 17:06:53,297 | INFO | speech length: 65920
2026-01-29 17:06:53,329 | INFO | decoder input length: 102
2026-01-29 17:06:53,329 | INFO | max output length: 102
2026-01-29 17:06:53,329 | INFO | min output length: 10
2026-01-29 17:06:54,720 | INFO | end detected at 35
2026-01-29 17:06:54,722 | INFO |  -2.27 * 0.5 =  -1.13 for decoder
2026-01-29 17:06:54,722 | INFO |  -0.85 * 0.5 =  -0.42 for ctc
2026-01-29 17:06:54,722 | INFO | total log probability: -1.56
2026-01-29 17:06:54,722 | INFO | normalized log probability: -0.05
2026-01-29 17:06:54,722 | INFO | total number of ended hypotheses: 162
2026-01-29 17:06:54,722 | INFO | best hypo: ▁tout▁spécialement▁les▁ministres▁de▁la▁défense▁qui▁sont▁parmi▁nous▁ce▁soir

2026-01-29 17:06:54,724 | INFO | speech length: 38560
2026-01-29 17:06:54,751 | INFO | decoder input length: 59
2026-01-29 17:06:54,751 | INFO | max output length: 59
2026-01-29 17:06:54,751 | INFO | min output length: 5
2026-01-29 17:06:55,566 | INFO | end detected at 23
2026-01-29 17:06:55,567 | INFO |  -1.30 * 0.5 =  -0.65 for decoder
2026-01-29 17:06:55,567 | INFO |  -0.06 * 0.5 =  -0.03 for ctc
2026-01-29 17:06:55,567 | INFO | total log probability: -0.68
2026-01-29 17:06:55,567 | INFO | normalized log probability: -0.04
2026-01-29 17:06:55,567 | INFO | total number of ended hypotheses: 146
2026-01-29 17:06:55,568 | INFO | best hypo: ▁je▁leur▁adresse▁un▁salut▁très▁amical

2026-01-29 17:06:55,569 | INFO | speech length: 40960
2026-01-29 17:06:55,600 | INFO | decoder input length: 63
2026-01-29 17:06:55,600 | INFO | max output length: 63
2026-01-29 17:06:55,600 | INFO | min output length: 6
2026-01-29 17:06:56,835 | INFO | end detected at 31
2026-01-29 17:06:56,837 | INFO |  -1.87 * 0.5 =  -0.94 for decoder
2026-01-29 17:06:56,837 | INFO |  -4.31 * 0.5 =  -2.16 for ctc
2026-01-29 17:06:56,837 | INFO | total log probability: -3.09
2026-01-29 17:06:56,837 | INFO | normalized log probability: -0.12
2026-01-29 17:06:56,837 | INFO | total number of ended hypotheses: 170
2026-01-29 17:06:56,837 | INFO | best hypo: ▁c'est▁l'image▁d'une▁robe▁qui▁repart▁d'un▁même▁pas

2026-01-29 17:06:56,839 | INFO | speech length: 74880
2026-01-29 17:06:56,872 | INFO | decoder input length: 116
2026-01-29 17:06:56,872 | INFO | max output length: 116
2026-01-29 17:06:56,872 | INFO | min output length: 11
2026-01-29 17:06:58,458 | INFO | end detected at 39
2026-01-29 17:06:58,459 | INFO |  -3.31 * 0.5 =  -1.66 for decoder
2026-01-29 17:06:58,459 | INFO |  -8.18 * 0.5 =  -4.09 for ctc
2026-01-29 17:06:58,459 | INFO | total log probability: -5.75
2026-01-29 17:06:58,459 | INFO | normalized log probability: -0.17
2026-01-29 17:06:58,459 | INFO | total number of ended hypotheses: 176
2026-01-29 17:06:58,459 | INFO | best hypo: ▁une▁robe▁résolue▁à▁se▁remettre▁en▁mouvement▁après▁deux▁années▁d'immobilisme

2026-01-29 17:06:58,461 | INFO | speech length: 95840
2026-01-29 17:06:58,494 | INFO | decoder input length: 149
2026-01-29 17:06:58,495 | INFO | max output length: 149
2026-01-29 17:06:58,495 | INFO | min output length: 14
2026-01-29 17:07:00,939 | INFO | end detected at 57
2026-01-29 17:07:00,940 | INFO |  -4.33 * 0.5 =  -2.16 for decoder
2026-01-29 17:07:00,940 | INFO |  -0.58 * 0.5 =  -0.29 for ctc
2026-01-29 17:07:00,940 | INFO | total log probability: -2.45
2026-01-29 17:07:00,940 | INFO | normalized log probability: -0.05
2026-01-29 17:07:00,940 | INFO | total number of ended hypotheses: 162
2026-01-29 17:07:00,941 | INFO | best hypo: ▁une▁robe▁vigilante▁qui▁n'oublie▁pas▁l'impératif▁de▁protection▁dans▁un▁monde▁plus▁instable▁et▁moins▁prévisible

2026-01-29 17:07:00,943 | INFO | speech length: 114560
2026-01-29 17:07:00,977 | INFO | decoder input length: 178
2026-01-29 17:07:00,977 | INFO | max output length: 178
2026-01-29 17:07:00,977 | INFO | min output length: 17
2026-01-29 17:07:03,841 | INFO | end detected at 63
2026-01-29 17:07:03,842 | INFO |  -5.58 * 0.5 =  -2.79 for decoder
2026-01-29 17:07:03,842 | INFO |  -4.77 * 0.5 =  -2.39 for ctc
2026-01-29 17:07:03,842 | INFO | total log probability: -5.18
2026-01-29 17:07:03,843 | INFO | normalized log probability: -0.09
2026-01-29 17:07:03,843 | INFO | total number of ended hypotheses: 158
2026-01-29 17:07:03,843 | INFO | best hypo: ▁une▁europe▁prête▁à▁assumer▁ses▁responsabilités▁et▁au▁sein▁de▁laquelle▁la▁france▁a▁bien▁l'intention▁de▁tenir▁son▁rang

2026-01-29 17:07:03,846 | INFO | speech length: 33920
2026-01-29 17:07:03,879 | INFO | decoder input length: 52
2026-01-29 17:07:03,879 | INFO | max output length: 52
2026-01-29 17:07:03,879 | INFO | min output length: 5
2026-01-29 17:07:04,775 | INFO | end detected at 27
2026-01-29 17:07:04,777 | INFO |  -1.60 * 0.5 =  -0.80 for decoder
2026-01-29 17:07:04,777 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-29 17:07:04,777 | INFO | total log probability: -0.81
2026-01-29 17:07:04,777 | INFO | normalized log probability: -0.04
2026-01-29 17:07:04,777 | INFO | total number of ended hypotheses: 146
2026-01-29 17:07:04,777 | INFO | best hypo: ▁les▁bases▁d'une▁défense▁européenne▁existent

2026-01-29 17:07:04,779 | INFO | speech length: 21120
2026-01-29 17:07:04,811 | INFO | decoder input length: 32
2026-01-29 17:07:04,811 | INFO | max output length: 32
2026-01-29 17:07:04,811 | INFO | min output length: 3
2026-01-29 17:07:05,312 | INFO | end detected at 15
2026-01-29 17:07:05,313 | INFO |  -0.83 * 0.5 =  -0.42 for decoder
2026-01-29 17:07:05,313 | INFO |  -0.06 * 0.5 =  -0.03 for ctc
2026-01-29 17:07:05,313 | INFO | total log probability: -0.45
2026-01-29 17:07:05,313 | INFO | normalized log probability: -0.04
2026-01-29 17:07:05,313 | INFO | total number of ended hypotheses: 140
2026-01-29 17:07:05,313 | INFO | best hypo: ▁il▁faut▁les▁faire▁grandir

2026-01-29 17:07:05,315 | INFO | speech length: 30880
2026-01-29 17:07:05,344 | INFO | decoder input length: 47
2026-01-29 17:07:05,344 | INFO | max output length: 47
2026-01-29 17:07:05,344 | INFO | min output length: 4
2026-01-29 17:07:06,076 | INFO | end detected at 20
2026-01-29 17:07:06,077 | INFO |  -1.66 * 0.5 =  -0.83 for decoder
2026-01-29 17:07:06,077 | INFO |  -1.60 * 0.5 =  -0.80 for ctc
2026-01-29 17:07:06,077 | INFO | total log probability: -1.63
2026-01-29 17:07:06,077 | INFO | normalized log probability: -0.11
2026-01-29 17:07:06,077 | INFO | total number of ended hypotheses: 163
2026-01-29 17:07:06,077 | INFO | best hypo: ▁en▁quittant▁le▁terrain▁des▁maux

2026-01-29 17:07:06,079 | INFO | speech length: 25920
2026-01-29 17:07:06,111 | INFO | decoder input length: 40
2026-01-29 17:07:06,111 | INFO | max output length: 40
2026-01-29 17:07:06,111 | INFO | min output length: 4
2026-01-29 17:07:06,760 | INFO | end detected at 19
2026-01-29 17:07:06,761 | INFO |  -1.05 * 0.5 =  -0.53 for decoder
2026-01-29 17:07:06,761 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:07:06,761 | INFO | total log probability: -0.53
2026-01-29 17:07:06,761 | INFO | normalized log probability: -0.04
2026-01-29 17:07:06,761 | INFO | total number of ended hypotheses: 144
2026-01-29 17:07:06,761 | INFO | best hypo: ▁pour▁gagner▁celui▁de▁l'action

2026-01-29 17:07:06,763 | INFO | speech length: 134240
2026-01-29 17:07:06,794 | INFO | decoder input length: 209
2026-01-29 17:07:06,794 | INFO | max output length: 209
2026-01-29 17:07:06,794 | INFO | min output length: 20
2026-01-29 17:07:10,022 | INFO | end detected at 66
2026-01-29 17:07:10,023 | INFO |  -4.35 * 0.5 =  -2.17 for decoder
2026-01-29 17:07:10,023 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-29 17:07:10,023 | INFO | total log probability: -2.18
2026-01-29 17:07:10,023 | INFO | normalized log probability: -0.04
2026-01-29 17:07:10,023 | INFO | total number of ended hypotheses: 143
2026-01-29 17:07:10,024 | INFO | best hypo: ▁demain▁davantage▁qu'aujourd'hui▁je▁souhaite▁que▁l'europe▁soit▁capable▁d'assurer▁sa▁sécurité▁de▁façon▁autonome

2026-01-29 17:07:10,026 | INFO | speech length: 46080
2026-01-29 17:07:10,060 | INFO | decoder input length: 71
2026-01-29 17:07:10,060 | INFO | max output length: 71
2026-01-29 17:07:10,060 | INFO | min output length: 7
2026-01-29 17:07:11,355 | INFO | end detected at 37
2026-01-29 17:07:11,357 | INFO |  -2.90 * 0.5 =  -1.45 for decoder
2026-01-29 17:07:11,357 | INFO |  -0.51 * 0.5 =  -0.25 for ctc
2026-01-29 17:07:11,357 | INFO | total log probability: -1.70
2026-01-29 17:07:11,357 | INFO | normalized log probability: -0.05
2026-01-29 17:07:11,357 | INFO | total number of ended hypotheses: 163
2026-01-29 17:07:11,357 | INFO | best hypo: ▁alors▁si▁la▁france▁pèse▁aujourd'hui▁sur▁la▁scène▁internationale

2026-01-29 17:07:11,359 | INFO | speech length: 80320
2026-01-29 17:07:11,392 | INFO | decoder input length: 125
2026-01-29 17:07:11,392 | INFO | max output length: 125
2026-01-29 17:07:11,392 | INFO | min output length: 12
2026-01-29 17:07:12,928 | INFO | end detected at 37
2026-01-29 17:07:12,930 | INFO |  -2.35 * 0.5 =  -1.18 for decoder
2026-01-29 17:07:12,930 | INFO |  -0.14 * 0.5 =  -0.07 for ctc
2026-01-29 17:07:12,930 | INFO | total log probability: -1.25
2026-01-29 17:07:12,930 | INFO | normalized log probability: -0.04
2026-01-29 17:07:12,930 | INFO | total number of ended hypotheses: 145
2026-01-29 17:07:12,931 | INFO | best hypo: ▁il▁lui▁faut▁un▁outil▁de▁défense▁adapté▁à▁la▁hauteur▁de▁ses▁ambitions

2026-01-29 17:07:12,933 | INFO | speech length: 55840
2026-01-29 17:07:12,986 | INFO | decoder input length: 86
2026-01-29 17:07:12,986 | INFO | max output length: 86
2026-01-29 17:07:12,986 | INFO | min output length: 8
2026-01-29 17:07:14,205 | INFO | end detected at 32
2026-01-29 17:07:14,206 | INFO |  -2.01 * 0.5 =  -1.00 for decoder
2026-01-29 17:07:14,206 | INFO |  -0.06 * 0.5 =  -0.03 for ctc
2026-01-29 17:07:14,206 | INFO | total log probability: -1.03
2026-01-29 17:07:14,206 | INFO | normalized log probability: -0.04
2026-01-29 17:07:14,206 | INFO | total number of ended hypotheses: 146
2026-01-29 17:07:14,207 | INFO | best hypo: ▁c'est▁aussi▁là▁l'intérêt▁fondamental▁de▁l'europe

2026-01-29 17:07:14,208 | INFO | speech length: 10400
2026-01-29 17:07:14,236 | INFO | decoder input length: 15
2026-01-29 17:07:14,236 | INFO | max output length: 15
2026-01-29 17:07:14,236 | INFO | min output length: 1
2026-01-29 17:07:14,656 | INFO | end detected at 13
2026-01-29 17:07:14,657 | INFO |  -1.28 * 0.5 =  -0.64 for decoder
2026-01-29 17:07:14,658 | INFO |  -1.60 * 0.5 =  -0.80 for ctc
2026-01-29 17:07:14,658 | INFO | total log probability: -1.44
2026-01-29 17:07:14,658 | INFO | normalized log probability: -0.21
2026-01-29 17:07:14,658 | INFO | total number of ended hypotheses: 166
2026-01-29 17:07:14,658 | INFO | best hypo: ▁et▁de▁nos▁pas

2026-01-29 17:07:14,660 | INFO | speech length: 88320
2026-01-29 17:07:14,692 | INFO | decoder input length: 137
2026-01-29 17:07:14,692 | INFO | max output length: 137
2026-01-29 17:07:14,692 | INFO | min output length: 13
2026-01-29 17:07:17,071 | INFO | end detected at 57
2026-01-29 17:07:17,072 | INFO |  -6.78 * 0.5 =  -3.39 for decoder
2026-01-29 17:07:17,072 | INFO |  -1.98 * 0.5 =  -0.99 for ctc
2026-01-29 17:07:17,072 | INFO | total log probability: -4.38
2026-01-29 17:07:17,072 | INFO | normalized log probability: -0.08
2026-01-29 17:07:17,072 | INFO | total number of ended hypotheses: 182
2026-01-29 17:07:17,073 | INFO | best hypo: ▁aujourd'hui▁plus▁jamais▁vous▁ne▁savez▁mieux▁que▁personne▁c'est▁sur▁le▁terrain▁que▁se▁gagne▁où▁se▁perd▁le▁combat

2026-01-29 17:07:17,074 | INFO | speech length: 20320
2026-01-29 17:07:17,106 | INFO | decoder input length: 31
2026-01-29 17:07:17,106 | INFO | max output length: 31
2026-01-29 17:07:17,106 | INFO | min output length: 3
2026-01-29 17:07:17,601 | INFO | end detected at 15
2026-01-29 17:07:17,602 | INFO |  -0.81 * 0.5 =  -0.41 for decoder
2026-01-29 17:07:17,602 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:07:17,602 | INFO | total log probability: -0.41
2026-01-29 17:07:17,602 | INFO | normalized log probability: -0.04
2026-01-29 17:07:17,602 | INFO | total number of ended hypotheses: 144
2026-01-29 17:07:17,603 | INFO | best hypo: ▁c'est▁sur▁le▁terrain

2026-01-29 17:07:17,604 | INFO | speech length: 127680
2026-01-29 17:07:17,636 | INFO | decoder input length: 199
2026-01-29 17:07:17,636 | INFO | max output length: 199
2026-01-29 17:07:17,636 | INFO | min output length: 19
2026-01-29 17:07:20,245 | INFO | end detected at 54
2026-01-29 17:07:20,246 | INFO |  -9.22 * 0.5 =  -4.61 for decoder
2026-01-29 17:07:20,246 | INFO |  -0.50 * 0.5 =  -0.25 for ctc
2026-01-29 17:07:20,246 | INFO | total log probability: -4.86
2026-01-29 17:07:20,246 | INFO | normalized log probability: -0.10
2026-01-29 17:07:20,246 | INFO | total number of ended hypotheses: 148
2026-01-29 17:07:20,247 | INFO | best hypo: ▁qu'une▁nation▁affirme▁son▁influence▁qu'elle▁pèse▁dans▁une▁coalition▁au▁travers▁des▁forces▁qu'elle▁est▁capable▁d'engager

2026-01-29 17:07:20,249 | INFO | speech length: 79200
2026-01-29 17:07:20,281 | INFO | decoder input length: 123
2026-01-29 17:07:20,282 | INFO | max output length: 123
2026-01-29 17:07:20,282 | INFO | min output length: 12
2026-01-29 17:07:21,881 | INFO | end detected at 39
2026-01-29 17:07:21,882 | INFO |  -2.53 * 0.5 =  -1.26 for decoder
2026-01-29 17:07:21,882 | INFO |  -0.04 * 0.5 =  -0.02 for ctc
2026-01-29 17:07:21,882 | INFO | total log probability: -1.28
2026-01-29 17:07:21,882 | INFO | normalized log probability: -0.04
2026-01-29 17:07:21,882 | INFO | total number of ended hypotheses: 143
2026-01-29 17:07:21,882 | INFO | best hypo: ▁cet▁engagement▁sur▁le▁terrain▁est▁pourvu▁de▁plus▁en▁plus▁difficile▁et▁de▁plus▁en▁plus▁dangereux

2026-01-29 17:07:21,884 | INFO | speech length: 17280
2026-01-29 17:07:21,916 | INFO | decoder input length: 26
2026-01-29 17:07:21,916 | INFO | max output length: 26
2026-01-29 17:07:21,916 | INFO | min output length: 2
2026-01-29 17:07:22,428 | INFO | end detected at 16
2026-01-29 17:07:22,429 | INFO |  -0.85 * 0.5 =  -0.43 for decoder
2026-01-29 17:07:22,429 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:07:22,429 | INFO | total log probability: -0.43
2026-01-29 17:07:22,430 | INFO | normalized log probability: -0.04
2026-01-29 17:07:22,430 | INFO | total number of ended hypotheses: 133
2026-01-29 17:07:22,430 | INFO | best hypo: ▁l'afghanistan

2026-01-29 17:07:22,431 | INFO | speech length: 16000
2026-01-29 17:07:22,464 | INFO | decoder input length: 24
2026-01-29 17:07:22,464 | INFO | max output length: 24
2026-01-29 17:07:22,464 | INFO | min output length: 2
2026-01-29 17:07:22,889 | INFO | end detected at 13
2026-01-29 17:07:22,889 | INFO |  -0.70 * 0.5 =  -0.35 for decoder
2026-01-29 17:07:22,890 | INFO |  -0.41 * 0.5 =  -0.21 for ctc
2026-01-29 17:07:22,890 | INFO | total log probability: -0.55
2026-01-29 17:07:22,890 | INFO | normalized log probability: -0.06
2026-01-29 17:07:22,890 | INFO | total number of ended hypotheses: 149
2026-01-29 17:07:22,890 | INFO | best hypo: ▁le▁proche▁orient

2026-01-29 17:07:22,891 | INFO | speech length: 106240
2026-01-29 17:07:22,924 | INFO | decoder input length: 165
2026-01-29 17:07:22,924 | INFO | max output length: 165
2026-01-29 17:07:22,924 | INFO | min output length: 16
2026-01-29 17:07:25,381 | INFO | end detected at 55
2026-01-29 17:07:25,383 | INFO |  -3.60 * 0.5 =  -1.80 for decoder
2026-01-29 17:07:25,383 | INFO |  -0.66 * 0.5 =  -0.33 for ctc
2026-01-29 17:07:25,383 | INFO | total log probability: -2.13
2026-01-29 17:07:25,383 | INFO | normalized log probability: -0.04
2026-01-29 17:07:25,383 | INFO | total number of ended hypotheses: 166
2026-01-29 17:07:25,383 | INFO | best hypo: ▁je▁connais▁la▁somme▁de▁courage▁et▁d'abnégation▁que▁requiert▁l'accomplissement▁de▁vos▁missions▁dans▁un▁tel▁contexte

2026-01-29 17:07:25,385 | INFO | speech length: 45920
2026-01-29 17:07:25,416 | INFO | decoder input length: 71
2026-01-29 17:07:25,416 | INFO | max output length: 71
2026-01-29 17:07:25,416 | INFO | min output length: 7
2026-01-29 17:07:26,358 | INFO | end detected at 26
2026-01-29 17:07:26,360 | INFO |  -1.51 * 0.5 =  -0.75 for decoder
2026-01-29 17:07:26,360 | INFO |  -0.04 * 0.5 =  -0.02 for ctc
2026-01-29 17:07:26,360 | INFO | total log probability: -0.77
2026-01-29 17:07:26,360 | INFO | normalized log probability: -0.04
2026-01-29 17:07:26,360 | INFO | total number of ended hypotheses: 166
2026-01-29 17:07:26,360 | INFO | best hypo: ▁je▁sais▁également▁ce▁que▁cela▁signifie▁pour▁vos▁familles

2026-01-29 17:07:26,362 | INFO | speech length: 28480
2026-01-29 17:07:26,391 | INFO | decoder input length: 44
2026-01-29 17:07:26,391 | INFO | max output length: 44
2026-01-29 17:07:26,391 | INFO | min output length: 4
2026-01-29 17:07:27,058 | INFO | end detected at 18
2026-01-29 17:07:27,059 | INFO |  -2.38 * 0.5 =  -1.19 for decoder
2026-01-29 17:07:27,059 | INFO |  -1.34 * 0.5 =  -0.67 for ctc
2026-01-29 17:07:27,059 | INFO | total log probability: -1.86
2026-01-29 17:07:27,059 | INFO | normalized log probability: -0.14
2026-01-29 17:07:27,059 | INFO | total number of ended hypotheses: 163
2026-01-29 17:07:27,059 | INFO | best hypo: ▁que▁je▁me▁saluais▁particulièrement

2026-01-29 17:07:27,061 | INFO | speech length: 82240
2026-01-29 17:07:27,092 | INFO | decoder input length: 128
2026-01-29 17:07:27,092 | INFO | max output length: 128
2026-01-29 17:07:27,092 | INFO | min output length: 12
2026-01-29 17:07:28,987 | INFO | end detected at 46
2026-01-29 17:07:28,988 | INFO |  -3.21 * 0.5 =  -1.61 for decoder
2026-01-29 17:07:28,988 | INFO |  -3.13 * 0.5 =  -1.57 for ctc
2026-01-29 17:07:28,988 | INFO | total log probability: -3.17
2026-01-29 17:07:28,988 | INFO | normalized log probability: -0.08
2026-01-29 17:07:28,988 | INFO | total number of ended hypotheses: 171
2026-01-29 17:07:28,989 | INFO | best hypo: ▁dont▁j'imagine▁qu'elles▁sont▁souvent▁confrontées▁à▁l'absence▁et▁parfois▁à▁l'angoisse

2026-01-29 17:07:28,991 | INFO | speech length: 62880
2026-01-29 17:07:29,021 | INFO | decoder input length: 97
2026-01-29 17:07:29,021 | INFO | max output length: 97
2026-01-29 17:07:29,021 | INFO | min output length: 9
2026-01-29 17:07:30,660 | INFO | end detected at 43
2026-01-29 17:07:30,662 | INFO |  -2.71 * 0.5 =  -1.35 for decoder
2026-01-29 17:07:30,662 | INFO |  -4.41 * 0.5 =  -2.21 for ctc
2026-01-29 17:07:30,662 | INFO | total log probability: -3.56
2026-01-29 17:07:30,662 | INFO | normalized log probability: -0.09
2026-01-29 17:07:30,662 | INFO | total number of ended hypotheses: 174
2026-01-29 17:07:30,663 | INFO | best hypo: ▁je▁sais▁aussi▁hélas▁le▁lourd▁tribut▁payé▁par▁certains▁de▁vos▁compagnons▁d'armes

2026-01-29 17:07:30,664 | INFO | speech length: 47840
2026-01-29 17:07:30,698 | INFO | decoder input length: 74
2026-01-29 17:07:30,698 | INFO | max output length: 74
2026-01-29 17:07:30,698 | INFO | min output length: 7
2026-01-29 17:07:31,806 | INFO | end detected at 30
2026-01-29 17:07:31,807 | INFO |  -2.81 * 0.5 =  -1.41 for decoder
2026-01-29 17:07:31,807 | INFO |  -3.78 * 0.5 =  -1.89 for ctc
2026-01-29 17:07:31,807 | INFO | total log probability: -3.29
2026-01-29 17:07:31,807 | INFO | normalized log probability: -0.13
2026-01-29 17:07:31,807 | INFO | total number of ended hypotheses: 163
2026-01-29 17:07:31,808 | INFO | best hypo: ▁tribut▁qui▁peut▁aller▁jusqu'au▁sacrifice▁ulté

2026-01-29 17:07:31,813 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:07:31,814 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:07:31,814 | INFO | Chunk: 2 | WER=8.333333 | S=0 D=1 I=0
2026-01-29 17:07:31,814 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:07:31,814 | INFO | Chunk: 4 | WER=25.000000 | S=2 D=0 I=0
2026-01-29 17:07:31,815 | INFO | Chunk: 5 | WER=9.090909 | S=2 D=0 I=0
2026-01-29 17:07:31,815 | INFO | Chunk: 6 | WER=17.647059 | S=2 D=1 I=0
2026-01-29 17:07:31,815 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:07:31,816 | INFO | Chunk: 8 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:07:31,816 | INFO | Chunk: 9 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:07:31,816 | INFO | Chunk: 10 | WER=16.666667 | S=1 D=0 I=0
2026-01-29 17:07:31,816 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:07:31,816 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:07:31,817 | INFO | Chunk: 13 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:07:31,817 | INFO | Chunk: 14 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:07:31,818 | INFO | Chunk: 15 | WER=10.000000 | S=1 D=0 I=1
2026-01-29 17:07:31,818 | INFO | Chunk: 16 | WER=5.882353 | S=1 D=0 I=0
2026-01-29 17:07:31,818 | INFO | Chunk: 17 | WER=50.000000 | S=0 D=1 I=0
2026-01-29 17:07:31,819 | INFO | Chunk: 18 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:07:31,819 | INFO | Chunk: 19 | WER=42.857143 | S=2 D=1 I=0
2026-01-29 17:07:31,819 | INFO | Chunk: 20 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:07:31,819 | INFO | Chunk: 21 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:07:31,820 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:07:31,820 | INFO | Chunk: 23 | WER=7.692308 | S=1 D=0 I=0
2026-01-29 17:07:31,820 | INFO | Chunk: 24 | WER=7.692308 | S=1 D=0 I=0
2026-01-29 17:07:31,821 | INFO | Chunk: 25 | WER=10.526316 | S=2 D=0 I=0
2026-01-29 17:07:31,821 | INFO | Chunk: 26 | WER=4.761905 | S=0 D=0 I=1
2026-01-29 17:07:31,821 | INFO | Chunk: 27 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:07:31,821 | INFO | Chunk: 28 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:07:31,822 | INFO | Chunk: 29 | WER=16.666667 | S=1 D=0 I=0
2026-01-29 17:07:31,822 | INFO | Chunk: 30 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:07:31,822 | INFO | Chunk: 31 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:07:31,823 | INFO | Chunk: 32 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:07:31,823 | INFO | Chunk: 33 | WER=7.142857 | S=1 D=0 I=0
2026-01-29 17:07:31,823 | INFO | Chunk: 34 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:07:31,823 | INFO | Chunk: 35 | WER=33.333333 | S=0 D=0 I=1
2026-01-29 17:07:31,824 | INFO | Chunk: 36 | WER=12.000000 | S=1 D=2 I=0
2026-01-29 17:07:31,824 | INFO | Chunk: 37 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:07:31,824 | INFO | Chunk: 38 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:07:31,825 | INFO | Chunk: 39 | WER=10.526316 | S=1 D=1 I=0
2026-01-29 17:07:31,825 | INFO | Chunk: 40 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:07:31,825 | INFO | Chunk: 41 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:07:31,826 | INFO | Chunk: 42 | WER=5.000000 | S=1 D=0 I=0
2026-01-29 17:07:31,826 | INFO | Chunk: 43 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:07:31,826 | INFO | Chunk: 44 | WER=40.000000 | S=2 D=0 I=0
2026-01-29 17:07:31,826 | INFO | Chunk: 45 | WER=6.666667 | S=0 D=0 I=1
2026-01-29 17:07:31,827 | INFO | Chunk: 46 | WER=6.666667 | S=1 D=0 I=0
2026-01-29 17:07:31,827 | INFO | Chunk: 47 | WER=12.500000 | S=1 D=0 I=0
2026-01-29 17:07:31,947 | INFO | File: Rhap-M2001.wav | WER=6.367041 | S=24 D=6 I=4
2026-01-29 17:07:31,947 | INFO | ------------------------------
2026-01-29 17:07:31,947 | INFO | Conf cv Done!
2026-01-29 17:07:32,114 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-29 17:07:32,133 | INFO | Vocabulary size: 47
2026-01-29 17:07:32,655 | INFO | Gradient checkpoint layers: []
2026-01-29 17:07:33,318 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 17:07:33,321 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 17:07:33,321 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 17:07:33,322 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-29 17:07:33,324 | INFO | speech length: 14880
2026-01-29 17:07:33,354 | INFO | decoder input length: 22
2026-01-29 17:07:33,354 | INFO | max output length: 22
2026-01-29 17:07:33,354 | INFO | min output length: 2
2026-01-29 17:07:33,948 | INFO | adding <eos> in the last position in the loop
2026-01-29 17:07:33,954 | INFO | no hypothesis. Finish decoding.
2026-01-29 17:07:33,955 | INFO |  -1.81 * 0.5 =  -0.90 for decoder
2026-01-29 17:07:33,955 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-29 17:07:33,955 | INFO | total log probability: -0.91
2026-01-29 17:07:33,955 | INFO | normalized log probability: -0.04
2026-01-29 17:07:33,955 | INFO | total number of ended hypotheses: 56
2026-01-29 17:07:33,955 | INFO | best hypo: mesdames<space>et<space>messieurs<sos/eos>

2026-01-29 17:07:33,955 | WARNING | best hypo length: 22 == max output length: 22
2026-01-29 17:07:33,955 | WARNING | decoding may be stopped by the max output length limitation, please consider to increase the maxlenratio.
2026-01-29 17:07:33,956 | INFO | speech length: 53600
2026-01-29 17:07:33,983 | INFO | decoder input length: 83
2026-01-29 17:07:33,983 | INFO | max output length: 83
2026-01-29 17:07:33,983 | INFO | min output length: 8
2026-01-29 17:07:35,864 | INFO | end detected at 57
2026-01-29 17:07:35,865 | INFO |  -4.09 * 0.5 =  -2.04 for decoder
2026-01-29 17:07:35,865 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:07:35,865 | INFO | total log probability: -2.04
2026-01-29 17:07:35,865 | INFO | normalized log probability: -0.04
2026-01-29 17:07:35,865 | INFO | total number of ended hypotheses: 169
2026-01-29 17:07:35,866 | INFO | best hypo: je<space>suis<space>heureux<space>de<space>me<space>retrouver<space>ce<space>soir<space>parmi<space>vous

2026-01-29 17:07:35,868 | INFO | speech length: 53280
2026-01-29 17:07:35,895 | INFO | decoder input length: 82
2026-01-29 17:07:35,895 | INFO | max output length: 82
2026-01-29 17:07:35,895 | INFO | min output length: 8
2026-01-29 17:07:37,936 | INFO | end detected at 63
2026-01-29 17:07:37,937 | INFO |  -5.49 * 0.5 =  -2.74 for decoder
2026-01-29 17:07:37,937 | INFO |  -1.78 * 0.5 =  -0.89 for ctc
2026-01-29 17:07:37,937 | INFO | total log probability: -3.63
2026-01-29 17:07:37,937 | INFO | normalized log probability: -0.06
2026-01-29 17:07:37,937 | INFO | total number of ended hypotheses: 159
2026-01-29 17:07:37,938 | INFO | best hypo: après<space>ma<space>visite<space>à<space>l'endivision<space>et<space>à<space>l'île<space>longue<space>ce<space>matin

2026-01-29 17:07:37,940 | INFO | speech length: 37440
2026-01-29 17:07:37,966 | INFO | decoder input length: 58
2026-01-29 17:07:37,966 | INFO | max output length: 58
2026-01-29 17:07:37,966 | INFO | min output length: 5
2026-01-29 17:07:39,170 | INFO | end detected at 37
2026-01-29 17:07:39,172 | INFO |  -2.48 * 0.5 =  -1.24 for decoder
2026-01-29 17:07:39,172 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-29 17:07:39,172 | INFO | total log probability: -1.24
2026-01-29 17:07:39,172 | INFO | normalized log probability: -0.04
2026-01-29 17:07:39,172 | INFO | total number of ended hypotheses: 171
2026-01-29 17:07:39,173 | INFO | best hypo: c'est<space>donc<space>une<space>journée<space>entière

2026-01-29 17:07:39,175 | INFO | speech length: 30400
2026-01-29 17:07:39,202 | INFO | decoder input length: 47
2026-01-29 17:07:39,202 | INFO | max output length: 47
2026-01-29 17:07:39,202 | INFO | min output length: 4
2026-01-29 17:07:40,549 | INFO | adding <eos> in the last position in the loop
2026-01-29 17:07:40,558 | INFO | no hypothesis. Finish decoding.
2026-01-29 17:07:40,559 | INFO |  -4.16 * 0.5 =  -2.08 for decoder
2026-01-29 17:07:40,559 | INFO |  -4.26 * 0.5 =  -2.13 for ctc
2026-01-29 17:07:40,559 | INFO | total log probability: -4.21
2026-01-29 17:07:40,559 | INFO | normalized log probability: -0.10
2026-01-29 17:07:40,559 | INFO | total number of ended hypotheses: 163
2026-01-29 17:07:40,560 | INFO | best hypo: que<space>j'aurais<space>passé<space>avec<space>nos<space>forces<space>armées

2026-01-29 17:07:40,561 | INFO | speech length: 88640
2026-01-29 17:07:40,588 | INFO | decoder input length: 138
2026-01-29 17:07:40,589 | INFO | max output length: 138
2026-01-29 17:07:40,589 | INFO | min output length: 13
2026-01-29 17:07:44,732 | INFO | end detected at 116
2026-01-29 17:07:44,733 | INFO |  -8.84 * 0.5 =  -4.42 for decoder
2026-01-29 17:07:44,733 | INFO |  -1.07 * 0.5 =  -0.54 for ctc
2026-01-29 17:07:44,733 | INFO | total log probability: -4.95
2026-01-29 17:07:44,733 | INFO | normalized log probability: -0.04
2026-01-29 17:07:44,733 | INFO | total number of ended hypotheses: 147
2026-01-29 17:07:44,734 | INFO | best hypo: c'est<space>bien<space>sûr<space>dans<space>mon<space>esprit<space>la<space>marque<space>du<space>lien<space>direct<space>qui<space>unit<space>le<space>président<space>de<space>la<space>république<space>chef<space>des<space>armées

2026-01-29 17:07:44,736 | INFO | speech length: 86880
2026-01-29 17:07:44,763 | INFO | decoder input length: 135
2026-01-29 17:07:44,764 | INFO | max output length: 135
2026-01-29 17:07:44,764 | INFO | min output length: 13
2026-01-29 17:07:48,319 | INFO | end detected at 97
2026-01-29 17:07:48,320 | INFO |  -7.48 * 0.5 =  -3.74 for decoder
2026-01-29 17:07:48,320 | INFO |  -1.41 * 0.5 =  -0.71 for ctc
2026-01-29 17:07:48,320 | INFO | total log probability: -4.45
2026-01-29 17:07:48,320 | INFO | normalized log probability: -0.05
2026-01-29 17:07:48,320 | INFO | total number of ended hypotheses: 183
2026-01-29 17:07:48,321 | INFO | best hypo: avec<space>toutes<space>celles<space>et<space>tous<space>ceux<space>qui<space>ont<space>la<space>difficile<space>mission<space>de<space>veiller<space>sur<space>nos<space>intérêts

2026-01-29 17:07:48,323 | INFO | speech length: 31360
2026-01-29 17:07:48,350 | INFO | decoder input length: 48
2026-01-29 17:07:48,350 | INFO | max output length: 48
2026-01-29 17:07:48,350 | INFO | min output length: 4
2026-01-29 17:07:49,683 | INFO | end detected at 43
2026-01-29 17:07:49,684 | INFO |  -3.07 * 0.5 =  -1.53 for decoder
2026-01-29 17:07:49,684 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:07:49,684 | INFO | total log probability: -1.54
2026-01-29 17:07:49,684 | INFO | normalized log probability: -0.04
2026-01-29 17:07:49,684 | INFO | total number of ended hypotheses: 152
2026-01-29 17:07:49,685 | INFO | best hypo: et<space>sur<space>la<space>sécurité<space>de<space>nos<space>concitoyens

2026-01-29 17:07:49,687 | INFO | speech length: 62720
2026-01-29 17:07:49,714 | INFO | decoder input length: 97
2026-01-29 17:07:49,714 | INFO | max output length: 97
2026-01-29 17:07:49,715 | INFO | min output length: 9
2026-01-29 17:07:52,630 | INFO | end detected at 87
2026-01-29 17:07:52,631 | INFO |  -6.55 * 0.5 =  -3.28 for decoder
2026-01-29 17:07:52,631 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-29 17:07:52,631 | INFO | total log probability: -3.29
2026-01-29 17:07:52,631 | INFO | normalized log probability: -0.04
2026-01-29 17:07:52,631 | INFO | total number of ended hypotheses: 160
2026-01-29 17:07:52,633 | INFO | best hypo: c'est<space>pour<space>cela<space>que<space>je<space>tenais<space>à<space>vous<space>rencontrer<space>la<space>veille<space>de<space>notre<space>fête<space>nationale

2026-01-29 17:07:52,635 | INFO | speech length: 50080
2026-01-29 17:07:52,664 | INFO | decoder input length: 77
2026-01-29 17:07:52,664 | INFO | max output length: 77
2026-01-29 17:07:52,664 | INFO | min output length: 7
2026-01-29 17:07:54,445 | INFO | end detected at 54
2026-01-29 17:07:54,446 | INFO |  -6.47 * 0.5 =  -3.23 for decoder
2026-01-29 17:07:54,446 | INFO |  -0.15 * 0.5 =  -0.07 for ctc
2026-01-29 17:07:54,446 | INFO | total log probability: -3.31
2026-01-29 17:07:54,446 | INFO | normalized log probability: -0.07
2026-01-29 17:07:54,446 | INFO | total number of ended hypotheses: 162
2026-01-29 17:07:54,446 | INFO | best hypo: le<space>quatorze<space>juillet<space>lors<space>du<space>traditionnel<space>définé

2026-01-29 17:07:54,448 | INFO | speech length: 25280
2026-01-29 17:07:54,475 | INFO | decoder input length: 39
2026-01-29 17:07:54,475 | INFO | max output length: 39
2026-01-29 17:07:54,475 | INFO | min output length: 3
2026-01-29 17:07:55,516 | INFO | end detected at 35
2026-01-29 17:07:55,517 | INFO |  -2.41 * 0.5 =  -1.20 for decoder
2026-01-29 17:07:55,517 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:07:55,517 | INFO | total log probability: -1.20
2026-01-29 17:07:55,517 | INFO | normalized log probability: -0.04
2026-01-29 17:07:55,517 | INFO | total number of ended hypotheses: 149
2026-01-29 17:07:55,518 | INFO | best hypo: c'est<space>la<space>nation<space>toute<space>entière

2026-01-29 17:07:55,519 | INFO | speech length: 13600
2026-01-29 17:07:55,547 | INFO | decoder input length: 20
2026-01-29 17:07:55,547 | INFO | max output length: 20
2026-01-29 17:07:55,547 | INFO | min output length: 2
2026-01-29 17:07:56,083 | INFO | adding <eos> in the last position in the loop
2026-01-29 17:07:56,089 | INFO | no hypothesis. Finish decoding.
2026-01-29 17:07:56,090 | INFO | -14.08 * 0.5 =  -7.04 for decoder
2026-01-29 17:07:56,090 | INFO |  -3.48 * 0.5 =  -1.74 for ctc
2026-01-29 17:07:56,090 | INFO | total log probability: -8.78
2026-01-29 17:07:56,090 | INFO | normalized log probability: -0.40
2026-01-29 17:07:56,090 | INFO | total number of ended hypotheses: 55
2026-01-29 17:07:56,090 | INFO | best hypo: qu<space>vous<space>rend<space>homage<sos/eos>

2026-01-29 17:07:56,090 | WARNING | best hypo length: 20 == max output length: 20
2026-01-29 17:07:56,090 | WARNING | decoding may be stopped by the max output length limitation, please consider to increase the maxlenratio.
2026-01-29 17:07:56,092 | INFO | speech length: 21920
2026-01-29 17:07:56,120 | INFO | decoder input length: 33
2026-01-29 17:07:56,120 | INFO | max output length: 33
2026-01-29 17:07:56,120 | INFO | min output length: 3
2026-01-29 17:07:56,923 | INFO | end detected at 27
2026-01-29 17:07:56,924 | INFO |  -1.81 * 0.5 =  -0.90 for decoder
2026-01-29 17:07:56,924 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:07:56,924 | INFO | total log probability: -0.91
2026-01-29 17:07:56,925 | INFO | normalized log probability: -0.04
2026-01-29 17:07:56,925 | INFO | total number of ended hypotheses: 129
2026-01-29 17:07:56,925 | INFO | best hypo: elle<space>salue<space>le<space>courage

2026-01-29 17:07:56,927 | INFO | speech length: 48480
2026-01-29 17:07:56,954 | INFO | decoder input length: 75
2026-01-29 17:07:56,954 | INFO | max output length: 75
2026-01-29 17:07:56,954 | INFO | min output length: 7
2026-01-29 17:07:58,988 | INFO | end detected at 53
2026-01-29 17:07:58,990 | INFO |  -3.79 * 0.5 =  -1.90 for decoder
2026-01-29 17:07:58,990 | INFO |  -0.03 * 0.5 =  -0.02 for ctc
2026-01-29 17:07:58,990 | INFO | total log probability: -1.91
2026-01-29 17:07:58,990 | INFO | normalized log probability: -0.04
2026-01-29 17:07:58,990 | INFO | total number of ended hypotheses: 161
2026-01-29 17:07:58,991 | INFO | best hypo: elle<space>salue<space>la<space>loyauté<space>elle<space>salue<space>le<space>dévouement

2026-01-29 17:07:58,993 | INFO | speech length: 147840
2026-01-29 17:07:59,031 | INFO | decoder input length: 230
2026-01-29 17:07:59,031 | INFO | max output length: 230
2026-01-29 17:07:59,031 | INFO | min output length: 23
2026-01-29 17:08:06,941 | INFO | end detected at 130
2026-01-29 17:08:06,943 | INFO | -10.00 * 0.5 =  -5.00 for decoder
2026-01-29 17:08:06,943 | INFO |  -0.36 * 0.5 =  -0.18 for ctc
2026-01-29 17:08:06,943 | INFO | total log probability: -5.18
2026-01-29 17:08:06,943 | INFO | normalized log probability: -0.04
2026-01-29 17:08:06,943 | INFO | total number of ended hypotheses: 166
2026-01-29 17:08:06,945 | INFO | best hypo: elle<space>salue<space>l'esprit<space>de<space>sacrifice<space>de<space>ceux<space>qui<space>ont<space>choisi<space>de<space>servir<space>sa<space>défense<space>ses<space>intérêts<space>et<space>ses<space>valeurs<space>à<space>travers<space>le<space>monde

2026-01-29 17:08:06,947 | INFO | speech length: 95200
2026-01-29 17:08:06,981 | INFO | decoder input length: 148
2026-01-29 17:08:06,981 | INFO | max output length: 148
2026-01-29 17:08:06,981 | INFO | min output length: 14
2026-01-29 17:08:12,716 | INFO | end detected at 119
2026-01-29 17:08:12,718 | INFO | -13.13 * 0.5 =  -6.56 for decoder
2026-01-29 17:08:12,718 | INFO |  -5.33 * 0.5 =  -2.67 for ctc
2026-01-29 17:08:12,718 | INFO | total log probability: -9.23
2026-01-29 17:08:12,718 | INFO | normalized log probability: -0.08
2026-01-29 17:08:12,718 | INFO | total number of ended hypotheses: 171
2026-01-29 17:08:12,720 | INFO | best hypo: et<space>c'est<space>la<space>raison<space>pour<space>laquelle<space>au<space>nom<space>de<space>la<space>république<space>je<space>norerai<space>certains<space>d'entre<space>vous<space>dans<space>quelques<space>instants

2026-01-29 17:08:12,723 | INFO | speech length: 69920
2026-01-29 17:08:12,757 | INFO | decoder input length: 108
2026-01-29 17:08:12,757 | INFO | max output length: 108
2026-01-29 17:08:12,757 | INFO | min output length: 10
2026-01-29 17:08:16,610 | INFO | end detected at 88
2026-01-29 17:08:16,612 | INFO |  -6.51 * 0.5 =  -3.25 for decoder
2026-01-29 17:08:16,612 | INFO |  -1.22 * 0.5 =  -0.61 for ctc
2026-01-29 17:08:16,612 | INFO | total log probability: -3.86
2026-01-29 17:08:16,612 | INFO | normalized log probability: -0.05
2026-01-29 17:08:16,612 | INFO | total number of ended hypotheses: 178
2026-01-29 17:08:16,614 | INFO | best hypo: cette<space>année<space>j'ai<space>tenu<space>à<space>ce<space>que<space>cette<space>fête<space>nationale<space>soit<space>aussi<space>celle<space>de<space>l'europe

2026-01-29 17:08:16,616 | INFO | speech length: 12160
2026-01-29 17:08:16,649 | INFO | decoder input length: 18
2026-01-29 17:08:16,649 | INFO | max output length: 18
2026-01-29 17:08:16,649 | INFO | min output length: 1
2026-01-29 17:08:17,201 | INFO | end detected at 15
2026-01-29 17:08:17,202 | INFO |  -0.82 * 0.5 =  -0.41 for decoder
2026-01-29 17:08:17,203 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:08:17,203 | INFO | total log probability: -0.41
2026-01-29 17:08:17,203 | INFO | normalized log probability: -0.04
2026-01-29 17:08:17,203 | INFO | total number of ended hypotheses: 139
2026-01-29 17:08:17,203 | INFO | best hypo: et<space>demain

2026-01-29 17:08:17,205 | INFO | speech length: 76000
2026-01-29 17:08:17,237 | INFO | decoder input length: 118
2026-01-29 17:08:17,237 | INFO | max output length: 118
2026-01-29 17:08:17,237 | INFO | min output length: 11
2026-01-29 17:08:21,875 | INFO | end detected at 104
2026-01-29 17:08:21,877 | INFO |  -8.86 * 0.5 =  -4.43 for decoder
2026-01-29 17:08:21,877 | INFO |  -0.99 * 0.5 =  -0.50 for ctc
2026-01-29 17:08:21,877 | INFO | total log probability: -4.92
2026-01-29 17:08:21,877 | INFO | normalized log probability: -0.05
2026-01-29 17:08:21,877 | INFO | total number of ended hypotheses: 180
2026-01-29 17:08:21,879 | INFO | best hypo: ce<space>sont<space>les<space>vingt<space>six<space>drapeaux<space>de<space>nos<space>partenaires<space>européens<space>qui<space>défileront<space>au<space>côté<space>de<space>nos<space>armées

2026-01-29 17:08:21,881 | INFO | speech length: 29120
2026-01-29 17:08:21,913 | INFO | decoder input length: 45
2026-01-29 17:08:21,913 | INFO | max output length: 45
2026-01-29 17:08:21,913 | INFO | min output length: 4
2026-01-29 17:08:23,539 | INFO | adding <eos> in the last position in the loop
2026-01-29 17:08:23,550 | INFO | no hypothesis. Finish decoding.
2026-01-29 17:08:23,552 | INFO |  -7.56 * 0.5 =  -3.78 for decoder
2026-01-29 17:08:23,552 | INFO |  -4.15 * 0.5 =  -2.07 for ctc
2026-01-29 17:08:23,552 | INFO | total log probability: -5.85
2026-01-29 17:08:23,552 | INFO | normalized log probability: -0.15
2026-01-29 17:08:23,552 | INFO | total number of ended hypotheses: 173
2026-01-29 17:08:23,553 | INFO | best hypo: je<space>vous<space>remercie<space>d'ailleurs<space>cher<space>hervé

2026-01-29 17:08:23,555 | INFO | speech length: 9600
2026-01-29 17:08:23,582 | INFO | decoder input length: 14
2026-01-29 17:08:23,582 | INFO | max output length: 14
2026-01-29 17:08:23,582 | INFO | min output length: 1
2026-01-29 17:08:24,046 | INFO | adding <eos> in the last position in the loop
2026-01-29 17:08:24,056 | INFO | no hypothesis. Finish decoding.
2026-01-29 17:08:24,057 | INFO |  -2.36 * 0.5 =  -1.18 for decoder
2026-01-29 17:08:24,057 | INFO |  -2.97 * 0.5 =  -1.48 for ctc
2026-01-29 17:08:24,057 | INFO | total log probability: -2.66
2026-01-29 17:08:24,057 | INFO | normalized log probability: -0.22
2026-01-29 17:08:24,057 | INFO | total number of ended hypotheses: 115
2026-01-29 17:08:24,057 | INFO | best hypo: chez<space>allin

2026-01-29 17:08:24,059 | INFO | speech length: 65920
2026-01-29 17:08:24,092 | INFO | decoder input length: 102
2026-01-29 17:08:24,092 | INFO | max output length: 102
2026-01-29 17:08:24,092 | INFO | min output length: 10
2026-01-29 17:08:27,575 | INFO | end detected at 80
2026-01-29 17:08:27,577 | INFO |  -5.92 * 0.5 =  -2.96 for decoder
2026-01-29 17:08:27,577 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:08:27,577 | INFO | total log probability: -2.96
2026-01-29 17:08:27,578 | INFO | normalized log probability: -0.04
2026-01-29 17:08:27,578 | INFO | total number of ended hypotheses: 182
2026-01-29 17:08:27,579 | INFO | best hypo: tout<space>spécialement<space>les<space>ministres<space>de<space>la<space>défense<space>qui<space>sont<space>parmi<space>nous<space>ce<space>soir

2026-01-29 17:08:27,581 | INFO | speech length: 38560
2026-01-29 17:08:27,613 | INFO | decoder input length: 59
2026-01-29 17:08:27,613 | INFO | max output length: 59
2026-01-29 17:08:27,613 | INFO | min output length: 5
2026-01-29 17:08:29,323 | INFO | end detected at 43
2026-01-29 17:08:29,325 | INFO |  -2.96 * 0.5 =  -1.48 for decoder
2026-01-29 17:08:29,325 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-29 17:08:29,325 | INFO | total log probability: -1.49
2026-01-29 17:08:29,325 | INFO | normalized log probability: -0.04
2026-01-29 17:08:29,325 | INFO | total number of ended hypotheses: 172
2026-01-29 17:08:29,326 | INFO | best hypo: je<space>leur<space>adresse<space>un<space>salut<space>très<space>amical

2026-01-29 17:08:29,327 | INFO | speech length: 40960
2026-01-29 17:08:29,361 | INFO | decoder input length: 63
2026-01-29 17:08:29,361 | INFO | max output length: 63
2026-01-29 17:08:29,361 | INFO | min output length: 6
2026-01-29 17:08:31,582 | INFO | end detected at 57
2026-01-29 17:08:31,583 | INFO |  -4.16 * 0.5 =  -2.08 for decoder
2026-01-29 17:08:31,584 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-29 17:08:31,584 | INFO | total log probability: -2.08
2026-01-29 17:08:31,584 | INFO | normalized log probability: -0.04
2026-01-29 17:08:31,584 | INFO | total number of ended hypotheses: 151
2026-01-29 17:08:31,585 | INFO | best hypo: c'est<space>l'image<space>d'une<space>europe<space>qui<space>repart<space>d'un<space>même<space>pas

2026-01-29 17:08:31,587 | INFO | speech length: 74880
2026-01-29 17:08:31,621 | INFO | decoder input length: 116
2026-01-29 17:08:31,621 | INFO | max output length: 116
2026-01-29 17:08:31,621 | INFO | min output length: 11
2026-01-29 17:08:35,422 | INFO | end detected at 84
2026-01-29 17:08:35,424 | INFO |  -6.55 * 0.5 =  -3.28 for decoder
2026-01-29 17:08:35,424 | INFO |  -0.30 * 0.5 =  -0.15 for ctc
2026-01-29 17:08:35,424 | INFO | total log probability: -3.43
2026-01-29 17:08:35,424 | INFO | normalized log probability: -0.04
2026-01-29 17:08:35,424 | INFO | total number of ended hypotheses: 155
2026-01-29 17:08:35,425 | INFO | best hypo: une<space>europe<space>résolue<space>à<space>se<space>remettre<space>au<space>mouvement<space>après<space>deux<space>années<space>d'immobilistes

2026-01-29 17:08:35,427 | INFO | speech length: 95840
2026-01-29 17:08:35,461 | INFO | decoder input length: 149
2026-01-29 17:08:35,461 | INFO | max output length: 149
2026-01-29 17:08:35,461 | INFO | min output length: 14
2026-01-29 17:08:41,166 | INFO | end detected at 117
2026-01-29 17:08:41,167 | INFO |  -8.96 * 0.5 =  -4.48 for decoder
2026-01-29 17:08:41,167 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-29 17:08:41,167 | INFO | total log probability: -4.49
2026-01-29 17:08:41,167 | INFO | normalized log probability: -0.04
2026-01-29 17:08:41,167 | INFO | total number of ended hypotheses: 147
2026-01-29 17:08:41,168 | INFO | best hypo: une<space>europe<space>vigilante<space>qui<space>n'oublie<space>pas<space>l'impératif<space>de<space>protection<space>dans<space>un<space>monde<space>plus<space>instable<space>et<space>moins<space>prévisible

2026-01-29 17:08:41,170 | INFO | speech length: 114560
2026-01-29 17:08:41,199 | INFO | decoder input length: 178
2026-01-29 17:08:41,199 | INFO | max output length: 178
2026-01-29 17:08:41,199 | INFO | min output length: 17
2026-01-29 17:08:46,274 | INFO | end detected at 123
2026-01-29 17:08:46,276 | INFO |  -9.34 * 0.5 =  -4.67 for decoder
2026-01-29 17:08:46,276 | INFO |  -0.07 * 0.5 =  -0.03 for ctc
2026-01-29 17:08:46,276 | INFO | total log probability: -4.71
2026-01-29 17:08:46,276 | INFO | normalized log probability: -0.04
2026-01-29 17:08:46,276 | INFO | total number of ended hypotheses: 182
2026-01-29 17:08:46,278 | INFO | best hypo: une<space>europe<space>prête<space>à<space>assumer<space>ses<space>responsabilités<space>et<space>au<space>sein<space>de<space>laquelle<space>la<space>france<space>a<space>bien<space>l'intention<space>de<space>tenir<space>son<space>rang

2026-01-29 17:08:46,281 | INFO | speech length: 33920
2026-01-29 17:08:46,318 | INFO | decoder input length: 52
2026-01-29 17:08:46,319 | INFO | max output length: 52
2026-01-29 17:08:46,319 | INFO | min output length: 5
2026-01-29 17:08:47,806 | INFO | end detected at 49
2026-01-29 17:08:47,807 | INFO |  -3.52 * 0.5 =  -1.76 for decoder
2026-01-29 17:08:47,807 | INFO |  -0.07 * 0.5 =  -0.04 for ctc
2026-01-29 17:08:47,807 | INFO | total log probability: -1.80
2026-01-29 17:08:47,807 | INFO | normalized log probability: -0.04
2026-01-29 17:08:47,807 | INFO | total number of ended hypotheses: 164
2026-01-29 17:08:47,808 | INFO | best hypo: les<space>bases<space>d'une<space>défense<space>européenne<space>existent

2026-01-29 17:08:47,810 | INFO | speech length: 21120
2026-01-29 17:08:47,838 | INFO | decoder input length: 32
2026-01-29 17:08:47,838 | INFO | max output length: 32
2026-01-29 17:08:47,838 | INFO | min output length: 3
2026-01-29 17:08:48,762 | INFO | adding <eos> in the last position in the loop
2026-01-29 17:08:48,771 | INFO | end detected at 31
2026-01-29 17:08:48,772 | INFO |  -2.78 * 0.5 =  -1.39 for decoder
2026-01-29 17:08:48,772 | INFO |  -0.18 * 0.5 =  -0.09 for ctc
2026-01-29 17:08:48,772 | INFO | total log probability: -1.48
2026-01-29 17:08:48,772 | INFO | normalized log probability: -0.05
2026-01-29 17:08:48,772 | INFO | total number of ended hypotheses: 176
2026-01-29 17:08:48,773 | INFO | best hypo: il<space>faut<space>les<space>faire<space>grandir

2026-01-29 17:08:48,775 | INFO | speech length: 30880
2026-01-29 17:08:48,803 | INFO | decoder input length: 47
2026-01-29 17:08:48,803 | INFO | max output length: 47
2026-01-29 17:08:48,803 | INFO | min output length: 4
2026-01-29 17:08:49,934 | INFO | end detected at 37
2026-01-29 17:08:49,935 | INFO |  -2.56 * 0.5 =  -1.28 for decoder
2026-01-29 17:08:49,935 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:08:49,935 | INFO | total log probability: -1.28
2026-01-29 17:08:49,936 | INFO | normalized log probability: -0.04
2026-01-29 17:08:49,936 | INFO | total number of ended hypotheses: 139
2026-01-29 17:08:49,936 | INFO | best hypo: en<space>quittant<space>le<space>terrain<space>des<space>mots

2026-01-29 17:08:49,938 | INFO | speech length: 25920
2026-01-29 17:08:49,967 | INFO | decoder input length: 40
2026-01-29 17:08:49,967 | INFO | max output length: 40
2026-01-29 17:08:49,967 | INFO | min output length: 4
2026-01-29 17:08:51,017 | INFO | end detected at 35
2026-01-29 17:08:51,018 | INFO |  -2.42 * 0.5 =  -1.21 for decoder
2026-01-29 17:08:51,018 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:08:51,018 | INFO | total log probability: -1.21
2026-01-29 17:08:51,019 | INFO | normalized log probability: -0.04
2026-01-29 17:08:51,019 | INFO | total number of ended hypotheses: 143
2026-01-29 17:08:51,019 | INFO | best hypo: pour<space>gagner<space>celui<space>de<space>l'action

2026-01-29 17:08:51,021 | INFO | speech length: 134240
2026-01-29 17:08:51,049 | INFO | decoder input length: 209
2026-01-29 17:08:51,049 | INFO | max output length: 209
2026-01-29 17:08:51,049 | INFO | min output length: 20
2026-01-29 17:08:56,130 | INFO | end detected at 116
2026-01-29 17:08:56,131 | INFO |  -8.85 * 0.5 =  -4.42 for decoder
2026-01-29 17:08:56,131 | INFO |  -0.08 * 0.5 =  -0.04 for ctc
2026-01-29 17:08:56,131 | INFO | total log probability: -4.46
2026-01-29 17:08:56,131 | INFO | normalized log probability: -0.04
2026-01-29 17:08:56,131 | INFO | total number of ended hypotheses: 165
2026-01-29 17:08:56,132 | INFO | best hypo: demain<space>davantage<space>qu'aujourd'hui<space>je<space>souhaite<space>que<space>l'europe<space>soit<space>capable<space>d'assurer<space>sa<space>sécurité<space>de<space>façon<space>autonome

2026-01-29 17:08:56,134 | INFO | speech length: 46080
2026-01-29 17:08:56,162 | INFO | decoder input length: 71
2026-01-29 17:08:56,162 | INFO | max output length: 71
2026-01-29 17:08:56,162 | INFO | min output length: 7
2026-01-29 17:08:58,302 | INFO | end detected at 69
2026-01-29 17:08:58,303 | INFO |  -5.09 * 0.5 =  -2.54 for decoder
2026-01-29 17:08:58,303 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-29 17:08:58,303 | INFO | total log probability: -2.55
2026-01-29 17:08:58,303 | INFO | normalized log probability: -0.04
2026-01-29 17:08:58,303 | INFO | total number of ended hypotheses: 161
2026-01-29 17:08:58,304 | INFO | best hypo: alors<space>si<space>la<space>france<space>pèse<space>aujourd'hui<space>sur<space>la<space>scène<space>internationale

2026-01-29 17:08:58,305 | INFO | speech length: 80320
2026-01-29 17:08:58,333 | INFO | decoder input length: 125
2026-01-29 17:08:58,333 | INFO | max output length: 125
2026-01-29 17:08:58,333 | INFO | min output length: 12
2026-01-29 17:09:01,069 | INFO | end detected at 75
2026-01-29 17:09:01,071 | INFO |  -5.53 * 0.5 =  -2.77 for decoder
2026-01-29 17:09:01,071 | INFO |  -0.10 * 0.5 =  -0.05 for ctc
2026-01-29 17:09:01,071 | INFO | total log probability: -2.81
2026-01-29 17:09:01,071 | INFO | normalized log probability: -0.04
2026-01-29 17:09:01,071 | INFO | total number of ended hypotheses: 164
2026-01-29 17:09:01,072 | INFO | best hypo: il<space>lui<space>faut<space>un<space>outil<space>de<space>défense<space>adapté<space>à<space>la<space>hauteur<space>de<space>ces<space>ambitions

2026-01-29 17:09:01,073 | INFO | speech length: 55840
2026-01-29 17:09:01,114 | INFO | decoder input length: 86
2026-01-29 17:09:01,114 | INFO | max output length: 86
2026-01-29 17:09:01,114 | INFO | min output length: 8
2026-01-29 17:09:03,793 | INFO | end detected at 55
2026-01-29 17:09:03,794 | INFO |  -3.91 * 0.5 =  -1.95 for decoder
2026-01-29 17:09:03,794 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-29 17:09:03,794 | INFO | total log probability: -1.96
2026-01-29 17:09:03,794 | INFO | normalized log probability: -0.04
2026-01-29 17:09:03,794 | INFO | total number of ended hypotheses: 173
2026-01-29 17:09:03,795 | INFO | best hypo: c'est<space>aussi<space>là<space>l'intérêt<space>fondamental<space>de<space>l'europe

2026-01-29 17:09:03,798 | INFO | speech length: 10400
2026-01-29 17:09:03,824 | INFO | decoder input length: 15
2026-01-29 17:09:03,824 | INFO | max output length: 15
2026-01-29 17:09:03,824 | INFO | min output length: 1
2026-01-29 17:09:04,223 | INFO | adding <eos> in the last position in the loop
2026-01-29 17:09:04,229 | INFO | no hypothesis. Finish decoding.
2026-01-29 17:09:04,229 | INFO |  -8.95 * 0.5 =  -4.47 for decoder
2026-01-29 17:09:04,229 | INFO | -10.11 * 0.5 =  -5.05 for ctc
2026-01-29 17:09:04,229 | INFO | total log probability: -9.53
2026-01-29 17:09:04,229 | INFO | normalized log probability: -0.64
2026-01-29 17:09:04,229 | INFO | total number of ended hypotheses: 51
2026-01-29 17:09:04,230 | INFO | best hypo: et<space>denous<space>par

2026-01-29 17:09:04,231 | INFO | speech length: 88320
2026-01-29 17:09:04,256 | INFO | decoder input length: 137
2026-01-29 17:09:04,256 | INFO | max output length: 137
2026-01-29 17:09:04,256 | INFO | min output length: 13
2026-01-29 17:09:08,580 | INFO | end detected at 122
2026-01-29 17:09:08,581 | INFO |  -9.32 * 0.5 =  -4.66 for decoder
2026-01-29 17:09:08,581 | INFO |  -1.04 * 0.5 =  -0.52 for ctc
2026-01-29 17:09:08,581 | INFO | total log probability: -5.18
2026-01-29 17:09:08,581 | INFO | normalized log probability: -0.04
2026-01-29 17:09:08,582 | INFO | total number of ended hypotheses: 171
2026-01-29 17:09:08,583 | INFO | best hypo: aujourd'hui<space>plus<space>que<space>jamais<space>vous<space>le<space>savez<space>mieux<space>que<space>personne<space>c'est<space>sur<space>le<space>terrain<space>que<space>se<space>gagne<space>ou<space>se<space>perd<space>le<space>combat

2026-01-29 17:09:08,585 | INFO | speech length: 20320
2026-01-29 17:09:08,614 | INFO | decoder input length: 31
2026-01-29 17:09:08,614 | INFO | max output length: 31
2026-01-29 17:09:08,614 | INFO | min output length: 3
2026-01-29 17:09:09,386 | INFO | end detected at 26
2026-01-29 17:09:09,387 | INFO |  -1.70 * 0.5 =  -0.85 for decoder
2026-01-29 17:09:09,387 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:09:09,387 | INFO | total log probability: -0.85
2026-01-29 17:09:09,387 | INFO | normalized log probability: -0.04
2026-01-29 17:09:09,387 | INFO | total number of ended hypotheses: 148
2026-01-29 17:09:09,387 | INFO | best hypo: c'est<space>sur<space>le<space>terrain

2026-01-29 17:09:09,389 | INFO | speech length: 127680
2026-01-29 17:09:09,415 | INFO | decoder input length: 199
2026-01-29 17:09:09,416 | INFO | max output length: 199
2026-01-29 17:09:09,416 | INFO | min output length: 19
2026-01-29 17:09:14,760 | INFO | end detected at 126
2026-01-29 17:09:14,761 | INFO |  -9.66 * 0.5 =  -4.83 for decoder
2026-01-29 17:09:14,761 | INFO |  -0.08 * 0.5 =  -0.04 for ctc
2026-01-29 17:09:14,761 | INFO | total log probability: -4.87
2026-01-29 17:09:14,761 | INFO | normalized log probability: -0.04
2026-01-29 17:09:14,761 | INFO | total number of ended hypotheses: 179
2026-01-29 17:09:14,763 | INFO | best hypo: qu'une<space>nation<space>affirme<space>son<space>influence<space>qu'elle<space>pèse<space>dans<space>une<space>coalition<space>au<space>travers<space>des<space>forces<space>qu'elle<space>est<space>capable<space>d'engager

2026-01-29 17:09:14,767 | INFO | speech length: 79200
2026-01-29 17:09:14,808 | INFO | decoder input length: 123
2026-01-29 17:09:14,808 | INFO | max output length: 123
2026-01-29 17:09:14,808 | INFO | min output length: 12
2026-01-29 17:09:18,659 | INFO | end detected at 104
2026-01-29 17:09:18,660 | INFO |  -7.92 * 0.5 =  -3.96 for decoder
2026-01-29 17:09:18,660 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-29 17:09:18,660 | INFO | total log probability: -3.97
2026-01-29 17:09:18,661 | INFO | normalized log probability: -0.04
2026-01-29 17:09:18,661 | INFO | total number of ended hypotheses: 153
2026-01-29 17:09:18,662 | INFO | best hypo: cet<space>engagement<space>sur<space>le<space>terrain<space>est<space>pour<space>vous<space>de<space>plus<space>en<space>plus<space>difficile<space>et<space>de<space>plus<space>en<space>plus<space>dangereux

2026-01-29 17:09:18,664 | INFO | speech length: 17280
2026-01-29 17:09:18,691 | INFO | decoder input length: 26
2026-01-29 17:09:18,691 | INFO | max output length: 26
2026-01-29 17:09:18,691 | INFO | min output length: 2
2026-01-29 17:09:19,264 | INFO | end detected at 19
2026-01-29 17:09:19,265 | INFO |  -1.15 * 0.5 =  -0.58 for decoder
2026-01-29 17:09:19,265 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-29 17:09:19,265 | INFO | total log probability: -0.58
2026-01-29 17:09:19,265 | INFO | normalized log probability: -0.04
2026-01-29 17:09:19,265 | INFO | total number of ended hypotheses: 137
2026-01-29 17:09:19,265 | INFO | best hypo: l'afghanistan

2026-01-29 17:09:19,267 | INFO | speech length: 16000
2026-01-29 17:09:19,292 | INFO | decoder input length: 24
2026-01-29 17:09:19,292 | INFO | max output length: 24
2026-01-29 17:09:19,292 | INFO | min output length: 2
2026-01-29 17:09:19,970 | INFO | end detected at 22
2026-01-29 17:09:19,971 | INFO |  -1.38 * 0.5 =  -0.69 for decoder
2026-01-29 17:09:19,971 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:09:19,971 | INFO | total log probability: -0.69
2026-01-29 17:09:19,971 | INFO | normalized log probability: -0.04
2026-01-29 17:09:19,971 | INFO | total number of ended hypotheses: 139
2026-01-29 17:09:19,971 | INFO | best hypo: le<space>proche<space>orient

2026-01-29 17:09:19,973 | INFO | speech length: 106240
2026-01-29 17:09:19,999 | INFO | decoder input length: 165
2026-01-29 17:09:19,999 | INFO | max output length: 165
2026-01-29 17:09:19,999 | INFO | min output length: 16
2026-01-29 17:09:24,747 | INFO | end detected at 123
2026-01-29 17:09:24,750 | INFO | -13.25 * 0.5 =  -6.62 for decoder
2026-01-29 17:09:24,750 | INFO |  -2.52 * 0.5 =  -1.26 for ctc
2026-01-29 17:09:24,750 | INFO | total log probability: -7.89
2026-01-29 17:09:24,750 | INFO | normalized log probability: -0.07
2026-01-29 17:09:24,750 | INFO | total number of ended hypotheses: 182
2026-01-29 17:09:24,752 | INFO | best hypo: je<space>connais<space>la<space>somme<space>de<space>courage<space>et<space>d'abnégation<space>que<space>requièrent<space>l'accomplissement<space>de<space>vos<space>missions<space>dans<space>un<space>tel<space>contexte

2026-01-29 17:09:24,756 | INFO | speech length: 45920
2026-01-29 17:09:24,795 | INFO | decoder input length: 71
2026-01-29 17:09:24,795 | INFO | max output length: 71
2026-01-29 17:09:24,795 | INFO | min output length: 7
2026-01-29 17:09:26,725 | INFO | end detected at 62
2026-01-29 17:09:26,726 | INFO |  -4.56 * 0.5 =  -2.28 for decoder
2026-01-29 17:09:26,726 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-29 17:09:26,726 | INFO | total log probability: -2.28
2026-01-29 17:09:26,726 | INFO | normalized log probability: -0.04
2026-01-29 17:09:26,727 | INFO | total number of ended hypotheses: 156
2026-01-29 17:09:26,727 | INFO | best hypo: je<space>sais<space>également<space>ce<space>que<space>cela<space>signifie<space>pour<space>vos<space>familles

2026-01-29 17:09:26,729 | INFO | speech length: 28480
2026-01-29 17:09:26,754 | INFO | decoder input length: 44
2026-01-29 17:09:26,754 | INFO | max output length: 44
2026-01-29 17:09:26,754 | INFO | min output length: 4
2026-01-29 17:09:27,971 | INFO | end detected at 41
2026-01-29 17:09:27,972 | INFO |  -2.88 * 0.5 =  -1.44 for decoder
2026-01-29 17:09:27,972 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:09:27,972 | INFO | total log probability: -1.44
2026-01-29 17:09:27,972 | INFO | normalized log probability: -0.04
2026-01-29 17:09:27,972 | INFO | total number of ended hypotheses: 145
2026-01-29 17:09:27,972 | INFO | best hypo: que<space>je<space>veux<space>saluer<space>particulièrement

2026-01-29 17:09:27,974 | INFO | speech length: 82240
2026-01-29 17:09:28,000 | INFO | decoder input length: 128
2026-01-29 17:09:28,000 | INFO | max output length: 128
2026-01-29 17:09:28,000 | INFO | min output length: 12
2026-01-29 17:09:31,265 | INFO | end detected at 90
2026-01-29 17:09:31,266 | INFO |  -6.99 * 0.5 =  -3.49 for decoder
2026-01-29 17:09:31,267 | INFO |  -0.17 * 0.5 =  -0.09 for ctc
2026-01-29 17:09:31,267 | INFO | total log probability: -3.58
2026-01-29 17:09:31,267 | INFO | normalized log probability: -0.04
2026-01-29 17:09:31,267 | INFO | total number of ended hypotheses: 156
2026-01-29 17:09:31,268 | INFO | best hypo: dont<space>j'imagine<space>qu'elles<space>sont<space>souvent<space>confrontées<space>à<space>l'absence<space>et<space>parfois<space>à<space>l'angoisse

2026-01-29 17:09:31,270 | INFO | speech length: 62880
2026-01-29 17:09:31,295 | INFO | decoder input length: 97
2026-01-29 17:09:31,295 | INFO | max output length: 97
2026-01-29 17:09:31,295 | INFO | min output length: 9
2026-01-29 17:09:34,096 | INFO | end detected at 85
2026-01-29 17:09:34,097 | INFO |  -6.34 * 0.5 =  -3.17 for decoder
2026-01-29 17:09:34,097 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-29 17:09:34,097 | INFO | total log probability: -3.17
2026-01-29 17:09:34,097 | INFO | normalized log probability: -0.04
2026-01-29 17:09:34,097 | INFO | total number of ended hypotheses: 161
2026-01-29 17:09:34,098 | INFO | best hypo: je<space>sais<space>aussi<space>hélas<space>le<space>lourd<space>tribut<space>payé<space>par<space>certains<space>de<space>vos<space>compagnons<space>d'armes

2026-01-29 17:09:34,100 | INFO | speech length: 47840
2026-01-29 17:09:34,125 | INFO | decoder input length: 74
2026-01-29 17:09:34,125 | INFO | max output length: 74
2026-01-29 17:09:34,125 | INFO | min output length: 7
2026-01-29 17:09:35,807 | INFO | end detected at 52
2026-01-29 17:09:35,808 | INFO |  -7.36 * 0.5 =  -3.68 for decoder
2026-01-29 17:09:35,808 | INFO |  -0.77 * 0.5 =  -0.39 for ctc
2026-01-29 17:09:35,808 | INFO | total log probability: -4.07
2026-01-29 17:09:35,808 | INFO | normalized log probability: -0.09
2026-01-29 17:09:35,808 | INFO | total number of ended hypotheses: 165
2026-01-29 17:09:35,809 | INFO | best hypo: tribut<space>qui<space>peut<space>aller<space>jusqu'au<space>sacrifice<space>ulté

2026-01-29 17:09:35,818 | INFO | Chunk: 0 | WER=66.666667 | S=0 D=0 I=2
2026-01-29 17:09:35,818 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:09:35,818 | INFO | Chunk: 2 | WER=16.666667 | S=1 D=0 I=1
2026-01-29 17:09:35,819 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:09:35,819 | INFO | Chunk: 4 | WER=25.000000 | S=2 D=0 I=0
2026-01-29 17:09:35,819 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:09:35,820 | INFO | Chunk: 6 | WER=5.882353 | S=0 D=1 I=0
2026-01-29 17:09:35,820 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:09:35,820 | INFO | Chunk: 8 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:09:35,820 | INFO | Chunk: 9 | WER=14.285714 | S=1 D=0 I=0
2026-01-29 17:09:35,821 | INFO | Chunk: 10 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:09:35,821 | INFO | Chunk: 11 | WER=100.000000 | S=2 D=0 I=2
2026-01-29 17:09:35,821 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:09:35,821 | INFO | Chunk: 13 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:09:35,822 | INFO | Chunk: 14 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:09:35,822 | INFO | Chunk: 15 | WER=15.000000 | S=2 D=0 I=1
2026-01-29 17:09:35,823 | INFO | Chunk: 16 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:09:35,823 | INFO | Chunk: 17 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:09:35,823 | INFO | Chunk: 18 | WER=11.764706 | S=2 D=0 I=0
2026-01-29 17:09:35,823 | INFO | Chunk: 19 | WER=28.571429 | S=2 D=0 I=0
2026-01-29 17:09:35,823 | INFO | Chunk: 20 | WER=100.000000 | S=2 D=0 I=0
2026-01-29 17:09:35,824 | INFO | Chunk: 21 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:09:35,824 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:09:35,824 | INFO | Chunk: 23 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:09:35,825 | INFO | Chunk: 24 | WER=15.384615 | S=2 D=0 I=0
2026-01-29 17:09:35,825 | INFO | Chunk: 25 | WER=5.263158 | S=1 D=0 I=0
2026-01-29 17:09:35,826 | INFO | Chunk: 26 | WER=4.761905 | S=0 D=0 I=1
2026-01-29 17:09:35,826 | INFO | Chunk: 27 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:09:35,826 | INFO | Chunk: 28 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:09:35,826 | INFO | Chunk: 29 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:09:35,826 | INFO | Chunk: 30 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:09:35,827 | INFO | Chunk: 31 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:09:35,827 | INFO | Chunk: 32 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:09:35,827 | INFO | Chunk: 33 | WER=14.285714 | S=2 D=0 I=0
2026-01-29 17:09:35,828 | INFO | Chunk: 34 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:09:35,828 | INFO | Chunk: 35 | WER=66.666667 | S=2 D=0 I=0
2026-01-29 17:09:35,828 | INFO | Chunk: 36 | WER=4.000000 | S=0 D=1 I=0
2026-01-29 17:09:35,828 | INFO | Chunk: 37 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:09:35,829 | INFO | Chunk: 38 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:09:35,829 | INFO | Chunk: 39 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:09:35,830 | INFO | Chunk: 40 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:09:35,830 | INFO | Chunk: 41 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:09:35,830 | INFO | Chunk: 42 | WER=5.000000 | S=1 D=0 I=0
2026-01-29 17:09:35,830 | INFO | Chunk: 43 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:09:35,831 | INFO | Chunk: 44 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:09:35,831 | INFO | Chunk: 45 | WER=6.666667 | S=0 D=0 I=1
2026-01-29 17:09:35,831 | INFO | Chunk: 46 | WER=6.666667 | S=1 D=0 I=0
2026-01-29 17:09:35,832 | INFO | Chunk: 47 | WER=12.500000 | S=1 D=0 I=0
2026-01-29 17:09:35,956 | INFO | File: Rhap-M2001.wav | WER=6.179775 | S=24 D=1 I=8
2026-01-29 17:09:35,956 | INFO | ------------------------------
2026-01-29 17:09:35,956 | INFO | Conf ester Done!
2026-01-29 17:12:53,991 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:12:53,992 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:12:53,992 | INFO | Chunk: 2 | WER=33.333333 | S=3 D=0 I=1
2026-01-29 17:12:53,993 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:12:53,993 | INFO | Chunk: 4 | WER=25.000000 | S=2 D=0 I=0
2026-01-29 17:12:53,994 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:12:53,994 | INFO | Chunk: 6 | WER=5.882353 | S=0 D=1 I=0
2026-01-29 17:12:53,995 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:12:53,995 | INFO | Chunk: 8 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:12:53,996 | INFO | Chunk: 9 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:12:53,996 | INFO | Chunk: 10 | WER=16.666667 | S=1 D=0 I=0
2026-01-29 17:12:53,996 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:12:53,997 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:12:53,997 | INFO | Chunk: 13 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:12:53,998 | INFO | Chunk: 14 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:12:53,999 | INFO | Chunk: 15 | WER=30.000000 | S=3 D=0 I=3
2026-01-29 17:12:53,999 | INFO | Chunk: 16 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:12:53,999 | INFO | Chunk: 17 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:12:53,999 | INFO | Chunk: 18 | WER=11.764706 | S=1 D=1 I=0
2026-01-29 17:12:54,000 | INFO | Chunk: 19 | WER=57.142857 | S=3 D=1 I=0
2026-01-29 17:12:54,000 | INFO | Chunk: 20 | WER=100.000000 | S=2 D=0 I=0
2026-01-29 17:12:54,000 | INFO | Chunk: 21 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:12:54,000 | INFO | Chunk: 22 | WER=14.285714 | S=1 D=0 I=0
2026-01-29 17:12:54,001 | INFO | Chunk: 23 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:12:54,001 | INFO | Chunk: 24 | WER=7.692308 | S=1 D=0 I=0
2026-01-29 17:12:54,001 | INFO | Chunk: 25 | WER=5.263158 | S=1 D=0 I=0
2026-01-29 17:12:54,002 | INFO | Chunk: 26 | WER=4.761905 | S=0 D=0 I=1
2026-01-29 17:12:54,002 | INFO | Chunk: 27 | WER=14.285714 | S=1 D=0 I=0
2026-01-29 17:12:54,002 | INFO | Chunk: 28 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:12:54,002 | INFO | Chunk: 29 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:12:54,003 | INFO | Chunk: 30 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:12:54,003 | INFO | Chunk: 31 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:12:54,003 | INFO | Chunk: 32 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:12:54,004 | INFO | Chunk: 33 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:12:54,004 | INFO | Chunk: 34 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:12:54,004 | INFO | Chunk: 35 | WER=33.333333 | S=0 D=0 I=1
2026-01-29 17:12:54,005 | INFO | Chunk: 36 | WER=8.000000 | S=1 D=1 I=0
2026-01-29 17:12:54,005 | INFO | Chunk: 37 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:12:54,005 | INFO | Chunk: 38 | WER=13.636364 | S=2 D=1 I=0
2026-01-29 17:12:54,006 | INFO | Chunk: 39 | WER=10.526316 | S=2 D=0 I=0
2026-01-29 17:12:54,006 | INFO | Chunk: 40 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:12:54,006 | INFO | Chunk: 41 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:12:54,007 | INFO | Chunk: 42 | WER=5.000000 | S=1 D=0 I=0
2026-01-29 17:12:54,007 | INFO | Chunk: 43 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:12:54,007 | INFO | Chunk: 44 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:12:54,007 | INFO | Chunk: 45 | WER=6.666667 | S=1 D=0 I=0
2026-01-29 17:12:54,008 | INFO | Chunk: 46 | WER=6.666667 | S=1 D=0 I=0
2026-01-29 17:12:54,008 | INFO | Chunk: 47 | WER=12.500000 | S=1 D=0 I=0
2026-01-29 17:12:54,132 | INFO | File: Rhap-M2001.wav | WER=7.116105 | S=28 D=4 I=6
2026-01-29 17:12:54,132 | INFO | ------------------------------
2026-01-29 17:12:54,132 | INFO | hmm_tdnn Done!
2026-01-29 17:12:54,297 | INFO | ==================================Rhap-M2002.wav=========================================
2026-01-29 17:12:54,444 | INFO | Using rVAD model
2026-01-29 17:13:21,555 | INFO | Chunk: 0 | WER=15.384615 | S=2 D=4 I=0
2026-01-29 17:13:21,560 | INFO | Chunk: 1 | WER=15.533981 | S=6 D=10 I=0
2026-01-29 17:13:21,563 | INFO | Chunk: 2 | WER=25.806452 | S=4 D=20 I=0
2026-01-29 17:13:21,567 | INFO | Chunk: 3 | WER=17.021277 | S=4 D=12 I=0
2026-01-29 17:13:21,570 | INFO | Chunk: 4 | WER=40.425532 | S=14 D=24 I=0
2026-01-29 17:13:21,572 | INFO | Chunk: 5 | WER=17.187500 | S=3 D=8 I=0
2026-01-29 17:13:21,578 | INFO | Chunk: 6 | WER=17.857143 | S=7 D=12 I=1
2026-01-29 17:13:21,581 | INFO | Chunk: 7 | WER=24.096386 | S=6 D=14 I=0
2026-01-29 17:13:21,585 | INFO | Chunk: 8 | WER=8.791209 | S=3 D=5 I=0
2026-01-29 17:13:21,589 | INFO | Chunk: 9 | WER=19.266055 | S=6 D=15 I=0
2026-01-29 17:13:21,594 | INFO | Chunk: 10 | WER=14.285714 | S=7 D=7 I=0
2026-01-29 17:13:21,597 | INFO | Chunk: 11 | WER=21.428571 | S=6 D=12 I=0
2026-01-29 17:13:21,600 | INFO | Chunk: 12 | WER=14.606742 | S=3 D=10 I=0
2026-01-29 17:13:21,603 | INFO | Chunk: 13 | WER=20.779221 | S=4 D=12 I=0
2026-01-29 17:13:21,606 | INFO | Chunk: 14 | WER=20.253165 | S=6 D=9 I=1
2026-01-29 17:13:22,223 | INFO | File: Rhap-M2002.wav | WER=15.673469 | S=84 D=98 I=10
2026-01-29 17:13:22,223 | INFO | ------------------------------
2026-01-29 17:13:22,223 | INFO | w2vec vad chunk Done!
2026-01-29 17:13:53,366 | INFO | Chunk: 0 | WER=41.025641 | S=0 D=16 I=0
2026-01-29 17:13:53,369 | INFO | Chunk: 1 | WER=64.077670 | S=13 D=53 I=0
2026-01-29 17:13:53,371 | INFO | Chunk: 2 | WER=59.139785 | S=1 D=54 I=0
2026-01-29 17:13:53,373 | INFO | Chunk: 3 | WER=71.276596 | S=0 D=67 I=0
2026-01-29 17:13:53,375 | INFO | Chunk: 4 | WER=62.765957 | S=22 D=37 I=0
2026-01-29 17:13:53,377 | INFO | Chunk: 5 | WER=40.625000 | S=3 D=23 I=0
2026-01-29 17:13:53,380 | INFO | Chunk: 6 | WER=66.071429 | S=11 D=63 I=0
2026-01-29 17:13:53,382 | INFO | Chunk: 7 | WER=61.445783 | S=4 D=47 I=0
2026-01-29 17:13:53,383 | INFO | Chunk: 8 | WER=84.615385 | S=2 D=75 I=0
2026-01-29 17:13:53,386 | INFO | Chunk: 9 | WER=55.045872 | S=12 D=48 I=0
2026-01-29 17:13:53,388 | INFO | Chunk: 10 | WER=72.448980 | S=0 D=71 I=0
2026-01-29 17:13:53,390 | INFO | Chunk: 11 | WER=58.333333 | S=5 D=44 I=0
2026-01-29 17:13:53,393 | INFO | Chunk: 12 | WER=65.168539 | S=19 D=39 I=0
2026-01-29 17:13:53,394 | INFO | Chunk: 13 | WER=53.246753 | S=1 D=40 I=0
2026-01-29 17:13:53,396 | INFO | Chunk: 14 | WER=56.962025 | S=0 D=45 I=0
2026-01-29 17:13:53,714 | INFO | File: Rhap-M2002.wav | WER=59.755102 | S=94 D=638 I=0
2026-01-29 17:13:53,715 | INFO | ------------------------------
2026-01-29 17:13:53,715 | INFO | whisper med Done!
2026-01-29 17:14:32,468 | INFO | Chunk: 0 | WER=10.256410 | S=0 D=4 I=0
2026-01-29 17:14:32,470 | INFO | Chunk: 1 | WER=68.932039 | S=1 D=70 I=0
2026-01-29 17:14:32,473 | INFO | Chunk: 2 | WER=50.537634 | S=3 D=44 I=0
2026-01-29 17:14:32,474 | INFO | Chunk: 3 | WER=71.276596 | S=0 D=67 I=0
2026-01-29 17:14:32,476 | INFO | Chunk: 4 | WER=70.212766 | S=3 D=63 I=0
2026-01-29 17:14:32,478 | INFO | Chunk: 5 | WER=40.625000 | S=2 D=24 I=0
2026-01-29 17:14:32,481 | INFO | Chunk: 6 | WER=53.571429 | S=15 D=45 I=0
2026-01-29 17:14:32,484 | INFO | Chunk: 7 | WER=43.373494 | S=3 D=33 I=0
2026-01-29 17:14:32,485 | INFO | Chunk: 8 | WER=85.714286 | S=3 D=75 I=0
2026-01-29 17:14:32,487 | INFO | Chunk: 9 | WER=63.302752 | S=4 D=65 I=0
2026-01-29 17:14:32,489 | INFO | Chunk: 10 | WER=73.469388 | S=1 D=71 I=0
2026-01-29 17:14:32,491 | INFO | Chunk: 11 | WER=61.904762 | S=6 D=46 I=0
2026-01-29 17:14:32,493 | INFO | Chunk: 12 | WER=68.539326 | S=7 D=54 I=0
2026-01-29 17:14:32,495 | INFO | Chunk: 13 | WER=53.246753 | S=19 D=22 I=0
2026-01-29 17:14:32,497 | INFO | Chunk: 14 | WER=63.291139 | S=1 D=49 I=0
2026-01-29 17:14:32,814 | INFO | File: Rhap-M2002.wav | WER=58.612245 | S=68 D=649 I=1
2026-01-29 17:14:32,814 | INFO | ------------------------------
2026-01-29 17:14:32,814 | INFO | whisper large Done!
2026-01-29 17:14:32,970 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-29 17:14:33,000 | INFO | Vocabulary size: 350
2026-01-29 17:14:33,699 | INFO | Gradient checkpoint layers: []
2026-01-29 17:14:34,381 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 17:14:34,385 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 17:14:34,385 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 17:14:34,385 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-29 17:14:34,385 | INFO | speech length: 179360
2026-01-29 17:14:34,424 | INFO | decoder input length: 279
2026-01-29 17:14:34,424 | INFO | max output length: 279
2026-01-29 17:14:34,424 | INFO | min output length: 27
2026-01-29 17:14:39,417 | INFO | end detected at 90
2026-01-29 17:14:39,419 | INFO | -32.84 * 0.5 = -16.42 for decoder
2026-01-29 17:14:39,419 | INFO |  -6.22 * 0.5 =  -3.11 for ctc
2026-01-29 17:14:39,419 | INFO | total log probability: -19.53
2026-01-29 17:14:39,419 | INFO | normalized log probability: -0.24
2026-01-29 17:14:39,419 | INFO | total number of ended hypotheses: 215
2026-01-29 17:14:39,420 | INFO | best hypo: ▁c'est▁une▁conférence▁de▁de▁philosophie▁entre▁guillemet▁et▁c'est▁aussi▁une▁conférence▁d'histoire▁de▁l'art▁conférence▁d'esthétique▁parce▁que▁comme▁vous▁verrez▁chez▁une▁approche▁de

2026-01-29 17:14:39,423 | INFO | speech length: 447840
2026-01-29 17:14:39,455 | INFO | decoder input length: 699
2026-01-29 17:14:39,455 | INFO | max output length: 699
2026-01-29 17:14:39,455 | INFO | min output length: 69
2026-01-29 17:14:59,720 | INFO | end detected at 211
2026-01-29 17:14:59,722 | INFO | -539.48 * 0.5 = -269.74 for decoder
2026-01-29 17:14:59,722 | INFO | -158.07 * 0.5 = -79.03 for ctc
2026-01-29 17:14:59,722 | INFO | total log probability: -348.77
2026-01-29 17:14:59,722 | INFO | normalized log probability: -1.71
2026-01-29 17:14:59,722 | INFO | total number of ended hypotheses: 167
2026-01-29 17:14:59,724 | INFO | best hypo: ▁qu'est▁d'assez▁assez▁généraliste▁et▁qui▁mêle▁plusieurs▁points▁de▁vue▁et▁donc▁pas▁seulement▁le▁point▁de▁vue▁de▁la▁philosophie▁mais▁aussi▁celui▁de▁l'histoire▁celui▁de▁la▁sociologie▁etc▁etc▁vous▁j'avais▁traitement▁mon▁sujet▁injuieux▁comme▁je▁suis▁quelqu'un▁de▁la▁disciplinée▁j'vais▁traités▁mon▁sujet▁l'artr▁est▁il▁le▁refle▁de▁la▁société▁et▁la▁réponseu▁questions▁réponse▁que▁vous▁allez▁voir▁pas▁tout▁de▁suite▁'est▁hui▁là▁refle▁de▁la▁société▁mais▁pas▁tout▁le▁temps

2026-01-29 17:14:59,726 | INFO | speech length: 356320
2026-01-29 17:14:59,760 | INFO | decoder input length: 556
2026-01-29 17:14:59,760 | INFO | max output length: 556
2026-01-29 17:14:59,760 | INFO | min output length: 55
2026-01-29 17:15:13,264 | INFO | end detected at 164
2026-01-29 17:15:13,265 | INFO | -234.60 * 0.5 = -117.30 for decoder
2026-01-29 17:15:13,266 | INFO | -72.10 * 0.5 = -36.05 for ctc
2026-01-29 17:15:13,266 | INFO | total log probability: -153.35
2026-01-29 17:15:13,266 | INFO | normalized log probability: -0.98
2026-01-29 17:15:13,266 | INFO | total number of ended hypotheses: 169
2026-01-29 17:15:13,268 | INFO | best hypo: ▁eh▁pas▁et▁pas▁sous▁tous▁les▁aspects▁lire▁c'est▁une▁réponse▁un▁peu▁de▁à▁peu▁trop▁nuancé▁mais▁il▁n'est▁pas▁tout▁le▁temps▁et▁vous▁verrez▁qu'à▁des▁formes▁d'art▁qui▁ne▁sont▁pas▁le▁reflet▁de▁la▁société▁et▁tout▁cas▁même▁dans▁l'art▁est▁le▁reflet▁de▁la▁société▁il▁n'est▁pas▁sou▁toutes▁ces▁dimensions▁mais▁sur▁tous▁ces▁apects▁je▁commencerais▁par▁une▁remarque▁préalable

2026-01-29 17:15:13,270 | INFO | speech length: 424800
2026-01-29 17:15:13,304 | INFO | decoder input length: 663
2026-01-29 17:15:13,304 | INFO | max output length: 663
2026-01-29 17:15:13,304 | INFO | min output length: 66
2026-01-29 17:15:31,780 | INFO | end detected at 200
2026-01-29 17:15:31,781 | INFO | -571.54 * 0.5 = -285.77 for decoder
2026-01-29 17:15:31,781 | INFO | -134.12 * 0.5 = -67.06 for ctc
2026-01-29 17:15:31,781 | INFO | total log probability: -352.83
2026-01-29 17:15:31,781 | INFO | normalized log probability: -1.83
2026-01-29 17:15:31,781 | INFO | total number of ended hypotheses: 158
2026-01-29 17:15:31,784 | INFO | best hypo: ▁qui▁est▁très▁importante▁mais▁que▁les▁philosophes▁ont▁beaucoup▁de▁mal▁à▁faire▁c'est▁d'admettre▁que▁la▁notion▁d'art▁est▁une▁notion▁floue▁de▁dire▁que▁elle▁est▁informe▁indique▁que▁'on▁peut▁dire▁'importe▁quoire▁mais▁c'est▁un▁concept▁c'est▁ce▁qu'on▁apptelle▁un▁concept▁flo▁c'est▁un▁philophe▁américain'▁disciple▁du▁philosophe▁anglo▁autrichien▁wit▁guinenstein▁qui▁a▁avancé▁cette▁idée▁de▁l'artau▁comme▁concept▁est▁de▁dire

2026-01-29 17:15:31,785 | INFO | speech length: 420960
2026-01-29 17:15:31,822 | INFO | decoder input length: 657
2026-01-29 17:15:31,822 | INFO | max output length: 657
2026-01-29 17:15:31,822 | INFO | min output length: 65
2026-01-29 17:15:48,757 | INFO | end detected at 180
2026-01-29 17:15:48,758 | INFO | -402.77 * 0.5 = -201.38 for decoder
2026-01-29 17:15:48,758 | INFO | -110.12 * 0.5 = -55.06 for ctc
2026-01-29 17:15:48,758 | INFO | total log probability: -256.45
2026-01-29 17:15:48,758 | INFO | normalized log probability: -1.47
2026-01-29 17:15:48,758 | INFO | total number of ended hypotheses: 157
2026-01-29 17:15:48,760 | INFO | best hypo: ▁un▁art▁dont▁les▁l'art▁est▁un▁concept▁dont▁les▁contours▁sont▁changeants▁s'échangeant▁selon▁les▁sociétés▁s'échangeant▁selon▁selon▁les▁formes▁d'art▁considérées▁et▁'▁il▁faut▁faut▁apprendre▁à▁s'i▁'▁diresse▁se▁fairent▁à▁ces▁ris▁et▁il▁est▁évident▁qui▁appartent▁des▁formes▁d'art▁qui▁sont▁très▁liés▁à▁la▁connaiss▁par▁l'art▁de▁la▁renaiss▁l'art▁de▁la▁renaissance▁est▁vu▁comme▁un

2026-01-29 17:15:48,762 | INFO | speech length: 319840
2026-01-29 17:15:48,796 | INFO | decoder input length: 499
2026-01-29 17:15:48,796 | INFO | max output length: 499
2026-01-29 17:15:48,796 | INFO | min output length: 49
2026-01-29 17:15:59,178 | INFO | end detected at 134
2026-01-29 17:15:59,180 | INFO | -231.26 * 0.5 = -115.63 for decoder
2026-01-29 17:15:59,180 | INFO | -45.88 * 0.5 = -22.94 for ctc
2026-01-29 17:15:59,180 | INFO | total log probability: -138.57
2026-01-29 17:15:59,180 | INFO | normalized log probability: -1.07
2026-01-29 17:15:59,180 | INFO | total number of ended hypotheses: 172
2026-01-29 17:15:59,182 | INFO | best hypo: ▁un▁art▁descriptif▁qui▁doit▁nous▁représenter▁les▁choses▁telles▁qu'elles▁sont▁elles▁sont▁correctement▁vues▁et▁puis▁à▁des▁formes▁d'art▁qui▁sont▁beaucoup▁plus▁édonistes▁et▁qui▁se▁préoccupent▁très▁peu▁d'eux▁de▁de▁la▁connaissance▁l'ar▁est▁un▁concept▁flou▁et▁il▁faut▁faire▁un▁certain▁nombre▁de▁distinctions

2026-01-29 17:15:59,183 | INFO | speech length: 474240
2026-01-29 17:15:59,218 | INFO | decoder input length: 740
2026-01-29 17:15:59,218 | INFO | max output length: 740
2026-01-29 17:15:59,218 | INFO | min output length: 74
2026-01-29 17:16:22,169 | INFO | end detected at 228
2026-01-29 17:16:22,170 | INFO | -641.24 * 0.5 = -320.62 for decoder
2026-01-29 17:16:22,170 | INFO | -173.82 * 0.5 = -86.91 for ctc
2026-01-29 17:16:22,170 | INFO | total log probability: -407.53
2026-01-29 17:16:22,170 | INFO | normalized log probability: -1.83
2026-01-29 17:16:22,170 | INFO | total number of ended hypotheses: 154
2026-01-29 17:16:22,173 | INFO | best hypo: ▁ce▁sera▁juste▁des▁distinctions▁préliminaires▁d'abord▁une▁grande▁distinction▁que▁l'on▁oublie▁alors▁que▁tout▁le▁monde▁la▁connaît▁et▁tout▁le▁monde▁c'est▁de▁quoi▁il▁s'agit▁pour▁la▁distinction▁entre▁l'art▁en▁tant▁qu'on▁le▁fabrique▁qu'on▁le▁produit▁qu'on▁le▁pratique▁depuis▁l'art▁à▁tant▁qu'on▁le▁reçoit▁qu'on▁l'expérimente▁et▁qu'on▁et▁qu'on▁l'éprouve▁c'est▁à▁une▁dist'inction▁qui▁dait▁depuis▁le▁la▁plus▁haute▁et▁anticité▁et▁tout▁le▁cas▁est▁depuis▁platon▁c'est▁qu'est▁à▁un▁art▁qui▁cons▁à▁fabriquer▁per▁six▁faire

2026-01-29 17:16:22,175 | INFO | speech length: 464480
2026-01-29 17:16:22,211 | INFO | decoder input length: 725
2026-01-29 17:16:22,211 | INFO | max output length: 725
2026-01-29 17:16:22,211 | INFO | min output length: 72
2026-01-29 17:16:41,131 | INFO | end detected at 187
2026-01-29 17:16:41,132 | INFO | -409.41 * 0.5 = -204.70 for decoder
2026-01-29 17:16:41,132 | INFO | -109.39 * 0.5 = -54.69 for ctc
2026-01-29 17:16:41,132 | INFO | total log probability: -259.40
2026-01-29 17:16:41,132 | INFO | normalized log probability: -1.43
2026-01-29 17:16:41,132 | INFO | total number of ended hypotheses: 164
2026-01-29 17:16:41,134 | INFO | best hypo: ▁un▁de▁la▁vient▁en▁termes▁de▁poésie▁aussi▁curieux▁que▁s'a▁puisse▁paraître▁et▁puis▁il▁a▁un▁autre▁aspect▁qui▁consiste▁à▁éprouver▁l'art▁le▁ressentir▁à▁en▁faire▁l'expérience▁avoir▁et▁sorte▁de▁vécu▁de▁l'expérience▁artisique▁et▁c'est▁ce▁qu'on▁appeelle▁de▁l'aïstéss▁ishésis▁ça▁veut▁dire▁sensation▁en▁grec▁et▁çara▁ça▁à▁cesser▁le▁phénomène▁de▁de▁recevoir▁sans▁ressentir

2026-01-29 17:16:41,136 | INFO | speech length: 424160
2026-01-29 17:16:41,172 | INFO | decoder input length: 662
2026-01-29 17:16:41,172 | INFO | max output length: 662
2026-01-29 17:16:41,172 | INFO | min output length: 66
2026-01-29 17:17:01,542 | INFO | end detected at 220
2026-01-29 17:17:01,544 | INFO | -482.60 * 0.5 = -241.30 for decoder
2026-01-29 17:17:01,544 | INFO | -144.83 * 0.5 = -72.42 for ctc
2026-01-29 17:17:01,544 | INFO | total log probability: -313.71
2026-01-29 17:17:01,544 | INFO | normalized log probability: -1.47
2026-01-29 17:17:01,544 | INFO | total number of ended hypotheses: 182
2026-01-29 17:17:01,547 | INFO | best hypo: ▁et▁et▁ça▁a▁donné▁sipoïésis▁a▁donné▁le▁mot▁poésie▁aïstécis▁a▁donné▁le▁terme▁esthétique▁et▁ce▁qui▁est▁intéressant▁à▁souligner▁eux▁si▁vous▁poursuivez▁des▁études▁d'histoire▁de▁l'art▁par▁exemple▁oule▁verrez▁très▁bien▁c'est▁pendant▁très▁l'gtemps▁n'a▁a▁pas▁beaucoup▁parlée▁d'esthétique▁on▁n'a▁a▁pas▁beaucoup▁parlé▁de▁l'expériencee▁restentie▁de▁l'art▁et▁on▁a▁parlé▁de▁surtout▁de▁l'art▁en▁tant▁de▁fabrication▁parce▁que▁c'était▁le▁une▁afaire▁de▁spécialiste

2026-01-29 17:17:01,549 | INFO | speech length: 425760
2026-01-29 17:17:01,580 | INFO | decoder input length: 664
2026-01-29 17:17:01,580 | INFO | max output length: 664
2026-01-29 17:17:01,580 | INFO | min output length: 66
2026-01-29 17:17:22,849 | INFO | end detected at 231
2026-01-29 17:17:22,851 | INFO | -427.14 * 0.5 = -213.57 for decoder
2026-01-29 17:17:22,851 | INFO | -127.73 * 0.5 = -63.86 for ctc
2026-01-29 17:17:22,851 | INFO | total log probability: -277.43
2026-01-29 17:17:22,851 | INFO | normalized log probability: -1.23
2026-01-29 17:17:22,851 | INFO | total number of ended hypotheses: 169
2026-01-29 17:17:22,854 | INFO | best hypo: ▁c'était▁une▁affaire▁aussi▁de▁commanditaire▁les▁qui▁ce▁qui▁s'est▁qui▁commandaient▁les▁oeuvres▁nains▁et▁qui▁les▁achetaient▁en▁quelque▁sorte▁mais▁que▁le▁point▁de▁vueut▁du▁spectateur▁n'avait▁pas▁beaucoupe▁d'importance▁eux▁jusqu'à▁la▁renaissant▁et▁y▁compris▁un▁peulont▁jusqu'au▁vie▁siècle▁le▁point▁de▁vue▁de▁la▁fabriction▁le▁point▁de▁vue▁de▁la▁production▁de▁l'art▁et▁plus▁important▁que▁le▁point▁de▁vue▁de▁l'expris▁de▁la▁réception▁de▁l'art▁le▁point▁de▁vue▁du▁spectateur▁point▁de▁vue▁du▁spectateur▁point▁de▁vue▁esthétique

2026-01-29 17:17:22,855 | INFO | speech length: 451360
2026-01-29 17:17:22,892 | INFO | decoder input length: 704
2026-01-29 17:17:22,892 | INFO | max output length: 704
2026-01-29 17:17:22,892 | INFO | min output length: 70
2026-01-29 17:17:43,288 | INFO | end detected at 210
2026-01-29 17:17:43,290 | INFO | -437.81 * 0.5 = -218.90 for decoder
2026-01-29 17:17:43,290 | INFO | -109.12 * 0.5 = -54.56 for ctc
2026-01-29 17:17:43,290 | INFO | total log probability: -273.47
2026-01-29 17:17:43,290 | INFO | normalized log probability: -1.35
2026-01-29 17:17:43,290 | INFO | total number of ended hypotheses: 182
2026-01-29 17:17:43,292 | INFO | best hypo: ▁se▁dégage▁à▁la▁fin▁du▁dix▁septième▁siècle▁au▁xviiième▁siècle▁et▁puis▁va▁c'est▁quelque▁chose▁que▁nous▁sommes▁que▁nous▁avons▁bien▁que▁nous▁connaissons▁bien▁mais▁faites▁vous▁même▁à▁différence▁entre▁votre▁expérience▁pour▁exemple▁si▁vous▁pratiquez▁la▁musique▁ou▁votre▁expérience▁en▁tant▁que▁pratiquant▁la▁musique'▁et▁votre▁expérience▁en▁tant▁que▁connommateurs▁de▁la▁musiques▁qui▁suffit▁que▁vous▁ayez▁un▁baladeur▁pour▁vous▁connaissiez▁un▁expérience▁de▁consmateur▁de▁musique▁comme▁expérience▁de▁la▁réception▁et▁si▁vous▁faites▁de▁la▁musique

2026-01-29 17:17:43,295 | INFO | speech length: 436160
2026-01-29 17:17:43,330 | INFO | decoder input length: 681
2026-01-29 17:17:43,330 | INFO | max output length: 681
2026-01-29 17:17:43,330 | INFO | min output length: 68
2026-01-29 17:18:02,025 | INFO | end detected at 197
2026-01-29 17:18:02,026 | INFO | -392.12 * 0.5 = -196.06 for decoder
2026-01-29 17:18:02,026 | INFO | -114.35 * 0.5 = -57.17 for ctc
2026-01-29 17:18:02,026 | INFO | total log probability: -253.23
2026-01-29 17:18:02,027 | INFO | normalized log probability: -1.33
2026-01-29 17:18:02,027 | INFO | total number of ended hypotheses: 181
2026-01-29 17:18:02,029 | INFO | best hypo: ▁eh▁bien▁vous▁avez▁l'expérience▁de▁la▁poétise▁de▁de▁la▁production▁musicale▁une▁première▁distinction▁fondamentale▁parce▁que▁lire▁selon▁les▁sociétés▁je▁dirait▁que▁là▁la▁la▁composante▁de▁production▁ou▁la▁composante▁de▁réception▁e▁varie▁beaucoup▁la▁deuxième▁distinction▁'h▁bien▁et▁faut▁voir▁et▁c'est▁en▁sens▁que▁l'art▁est▁un▁concept▁de▁flo▁c'est▁que▁l'art▁a▁des▁fonnction▁extrêmement▁diverses▁selon▁les▁époque▁et▁selon▁les▁sociétés

2026-01-29 17:18:02,031 | INFO | speech length: 435520
2026-01-29 17:18:02,066 | INFO | decoder input length: 680
2026-01-29 17:18:02,066 | INFO | max output length: 680
2026-01-29 17:18:02,066 | INFO | min output length: 68
2026-01-29 17:18:23,175 | INFO | end detected at 226
2026-01-29 17:18:23,176 | INFO | -488.78 * 0.5 = -244.39 for decoder
2026-01-29 17:18:23,176 | INFO | -136.20 * 0.5 = -68.10 for ctc
2026-01-29 17:18:23,176 | INFO | total log probability: -312.49
2026-01-29 17:18:23,176 | INFO | normalized log probability: -1.42
2026-01-29 17:18:23,176 | INFO | total number of ended hypotheses: 175
2026-01-29 17:18:23,179 | INFO | best hypo: ▁mais▁il▁a▁des▁fonctions▁religieuses▁et▁des▁fonctions▁presque▁sacrées▁quelquefois▁l'art▁est▁utilisé▁dans▁les▁cérémonies▁et▁il▁ne▁doit▁pas▁être▁utilisé▁en▁dehors▁des▁cérémonies▁c'est▁vraix▁encore▁pour▁pas▁mal▁de▁masques▁africains▁par▁exemple▁eux▁bien▁évidemment▁s'est▁vrai▁pour▁la▁peinte▁religieuse'▁en▁occcent▁et▁la▁peinture▁de▁la▁renaissance▁jusqu'au'▁jusqu'au▁sizième▁siècle▁la▁pein▁de▁la▁renaissance▁et▁une▁peinture▁éminemment▁religieuse▁avec▁des▁fonction▁religieuse▁ou▁en▁pas▁vraie

2026-01-29 17:18:23,181 | INFO | speech length: 412160
2026-01-29 17:18:23,216 | INFO | decoder input length: 643
2026-01-29 17:18:23,216 | INFO | max output length: 643
2026-01-29 17:18:23,216 | INFO | min output length: 64
2026-01-29 17:18:38,248 | INFO | end detected at 163
2026-01-29 17:18:38,249 | INFO | -412.31 * 0.5 = -206.15 for decoder
2026-01-29 17:18:38,249 | INFO | -173.54 * 0.5 = -86.77 for ctc
2026-01-29 17:18:38,249 | INFO | total log probability: -292.92
2026-01-29 17:18:38,249 | INFO | normalized log probability: -1.88
2026-01-29 17:18:38,249 | INFO | total number of ended hypotheses: 165
2026-01-29 17:18:38,251 | INFO | best hypo: ▁et▁puis▁il▁peut▁avoir▁des▁fonctions▁politiques▁aussi▁larpeur▁des▁fonctions▁politiques▁et▁peut▁peut▁avoir▁des▁fonctions▁décoratives▁aussi▁penser▁aux▁rôles▁de▁la▁de▁la▁décoration▁par▁exemple▁eux▁quand▁vous▁regardez▁les▁es▁stations▁de▁métro▁qui▁sont▁décorés▁par▁des▁oeuvres▁d'arts▁ou▁avec▁des▁'▁et▁les▁travaux▁et▁des▁'▁il▁peut'avoir▁des▁l'arts▁peu▁avoir▁des▁fonctions▁de▁renaissions▁aussi

2026-01-29 17:18:38,253 | INFO | speech length: 377600
2026-01-29 17:18:38,288 | INFO | decoder input length: 589
2026-01-29 17:18:38,288 | INFO | max output length: 589
2026-01-29 17:18:38,288 | INFO | min output length: 58
2026-01-29 17:18:55,270 | INFO | end detected at 199
2026-01-29 17:18:55,271 | INFO | -313.32 * 0.5 = -156.66 for decoder
2026-01-29 17:18:55,271 | INFO | -37.42 * 0.5 = -18.71 for ctc
2026-01-29 17:18:55,271 | INFO | total log probability: -175.37
2026-01-29 17:18:55,271 | INFO | normalized log probability: -0.90
2026-01-29 17:18:55,271 | INFO | total number of ended hypotheses: 160
2026-01-29 17:18:55,273 | INFO | best hypo: ▁les▁fonctions▁de▁connaissance▁des▁fonctions▁de▁critiques▁y▁a▁beaucoup▁de▁fonctions▁différentes▁dans▁l'art▁et▁encore▁une▁fois▁elles▁sont▁pas▁toutes▁également▁présentes▁selon▁les▁sociétés▁et▁selon▁les▁époques▁et▁a▁des▁sociétés▁ou▁la▁dimensions▁de▁connaissances▁que▁j'aperaiss▁à▁la▁dimension▁cognitives▁et▁plus▁importantes▁y▁a▁des▁sociétés▁où▁la▁dimension▁des▁données▁édonistiques▁le▁plaisir▁que▁donnent▁l'expérience▁artisique▁sont▁plus▁importantes

2026-01-29 17:18:55,279 | INFO | Chunk: 0 | WER=25.641026 | S=3 D=7 I=0
2026-01-29 17:18:55,284 | INFO | Chunk: 1 | WER=36.893204 | S=19 D=16 I=3
2026-01-29 17:18:55,287 | INFO | Chunk: 2 | WER=30.107527 | S=13 D=15 I=0
2026-01-29 17:18:55,291 | INFO | Chunk: 3 | WER=26.595745 | S=10 D=15 I=0
2026-01-29 17:18:55,294 | INFO | Chunk: 4 | WER=37.234043 | S=15 D=20 I=0
2026-01-29 17:18:55,296 | INFO | Chunk: 5 | WER=20.312500 | S=3 D=9 I=1
2026-01-29 17:18:55,302 | INFO | Chunk: 6 | WER=26.785714 | S=14 D=9 I=7
2026-01-29 17:18:55,305 | INFO | Chunk: 7 | WER=33.734940 | S=14 D=12 I=2
2026-01-29 17:18:55,309 | INFO | Chunk: 8 | WER=24.175824 | S=12 D=5 I=5
2026-01-29 17:18:55,314 | INFO | Chunk: 9 | WER=22.935780 | S=16 D=8 I=1
2026-01-29 17:18:55,319 | INFO | Chunk: 10 | WER=18.367347 | S=9 D=7 I=2
2026-01-29 17:18:55,322 | INFO | Chunk: 11 | WER=23.809524 | S=11 D=7 I=2
2026-01-29 17:18:55,326 | INFO | Chunk: 12 | WER=26.966292 | S=17 D=6 I=1
2026-01-29 17:18:55,329 | INFO | Chunk: 13 | WER=31.168831 | S=16 D=8 I=0
2026-01-29 17:18:55,332 | INFO | Chunk: 14 | WER=27.848101 | S=14 D=7 I=1
2026-01-29 17:18:55,968 | INFO | File: Rhap-M2002.wav | WER=24.816327 | S=168 D=89 I=47
2026-01-29 17:18:55,968 | INFO | ------------------------------
2026-01-29 17:18:55,969 | INFO | Conf cv Done!
2026-01-29 17:18:56,095 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-29 17:18:56,113 | INFO | Vocabulary size: 47
2026-01-29 17:18:56,703 | INFO | Gradient checkpoint layers: []
2026-01-29 17:18:57,349 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 17:18:57,353 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 17:18:57,353 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 17:18:57,353 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-29 17:18:57,356 | INFO | speech length: 179360
2026-01-29 17:18:57,386 | INFO | decoder input length: 279
2026-01-29 17:18:57,386 | INFO | max output length: 279
2026-01-29 17:18:57,386 | INFO | min output length: 27
2026-01-29 17:19:07,017 | INFO | end detected at 208
2026-01-29 17:19:07,018 | INFO | -16.18 * 0.5 =  -8.09 for decoder
2026-01-29 17:19:07,018 | INFO |  -0.46 * 0.5 =  -0.23 for ctc
2026-01-29 17:19:07,018 | INFO | total log probability: -8.32
2026-01-29 17:19:07,019 | INFO | normalized log probability: -0.04
2026-01-29 17:19:07,019 | INFO | total number of ended hypotheses: 189
2026-01-29 17:19:07,021 | INFO | best hypo: c'est<space>une<space>conférence<space>de<space>de<space>philosophie<space>entre<space>guillemets<space>mais<space>c'est<space>aussi<space>une<space>conférence<space>de<space>d'histoire<space>de<space>l'art<space>une<space>conférence<space>d'esthétique<space>parce<space>que<space>euh<space>comme<space>vous<space>verrez<space>j'ai<space>une<space>approche<space>de<space>de<space>l'art

2026-01-29 17:19:07,023 | INFO | speech length: 447840
2026-01-29 17:19:07,050 | INFO | decoder input length: 699
2026-01-29 17:19:07,050 | INFO | max output length: 699
2026-01-29 17:19:07,050 | INFO | min output length: 69
2026-01-29 17:19:46,896 | INFO | end detected at 500
2026-01-29 17:19:46,897 | INFO | -485.43 * 0.5 = -242.72 for decoder
2026-01-29 17:19:46,897 | INFO | -30.29 * 0.5 = -15.15 for ctc
2026-01-29 17:19:46,897 | INFO | total log probability: -257.86
2026-01-29 17:19:46,897 | INFO | normalized log probability: -0.52
2026-01-29 17:19:46,897 | INFO | total number of ended hypotheses: 155
2026-01-29 17:19:46,903 | INFO | best hypo: qui<space>est<space>assez<space>assez<space>généraliste<space>et<space>qui<space>euh<space>mêle<space>plusieurs<space>euh<space>euh<space>plusieurs<space>points<space>de<space>vue<space>et<space>donc<space>pas<space>seulement<space>le<space>point<space>de<space>vue<space>de<space>la<space>philosophie<space>mais<space>aussi<space>celui<space>de<space>l'histoire<space>celui<space>de<space>la<space>sociologie<space>etc<space>etc<space>donc<space>je<space>vais<space>traider<space>mon<space>sujet<space>hein<space>je<space>veux<space>dire<space>comme<space>je<space>suis<space>quelqu'un<space>de<space>de<space>discipliné<space>jevais<space>traiter<space>euh<space>mon<space>sujet<space>là<space>arêt<space>il<space>le<space>reflet<space>de<space>la<space>société<space>et<space>euh<space>la<space>réponse<space>question<space>réponse<space>vous<space>alez<space>voir<space>tout<space>de<space>suite<space>c'est<space>ou<space>la<space>ré<space>le<space>reflet<space>de<space>la<space>société<space>mais<space>euh<space>pas<space>tout<space>le<space>temps

2026-01-29 17:19:46,905 | INFO | speech length: 356320
2026-01-29 17:19:46,936 | INFO | decoder input length: 556
2026-01-29 17:19:46,936 | INFO | max output length: 556
2026-01-29 17:19:46,936 | INFO | min output length: 55
2026-01-29 17:20:14,185 | INFO | end detected at 398
2026-01-29 17:20:14,186 | INFO | -54.85 * 0.5 = -27.42 for decoder
2026-01-29 17:20:14,186 | INFO |  -8.02 * 0.5 =  -4.01 for ctc
2026-01-29 17:20:14,186 | INFO | total log probability: -31.44
2026-01-29 17:20:14,186 | INFO | normalized log probability: -0.08
2026-01-29 17:20:14,186 | INFO | total number of ended hypotheses: 160
2026-01-29 17:20:14,190 | INFO | best hypo: et<space>pas<space>euh<space>et<space>pas<space>sous<space>tous<space>les<space>aspects<space>hein<space>c'est<space>une<space>réponse<space>un<space>peu<space>de<space>un<space>peu<space>trop<space>nuancée<space>hein<space>mais<space>il<space>est<space>pas<space>tout<space>le<space>temps<space>y<space>a<space>d<space>vous<space>verrez<space>qu'<space>y<space>a<space>des<space>formes<space>d'art<space>qui<space>ne<space>sont<space>pas<space>le<space>reflet<space>de<space>la<space>société<space>et<space>en<space>tout<space>cas<space>même<space>quand<space>l'art<space>et<space>le<space>réflet<space>de<space>la<space>société<space>il<space>n'est<space>pas<space>euh<space>sous<space>toutes<space>ses<space>dimensions<space>ni<space>sur<space>tous<space>ces<space>aspects<space>alors<space>je<space>commencerais<space>par<space>euh<space>une<space>remarque<space>préalable

2026-01-29 17:20:14,193 | INFO | speech length: 424800
2026-01-29 17:20:14,220 | INFO | decoder input length: 663
2026-01-29 17:20:14,220 | INFO | max output length: 663
2026-01-29 17:20:14,220 | INFO | min output length: 66
2026-01-29 17:20:50,300 | INFO | end detected at 469
2026-01-29 17:20:50,301 | INFO | -537.95 * 0.5 = -268.97 for decoder
2026-01-29 17:20:50,301 | INFO | -18.00 * 0.5 =  -9.00 for ctc
2026-01-29 17:20:50,301 | INFO | total log probability: -277.97
2026-01-29 17:20:50,301 | INFO | normalized log probability: -0.60
2026-01-29 17:20:50,301 | INFO | total number of ended hypotheses: 175
2026-01-29 17:20:50,306 | INFO | best hypo: qui<space>est<space>très<space>importante<space>mais<space>que<space>les<space>philosophes<space>ont<space>beaucoup<space>de<space>mal<space>à<space>faire<space>euh<space>c'est<space>d'admettre<space>que<space>la<space>notion<space>d'art<space>euh<space>est<space>une<space>notion<space>floue<space>je<space>veux<space>pas<space>lui<space>dire<space>que<space>elle<space>est<space>informe<space>hein<space>dit<space>que<space>on<space>peut<space>dire<space>n'importe<space>quoi<space>mais<space>c'est<space>un<space>concept<space>c'est<space>ce<space>qu'on<space>appelle<space>un<space>concept<space>de<space>flou<space>c'est<space>un<space>philosophe<space>euh<space>américain<space>euh<space>discible<space>du<space>philosophe<space>euh<space>anglos<space>au<space>autrichien<space>euh<space>wid<space>gunshine<space>qui<space>a<space>euh<space>avancé<space>cette<space>idée<space>de<space>l'art<space>comme<space>concept<space>flou<space>c'est<space>à<space>dire

2026-01-29 17:20:50,309 | INFO | speech length: 420960
2026-01-29 17:20:50,338 | INFO | decoder input length: 657
2026-01-29 17:20:50,338 | INFO | max output length: 657
2026-01-29 17:20:50,338 | INFO | min output length: 65
2026-01-29 17:21:24,513 | INFO | end detected at 436
2026-01-29 17:21:24,514 | INFO | -423.79 * 0.5 = -211.90 for decoder
2026-01-29 17:21:24,514 | INFO | -39.87 * 0.5 = -19.93 for ctc
2026-01-29 17:21:24,514 | INFO | total log probability: -231.83
2026-01-29 17:21:24,515 | INFO | normalized log probability: -0.54
2026-01-29 17:21:24,515 | INFO | total number of ended hypotheses: 156
2026-01-29 17:21:24,519 | INFO | best hypo: un<space>art<space>dont<space>les<space>ann<space>l'art<space>est<space>un<space>concept<space>dont<space>les<space>contours<space>sont<space>changeants<space>échangeants<space>selon<space>les<space>sociétés<space>euh<space>s'échangeant<space>selon<space>euh<space>euh<space>selon<space>les<space>formes<space>d'art<space>euh<space>considérées<space>et<space>euh<space>i<space>faut<space>i<space>faut<space>apprendre<space>à<space>à<space>s<space>je<space>dirais<space>se<space>se<space>faire<space>à<space>ces<space>variations<space>hein<space>il<space>est<space>évident<space>qu'il<space>y<space>a<space>par<space>empre<space>des<space>des<space>formes<space>d'arts<space>qui<space>sont<space>très<space>liées<space>à<space>la<space>connaissance<space>euh<space>part<space>dans<space>l'art<space>de<space>la<space>renaissance<space>l'art<space>d<space>la<space>resaissance<space>est<space>vu<space>commeun

2026-01-29 17:21:24,522 | INFO | speech length: 319840
2026-01-29 17:21:24,551 | INFO | decoder input length: 499
2026-01-29 17:21:24,551 | INFO | max output length: 499
2026-01-29 17:21:24,551 | INFO | min output length: 49
2026-01-29 17:21:46,604 | INFO | end detected at 343
2026-01-29 17:21:46,605 | INFO | -45.58 * 0.5 = -22.79 for decoder
2026-01-29 17:21:46,606 | INFO |  -6.04 * 0.5 =  -3.02 for ctc
2026-01-29 17:21:46,606 | INFO | total log probability: -25.81
2026-01-29 17:21:46,606 | INFO | normalized log probability: -0.08
2026-01-29 17:21:46,606 | INFO | total number of ended hypotheses: 209
2026-01-29 17:21:46,609 | INFO | best hypo: un<space>art<space>descriptif<space>et<space>qui<space>doit<space>nous<space>représenter<space>les<space>choses<space>telles<space>qu'elles<space>sont<space>euh<space>quand<space>elles<space>sont<space>correctement<space>vues<space>et<space>puis<space>y<space>a<space>des<space>formes<space>d'art<space>qui<space>sont<space>beaucoup<space>plus<space>édonistes<space>et<space>qui<space>se<space>préoccupent<space>très<space>peu<space>euh<space>de<space>euh<space>de<space>la<space>connaissance<space>alors<space>euh<space>là<space>est<space>un<space>concept<space>euh<space>flou<space>et<space>euh<space>i<space>faut<space>faire<space>un<space>certain<space>nombre<space>de<space>distinctions

2026-01-29 17:21:46,612 | INFO | speech length: 474240
2026-01-29 17:21:46,641 | INFO | decoder input length: 740
2026-01-29 17:21:46,641 | INFO | max output length: 740
2026-01-29 17:21:46,641 | INFO | min output length: 74
2026-01-29 17:22:30,804 | INFO | end detected at 526
2026-01-29 17:22:30,805 | INFO | -468.22 * 0.5 = -234.11 for decoder
2026-01-29 17:22:30,805 | INFO | -47.21 * 0.5 = -23.60 for ctc
2026-01-29 17:22:30,805 | INFO | total log probability: -257.71
2026-01-29 17:22:30,805 | INFO | normalized log probability: -0.50
2026-01-29 17:22:30,805 | INFO | total number of ended hypotheses: 171
2026-01-29 17:22:30,811 | INFO | best hypo: euh<space>ce<space>sera<space>juste<space>des<space>distinctions<space>préliminaires<space>d'abord<space>y<space>a<space>une<space>grande<space>distinction<space>euh<space>que<space>l'on<space>oublie<space>euh<space>alors<space>que<space>tout<space>le<space>monde<space>la<space>connaît<space>et<space>tout<space>le<space>monde<space>sait<space>de<space>quoi<space>il<space>s'agit<space>pour<space>la<space>distinction<space>entre<space>l'art<space>en<space>tant<space>qu'on<space>le<space>fabrique<space>qu'on<space>le<space>produit<space>qu'on<space>le<space>pratique<space>et<space>puis<space>là<space>en<space>tant<space>qu'on<space>euh<space>le<space>reçoit<space>qu'on<space>l'expérimente<space>et<space>qu'on<space>et<space>qu'on<space>l'éprouve<space>c'est<space>une<space>distinction<space>euh<space>qui<space>date<space>depuis<space>h<space>la<space>plus<space>haute<space>hantiquité<space>e<space>n<space>tout<space>cas<space>de<space>puis<space>platon<space>c'est<space>qui<space>y<space>a<space>un<space>art<space>qui<space>consiste<space>à<space>fabriquer<space>poy<space>ci<space>faire

2026-01-29 17:22:30,813 | INFO | speech length: 464480
2026-01-29 17:22:30,840 | INFO | decoder input length: 725
2026-01-29 17:22:30,840 | INFO | max output length: 725
2026-01-29 17:22:30,840 | INFO | min output length: 72
2026-01-29 17:23:06,286 | INFO | end detected at 403
2026-01-29 17:23:06,287 | INFO | -203.33 * 0.5 = -101.67 for decoder
2026-01-29 17:23:06,287 | INFO | -18.83 * 0.5 =  -9.41 for ctc
2026-01-29 17:23:06,288 | INFO | total log probability: -111.08
2026-01-29 17:23:06,288 | INFO | normalized log probability: -0.28
2026-01-29 17:23:06,288 | INFO | total number of ended hypotheses: 174
2026-01-29 17:23:06,292 | INFO | best hypo: euh<space>de<space>la<space>viande<space>terme<space>de<space>poésie<space>hein<space>euh<space>aussi<space>curieux<space>que<space>ça<space>puisse<space>paraître<space>et<space>puis<space>y<space>a<space>un<space>autre<space>aspect<space>qui<space>consiste<space>à<space>exp<space>éprouver<space>l'art<space>le<space>ressentir<space>euh<space>en<space>faire<space>l'expérience<space>avoir<space>une<space>sorte<space>de<space>de<space>vécu<space>de<space>l'expérience<space>artistique<space>et<space>c'est<space>ce<space>qu'on<space>appelle<space>l'aïstécis<space>euh<space>haïstécis<space>ça<space>veut<space>dire<space>sensation<space>euh<space>en<space>en<space>grec<space>euh<space>et<space>ça<space>ça<space>c'est<space>c'est<space>le<space>phénomène<space>de<space>de<space>recevoir<space>euh<space>s'en<space>ressentir

2026-01-29 17:23:06,294 | INFO | speech length: 424160
2026-01-29 17:23:06,323 | INFO | decoder input length: 662
2026-01-29 17:23:06,323 | INFO | max output length: 662
2026-01-29 17:23:06,324 | INFO | min output length: 66
2026-01-29 17:23:42,622 | INFO | end detected at 462
2026-01-29 17:23:42,623 | INFO | -380.76 * 0.5 = -190.38 for decoder
2026-01-29 17:23:42,624 | INFO | -20.83 * 0.5 = -10.42 for ctc
2026-01-29 17:23:42,624 | INFO | total log probability: -200.80
2026-01-29 17:23:42,624 | INFO | normalized log probability: -0.44
2026-01-29 17:23:42,624 | INFO | total number of ended hypotheses: 159
2026-01-29 17:23:42,629 | INFO | best hypo: et<space>ça<space>a<space>donné<space>euh<space>si<space>poyécis<space>a<space>donné<space>le<space>mot<space>poésie<space>euh<space>a<space>istécis<space>a<space>donné<space>le<space>terme<space>esthétique<space>et<space>ce<space>qui<space>est<space>intéressant<space>à<space>souligner<space>euh<space>si<space>vous<space>poursuivez<space>des<space>études<space>d'histoire<space>de<space>l'art<space>par<space>exemple<space>vous<space>le<space>verrez<space>très<space>bien<space>c'est<space>que<space>pendant<space>très<space>longtemps<space>on<space>n'a<space>pas<space>beaucoup<space>parlé<space>d'esthétique<space>on<space>n'a<space>pas<space>beaucoup<space>parlé<space>de<space>l'expérience<space>ressentie<space>de<space>l'art<space>euh<space>on<space>a<space>parlé<space>surtout<space>de<space>l'art<space>en<space>tan<space>que<space>fabrication<space>parce<space>que<space>c'était<space>une<space>affaire<space>de<space>spécialistes

2026-01-29 17:23:42,631 | INFO | speech length: 425760
2026-01-29 17:23:42,661 | INFO | decoder input length: 664
2026-01-29 17:23:42,661 | INFO | max output length: 664
2026-01-29 17:23:42,661 | INFO | min output length: 66
2026-01-29 17:24:22,556 | INFO | end detected at 540
2026-01-29 17:24:22,558 | INFO | -436.63 * 0.5 = -218.31 for decoder
2026-01-29 17:24:22,558 | INFO | -41.52 * 0.5 = -20.76 for ctc
2026-01-29 17:24:22,558 | INFO | total log probability: -239.07
2026-01-29 17:24:22,558 | INFO | normalized log probability: -0.45
2026-01-29 17:24:22,558 | INFO | total number of ended hypotheses: 204
2026-01-29 17:24:22,564 | INFO | best hypo: c'était<space>une<space>affaire<space>aussi<space>de<space>commanditaires<space>qui<space>s<space>qui<space>s'est<space>qui<space>commandait<space>les<space>oeuvres<space>d'art<space>et<space>qui<space>les<space>achetaient<space>en<space>quelque<space>sorte<space>mais<space>que<space>le<space>point<space>de<space>vue<space>du<space>spectateur<space>avait<space>pas<space>beaucoup<space>d'importance<space>euh<space>jusqu'à<space>la<space>renaissance<space>et<space>y<space>compris<space>l<space>un<space>peu<space>long<space>un<space>p<space>jusqu'au<space>dix<space>huitième<space>siècle<space>le<space>point<space>de<space>vue<space>du<space>de<space>la<space>fabrication<space>le<space>point<space>de<space>vue<space>de<space>la<space>production<space>de<space>l'art<space>est<space>plus<space>important<space>que<space>le<space>point<space>de<space>vue<space>de<space>l'expérience<space>e<space>la<space>réception<space>de<space>l'art<space>euh<space>le<space>point<space>de<space>vue<space>de<space>pectateur<space>point<space>de<space>vue<space>spectateur<space>point<space>de<space>vue<space>esthétique

2026-01-29 17:24:22,566 | INFO | speech length: 451360
2026-01-29 17:24:22,602 | INFO | decoder input length: 704
2026-01-29 17:24:22,602 | INFO | max output length: 704
2026-01-29 17:24:22,602 | INFO | min output length: 70
2026-01-29 17:25:07,371 | INFO | end detected at 568
2026-01-29 17:25:07,372 | INFO | -717.65 * 0.5 = -358.82 for decoder
2026-01-29 17:25:07,372 | INFO | -90.56 * 0.5 = -45.28 for ctc
2026-01-29 17:25:07,372 | INFO | total log probability: -404.11
2026-01-29 17:25:07,372 | INFO | normalized log probability: -0.72
2026-01-29 17:25:07,372 | INFO | total number of ended hypotheses: 167
2026-01-29 17:25:07,379 | INFO | best hypo: se<space>dégagent<space>euh<space>à<space>la<space>fin<space>du<space>dix<space>septième<space>siècle<space>et<space>au<space>dix<space>huitième<space>siècle<space>et<space>puis<space>eh<space>ben<space>c'est<space>quelque<space>chose<space>que<space>nous<space>sommes<space>euh<space>que<space>nous<space>avons<space>bien<space>euh<space>que<space>nous<space>connaissons<space>bien<space>mais<space>faites<space>vous<space>même<space>la<space>différence<space>entre<space>votre<space>expérience<space>par<space>exemple<space>si<space>vous<space>pratiquez<space>la<space>musique<space>votre<space>expérience<space>en<space>tant<space>que<space>pratiquant<space>la<space>musique<space>et<space>votre<space>expérience<space>en<space>tant<space>que<space>consommateur<space>de<space>musique<space>qu'i<space>suffut<space>que<space>vous<space>ayez<space>un<space>falla<space>deur<space>poue<space>que<space>vous<space>connaissiez<space>e<space>de<space>dexpérience<space>de<space>consommateurs<space>de<space>musique<space>cone<space>dexpérence<space>de<space>la<space>réception<space>et<space>si<space>vous<space>faites<space>de<space>la<space>musique

2026-01-29 17:25:07,380 | INFO | speech length: 436160
2026-01-29 17:25:07,409 | INFO | decoder input length: 681
2026-01-29 17:25:07,409 | INFO | max output length: 681
2026-01-29 17:25:07,409 | INFO | min output length: 68
2026-01-29 17:25:43,260 | INFO | end detected at 445
2026-01-29 17:25:43,261 | INFO | -225.96 * 0.5 = -112.98 for decoder
2026-01-29 17:25:43,261 | INFO | -14.59 * 0.5 =  -7.30 for ctc
2026-01-29 17:25:43,261 | INFO | total log probability: -120.27
2026-01-29 17:25:43,262 | INFO | normalized log probability: -0.27
2026-01-29 17:25:43,262 | INFO | total number of ended hypotheses: 151
2026-01-29 17:25:43,266 | INFO | best hypo: eh<space>bien<space>vous<space>avez<space>l'expérience<space>de<space>la<space>poésie<space>de<space>de<space>la<space>production<space>musicale<space>euh<space>première<space>distinction<space>donc<space>fondamentale<space>parce<space>que<space>dire<space>selon<space>les<space>sociétés<space>je<space>dirais<space>que<space>la<space>euh<space>la<space>composante<space>de<space>production<space>ou<space>la<space>composante<space>de<space>réception<space>euh<space>varie<space>beaucoup<space>deuxième<space>distinction<space>eh<space>bien<space>euh<space>i<space>faut<space>voir<space>et<space>c'est<space>en<space>ce<space>sens<space>que<space>là<space>est<space>un<space>concept<space>flou<space>c'est<space>que<space>l'art<space>a<space>des<space>fonctions<space>extrêmement<space>diverses<space>euh<space>selon<space>les<space>époques<space>et<space>selon<space>les<space>sociétés

2026-01-29 17:25:43,269 | INFO | speech length: 435520
2026-01-29 17:25:43,298 | INFO | decoder input length: 680
2026-01-29 17:25:43,298 | INFO | max output length: 680
2026-01-29 17:25:43,298 | INFO | min output length: 68
2026-01-29 17:26:23,387 | INFO | end detected at 523
2026-01-29 17:26:23,389 | INFO | -528.95 * 0.5 = -264.48 for decoder
2026-01-29 17:26:23,389 | INFO | -35.50 * 0.5 = -17.75 for ctc
2026-01-29 17:26:23,389 | INFO | total log probability: -282.23
2026-01-29 17:26:23,389 | INFO | normalized log probability: -0.55
2026-01-29 17:26:23,389 | INFO | total number of ended hypotheses: 204
2026-01-29 17:26:23,395 | INFO | best hypo: euh<space>il<space>y<space>a<space>des<space>fonctions<space>euh<space>religieuses<space>euh<space>des<space>fonctions<space>presque<space>sacrées<space>quelquefois<space>euh<space>l'art<space>est<space>utilisée<space>dans<space>des<space>cérémonies<space>et<space>il<space>ne<space>doit<space>pas<space>être<space>utilisée<space>en<space>dehors<space>des<space>cérémonies<space>c'est<space>vrai<space>encore<space>pour<space>pas<space>mal<space>de<space>masques<space>africains<space>par<space>exemple<space>euh<space>bien<space>évidemment<space>c'est<space>vrai<space>pour<space>la<space>peinture<space>religieuse<space>euh<space>en<space>occident<space>euh<space>la<space>peinture<space>de<space>la<space>renaissance<space>euh<space>jusqu'au<space>jusqu'au<space>seizième<space>siècle<space>la<space>peinture<space>de<space>la<space>re<space>naissance<space>est<space>une<space>painture<space>émidemment<space>religieuse<space>avec<space>des<space>fonctions<space>religieuses<space>vous<space>en<space>parait

2026-01-29 17:26:23,397 | INFO | speech length: 412160
2026-01-29 17:26:23,426 | INFO | decoder input length: 643
2026-01-29 17:26:23,426 | INFO | max output length: 643
2026-01-29 17:26:23,426 | INFO | min output length: 64
2026-01-29 17:26:56,467 | INFO | end detected at 432
2026-01-29 17:26:56,468 | INFO | -224.50 * 0.5 = -112.25 for decoder
2026-01-29 17:26:56,468 | INFO | -22.81 * 0.5 = -11.40 for ctc
2026-01-29 17:26:56,468 | INFO | total log probability: -123.66
2026-01-29 17:26:56,468 | INFO | normalized log probability: -0.29
2026-01-29 17:26:56,468 | INFO | total number of ended hypotheses: 173
2026-01-29 17:26:56,473 | INFO | best hypo: et<space>puis<space>i<space>veut<space>avoir<space>des<space>fonctions<space>politiques<space>aussi<space>l'art<space>peut<space>avoir<space>des<space>fonctions<space>politiques<space>et<space>peut<space>peut<space>avoir<space>des<space>fonctions<space>décoratives<space>aussi<space>euh<space>penser<space>au<space>rôle<space>de<space>la<space>de<space>la<space>décoration<space>par<space>exemple<space>euh<space>quand<space>vous<space>regardez<space>dans<space>les<space>euh<space>certaines<space>stations<space>de<space>métro<space>qui<space>sont<space>euh<space>décorées<space>avec<space>des<space>oeuvres<space>d'art<space>ou<space>avec<space>des<space>des<space>des<space>travaux<space>de<space>design<space>il<space>peut<space>avoir<space>des<space>lart<space>peut<space>avoir<space>des<space>fonctions<space>de<space>connaissance<space>aussi<space>dans<space>des

2026-01-29 17:26:56,475 | INFO | speech length: 377600
2026-01-29 17:26:56,504 | INFO | decoder input length: 589
2026-01-29 17:26:56,504 | INFO | max output length: 589
2026-01-29 17:26:56,504 | INFO | min output length: 58
2026-01-29 17:27:28,558 | INFO | end detected at 461
2026-01-29 17:27:28,559 | INFO | -206.79 * 0.5 = -103.39 for decoder
2026-01-29 17:27:28,559 | INFO | -44.30 * 0.5 = -22.15 for ctc
2026-01-29 17:27:28,559 | INFO | total log probability: -125.55
2026-01-29 17:27:28,559 | INFO | normalized log probability: -0.28
2026-01-29 17:27:28,559 | INFO | total number of ended hypotheses: 179
2026-01-29 17:27:28,565 | INFO | best hypo: des<space>fonctions<space>de<space>connaissance<space>des<space>fonctions<space>de<space>critique<space>y<space>a<space>beaucoup<space>de<space>fonctions<space>différentes<space>dans<space>l'art<space>et<space>encore<space>une<space>fois<space>elles<space>sont<space>pas<space>toutes<space>également<space>présentes<space>euh<space>selon<space>les<space>sociétés<space>selon<space>les<space>efforts<space>qu'<space>y<space>a<space>des<space>sociétés<space>où<space>la<space>dimension<space>de<space>connaissance<space>que<space>j'appellerais<space>à<space>des<space>intentions<space>cogmitives<space>et<space>plus<space>importantes<space>y<space>y<space>a<space>des<space>sociétés<space>où<space>la<space>dimension<space>euh<space>des<space>don<space>et<space>donistiques<space>le<space>plaisir<space>que<space>donne<space>l'expérince<space>artistique<space>sont<space>plus<space>importantes

2026-01-29 17:27:28,572 | INFO | Chunk: 0 | WER=5.128205 | S=0 D=1 I=1
2026-01-29 17:27:28,578 | INFO | Chunk: 1 | WER=16.504854 | S=10 D=6 I=1
2026-01-29 17:27:28,583 | INFO | Chunk: 2 | WER=12.903226 | S=5 D=7 I=0
2026-01-29 17:27:28,589 | INFO | Chunk: 3 | WER=11.702128 | S=8 D=1 I=2
2026-01-29 17:27:28,594 | INFO | Chunk: 4 | WER=21.276596 | S=14 D=6 I=0
2026-01-29 17:27:28,597 | INFO | Chunk: 5 | WER=9.375000 | S=3 D=2 I=1
2026-01-29 17:27:28,604 | INFO | Chunk: 6 | WER=15.178571 | S=8 D=4 I=5
2026-01-29 17:27:28,608 | INFO | Chunk: 7 | WER=9.638554 | S=5 D=2 I=1
2026-01-29 17:27:28,614 | INFO | Chunk: 8 | WER=5.494505 | S=4 D=0 I=1
2026-01-29 17:27:28,621 | INFO | Chunk: 9 | WER=12.844037 | S=8 D=4 I=2
2026-01-29 17:27:28,627 | INFO | Chunk: 10 | WER=14.285714 | S=11 D=0 I=3
2026-01-29 17:27:28,631 | INFO | Chunk: 11 | WER=8.333333 | S=3 D=4 I=0
2026-01-29 17:27:28,636 | INFO | Chunk: 12 | WER=12.359551 | S=7 D=2 I=2
2026-01-29 17:27:28,640 | INFO | Chunk: 13 | WER=9.090909 | S=5 D=1 I=1
2026-01-29 17:27:28,644 | INFO | Chunk: 14 | WER=24.050633 | S=13 D=4 I=2
2026-01-29 17:27:29,350 | INFO | File: Rhap-M2002.wav | WER=20.000000 | S=107 D=38 I=100
2026-01-29 17:27:29,350 | INFO | ------------------------------
2026-01-29 17:27:29,350 | INFO | Conf ester Done!
2026-01-29 17:30:20,061 | INFO | Chunk: 0 | WER=20.512821 | S=4 D=4 I=0
2026-01-29 17:30:20,069 | INFO | Chunk: 1 | WER=20.388350 | S=11 D=9 I=1
2026-01-29 17:30:20,073 | INFO | Chunk: 2 | WER=19.354839 | S=7 D=11 I=0
2026-01-29 17:30:20,077 | INFO | Chunk: 3 | WER=21.276596 | S=6 D=14 I=0
2026-01-29 17:30:20,081 | INFO | Chunk: 4 | WER=30.851064 | S=15 D=14 I=0
2026-01-29 17:30:20,083 | INFO | Chunk: 5 | WER=14.062500 | S=2 D=7 I=0
2026-01-29 17:30:20,089 | INFO | Chunk: 6 | WER=16.964286 | S=7 D=10 I=2
2026-01-29 17:30:20,092 | INFO | Chunk: 7 | WER=32.530120 | S=14 D=12 I=1
2026-01-29 17:30:20,096 | INFO | Chunk: 8 | WER=18.681319 | S=13 D=3 I=1
2026-01-29 17:30:20,102 | INFO | Chunk: 9 | WER=19.266055 | S=11 D=10 I=0
2026-01-29 17:30:20,107 | INFO | Chunk: 10 | WER=12.244898 | S=4 D=6 I=2
2026-01-29 17:30:20,110 | INFO | Chunk: 11 | WER=14.285714 | S=4 D=8 I=0
2026-01-29 17:30:20,114 | INFO | Chunk: 12 | WER=16.853933 | S=6 D=9 I=0
2026-01-29 17:30:20,117 | INFO | Chunk: 13 | WER=20.779221 | S=9 D=7 I=0
2026-01-29 17:30:20,120 | INFO | Chunk: 14 | WER=24.050633 | S=13 D=5 I=1
2026-01-29 17:30:20,760 | INFO | File: Rhap-M2002.wav | WER=16.897959 | S=122 D=61 I=24
2026-01-29 17:30:20,760 | INFO | ------------------------------
2026-01-29 17:30:20,760 | INFO | hmm_tdnn Done!
2026-01-29 17:30:20,900 | INFO | ==================================Rhap-M2003.wav=========================================
2026-01-29 17:30:20,995 | INFO | Using rVAD model
2026-01-29 17:30:41,150 | INFO | Chunk: 0 | WER=3.225806 | S=0 D=0 I=1
2026-01-29 17:30:41,150 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:30:41,150 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:30:41,151 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:30:41,151 | INFO | Chunk: 4 | WER=3.846154 | S=1 D=0 I=0
2026-01-29 17:30:41,152 | INFO | Chunk: 5 | WER=16.666667 | S=2 D=0 I=0
2026-01-29 17:30:41,152 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:30:41,152 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:30:41,152 | INFO | Chunk: 8 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:30:41,152 | INFO | Chunk: 9 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:30:41,153 | INFO | Chunk: 10 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:30:41,153 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:30:41,154 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:30:41,154 | INFO | Chunk: 13 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:30:41,155 | INFO | Chunk: 14 | WER=5.263158 | S=0 D=1 I=0
2026-01-29 17:30:41,155 | INFO | Chunk: 15 | WER=6.250000 | S=1 D=0 I=0
2026-01-29 17:30:41,155 | INFO | Chunk: 16 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:30:41,156 | INFO | Chunk: 17 | WER=3.125000 | S=1 D=0 I=0
2026-01-29 17:30:41,157 | INFO | Chunk: 18 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:30:41,157 | INFO | Chunk: 19 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:30:41,158 | INFO | Chunk: 20 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:30:41,160 | INFO | Chunk: 21 | WER=4.918033 | S=3 D=0 I=0
2026-01-29 17:30:41,160 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:30:41,161 | INFO | Chunk: 23 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:30:41,161 | INFO | Chunk: 24 | WER=6.250000 | S=1 D=0 I=0
2026-01-29 17:30:41,161 | INFO | Chunk: 25 | WER=10.000000 | S=0 D=0 I=1
2026-01-29 17:30:41,161 | INFO | Chunk: 26 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:30:41,162 | INFO | Chunk: 27 | WER=7.142857 | S=1 D=0 I=0
2026-01-29 17:30:41,162 | INFO | Chunk: 28 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:30:41,163 | INFO | Chunk: 29 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:30:41,165 | INFO | Chunk: 30 | WER=3.448276 | S=2 D=0 I=0
2026-01-29 17:30:41,165 | INFO | Chunk: 31 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:30:41,166 | INFO | Chunk: 32 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:30:41,168 | INFO | Chunk: 33 | WER=4.477612 | S=2 D=1 I=0
2026-01-29 17:30:41,168 | INFO | Chunk: 34 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:30:41,169 | INFO | Chunk: 35 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:30:41,390 | INFO | File: Rhap-M2003.wav | WER=2.624309 | S=14 D=2 I=3
2026-01-29 17:30:41,390 | INFO | ------------------------------
2026-01-29 17:30:41,391 | INFO | w2vec vad chunk Done!
2026-01-29 17:31:21,191 | INFO | Chunk: 0 | WER=3.225806 | S=0 D=0 I=1
2026-01-29 17:31:21,192 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:31:21,192 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:31:21,192 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:31:21,193 | INFO | Chunk: 4 | WER=7.692308 | S=2 D=0 I=0
2026-01-29 17:31:21,193 | INFO | Chunk: 5 | WER=8.333333 | S=1 D=0 I=0
2026-01-29 17:31:21,193 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:31:21,194 | INFO | Chunk: 7 | WER=20.000000 | S=2 D=0 I=0
2026-01-29 17:31:21,194 | INFO | Chunk: 8 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:31:21,194 | INFO | Chunk: 9 | WER=14.285714 | S=1 D=1 I=0
2026-01-29 17:31:21,195 | INFO | Chunk: 10 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:31:21,195 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:31:21,196 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:31:21,196 | INFO | Chunk: 13 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:31:21,197 | INFO | Chunk: 14 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:31:21,197 | INFO | Chunk: 15 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:31:21,197 | INFO | Chunk: 16 | WER=33.333333 | S=1 D=0 I=0
2026-01-29 17:31:21,198 | INFO | Chunk: 17 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:31:21,199 | INFO | Chunk: 18 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:31:21,199 | INFO | Chunk: 19 | WER=20.000000 | S=1 D=0 I=0
2026-01-29 17:31:21,200 | INFO | Chunk: 20 | WER=40.000000 | S=0 D=14 I=0
2026-01-29 17:31:21,201 | INFO | Chunk: 21 | WER=40.983607 | S=7 D=18 I=0
2026-01-29 17:31:21,201 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:31:21,202 | INFO | Chunk: 23 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:31:21,202 | INFO | Chunk: 24 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:31:21,202 | INFO | Chunk: 25 | WER=10.000000 | S=0 D=0 I=1
2026-01-29 17:31:21,202 | INFO | Chunk: 26 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:31:21,203 | INFO | Chunk: 27 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:31:21,203 | INFO | Chunk: 28 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:31:21,204 | INFO | Chunk: 29 | WER=2.500000 | S=1 D=0 I=0
2026-01-29 17:31:21,205 | INFO | Chunk: 30 | WER=43.103448 | S=4 D=19 I=2
2026-01-29 17:31:21,206 | INFO | Chunk: 31 | WER=12.500000 | S=1 D=0 I=0
2026-01-29 17:31:21,206 | INFO | Chunk: 32 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:31:21,208 | INFO | Chunk: 33 | WER=26.865672 | S=1 D=17 I=0
2026-01-29 17:31:21,208 | INFO | Chunk: 34 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:31:21,209 | INFO | Chunk: 35 | WER=3.125000 | S=1 D=0 I=0
2026-01-29 17:31:21,411 | INFO | File: Rhap-M2003.wav | WER=13.121547 | S=23 D=68 I=4
2026-01-29 17:31:21,411 | INFO | ------------------------------
2026-01-29 17:31:21,411 | INFO | whisper med Done!
2026-01-29 17:32:17,864 | INFO | Chunk: 0 | WER=9.677419 | S=1 D=0 I=2
2026-01-29 17:32:17,865 | INFO | Chunk: 1 | WER=5.263158 | S=1 D=0 I=0
2026-01-29 17:32:17,865 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:32:17,865 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:32:17,866 | INFO | Chunk: 4 | WER=3.846154 | S=1 D=0 I=0
2026-01-29 17:32:17,866 | INFO | Chunk: 5 | WER=8.333333 | S=1 D=0 I=0
2026-01-29 17:32:17,866 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:32:17,867 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:32:17,867 | INFO | Chunk: 8 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:32:17,867 | INFO | Chunk: 9 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:32:17,867 | INFO | Chunk: 10 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:32:17,868 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:32:17,868 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:32:17,869 | INFO | Chunk: 13 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:32:17,869 | INFO | Chunk: 14 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:32:17,870 | INFO | Chunk: 15 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:32:17,870 | INFO | Chunk: 16 | WER=33.333333 | S=1 D=0 I=0
2026-01-29 17:32:17,871 | INFO | Chunk: 17 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:32:17,871 | INFO | Chunk: 18 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:32:17,872 | INFO | Chunk: 19 | WER=20.000000 | S=1 D=0 I=0
2026-01-29 17:32:17,872 | INFO | Chunk: 20 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:32:17,875 | INFO | Chunk: 21 | WER=22.950820 | S=8 D=1 I=5
2026-01-29 17:32:17,875 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:32:17,875 | INFO | Chunk: 23 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:32:17,875 | INFO | Chunk: 24 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:32:17,876 | INFO | Chunk: 25 | WER=10.000000 | S=0 D=0 I=1
2026-01-29 17:32:17,876 | INFO | Chunk: 26 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:32:17,876 | INFO | Chunk: 27 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:32:17,876 | INFO | Chunk: 28 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:32:17,877 | INFO | Chunk: 29 | WER=7.500000 | S=2 D=0 I=1
2026-01-29 17:32:17,879 | INFO | Chunk: 30 | WER=20.689655 | S=6 D=6 I=0
2026-01-29 17:32:17,879 | INFO | Chunk: 31 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:32:17,880 | INFO | Chunk: 32 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:32:17,882 | INFO | Chunk: 33 | WER=17.910448 | S=7 D=0 I=5
2026-01-29 17:32:17,883 | INFO | Chunk: 34 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:32:17,883 | INFO | Chunk: 35 | WER=12.500000 | S=4 D=0 I=0
2026-01-29 17:32:18,107 | INFO | File: Rhap-M2003.wav | WER=7.596685 | S=33 D=7 I=15
2026-01-29 17:32:18,107 | INFO | ------------------------------
2026-01-29 17:32:18,107 | INFO | whisper large Done!
2026-01-29 17:32:18,266 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-29 17:32:18,296 | INFO | Vocabulary size: 350
2026-01-29 17:32:18,938 | INFO | Gradient checkpoint layers: []
2026-01-29 17:32:19,621 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 17:32:19,625 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 17:32:19,625 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 17:32:19,626 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-29 17:32:19,626 | INFO | speech length: 216960
2026-01-29 17:32:19,668 | INFO | decoder input length: 338
2026-01-29 17:32:19,668 | INFO | max output length: 338
2026-01-29 17:32:19,668 | INFO | min output length: 33
2026-01-29 17:32:26,341 | INFO | end detected at 79
2026-01-29 17:32:26,342 | INFO | -32.74 * 0.5 = -16.37 for decoder
2026-01-29 17:32:26,342 | INFO |  -0.36 * 0.5 =  -0.18 for ctc
2026-01-29 17:32:26,342 | INFO | total log probability: -16.55
2026-01-29 17:32:26,342 | INFO | normalized log probability: -0.22
2026-01-29 17:32:26,342 | INFO | total number of ended hypotheses: 137
2026-01-29 17:32:26,343 | INFO | best hypo: ▁c'est▁la▁première▁journée▁de▁jésus▁que▁nous▁venons▁d'entendre▁la▁première▁journée▁où▁il▁manifeste▁son▁autorité▁et▁la▁nouveauté▁la▁bonne▁nouvelle▁l'évangile▁qui▁arrive▁avec▁lui

2026-01-29 17:32:26,346 | INFO | speech length: 107360
2026-01-29 17:32:26,384 | INFO | decoder input length: 167
2026-01-29 17:32:26,384 | INFO | max output length: 167
2026-01-29 17:32:26,384 | INFO | min output length: 16
2026-01-29 17:32:29,643 | INFO | end detected at 55
2026-01-29 17:32:29,644 | INFO |  -3.60 * 0.5 =  -1.80 for decoder
2026-01-29 17:32:29,644 | INFO |  -0.33 * 0.5 =  -0.16 for ctc
2026-01-29 17:32:29,644 | INFO | total log probability: -1.96
2026-01-29 17:32:29,644 | INFO | normalized log probability: -0.04
2026-01-29 17:32:29,644 | INFO | total number of ended hypotheses: 150
2026-01-29 17:32:29,645 | INFO | best hypo: ▁et▁vous▁l'avez▁remarqué▁en▁écoutant▁l'évangile▁jésus▁ne▁dit▁rien▁avant▁la▁fin▁de▁l'épisode

2026-01-29 17:32:29,647 | INFO | speech length: 15840
2026-01-29 17:32:29,682 | INFO | decoder input length: 24
2026-01-29 17:32:29,682 | INFO | max output length: 24
2026-01-29 17:32:29,682 | INFO | min output length: 2
2026-01-29 17:32:30,107 | INFO | end detected at 10
2026-01-29 17:32:30,108 | INFO |  -0.41 * 0.5 =  -0.20 for decoder
2026-01-29 17:32:30,108 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:32:30,108 | INFO | total log probability: -0.21
2026-01-29 17:32:30,109 | INFO | normalized log probability: -0.03
2026-01-29 17:32:30,109 | INFO | total number of ended hypotheses: 137
2026-01-29 17:32:30,109 | INFO | best hypo: ▁il▁agit

2026-01-29 17:32:30,110 | INFO | speech length: 85120
2026-01-29 17:32:30,147 | INFO | decoder input length: 132
2026-01-29 17:32:30,147 | INFO | max output length: 132
2026-01-29 17:32:30,147 | INFO | min output length: 13
2026-01-29 17:32:32,037 | INFO | end detected at 34
2026-01-29 17:32:32,038 | INFO |  -2.23 * 0.5 =  -1.12 for decoder
2026-01-29 17:32:32,038 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-29 17:32:32,038 | INFO | total log probability: -1.13
2026-01-29 17:32:32,038 | INFO | normalized log probability: -0.04
2026-01-29 17:32:32,038 | INFO | total number of ended hypotheses: 141
2026-01-29 17:32:32,039 | INFO | best hypo: ▁il▁fait▁d'abord▁lever▁la▁belle▁mère▁de▁pierre▁et▁sa▁fièvre▁la▁quitte

2026-01-29 17:32:32,041 | INFO | speech length: 181760
2026-01-29 17:32:32,080 | INFO | decoder input length: 283
2026-01-29 17:32:32,080 | INFO | max output length: 283
2026-01-29 17:32:32,080 | INFO | min output length: 28
2026-01-29 17:32:37,130 | INFO | end detected at 66
2026-01-29 17:32:37,132 | INFO | -11.37 * 0.5 =  -5.68 for decoder
2026-01-29 17:32:37,132 | INFO |  -0.15 * 0.5 =  -0.08 for ctc
2026-01-29 17:32:37,132 | INFO | total log probability: -5.76
2026-01-29 17:32:37,132 | INFO | normalized log probability: -0.09
2026-01-29 17:32:37,132 | INFO | total number of ended hypotheses: 153
2026-01-29 17:32:37,133 | INFO | best hypo: ▁il▁guérit▁ensuite▁toutes▁sortes▁de▁malades▁comme▁plus▁tard▁dans▁l'évangile▁il▁acceptera▁de▁se▁laisser▁toucher▁ou▁de▁bénir▁en▁imposant▁les▁mains

2026-01-29 17:32:37,135 | INFO | speech length: 57120
2026-01-29 17:32:37,173 | INFO | decoder input length: 88
2026-01-29 17:32:37,173 | INFO | max output length: 88
2026-01-29 17:32:37,173 | INFO | min output length: 8
2026-01-29 17:32:38,691 | INFO | end detected at 30
2026-01-29 17:32:38,692 | INFO |  -1.81 * 0.5 =  -0.91 for decoder
2026-01-29 17:32:38,692 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-29 17:32:38,692 | INFO | total log probability: -0.92
2026-01-29 17:32:38,692 | INFO | normalized log probability: -0.04
2026-01-29 17:32:38,692 | INFO | total number of ended hypotheses: 139
2026-01-29 17:32:38,693 | INFO | best hypo: ▁il▁chasse▁beaucoup▁d'esprit▁ce▁qui▁fait▁du▁mal▁aux▁gens

2026-01-29 17:32:38,694 | INFO | speech length: 19040
2026-01-29 17:32:38,731 | INFO | decoder input length: 29
2026-01-29 17:32:38,731 | INFO | max output length: 29
2026-01-29 17:32:38,731 | INFO | min output length: 2
2026-01-29 17:32:39,319 | INFO | end detected at 14
2026-01-29 17:32:39,320 | INFO |  -0.76 * 0.5 =  -0.38 for decoder
2026-01-29 17:32:39,320 | INFO |  -0.45 * 0.5 =  -0.22 for ctc
2026-01-29 17:32:39,320 | INFO | total log probability: -0.60
2026-01-29 17:32:39,321 | INFO | normalized log probability: -0.07
2026-01-29 17:32:39,321 | INFO | total number of ended hypotheses: 165
2026-01-29 17:32:39,321 | INFO | best hypo: ▁avant▁de▁parler

2026-01-29 17:32:39,322 | INFO | speech length: 58080
2026-01-29 17:32:39,359 | INFO | decoder input length: 90
2026-01-29 17:32:39,360 | INFO | max output length: 90
2026-01-29 17:32:39,360 | INFO | min output length: 9
2026-01-29 17:32:40,698 | INFO | end detected at 27
2026-01-29 17:32:40,699 | INFO |  -1.73 * 0.5 =  -0.87 for decoder
2026-01-29 17:32:40,699 | INFO |  -0.07 * 0.5 =  -0.04 for ctc
2026-01-29 17:32:40,699 | INFO | total log probability: -0.90
2026-01-29 17:32:40,699 | INFO | normalized log probability: -0.04
2026-01-29 17:32:40,699 | INFO | total number of ended hypotheses: 147
2026-01-29 17:32:40,700 | INFO | best hypo: ▁avant▁toute▁parole▁la▁vie▁de▁jésus▁est▁une▁action

2026-01-29 17:32:40,701 | INFO | speech length: 28000
2026-01-29 17:32:40,738 | INFO | decoder input length: 43
2026-01-29 17:32:40,738 | INFO | max output length: 43
2026-01-29 17:32:40,738 | INFO | min output length: 4
2026-01-29 17:32:41,483 | INFO | end detected at 16
2026-01-29 17:32:41,484 | INFO |  -0.90 * 0.5 =  -0.45 for decoder
2026-01-29 17:32:41,484 | INFO |  -1.04 * 0.5 =  -0.52 for ctc
2026-01-29 17:32:41,484 | INFO | total log probability: -0.97
2026-01-29 17:32:41,484 | INFO | normalized log probability: -0.08
2026-01-29 17:32:41,484 | INFO | total number of ended hypotheses: 152
2026-01-29 17:32:41,484 | INFO | best hypo: ▁c'est▁une▁action▁salutaire

2026-01-29 17:32:41,486 | INFO | speech length: 83680
2026-01-29 17:32:41,523 | INFO | decoder input length: 130
2026-01-29 17:32:41,523 | INFO | max output length: 130
2026-01-29 17:32:41,523 | INFO | min output length: 13
2026-01-29 17:32:43,518 | INFO | end detected at 36
2026-01-29 17:32:43,521 | INFO |  -2.10 * 0.5 =  -1.05 for decoder
2026-01-29 17:32:43,521 | INFO |  -2.86 * 0.5 =  -1.43 for ctc
2026-01-29 17:32:43,521 | INFO | total log probability: -2.48
2026-01-29 17:32:43,521 | INFO | normalized log probability: -0.09
2026-01-29 17:32:43,521 | INFO | total number of ended hypotheses: 193
2026-01-29 17:32:43,522 | INFO | best hypo: ▁tout▁chez▁lui▁est▁fait▁de▁compassion▁et▁non▁de▁violence▁ou▁de▁châtiment

2026-01-29 17:32:43,524 | INFO | speech length: 93600
2026-01-29 17:32:43,562 | INFO | decoder input length: 145
2026-01-29 17:32:43,562 | INFO | max output length: 145
2026-01-29 17:32:43,562 | INFO | min output length: 14
2026-01-29 17:32:45,955 | INFO | end detected at 41
2026-01-29 17:32:45,957 | INFO |  -2.82 * 0.5 =  -1.41 for decoder
2026-01-29 17:32:45,957 | INFO |  -0.62 * 0.5 =  -0.31 for ctc
2026-01-29 17:32:45,957 | INFO | total log probability: -1.72
2026-01-29 17:32:45,957 | INFO | normalized log probability: -0.05
2026-01-29 17:32:45,957 | INFO | total number of ended hypotheses: 178
2026-01-29 17:32:45,958 | INFO | best hypo: ▁en▁fait▁tout▁l'évangile▁nous▁montre▁un▁jésus▁qui▁remet▁l'humanité▁debout

2026-01-29 17:32:45,960 | INFO | speech length: 104480
2026-01-29 17:32:45,997 | INFO | decoder input length: 162
2026-01-29 17:32:45,997 | INFO | max output length: 162
2026-01-29 17:32:45,997 | INFO | min output length: 16
2026-01-29 17:32:48,779 | INFO | end detected at 47
2026-01-29 17:32:48,781 | INFO |  -3.28 * 0.5 =  -1.64 for decoder
2026-01-29 17:32:48,781 | INFO |  -0.03 * 0.5 =  -0.01 for ctc
2026-01-29 17:32:48,781 | INFO | total log probability: -1.66
2026-01-29 17:32:48,781 | INFO | normalized log probability: -0.04
2026-01-29 17:32:48,781 | INFO | total number of ended hypotheses: 149
2026-01-29 17:32:48,782 | INFO | best hypo: ▁et▁aujourd'hui▁dans▁sa▁première▁journée▁il▁fait▁de▁la▁belle▁mère▁de▁pierre▁une▁des▁toutes▁premières▁disciples

2026-01-29 17:32:48,784 | INFO | speech length: 132320
2026-01-29 17:32:48,823 | INFO | decoder input length: 206
2026-01-29 17:32:48,823 | INFO | max output length: 206
2026-01-29 17:32:48,823 | INFO | min output length: 20
2026-01-29 17:32:52,800 | INFO | end detected at 61
2026-01-29 17:32:52,802 | INFO |  -4.37 * 0.5 =  -2.18 for decoder
2026-01-29 17:32:52,802 | INFO |  -1.61 * 0.5 =  -0.80 for ctc
2026-01-29 17:32:52,802 | INFO | total log probability: -2.99
2026-01-29 17:32:52,802 | INFO | normalized log probability: -0.05
2026-01-29 17:32:52,802 | INFO | total number of ended hypotheses: 149
2026-01-29 17:32:52,803 | INFO | best hypo: ▁l'évangile▁la▁bonne▁nouvelle▁comme▁le▁répète▁l'apôtre▁paul▁la▁bonne▁nouvelle▁en▁la▁personne▁de▁jésus▁c'est▁d'abord▁une▁action

2026-01-29 17:32:52,805 | INFO | speech length: 153760
2026-01-29 17:32:52,844 | INFO | decoder input length: 239
2026-01-29 17:32:52,844 | INFO | max output length: 239
2026-01-29 17:32:52,844 | INFO | min output length: 23
2026-01-29 17:32:56,969 | INFO | end detected at 58
2026-01-29 17:32:56,971 | INFO |  -4.78 * 0.5 =  -2.39 for decoder
2026-01-29 17:32:56,971 | INFO |  -0.45 * 0.5 =  -0.23 for ctc
2026-01-29 17:32:56,971 | INFO | total log probability: -2.61
2026-01-29 17:32:56,971 | INFO | normalized log probability: -0.05
2026-01-29 17:32:56,971 | INFO | total number of ended hypotheses: 146
2026-01-29 17:32:56,972 | INFO | best hypo: ▁une▁action▁pour▁restaurer▁la▁dignité▁de▁la▁personne▁humaine▁une▁action▁pour▁rétablir▁la▁personne▁en▁sa▁qualité▁de▁sujet▁de▁parole

2026-01-29 17:32:56,973 | INFO | speech length: 99360
2026-01-29 17:32:57,012 | INFO | decoder input length: 154
2026-01-29 17:32:57,012 | INFO | max output length: 154
2026-01-29 17:32:57,012 | INFO | min output length: 15
2026-01-29 17:32:59,399 | INFO | end detected at 40
2026-01-29 17:32:59,401 | INFO |  -2.79 * 0.5 =  -1.39 for decoder
2026-01-29 17:32:59,401 | INFO |  -0.07 * 0.5 =  -0.03 for ctc
2026-01-29 17:32:59,401 | INFO | total log probability: -1.43
2026-01-29 17:32:59,401 | INFO | normalized log probability: -0.04
2026-01-29 17:32:59,401 | INFO | total number of ended hypotheses: 167
2026-01-29 17:32:59,402 | INFO | best hypo: ▁une▁action▁pour▁rétablir▁les▁uns▁et▁les▁autres▁dans▁les▁relations▁avec▁les▁uns▁et▁avec▁les▁autres

2026-01-29 17:32:59,404 | INFO | speech length: 88160
2026-01-29 17:32:59,440 | INFO | decoder input length: 137
2026-01-29 17:32:59,440 | INFO | max output length: 137
2026-01-29 17:32:59,440 | INFO | min output length: 13
2026-01-29 17:33:01,702 | INFO | end detected at 40
2026-01-29 17:33:01,707 | INFO |  -3.96 * 0.5 =  -1.98 for decoder
2026-01-29 17:33:01,707 | INFO |  -1.71 * 0.5 =  -0.85 for ctc
2026-01-29 17:33:01,708 | INFO | total log probability: -2.84
2026-01-29 17:33:01,708 | INFO | normalized log probability: -0.08
2026-01-29 17:33:01,708 | INFO | total number of ended hypotheses: 165
2026-01-29 17:33:01,709 | INFO | best hypo: ▁jésus▁ne▁pose▁même▁pas▁ces▁gestes▁de▁bonté▁comme▁des▁appels▁à▁la▁fois▁non

2026-01-29 17:33:01,711 | INFO | speech length: 20800
2026-01-29 17:33:01,769 | INFO | decoder input length: 32
2026-01-29 17:33:01,769 | INFO | max output length: 32
2026-01-29 17:33:01,769 | INFO | min output length: 3
2026-01-29 17:33:02,289 | INFO | end detected at 12
2026-01-29 17:33:02,291 | INFO |  -1.82 * 0.5 =  -0.91 for decoder
2026-01-29 17:33:02,291 | INFO |  -1.15 * 0.5 =  -0.57 for ctc
2026-01-29 17:33:02,291 | INFO | total log probability: -1.48
2026-01-29 17:33:02,291 | INFO | normalized log probability: -0.19
2026-01-29 17:33:02,291 | INFO | total number of ended hypotheses: 149
2026-01-29 17:33:02,291 | INFO | best hypo: ▁en▁ces▁gestes

2026-01-29 17:33:02,293 | INFO | speech length: 214560
2026-01-29 17:33:02,336 | INFO | decoder input length: 334
2026-01-29 17:33:02,337 | INFO | max output length: 334
2026-01-29 17:33:02,337 | INFO | min output length: 33
2026-01-29 17:33:08,832 | INFO | end detected at 75
2026-01-29 17:33:08,833 | INFO | -39.87 * 0.5 = -19.94 for decoder
2026-01-29 17:33:08,833 | INFO |  -0.25 * 0.5 =  -0.12 for ctc
2026-01-29 17:33:08,833 | INFO | total log probability: -20.06
2026-01-29 17:33:08,833 | INFO | normalized log probability: -0.28
2026-01-29 17:33:08,833 | INFO | total number of ended hypotheses: 136
2026-01-29 17:33:08,834 | INFO | best hypo: ▁dans▁ce▁premier▁jour▁il▁inaugure▁en▁sa▁personne▁la▁venue▁de▁dieu▁parmi▁les▁hommes▁la▁venue▁définitive▁de▁dieu▁pour▁rétablir▁l'humanité▁dans▁sa▁vocation▁et▁dans▁sa▁dignité

2026-01-29 17:33:08,836 | INFO | speech length: 186880
2026-01-29 17:33:08,876 | INFO | decoder input length: 291
2026-01-29 17:33:08,876 | INFO | max output length: 291
2026-01-29 17:33:08,877 | INFO | min output length: 29
2026-01-29 17:33:15,428 | INFO | end detected at 86
2026-01-29 17:33:15,429 | INFO | -32.06 * 0.5 = -16.03 for decoder
2026-01-29 17:33:15,429 | INFO |  -0.63 * 0.5 =  -0.31 for ctc
2026-01-29 17:33:15,430 | INFO | total log probability: -16.34
2026-01-29 17:33:15,430 | INFO | normalized log probability: -0.20
2026-01-29 17:33:15,430 | INFO | total number of ended hypotheses: 162
2026-01-29 17:33:15,431 | INFO | best hypo: ▁son▁message▁initial▁au▁tout▁début▁de▁l'évangile▁convertissez▁vous▁le▁royaume▁de▁dieu▁est▁proche▁de▁vous▁son▁message▁s'inscrit▁aujourd'hui▁dans▁sa▁personne▁il▁trouve▁sa▁traduction▁en▁action

2026-01-29 17:33:15,433 | INFO | speech length: 28000
2026-01-29 17:33:15,468 | INFO | decoder input length: 43
2026-01-29 17:33:15,468 | INFO | max output length: 43
2026-01-29 17:33:15,468 | INFO | min output length: 4
2026-01-29 17:33:16,329 | INFO | end detected at 19
2026-01-29 17:33:16,330 | INFO |  -1.25 * 0.5 =  -0.63 for decoder
2026-01-29 17:33:16,330 | INFO |  -0.32 * 0.5 =  -0.16 for ctc
2026-01-29 17:33:16,330 | INFO | total log probability: -0.79
2026-01-29 17:33:16,330 | INFO | normalized log probability: -0.05
2026-01-29 17:33:16,331 | INFO | total number of ended hypotheses: 146
2026-01-29 17:33:16,331 | INFO | best hypo: ▁dans▁ces▁gestes▁de▁libération

2026-01-29 17:33:16,332 | INFO | speech length: 202400
2026-01-29 17:33:16,372 | INFO | decoder input length: 315
2026-01-29 17:33:16,372 | INFO | max output length: 315
2026-01-29 17:33:16,372 | INFO | min output length: 31
2026-01-29 17:33:23,579 | INFO | end detected at 91
2026-01-29 17:33:23,580 | INFO | -18.90 * 0.5 =  -9.45 for decoder
2026-01-29 17:33:23,580 | INFO |  -2.16 * 0.5 =  -1.08 for ctc
2026-01-29 17:33:23,580 | INFO | total log probability: -10.53
2026-01-29 17:33:23,580 | INFO | normalized log probability: -0.12
2026-01-29 17:33:23,580 | INFO | total number of ended hypotheses: 150
2026-01-29 17:33:23,582 | INFO | best hypo: ▁plus▁encore▁dans▁l'évangile▁d'aujourd'hui▁jésus▁s'accorde▁le▁pouvoir▁de▁guérir▁et▁de▁chasser▁les▁esprits▁mauvais▁il▁n'en▁appelle▁plus▁comme▁on▁le▁faisait▁en▁son▁temps▁à▁salomon

2026-01-29 17:33:23,583 | INFO | speech length: 346560
2026-01-29 17:33:23,624 | INFO | decoder input length: 541
2026-01-29 17:33:23,624 | INFO | max output length: 541
2026-01-29 17:33:23,624 | INFO | min output length: 54
2026-01-29 17:33:39,209 | INFO | end detected at 139
2026-01-29 17:33:39,211 | INFO | -189.22 * 0.5 = -94.61 for decoder
2026-01-29 17:33:39,211 | INFO | -15.54 * 0.5 =  -7.77 for ctc
2026-01-29 17:33:39,211 | INFO | total log probability: -102.38
2026-01-29 17:33:39,211 | INFO | normalized log probability: -0.76
2026-01-29 17:33:39,211 | INFO | total number of ended hypotheses: 163
2026-01-29 17:33:39,213 | INFO | best hypo: ▁comme▁le▁faisaient▁des▁guérisseurs▁de▁son▁temps▁il▁guérit▁de▁son▁propre▁chef▁les▁malades▁c'est▁sa▁propre▁vie▁qu'il▁communiquent▁à▁ce▁qu'il▁rencontre▁c'est▁sa▁propre▁autorité▁qui▁l'engage▁l'autorité▁de▁sa▁vie▁je▁le▁veux▁sois▁purifié▁entendant▁dans▁l'évangile▁ou▁encore▁fillette▁je▁te▁le▁dis▁lève▁toi

2026-01-29 17:33:39,215 | INFO | speech length: 42880
2026-01-29 17:33:39,252 | INFO | decoder input length: 66
2026-01-29 17:33:39,253 | INFO | max output length: 66
2026-01-29 17:33:39,253 | INFO | min output length: 6
2026-01-29 17:33:40,223 | INFO | end detected at 21
2026-01-29 17:33:40,224 | INFO |  -1.38 * 0.5 =  -0.69 for decoder
2026-01-29 17:33:40,224 | INFO |  -0.16 * 0.5 =  -0.08 for ctc
2026-01-29 17:33:40,224 | INFO | total log probability: -0.77
2026-01-29 17:33:40,224 | INFO | normalized log probability: -0.05
2026-01-29 17:33:40,224 | INFO | total number of ended hypotheses: 143
2026-01-29 17:33:40,225 | INFO | best hypo: ▁ou▁encore▁moi▁moi▁je▁te▁l'ordonne

2026-01-29 17:33:40,226 | INFO | speech length: 40640
2026-01-29 17:33:40,262 | INFO | decoder input length: 63
2026-01-29 17:33:40,262 | INFO | max output length: 63
2026-01-29 17:33:40,262 | INFO | min output length: 6
2026-01-29 17:33:41,187 | INFO | end detected at 20
2026-01-29 17:33:41,188 | INFO |  -1.15 * 0.5 =  -0.57 for decoder
2026-01-29 17:33:41,188 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:33:41,189 | INFO | total log probability: -0.58
2026-01-29 17:33:41,189 | INFO | normalized log probability: -0.04
2026-01-29 17:33:41,189 | INFO | total number of ended hypotheses: 146
2026-01-29 17:33:41,189 | INFO | best hypo: ▁c'est▁désormais▁en▁son▁propre▁nom

2026-01-29 17:33:41,190 | INFO | speech length: 101120
2026-01-29 17:33:41,229 | INFO | decoder input length: 157
2026-01-29 17:33:41,229 | INFO | max output length: 157
2026-01-29 17:33:41,229 | INFO | min output length: 15
2026-01-29 17:33:44,073 | INFO | end detected at 48
2026-01-29 17:33:44,076 | INFO |  -3.49 * 0.5 =  -1.75 for decoder
2026-01-29 17:33:44,076 | INFO |  -1.01 * 0.5 =  -0.50 for ctc
2026-01-29 17:33:44,076 | INFO | total log probability: -2.25
2026-01-29 17:33:44,076 | INFO | normalized log probability: -0.05
2026-01-29 17:33:44,076 | INFO | total number of ended hypotheses: 147
2026-01-29 17:33:44,077 | INFO | best hypo: ▁que▁jésus▁chasse▁les▁démons▁il▁enseigne▁avec▁l'autorité▁pas▁comme▁les▁pharisiens▁et▁les▁scribes

2026-01-29 17:33:44,078 | INFO | speech length: 64960
2026-01-29 17:33:44,115 | INFO | decoder input length: 101
2026-01-29 17:33:44,115 | INFO | max output length: 101
2026-01-29 17:33:44,115 | INFO | min output length: 10
2026-01-29 17:33:45,643 | INFO | end detected at 30
2026-01-29 17:33:45,645 | INFO |  -1.86 * 0.5 =  -0.93 for decoder
2026-01-29 17:33:45,646 | INFO |  -0.86 * 0.5 =  -0.43 for ctc
2026-01-29 17:33:45,646 | INFO | total log probability: -1.36
2026-01-29 17:33:45,646 | INFO | normalized log probability: -0.05
2026-01-29 17:33:45,646 | INFO | total number of ended hypotheses: 155
2026-01-29 17:33:45,646 | INFO | best hypo: ▁il▁guérit▁en▁donnant▁la▁vie▁en▁donnant▁sa▁propre▁vue

2026-01-29 17:33:45,648 | INFO | speech length: 12960
2026-01-29 17:33:45,683 | INFO | decoder input length: 19
2026-01-29 17:33:45,683 | INFO | max output length: 19
2026-01-29 17:33:45,683 | INFO | min output length: 1
2026-01-29 17:33:46,142 | INFO | end detected at 11
2026-01-29 17:33:46,143 | INFO |  -1.16 * 0.5 =  -0.58 for decoder
2026-01-29 17:33:46,143 | INFO |  -0.07 * 0.5 =  -0.04 for ctc
2026-01-29 17:33:46,143 | INFO | total log probability: -0.62
2026-01-29 17:33:46,143 | INFO | normalized log probability: -0.10
2026-01-29 17:33:46,144 | INFO | total number of ended hypotheses: 155
2026-01-29 17:33:46,144 | INFO | best hypo: ▁me▁voilà

2026-01-29 17:33:46,145 | INFO | speech length: 71520
2026-01-29 17:33:46,182 | INFO | decoder input length: 111
2026-01-29 17:33:46,182 | INFO | max output length: 111
2026-01-29 17:33:46,182 | INFO | min output length: 11
2026-01-29 17:33:48,246 | INFO | end detected at 40
2026-01-29 17:33:48,247 | INFO |  -2.69 * 0.5 =  -1.35 for decoder
2026-01-29 17:33:48,247 | INFO |  -0.12 * 0.5 =  -0.06 for ctc
2026-01-29 17:33:48,247 | INFO | total log probability: -1.41
2026-01-29 17:33:48,247 | INFO | normalized log probability: -0.04
2026-01-29 17:33:48,247 | INFO | total number of ended hypotheses: 149
2026-01-29 17:33:48,248 | INFO | best hypo: ▁le▁mal▁est▁toujours▁présent▁au▁milieu▁de▁nous▁depuis▁le▁temps▁de▁jésus

2026-01-29 17:33:48,250 | INFO | speech length: 29120
2026-01-29 17:33:48,281 | INFO | decoder input length: 45
2026-01-29 17:33:48,281 | INFO | max output length: 45
2026-01-29 17:33:48,281 | INFO | min output length: 4
2026-01-29 17:33:49,153 | INFO | end detected at 20
2026-01-29 17:33:49,155 | INFO |  -1.13 * 0.5 =  -0.57 for decoder
2026-01-29 17:33:49,155 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-29 17:33:49,155 | INFO | total log probability: -0.57
2026-01-29 17:33:49,155 | INFO | normalized log probability: -0.04
2026-01-29 17:33:49,155 | INFO | total number of ended hypotheses: 168
2026-01-29 17:33:49,155 | INFO | best hypo: ▁il▁y▁a▁le▁mal▁dans▁l'humanité

2026-01-29 17:33:49,157 | INFO | speech length: 225440
2026-01-29 17:33:49,195 | INFO | decoder input length: 351
2026-01-29 17:33:49,195 | INFO | max output length: 351
2026-01-29 17:33:49,195 | INFO | min output length: 35
2026-01-29 17:33:57,454 | INFO | end detected at 96
2026-01-29 17:33:57,457 | INFO | -127.83 * 0.5 = -63.92 for decoder
2026-01-29 17:33:57,457 | INFO | -25.94 * 0.5 = -12.97 for ctc
2026-01-29 17:33:57,457 | INFO | total log probability: -76.89
2026-01-29 17:33:57,457 | INFO | normalized log probability: -0.84
2026-01-29 17:33:57,457 | INFO | total number of ended hypotheses: 157
2026-01-29 17:33:57,458 | INFO | best hypo: ▁le▁mal▁moral▁celui▁que▁l'on▁fait▁en▁faisant▁du▁mal▁aux▁autres▁le▁mal▁physique▁qu'en▁surviennent▁aussi▁des▁catastrophes▁naturelles▁le▁cri▁de▁jobes▁que▁nous▁avons▁entendus▁dans▁la▁première▁lectures▁retentit▁à▁nos▁oreilles

2026-01-29 17:33:57,460 | INFO | speech length: 319360
2026-01-29 17:33:57,500 | INFO | decoder input length: 498
2026-01-29 17:33:57,500 | INFO | max output length: 498
2026-01-29 17:33:57,500 | INFO | min output length: 49
2026-01-29 17:34:11,481 | INFO | end detected at 132
2026-01-29 17:34:11,484 | INFO | -178.74 * 0.5 = -89.37 for decoder
2026-01-29 17:34:11,484 | INFO | -40.61 * 0.5 = -20.30 for ctc
2026-01-29 17:34:11,484 | INFO | total log probability: -109.67
2026-01-29 17:34:11,484 | INFO | normalized log probability: -0.87
2026-01-29 17:34:11,484 | INFO | total number of ended hypotheses: 173
2026-01-29 17:34:11,486 | INFO | best hypo: ▁vraiment▁dit▁job▁la▁vie▁de▁l'homme▁sur▁terre▁est▁une▁corvée▁il▁fait▁des▁journées▁de▁manoeuvre▁comme▁l'esclave▁qui▁désire▁un▁peu▁d'ombre▁comme▁le▁manoeuvre▁qui▁attend▁sa▁paye▁depuis▁des▁mois▁dit▁jobe▁je▁n'ay▁rien▁réagnée▁sinon▁que▁du▁néant▁je▁ne▁compte▁que▁des▁nuits▁de▁souffrance

2026-01-29 17:34:11,488 | INFO | speech length: 44160
2026-01-29 17:34:11,527 | INFO | decoder input length: 68
2026-01-29 17:34:11,527 | INFO | max output length: 68
2026-01-29 17:34:11,527 | INFO | min output length: 6
2026-01-29 17:34:12,726 | INFO | end detected at 26
2026-01-29 17:34:12,728 | INFO |  -2.06 * 0.5 =  -1.03 for decoder
2026-01-29 17:34:12,728 | INFO |  -1.59 * 0.5 =  -0.79 for ctc
2026-01-29 17:34:12,728 | INFO | total log probability: -1.82
2026-01-29 17:34:12,728 | INFO | normalized log probability: -0.09
2026-01-29 17:34:12,728 | INFO | total number of ended hypotheses: 173
2026-01-29 17:34:12,728 | INFO | best hypo: ▁que▁de▁nuits▁de▁souffrance▁dans▁notre▁humanité

2026-01-29 17:34:12,730 | INFO | speech length: 116320
2026-01-29 17:34:12,766 | INFO | decoder input length: 181
2026-01-29 17:34:12,766 | INFO | max output length: 181
2026-01-29 17:34:12,766 | INFO | min output length: 18
2026-01-29 17:34:16,762 | INFO | end detected at 65
2026-01-29 17:34:16,763 | INFO |  -7.62 * 0.5 =  -3.81 for decoder
2026-01-29 17:34:16,764 | INFO |  -0.11 * 0.5 =  -0.05 for ctc
2026-01-29 17:34:16,764 | INFO | total log probability: -3.86
2026-01-29 17:34:16,764 | INFO | normalized log probability: -0.06
2026-01-29 17:34:16,764 | INFO | total number of ended hypotheses: 155
2026-01-29 17:34:16,765 | INFO | best hypo: ▁et▁beaucoup▁il▁faut▁l'avouer▁beaucoup▁s'éloignent▁de▁la▁foi▁à▁cause▁de▁ce▁mal▁à▁cause▁de▁cette▁souffrance▁sans▁répit

2026-01-29 17:34:16,766 | INFO | speech length: 343520
2026-01-29 17:34:16,802 | INFO | decoder input length: 536
2026-01-29 17:34:16,802 | INFO | max output length: 536
2026-01-29 17:34:16,802 | INFO | min output length: 53
2026-01-29 17:34:35,152 | INFO | end detected at 167
2026-01-29 17:34:35,153 | INFO | -304.64 * 0.5 = -152.32 for decoder
2026-01-29 17:34:35,153 | INFO | -175.87 * 0.5 = -87.94 for ctc
2026-01-29 17:34:35,153 | INFO | total log probability: -240.26
2026-01-29 17:34:35,153 | INFO | normalized log probability: -1.49
2026-01-29 17:34:35,153 | INFO | total number of ended hypotheses: 153
2026-01-29 17:34:35,155 | INFO | best hypo: ▁aussi▁à▁l'approche▁de▁la▁journée▁des▁malades▁voulus▁par▁jean▁paul▁deux▁le▁onze▁février▁on▁la▁fête▁de▁notre▁dame▁de▁lourdes▁en▁ce▁jour▁de▁la▁pastorale▁de▁la▁santé▁la▁puissance▁de▁guérison▁et▁de▁jénus▁et▁le▁don▁de▁la▁vie▁qu'il▁communique▁interroge▁vraiment▁vraiment▁la▁restonsnabilité▁de▁l'évlisie▁c'est▁à▁dire▁la▁restonsbilité▁de▁chacun'entre▁nous

2026-01-29 17:34:35,157 | INFO | speech length: 49600
2026-01-29 17:34:35,193 | INFO | decoder input length: 77
2026-01-29 17:34:35,193 | INFO | max output length: 77
2026-01-29 17:34:35,193 | INFO | min output length: 7
2026-01-29 17:34:36,331 | INFO | end detected at 24
2026-01-29 17:34:36,332 | INFO |  -1.45 * 0.5 =  -0.72 for decoder
2026-01-29 17:34:36,332 | INFO |  -0.03 * 0.5 =  -0.02 for ctc
2026-01-29 17:34:36,332 | INFO | total log probability: -0.74
2026-01-29 17:34:36,333 | INFO | normalized log probability: -0.04
2026-01-29 17:34:36,333 | INFO | total number of ended hypotheses: 140
2026-01-29 17:34:36,333 | INFO | best hypo: ▁pour▁que▁la▁parole▁des▁chrétiens▁soit▁entendue

2026-01-29 17:34:36,334 | INFO | speech length: 186880
2026-01-29 17:34:36,367 | INFO | decoder input length: 291
2026-01-29 17:34:36,367 | INFO | max output length: 291
2026-01-29 17:34:36,367 | INFO | min output length: 29
2026-01-29 17:34:42,020 | INFO | end detected at 72
2026-01-29 17:34:42,021 | INFO | -13.80 * 0.5 =  -6.90 for decoder
2026-01-29 17:34:42,021 | INFO |  -0.70 * 0.5 =  -0.35 for ctc
2026-01-29 17:34:42,021 | INFO | total log probability: -7.25
2026-01-29 17:34:42,021 | INFO | normalized log probability: -0.11
2026-01-29 17:34:42,022 | INFO | total number of ended hypotheses: 149
2026-01-29 17:34:42,023 | INFO | best hypo: ▁comme▁la▁parole▁du▁christ▁qui▁donne▁la▁vie▁pour▁que▁notre▁parole▁soit▁entendue▁pour▁qu'elle▁soit▁comprise▁ne▁faut▁il▁pas▁comme▁pour▁jésus▁que▁d'abord▁nous▁agissions

2026-01-29 17:34:42,028 | INFO | Chunk: 0 | WER=3.225806 | S=0 D=0 I=1
2026-01-29 17:34:42,029 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:34:42,029 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:34:42,029 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:34:42,030 | INFO | Chunk: 4 | WER=3.846154 | S=1 D=0 I=0
2026-01-29 17:34:42,030 | INFO | Chunk: 5 | WER=8.333333 | S=1 D=0 I=0
2026-01-29 17:34:42,030 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:34:42,031 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:34:42,031 | INFO | Chunk: 8 | WER=25.000000 | S=0 D=0 I=1
2026-01-29 17:34:42,031 | INFO | Chunk: 9 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:34:42,032 | INFO | Chunk: 10 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:34:42,032 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:34:42,033 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:34:42,033 | INFO | Chunk: 13 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:34:42,034 | INFO | Chunk: 14 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:34:42,034 | INFO | Chunk: 15 | WER=12.500000 | S=2 D=0 I=0
2026-01-29 17:34:42,034 | INFO | Chunk: 16 | WER=33.333333 | S=1 D=0 I=0
2026-01-29 17:34:42,035 | INFO | Chunk: 17 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:34:42,036 | INFO | Chunk: 18 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:34:42,036 | INFO | Chunk: 19 | WER=20.000000 | S=1 D=0 I=0
2026-01-29 17:34:42,037 | INFO | Chunk: 20 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:34:42,039 | INFO | Chunk: 21 | WER=9.836066 | S=5 D=1 I=0
2026-01-29 17:34:42,039 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:34:42,039 | INFO | Chunk: 23 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:34:42,040 | INFO | Chunk: 24 | WER=6.250000 | S=0 D=0 I=1
2026-01-29 17:34:42,040 | INFO | Chunk: 25 | WER=10.000000 | S=0 D=0 I=1
2026-01-29 17:34:42,040 | INFO | Chunk: 26 | WER=50.000000 | S=1 D=0 I=0
2026-01-29 17:34:42,040 | INFO | Chunk: 27 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:34:42,041 | INFO | Chunk: 28 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:34:42,042 | INFO | Chunk: 29 | WER=15.000000 | S=4 D=1 I=1
2026-01-29 17:34:42,043 | INFO | Chunk: 30 | WER=6.896552 | S=3 D=1 I=0
2026-01-29 17:34:42,044 | INFO | Chunk: 31 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:34:42,044 | INFO | Chunk: 32 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:34:42,047 | INFO | Chunk: 33 | WER=13.432836 | S=6 D=1 I=2
2026-01-29 17:34:42,047 | INFO | Chunk: 34 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:34:42,048 | INFO | Chunk: 35 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:34:42,269 | INFO | File: Rhap-M2003.wav | WER=5.110497 | S=25 D=4 I=8
2026-01-29 17:34:42,269 | INFO | ------------------------------
2026-01-29 17:34:42,269 | INFO | Conf cv Done!
2026-01-29 17:34:42,408 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-29 17:34:42,428 | INFO | Vocabulary size: 47
2026-01-29 17:34:42,949 | INFO | Gradient checkpoint layers: []
2026-01-29 17:34:43,597 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 17:34:43,600 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 17:34:43,601 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 17:34:43,601 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-29 17:34:43,603 | INFO | speech length: 216960
2026-01-29 17:34:43,639 | INFO | decoder input length: 338
2026-01-29 17:34:43,639 | INFO | max output length: 338
2026-01-29 17:34:43,639 | INFO | min output length: 33
2026-01-29 17:34:56,825 | INFO | end detected at 181
2026-01-29 17:34:56,826 | INFO | -14.69 * 0.5 =  -7.34 for decoder
2026-01-29 17:34:56,826 | INFO |  -0.18 * 0.5 =  -0.09 for ctc
2026-01-29 17:34:56,826 | INFO | total log probability: -7.43
2026-01-29 17:34:56,826 | INFO | normalized log probability: -0.04
2026-01-29 17:34:56,826 | INFO | total number of ended hypotheses: 137
2026-01-29 17:34:56,829 | INFO | best hypo: c'est<space>la<space>première<space>journée<space>de<space>jésus<space>que<space>nous<space>venons<space>d'entendre<space>la<space>première<space>journée<space>où<space>il<space>manifeste<space>son<space>autorité<space>et<space>la<space>nouveauté<space>la<space>bonne<space>nouvelle<space>l'évangile<space>qui<space>arrive<space>avec<space>lui

2026-01-29 17:34:56,831 | INFO | speech length: 107360
2026-01-29 17:34:56,865 | INFO | decoder input length: 167
2026-01-29 17:34:56,865 | INFO | max output length: 167
2026-01-29 17:34:56,865 | INFO | min output length: 16
2026-01-29 17:35:02,020 | INFO | end detected at 97
2026-01-29 17:35:02,021 | INFO |  -7.37 * 0.5 =  -3.69 for decoder
2026-01-29 17:35:02,021 | INFO |  -0.39 * 0.5 =  -0.20 for ctc
2026-01-29 17:35:02,021 | INFO | total log probability: -3.88
2026-01-29 17:35:02,021 | INFO | normalized log probability: -0.04
2026-01-29 17:35:02,021 | INFO | total number of ended hypotheses: 162
2026-01-29 17:35:02,023 | INFO | best hypo: et<space>vous<space>l'avez<space>remarqué<space>en<space>écoutant<space>l'évangile<space>jésus<space>ne<space>dit<space>rien<space>avant<space>la<space>fin<space>de<space>l'épisode

2026-01-29 17:35:02,024 | INFO | speech length: 15840
2026-01-29 17:35:02,056 | INFO | decoder input length: 24
2026-01-29 17:35:02,056 | INFO | max output length: 24
2026-01-29 17:35:02,056 | INFO | min output length: 2
2026-01-29 17:35:02,593 | INFO | end detected at 14
2026-01-29 17:35:02,594 | INFO |  -0.67 * 0.5 =  -0.33 for decoder
2026-01-29 17:35:02,594 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-29 17:35:02,594 | INFO | total log probability: -0.34
2026-01-29 17:35:02,594 | INFO | normalized log probability: -0.04
2026-01-29 17:35:02,594 | INFO | total number of ended hypotheses: 164
2026-01-29 17:35:02,594 | INFO | best hypo: il<space>agit

2026-01-29 17:35:02,596 | INFO | speech length: 85120
2026-01-29 17:35:02,628 | INFO | decoder input length: 132
2026-01-29 17:35:02,628 | INFO | max output length: 132
2026-01-29 17:35:02,628 | INFO | min output length: 13
2026-01-29 17:35:06,274 | INFO | end detected at 75
2026-01-29 17:35:06,276 | INFO |  -7.17 * 0.5 =  -3.59 for decoder
2026-01-29 17:35:06,276 | INFO |  -2.96 * 0.5 =  -1.48 for ctc
2026-01-29 17:35:06,276 | INFO | total log probability: -5.07
2026-01-29 17:35:06,276 | INFO | normalized log probability: -0.07
2026-01-29 17:35:06,276 | INFO | total number of ended hypotheses: 188
2026-01-29 17:35:06,277 | INFO | best hypo: il<space>fait<space>d'abord<space>lever<space>la<space>belle<space>mère<space>de<space>pierre<space>et<space>sa<space>fièvre<space>l'acquit

2026-01-29 17:35:06,279 | INFO | speech length: 181760
2026-01-29 17:35:06,312 | INFO | decoder input length: 283
2026-01-29 17:35:06,312 | INFO | max output length: 283
2026-01-29 17:35:06,312 | INFO | min output length: 28
2026-01-29 17:35:16,449 | INFO | end detected at 152
2026-01-29 17:35:16,452 | INFO | -12.10 * 0.5 =  -6.05 for decoder
2026-01-29 17:35:16,452 | INFO |  -3.51 * 0.5 =  -1.76 for ctc
2026-01-29 17:35:16,452 | INFO | total log probability: -7.81
2026-01-29 17:35:16,452 | INFO | normalized log probability: -0.05
2026-01-29 17:35:16,452 | INFO | total number of ended hypotheses: 176
2026-01-29 17:35:16,454 | INFO | best hypo: il<space>guérit<space>ensuite<space>toutes<space>sortes<space>de<space>malades<space>comme<space>plus<space>tard<space>dans<space>l'evangile<space>il<space>acceptera<space>de<space>se<space>laisser<space>toucher<space>ou<space>de<space>bainir<space>en<space>imposant<space>les<space>mains

2026-01-29 17:35:16,456 | INFO | speech length: 57120
2026-01-29 17:35:16,489 | INFO | decoder input length: 88
2026-01-29 17:35:16,489 | INFO | max output length: 88
2026-01-29 17:35:16,489 | INFO | min output length: 8
2026-01-29 17:35:19,108 | INFO | end detected at 62
2026-01-29 17:35:19,109 | INFO |  -4.90 * 0.5 =  -2.45 for decoder
2026-01-29 17:35:19,109 | INFO |  -0.28 * 0.5 =  -0.14 for ctc
2026-01-29 17:35:19,109 | INFO | total log probability: -2.59
2026-01-29 17:35:19,109 | INFO | normalized log probability: -0.05
2026-01-29 17:35:19,109 | INFO | total number of ended hypotheses: 159
2026-01-29 17:35:19,110 | INFO | best hypo: il<space>chasse<space>beaucoup<space>d'esprit<space>ce<space>qui<space>fait<space>du<space>mal<space>aux<space>gens

2026-01-29 17:35:19,112 | INFO | speech length: 19040
2026-01-29 17:35:19,144 | INFO | decoder input length: 29
2026-01-29 17:35:19,144 | INFO | max output length: 29
2026-01-29 17:35:19,144 | INFO | min output length: 2
2026-01-29 17:35:19,971 | INFO | end detected at 21
2026-01-29 17:35:19,972 | INFO |  -1.29 * 0.5 =  -0.65 for decoder
2026-01-29 17:35:19,972 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:35:19,972 | INFO | total log probability: -0.65
2026-01-29 17:35:19,972 | INFO | normalized log probability: -0.04
2026-01-29 17:35:19,972 | INFO | total number of ended hypotheses: 137
2026-01-29 17:35:19,972 | INFO | best hypo: avant<space>de<space>parler

2026-01-29 17:35:19,974 | INFO | speech length: 58080
2026-01-29 17:35:20,005 | INFO | decoder input length: 90
2026-01-29 17:35:20,005 | INFO | max output length: 90
2026-01-29 17:35:20,005 | INFO | min output length: 9
2026-01-29 17:35:22,455 | INFO | end detected at 56
2026-01-29 17:35:22,456 | INFO |  -4.02 * 0.5 =  -2.01 for decoder
2026-01-29 17:35:22,456 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-29 17:35:22,456 | INFO | total log probability: -2.02
2026-01-29 17:35:22,456 | INFO | normalized log probability: -0.04
2026-01-29 17:35:22,456 | INFO | total number of ended hypotheses: 167
2026-01-29 17:35:22,457 | INFO | best hypo: avant<space>toute<space>parole<space>la<space>vie<space>de<space>jésus<space>est<space>une<space>action

2026-01-29 17:35:22,459 | INFO | speech length: 28000
2026-01-29 17:35:22,490 | INFO | decoder input length: 43
2026-01-29 17:35:22,490 | INFO | max output length: 43
2026-01-29 17:35:22,490 | INFO | min output length: 4
2026-01-29 17:35:23,675 | INFO | end detected at 31
2026-01-29 17:35:23,677 | INFO |  -2.02 * 0.5 =  -1.01 for decoder
2026-01-29 17:35:23,677 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:35:23,677 | INFO | total log probability: -1.01
2026-01-29 17:35:23,677 | INFO | normalized log probability: -0.04
2026-01-29 17:35:23,677 | INFO | total number of ended hypotheses: 168
2026-01-29 17:35:23,678 | INFO | best hypo: est<space>une<space>action<space>salutaire

2026-01-29 17:35:23,680 | INFO | speech length: 83680
2026-01-29 17:35:23,713 | INFO | decoder input length: 130
2026-01-29 17:35:23,713 | INFO | max output length: 130
2026-01-29 17:35:23,713 | INFO | min output length: 13
2026-01-29 17:35:27,760 | INFO | end detected at 79
2026-01-29 17:35:27,762 | INFO |  -6.45 * 0.5 =  -3.23 for decoder
2026-01-29 17:35:27,762 | INFO |  -1.04 * 0.5 =  -0.52 for ctc
2026-01-29 17:35:27,762 | INFO | total log probability: -3.74
2026-01-29 17:35:27,762 | INFO | normalized log probability: -0.05
2026-01-29 17:35:27,763 | INFO | total number of ended hypotheses: 184
2026-01-29 17:35:27,764 | INFO | best hypo: tout<space>chez<space>lui<space>est<space>fait<space>de<space>compassion<space>et<space>non<space>de<space>violence<space>ou<space>de<space>châtiment

2026-01-29 17:35:27,766 | INFO | speech length: 93600
2026-01-29 17:35:27,799 | INFO | decoder input length: 145
2026-01-29 17:35:27,799 | INFO | max output length: 145
2026-01-29 17:35:27,799 | INFO | min output length: 14
2026-01-29 17:35:31,913 | INFO | end detected at 82
2026-01-29 17:35:31,915 | INFO |  -7.78 * 0.5 =  -3.89 for decoder
2026-01-29 17:35:31,915 | INFO |  -4.02 * 0.5 =  -2.01 for ctc
2026-01-29 17:35:31,915 | INFO | total log probability: -5.90
2026-01-29 17:35:31,915 | INFO | normalized log probability: -0.08
2026-01-29 17:35:31,915 | INFO | total number of ended hypotheses: 194
2026-01-29 17:35:31,916 | INFO | best hypo: en<space>fait<space>tous<space>l'évangile<space>nous<space>montrent<space>un<space>jésus<space>qui<space>remet<space>l'humanité<space>debout

2026-01-29 17:35:31,918 | INFO | speech length: 104480
2026-01-29 17:35:31,952 | INFO | decoder input length: 162
2026-01-29 17:35:31,952 | INFO | max output length: 162
2026-01-29 17:35:31,952 | INFO | min output length: 16
2026-01-29 17:35:37,785 | INFO | end detected at 115
2026-01-29 17:35:37,786 | INFO |  -8.73 * 0.5 =  -4.37 for decoder
2026-01-29 17:35:37,786 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-29 17:35:37,786 | INFO | total log probability: -4.37
2026-01-29 17:35:37,786 | INFO | normalized log probability: -0.04
2026-01-29 17:35:37,786 | INFO | total number of ended hypotheses: 150
2026-01-29 17:35:37,788 | INFO | best hypo: et<space>aujourd'hui<space>dans<space>sa<space>première<space>journée<space>il<space>fait<space>de<space>la<space>belle<space>mère<space>de<space>pierre<space>une<space>des<space>toutes<space>premières<space>disciples

2026-01-29 17:35:37,790 | INFO | speech length: 132320
2026-01-29 17:35:37,823 | INFO | decoder input length: 206
2026-01-29 17:35:37,823 | INFO | max output length: 206
2026-01-29 17:35:37,823 | INFO | min output length: 20
2026-01-29 17:35:45,185 | INFO | end detected at 130
2026-01-29 17:35:45,186 | INFO | -10.25 * 0.5 =  -5.13 for decoder
2026-01-29 17:35:45,186 | INFO |  -0.33 * 0.5 =  -0.17 for ctc
2026-01-29 17:35:45,186 | INFO | total log probability: -5.29
2026-01-29 17:35:45,186 | INFO | normalized log probability: -0.04
2026-01-29 17:35:45,186 | INFO | total number of ended hypotheses: 141
2026-01-29 17:35:45,188 | INFO | best hypo: l'évangile<space>la<space>bonne<space>nouvelle<space>comme<space>le<space>répète<space>la<space>pétropole<space>la<space>bonne<space>nouvelle<space>en<space>la<space>personne<space>de<space>jésus<space>c'est<space>d'abord<space>une<space>action

2026-01-29 17:35:45,190 | INFO | speech length: 153760
2026-01-29 17:35:45,223 | INFO | decoder input length: 239
2026-01-29 17:35:45,223 | INFO | max output length: 239
2026-01-29 17:35:45,223 | INFO | min output length: 23
2026-01-29 17:35:53,495 | INFO | end detected at 136
2026-01-29 17:35:53,497 | INFO | -10.38 * 0.5 =  -5.19 for decoder
2026-01-29 17:35:53,497 | INFO |  -0.54 * 0.5 =  -0.27 for ctc
2026-01-29 17:35:53,497 | INFO | total log probability: -5.46
2026-01-29 17:35:53,497 | INFO | normalized log probability: -0.04
2026-01-29 17:35:53,497 | INFO | total number of ended hypotheses: 164
2026-01-29 17:35:53,499 | INFO | best hypo: une<space>action<space>pour<space>restaurer<space>la<space>dignité<space>de<space>la<space>personne<space>humaine<space>une<space>action<space>pour<space>rétablir<space>la<space>personne<space>en<space>sa<space>qualité<space>de<space>sujet<space>de<space>parole

2026-01-29 17:35:53,501 | INFO | speech length: 99360
2026-01-29 17:35:53,534 | INFO | decoder input length: 154
2026-01-29 17:35:53,534 | INFO | max output length: 154
2026-01-29 17:35:53,534 | INFO | min output length: 15
2026-01-29 17:35:58,756 | INFO | end detected at 104
2026-01-29 17:35:58,757 | INFO |  -7.84 * 0.5 =  -3.92 for decoder
2026-01-29 17:35:58,758 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:35:58,758 | INFO | total log probability: -3.92
2026-01-29 17:35:58,758 | INFO | normalized log probability: -0.04
2026-01-29 17:35:58,758 | INFO | total number of ended hypotheses: 175
2026-01-29 17:35:58,759 | INFO | best hypo: une<space>action<space>pour<space>rétablir<space>les<space>uns<space>et<space>les<space>autres<space>dans<space>les<space>relations<space>avec<space>les<space>uns<space>et<space>avec<space>les<space>autres

2026-01-29 17:35:58,761 | INFO | speech length: 88160
2026-01-29 17:35:58,794 | INFO | decoder input length: 137
2026-01-29 17:35:58,794 | INFO | max output length: 137
2026-01-29 17:35:58,794 | INFO | min output length: 13
2026-01-29 17:36:02,695 | INFO | end detected at 80
2026-01-29 17:36:02,697 | INFO |  -6.20 * 0.5 =  -3.10 for decoder
2026-01-29 17:36:02,697 | INFO |  -0.34 * 0.5 =  -0.17 for ctc
2026-01-29 17:36:02,697 | INFO | total log probability: -3.27
2026-01-29 17:36:02,697 | INFO | normalized log probability: -0.04
2026-01-29 17:36:02,697 | INFO | total number of ended hypotheses: 165
2026-01-29 17:36:02,698 | INFO | best hypo: jésus<space>ne<space>pose<space>même<space>pas<space>ces<space>gestes<space>de<space>bonté<space>comme<space>des<space>appels<space>à<space>la<space>fois<space>non

2026-01-29 17:36:02,700 | INFO | speech length: 20800
2026-01-29 17:36:02,732 | INFO | decoder input length: 32
2026-01-29 17:36:02,732 | INFO | max output length: 32
2026-01-29 17:36:02,732 | INFO | min output length: 3
2026-01-29 17:36:03,455 | INFO | end detected at 19
2026-01-29 17:36:03,457 | INFO |  -1.14 * 0.5 =  -0.57 for decoder
2026-01-29 17:36:03,457 | INFO |  -0.03 * 0.5 =  -0.02 for ctc
2026-01-29 17:36:03,457 | INFO | total log probability: -0.59
2026-01-29 17:36:03,457 | INFO | normalized log probability: -0.04
2026-01-29 17:36:03,458 | INFO | total number of ended hypotheses: 135
2026-01-29 17:36:03,458 | INFO | best hypo: en<space>ces<space>gestes

2026-01-29 17:36:03,459 | INFO | speech length: 214560
2026-01-29 17:36:03,492 | INFO | decoder input length: 334
2026-01-29 17:36:03,492 | INFO | max output length: 334
2026-01-29 17:36:03,492 | INFO | min output length: 33
2026-01-29 17:36:16,353 | INFO | end detected at 176
2026-01-29 17:36:16,354 | INFO | -14.48 * 0.5 =  -7.24 for decoder
2026-01-29 17:36:16,355 | INFO |  -1.93 * 0.5 =  -0.97 for ctc
2026-01-29 17:36:16,355 | INFO | total log probability: -8.21
2026-01-29 17:36:16,355 | INFO | normalized log probability: -0.05
2026-01-29 17:36:16,355 | INFO | total number of ended hypotheses: 159
2026-01-29 17:36:16,357 | INFO | best hypo: dans<space>ce<space>premier<space>jour<space>il<space>inaugure<space>en<space>sa<space>personne<space>la<space>venue<space>de<space>dieu<space>parmi<space>les<space>hommes<space>la<space>venue<space>définitive<space>de<space>dieu<space>pour<space>rétablir<space>l'humanité<space>dans<space>sa<space>vocation<space>et<space>dans<space>sa<space>dignité

2026-01-29 17:36:16,359 | INFO | speech length: 186880
2026-01-29 17:36:16,390 | INFO | decoder input length: 291
2026-01-29 17:36:16,390 | INFO | max output length: 291
2026-01-29 17:36:16,391 | INFO | min output length: 29
2026-01-29 17:36:29,344 | INFO | end detected at 195
2026-01-29 17:36:29,346 | INFO | -15.15 * 0.5 =  -7.58 for decoder
2026-01-29 17:36:29,346 | INFO |  -0.06 * 0.5 =  -0.03 for ctc
2026-01-29 17:36:29,346 | INFO | total log probability: -7.61
2026-01-29 17:36:29,346 | INFO | normalized log probability: -0.04
2026-01-29 17:36:29,346 | INFO | total number of ended hypotheses: 178
2026-01-29 17:36:29,349 | INFO | best hypo: son<space>message<space>initial<space>au<space>tout<space>début<space>de<space>l'évangile<space>convertissez<space>vous<space>le<space>royaume<space>de<space>dieu<space>est<space>proche<space>de<space>vous<space>son<space>message<space>s'inscrit<space>aujourd'hui<space>dans<space>sa<space>personne<space>il<space>trouve<space>sa<space>traduction<space>en<space>action

2026-01-29 17:36:29,351 | INFO | speech length: 28000
2026-01-29 17:36:29,384 | INFO | decoder input length: 43
2026-01-29 17:36:29,384 | INFO | max output length: 43
2026-01-29 17:36:29,384 | INFO | min output length: 4
2026-01-29 17:36:30,697 | INFO | end detected at 35
2026-01-29 17:36:30,698 | INFO |  -2.67 * 0.5 =  -1.33 for decoder
2026-01-29 17:36:30,698 | INFO |  -0.15 * 0.5 =  -0.07 for ctc
2026-01-29 17:36:30,698 | INFO | total log probability: -1.41
2026-01-29 17:36:30,699 | INFO | normalized log probability: -0.05
2026-01-29 17:36:30,699 | INFO | total number of ended hypotheses: 144
2026-01-29 17:36:30,699 | INFO | best hypo: dans<space>ces<space>gestes<space>de<space>libération

2026-01-29 17:36:30,701 | INFO | speech length: 202400
2026-01-29 17:36:30,734 | INFO | decoder input length: 315
2026-01-29 17:36:30,734 | INFO | max output length: 315
2026-01-29 17:36:30,734 | INFO | min output length: 31
2026-01-29 17:36:43,622 | INFO | end detected at 186
2026-01-29 17:36:43,625 | INFO | -16.52 * 0.5 =  -8.26 for decoder
2026-01-29 17:36:43,625 | INFO |  -2.52 * 0.5 =  -1.26 for ctc
2026-01-29 17:36:43,625 | INFO | total log probability: -9.52
2026-01-29 17:36:43,625 | INFO | normalized log probability: -0.05
2026-01-29 17:36:43,625 | INFO | total number of ended hypotheses: 188
2026-01-29 17:36:43,628 | INFO | best hypo: plus<space>encore<space>dans<space>l'évangile<space>d'aujourd'hui<space>jésus<space>s'accorde<space>le<space>pouvoir<space>de<space>guérir<space>et<space>de<space>chasser<space>les<space>esprits<space>mauvais<space>il<space>n'en<space>appelle<space>plus<space>comme<space>on<space>le<space>faisait<space>en<space>son<space>temps<space>à<space>sa<space>loumont

2026-01-29 17:36:43,630 | INFO | speech length: 346560
2026-01-29 17:36:43,664 | INFO | decoder input length: 541
2026-01-29 17:36:43,664 | INFO | max output length: 541
2026-01-29 17:36:43,664 | INFO | min output length: 54
2026-01-29 17:37:13,166 | INFO | end detected at 307
2026-01-29 17:37:13,168 | INFO | -122.97 * 0.5 = -61.48 for decoder
2026-01-29 17:37:13,168 | INFO |  -9.31 * 0.5 =  -4.65 for ctc
2026-01-29 17:37:13,168 | INFO | total log probability: -66.14
2026-01-29 17:37:13,168 | INFO | normalized log probability: -0.22
2026-01-29 17:37:13,168 | INFO | total number of ended hypotheses: 196
2026-01-29 17:37:13,173 | INFO | best hypo: comme<space>le<space>faisait<space>des<space>guérisseurs<space>de<space>son<space>temps<space>il<space>guérit<space>de<space>son<space>propre<space>chef<space>les<space>malades<space>c'est<space>sa<space>propre<space>vie<space>qu'il<space>communique<space>à<space>ceux<space>qu'il<space>rencontre<space>c'est<space>sa<space>propre<space>autorité<space>qu'il<space>engage<space>l'autorité<space>de<space>sa<space>vie<space>je<space>le<space>veux<space>soit<space>purifiée<space>en<space>tentant<space>dans<space>l'evangile<space>ou<space>encore<space>fillette<space>je<space>te<space>le<space>dis<space>laf<space>tois

2026-01-29 17:37:13,175 | INFO | speech length: 42880
2026-01-29 17:37:13,208 | INFO | decoder input length: 66
2026-01-29 17:37:13,208 | INFO | max output length: 66
2026-01-29 17:37:13,208 | INFO | min output length: 6
2026-01-29 17:37:14,863 | INFO | end detected at 41
2026-01-29 17:37:14,865 | INFO |  -2.84 * 0.5 =  -1.42 for decoder
2026-01-29 17:37:14,865 | INFO |  -0.03 * 0.5 =  -0.01 for ctc
2026-01-29 17:37:14,865 | INFO | total log probability: -1.43
2026-01-29 17:37:14,865 | INFO | normalized log probability: -0.04
2026-01-29 17:37:14,865 | INFO | total number of ended hypotheses: 163
2026-01-29 17:37:14,866 | INFO | best hypo: ou<space>encore<space>moi<space>moi<space>je<space>te<space>leur<space>donne

2026-01-29 17:37:14,867 | INFO | speech length: 40640
2026-01-29 17:37:14,899 | INFO | decoder input length: 63
2026-01-29 17:37:14,899 | INFO | max output length: 63
2026-01-29 17:37:14,899 | INFO | min output length: 6
2026-01-29 17:37:16,474 | INFO | end detected at 39
2026-01-29 17:37:16,475 | INFO |  -2.74 * 0.5 =  -1.37 for decoder
2026-01-29 17:37:16,475 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:37:16,475 | INFO | total log probability: -1.37
2026-01-29 17:37:16,475 | INFO | normalized log probability: -0.04
2026-01-29 17:37:16,475 | INFO | total number of ended hypotheses: 142
2026-01-29 17:37:16,476 | INFO | best hypo: c'est<space>désormais<space>en<space>son<space>propre<space>nom

2026-01-29 17:37:16,477 | INFO | speech length: 101120
2026-01-29 17:37:16,510 | INFO | decoder input length: 157
2026-01-29 17:37:16,510 | INFO | max output length: 157
2026-01-29 17:37:16,510 | INFO | min output length: 15
2026-01-29 17:37:21,795 | INFO | end detected at 104
2026-01-29 17:37:21,798 | INFO | -10.43 * 0.5 =  -5.21 for decoder
2026-01-29 17:37:21,798 | INFO | -10.65 * 0.5 =  -5.32 for ctc
2026-01-29 17:37:21,798 | INFO | total log probability: -10.54
2026-01-29 17:37:21,798 | INFO | normalized log probability: -0.11
2026-01-29 17:37:21,798 | INFO | total number of ended hypotheses: 216
2026-01-29 17:37:21,800 | INFO | best hypo: que<space>jésus<space>chasse<space>les<space>démons<space>il<space>enseigne<space>avec<space>l'autorité<space>pas<space>comme<space>les<space>parisiens<space>et<space>l'escrit<space>ben

2026-01-29 17:37:21,802 | INFO | speech length: 64960
2026-01-29 17:37:21,834 | INFO | decoder input length: 101
2026-01-29 17:37:21,834 | INFO | max output length: 101
2026-01-29 17:37:21,834 | INFO | min output length: 10
2026-01-29 17:37:24,477 | INFO | end detected at 59
2026-01-29 17:37:24,479 | INFO |  -5.25 * 0.5 =  -2.63 for decoder
2026-01-29 17:37:24,479 | INFO |  -0.52 * 0.5 =  -0.26 for ctc
2026-01-29 17:37:24,479 | INFO | total log probability: -2.89
2026-01-29 17:37:24,479 | INFO | normalized log probability: -0.05
2026-01-29 17:37:24,479 | INFO | total number of ended hypotheses: 173
2026-01-29 17:37:24,480 | INFO | best hypo: il<space>guérit<space>en<space>donnant<space>la<space>vie<space>en<space>donnant<space>sa<space>propre<space>vue

2026-01-29 17:37:24,481 | INFO | speech length: 12960
2026-01-29 17:37:24,512 | INFO | decoder input length: 19
2026-01-29 17:37:24,512 | INFO | max output length: 19
2026-01-29 17:37:24,512 | INFO | min output length: 1
2026-01-29 17:37:25,261 | INFO | end detected at 16
2026-01-29 17:37:25,262 | INFO |  -0.88 * 0.5 =  -0.44 for decoder
2026-01-29 17:37:25,262 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:37:25,262 | INFO | total log probability: -0.44
2026-01-29 17:37:25,262 | INFO | normalized log probability: -0.04
2026-01-29 17:37:25,262 | INFO | total number of ended hypotheses: 138
2026-01-29 17:37:25,262 | INFO | best hypo: mais<space>voilà

2026-01-29 17:37:25,264 | INFO | speech length: 71520
2026-01-29 17:37:25,295 | INFO | decoder input length: 111
2026-01-29 17:37:25,295 | INFO | max output length: 111
2026-01-29 17:37:25,295 | INFO | min output length: 11
2026-01-29 17:37:28,790 | INFO | end detected at 77
2026-01-29 17:37:28,791 | INFO |  -5.69 * 0.5 =  -2.85 for decoder
2026-01-29 17:37:28,791 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-29 17:37:28,791 | INFO | total log probability: -2.85
2026-01-29 17:37:28,791 | INFO | normalized log probability: -0.04
2026-01-29 17:37:28,791 | INFO | total number of ended hypotheses: 173
2026-01-29 17:37:28,793 | INFO | best hypo: le<space>mal<space>est<space>toujours<space>présent<space>au<space>milieu<space>de<space>nous<space>depuis<space>le<space>temps<space>de<space>jésus

2026-01-29 17:37:28,794 | INFO | speech length: 29120
2026-01-29 17:37:28,826 | INFO | decoder input length: 45
2026-01-29 17:37:28,826 | INFO | max output length: 45
2026-01-29 17:37:28,826 | INFO | min output length: 4
2026-01-29 17:37:30,193 | INFO | end detected at 36
2026-01-29 17:37:30,195 | INFO |  -2.41 * 0.5 =  -1.20 for decoder
2026-01-29 17:37:30,195 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:37:30,195 | INFO | total log probability: -1.20
2026-01-29 17:37:30,195 | INFO | normalized log probability: -0.04
2026-01-29 17:37:30,195 | INFO | total number of ended hypotheses: 174
2026-01-29 17:37:30,196 | INFO | best hypo: il<space>y<space>a<space>le<space>mal<space>dans<space>l'humanité

2026-01-29 17:37:30,197 | INFO | speech length: 225440
2026-01-29 17:37:30,229 | INFO | decoder input length: 351
2026-01-29 17:37:30,229 | INFO | max output length: 351
2026-01-29 17:37:30,229 | INFO | min output length: 35
2026-01-29 17:37:46,319 | INFO | end detected at 224
2026-01-29 17:37:46,322 | INFO | -18.21 * 0.5 =  -9.10 for decoder
2026-01-29 17:37:46,322 | INFO |  -6.49 * 0.5 =  -3.24 for ctc
2026-01-29 17:37:46,322 | INFO | total log probability: -12.35
2026-01-29 17:37:46,322 | INFO | normalized log probability: -0.06
2026-01-29 17:37:46,322 | INFO | total number of ended hypotheses: 179
2026-01-29 17:37:46,325 | INFO | best hypo: le<space>mal<space>moral<space>celui<space>que<space>l'on<space>fait<space>en<space>faisant<space>du<space>mal<space>aux<space>autres<space>le<space>mal<space>physique<space>en<space>survienne<space>aussi<space>des<space>catastrophes<space>naturelles<space>et<space>le<space>cri<space>de<space>job<space>que<space>nous<space>avons<space>entendu<space>dans<space>la<space>première<space>lecture<space>retenti<space>à<space>nos<space>oreilles

2026-01-29 17:37:46,327 | INFO | speech length: 319360
2026-01-29 17:37:46,361 | INFO | decoder input length: 498
2026-01-29 17:37:46,361 | INFO | max output length: 498
2026-01-29 17:37:46,361 | INFO | min output length: 49
2026-01-29 17:38:12,477 | INFO | end detected at 287
2026-01-29 17:38:12,480 | INFO | -34.04 * 0.5 = -17.02 for decoder
2026-01-29 17:38:12,480 | INFO |  -5.32 * 0.5 =  -2.66 for ctc
2026-01-29 17:38:12,480 | INFO | total log probability: -19.68
2026-01-29 17:38:12,480 | INFO | normalized log probability: -0.07
2026-01-29 17:38:12,480 | INFO | total number of ended hypotheses: 215
2026-01-29 17:38:12,484 | INFO | best hypo: vraiment<space>dit<space>job<space>la<space>vie<space>de<space>l'homme<space>sur<space>terre<space>est<space>une<space>corvée<space>il<space>fait<space>des<space>journées<space>de<space>manoeuvre<space>comme<space>l'esclave<space>qui<space>désire<space>un<space>peu<space>d'ombre<space>comme<space>le<space>manoeuvre<space>qui<space>attend<space>sa<space>paye<space>depuis<space>des<space>mois<space>dit<space>job<space>je<space>n'ai<space>rien<space>gagné<space>sinon<space>que<space>du<space>néant<space>je<space>ne<space>compte<space>que<space>des<space>nuits<space>de<space>souffrance

2026-01-29 17:38:12,487 | INFO | speech length: 44160
2026-01-29 17:38:12,521 | INFO | decoder input length: 68
2026-01-29 17:38:12,521 | INFO | max output length: 68
2026-01-29 17:38:12,521 | INFO | min output length: 6
2026-01-29 17:38:14,550 | INFO | end detected at 51
2026-01-29 17:38:14,551 | INFO |  -3.83 * 0.5 =  -1.92 for decoder
2026-01-29 17:38:14,551 | INFO |  -0.79 * 0.5 =  -0.40 for ctc
2026-01-29 17:38:14,551 | INFO | total log probability: -2.31
2026-01-29 17:38:14,551 | INFO | normalized log probability: -0.05
2026-01-29 17:38:14,551 | INFO | total number of ended hypotheses: 151
2026-01-29 17:38:14,552 | INFO | best hypo: que<space>de<space>nuit<space>de<space>souffrance<space>dans<space>notre<space>humanité

2026-01-29 17:38:14,553 | INFO | speech length: 116320
2026-01-29 17:38:14,585 | INFO | decoder input length: 181
2026-01-29 17:38:14,585 | INFO | max output length: 181
2026-01-29 17:38:14,585 | INFO | min output length: 18
2026-01-29 17:38:21,094 | INFO | end detected at 123
2026-01-29 17:38:21,095 | INFO |  -9.41 * 0.5 =  -4.70 for decoder
2026-01-29 17:38:21,095 | INFO |  -0.80 * 0.5 =  -0.40 for ctc
2026-01-29 17:38:21,095 | INFO | total log probability: -5.10
2026-01-29 17:38:21,095 | INFO | normalized log probability: -0.04
2026-01-29 17:38:21,095 | INFO | total number of ended hypotheses: 175
2026-01-29 17:38:21,097 | INFO | best hypo: et<space>beaucoup<space>il<space>faut<space>l'avouer<space>beaucoup<space>s'éloignent<space>de<space>la<space>foi<space>à<space>cause<space>de<space>ce<space>mal<space>à<space>cause<space>de<space>cette<space>souffrance<space>sans<space>répit

2026-01-29 17:38:21,099 | INFO | speech length: 343520
2026-01-29 17:38:21,133 | INFO | decoder input length: 536
2026-01-29 17:38:21,133 | INFO | max output length: 536
2026-01-29 17:38:21,133 | INFO | min output length: 53
2026-01-29 17:38:53,495 | INFO | end detected at 347
2026-01-29 17:38:53,497 | INFO | -42.22 * 0.5 = -21.11 for decoder
2026-01-29 17:38:53,497 | INFO |  -6.50 * 0.5 =  -3.25 for ctc
2026-01-29 17:38:53,497 | INFO | total log probability: -24.36
2026-01-29 17:38:53,497 | INFO | normalized log probability: -0.07
2026-01-29 17:38:53,497 | INFO | total number of ended hypotheses: 186
2026-01-29 17:38:53,502 | INFO | best hypo: aussi<space>à<space>l'approche<space>de<space>la<space>journée<space>des<space>malades<space>voulus<space>par<space>jean<space>paul<space>ii<space>le<space>onze<space>février<space>en<space>la<space>fête<space>de<space>notre<space>dame<space>de<space>lourdes<space>en<space>ce<space>jour<space>de<space>la<space>pastorale<space>de<space>la<space>santé<space>la<space>puissance<space>de<space>guérison<space>de<space>jésus<space>le<space>don<space>de<space>la<space>vie<space>qu'il<space>communique<space>interroge<space>vraiment<space>vraiment<space>la<space>responsabilité<space>de<space>l'église<space>c'est<space>à<space>dire<space>la<space>responsabilité<space>de<space>chacun<space>d'entre<space>nous

2026-01-29 17:38:53,504 | INFO | speech length: 49600
2026-01-29 17:38:53,537 | INFO | decoder input length: 77
2026-01-29 17:38:53,537 | INFO | max output length: 77
2026-01-29 17:38:53,537 | INFO | min output length: 7
2026-01-29 17:38:55,711 | INFO | end detected at 53
2026-01-29 17:38:55,712 | INFO |  -3.79 * 0.5 =  -1.90 for decoder
2026-01-29 17:38:55,712 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-29 17:38:55,712 | INFO | total log probability: -1.90
2026-01-29 17:38:55,712 | INFO | normalized log probability: -0.04
2026-01-29 17:38:55,712 | INFO | total number of ended hypotheses: 167
2026-01-29 17:38:55,713 | INFO | best hypo: pour<space>que<space>la<space>parole<space>des<space>chrétiens<space>soit<space>entendue

2026-01-29 17:38:55,715 | INFO | speech length: 186880
2026-01-29 17:38:55,747 | INFO | decoder input length: 291
2026-01-29 17:38:55,748 | INFO | max output length: 291
2026-01-29 17:38:55,748 | INFO | min output length: 29
2026-01-29 17:39:07,173 | INFO | end detected at 171
2026-01-29 17:39:07,175 | INFO | -13.30 * 0.5 =  -6.65 for decoder
2026-01-29 17:39:07,175 | INFO |  -0.21 * 0.5 =  -0.11 for ctc
2026-01-29 17:39:07,175 | INFO | total log probability: -6.75
2026-01-29 17:39:07,175 | INFO | normalized log probability: -0.04
2026-01-29 17:39:07,175 | INFO | total number of ended hypotheses: 172
2026-01-29 17:39:07,178 | INFO | best hypo: comme<space>la<space>parole<space>du<space>christ<space>qui<space>donne<space>la<space>vie<space>pour<space>que<space>notre<space>parole<space>soit<space>entendue<space>pour<space>qu'elle<space>soit<space>comprise<space>ne<space>faut<space>il<space>pas<space>comme<space>pour<space>jésus<space>que<space>d'abord<space>nous<space>agissions

2026-01-29 17:39:07,185 | INFO | Chunk: 0 | WER=3.225806 | S=0 D=0 I=1
2026-01-29 17:39:07,185 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:39:07,185 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:39:07,186 | INFO | Chunk: 3 | WER=13.333333 | S=2 D=0 I=0
2026-01-29 17:39:07,186 | INFO | Chunk: 4 | WER=7.692308 | S=2 D=0 I=0
2026-01-29 17:39:07,187 | INFO | Chunk: 5 | WER=8.333333 | S=1 D=0 I=0
2026-01-29 17:39:07,187 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:39:07,187 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:39:07,187 | INFO | Chunk: 8 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:39:07,188 | INFO | Chunk: 9 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:39:07,188 | INFO | Chunk: 10 | WER=14.285714 | S=2 D=0 I=0
2026-01-29 17:39:07,188 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:39:07,189 | INFO | Chunk: 12 | WER=12.000000 | S=2 D=1 I=0
2026-01-29 17:39:07,189 | INFO | Chunk: 13 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:39:07,190 | INFO | Chunk: 14 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:39:07,190 | INFO | Chunk: 15 | WER=12.500000 | S=2 D=0 I=0
2026-01-29 17:39:07,190 | INFO | Chunk: 16 | WER=33.333333 | S=1 D=0 I=0
2026-01-29 17:39:07,191 | INFO | Chunk: 17 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:39:07,192 | INFO | Chunk: 18 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:39:07,192 | INFO | Chunk: 19 | WER=20.000000 | S=1 D=0 I=0
2026-01-29 17:39:07,193 | INFO | Chunk: 20 | WER=5.714286 | S=1 D=0 I=1
2026-01-29 17:39:07,195 | INFO | Chunk: 21 | WER=11.475410 | S=7 D=0 I=0
2026-01-29 17:39:07,195 | INFO | Chunk: 22 | WER=25.000000 | S=2 D=0 I=0
2026-01-29 17:39:07,196 | INFO | Chunk: 23 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:39:07,196 | INFO | Chunk: 24 | WER=31.250000 | S=3 D=0 I=2
2026-01-29 17:39:07,196 | INFO | Chunk: 25 | WER=10.000000 | S=0 D=0 I=1
2026-01-29 17:39:07,196 | INFO | Chunk: 26 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:39:07,197 | INFO | Chunk: 27 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:39:07,197 | INFO | Chunk: 28 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:39:07,198 | INFO | Chunk: 29 | WER=7.500000 | S=3 D=0 I=0
2026-01-29 17:39:07,200 | INFO | Chunk: 30 | WER=1.724138 | S=0 D=1 I=0
2026-01-29 17:39:07,200 | INFO | Chunk: 31 | WER=12.500000 | S=1 D=0 I=0
2026-01-29 17:39:07,200 | INFO | Chunk: 32 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:39:07,203 | INFO | Chunk: 33 | WER=2.985075 | S=2 D=0 I=0
2026-01-29 17:39:07,203 | INFO | Chunk: 34 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:39:07,204 | INFO | Chunk: 35 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:39:07,426 | INFO | File: Rhap-M2003.wav | WER=5.524862 | S=32 D=2 I=6
2026-01-29 17:39:07,427 | INFO | ------------------------------
2026-01-29 17:39:07,427 | INFO | Conf ester Done!
2026-01-29 17:42:09,858 | INFO | Chunk: 0 | WER=3.225806 | S=0 D=0 I=1
2026-01-29 17:42:09,859 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:42:09,859 | INFO | Chunk: 2 | WER=50.000000 | S=0 D=0 I=1
2026-01-29 17:42:09,860 | INFO | Chunk: 3 | WER=20.000000 | S=3 D=0 I=0
2026-01-29 17:42:09,861 | INFO | Chunk: 4 | WER=3.846154 | S=1 D=0 I=0
2026-01-29 17:42:09,862 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:42:09,862 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:42:09,862 | INFO | Chunk: 7 | WER=10.000000 | S=1 D=0 I=0
2026-01-29 17:42:09,863 | INFO | Chunk: 8 | WER=25.000000 | S=1 D=0 I=0
2026-01-29 17:42:09,863 | INFO | Chunk: 9 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:42:09,864 | INFO | Chunk: 10 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:42:09,864 | INFO | Chunk: 11 | WER=5.000000 | S=1 D=0 I=0
2026-01-29 17:42:09,865 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:42:09,866 | INFO | Chunk: 13 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:42:09,866 | INFO | Chunk: 14 | WER=10.526316 | S=1 D=1 I=0
2026-01-29 17:42:09,866 | INFO | Chunk: 15 | WER=12.500000 | S=2 D=0 I=0
2026-01-29 17:42:09,867 | INFO | Chunk: 16 | WER=33.333333 | S=1 D=0 I=0
2026-01-29 17:42:09,867 | INFO | Chunk: 17 | WER=3.125000 | S=1 D=0 I=0
2026-01-29 17:42:09,868 | INFO | Chunk: 18 | WER=8.823529 | S=1 D=0 I=2
2026-01-29 17:42:09,868 | INFO | Chunk: 19 | WER=20.000000 | S=1 D=0 I=0
2026-01-29 17:42:09,869 | INFO | Chunk: 20 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:42:09,871 | INFO | Chunk: 21 | WER=9.836066 | S=6 D=0 I=0
2026-01-29 17:42:09,872 | INFO | Chunk: 22 | WER=25.000000 | S=2 D=0 I=0
2026-01-29 17:42:09,872 | INFO | Chunk: 23 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:42:09,872 | INFO | Chunk: 24 | WER=12.500000 | S=2 D=0 I=0
2026-01-29 17:42:09,872 | INFO | Chunk: 25 | WER=10.000000 | S=0 D=0 I=1
2026-01-29 17:42:09,873 | INFO | Chunk: 26 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:42:09,873 | INFO | Chunk: 27 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:42:09,873 | INFO | Chunk: 28 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:42:09,874 | INFO | Chunk: 29 | WER=5.000000 | S=2 D=0 I=0
2026-01-29 17:42:09,876 | INFO | Chunk: 30 | WER=3.448276 | S=1 D=1 I=0
2026-01-29 17:42:09,876 | INFO | Chunk: 31 | WER=37.500000 | S=2 D=0 I=1
2026-01-29 17:42:09,877 | INFO | Chunk: 32 | WER=4.166667 | S=1 D=0 I=0
2026-01-29 17:42:09,879 | INFO | Chunk: 33 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:42:09,880 | INFO | Chunk: 34 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:42:09,880 | INFO | Chunk: 35 | WER=3.125000 | S=1 D=0 I=0
2026-01-29 17:42:10,106 | INFO | File: Rhap-M2003.wav | WER=5.524862 | S=31 D=2 I=7
2026-01-29 17:42:10,106 | INFO | ------------------------------
2026-01-29 17:42:10,106 | INFO | hmm_tdnn Done!
2026-01-29 17:42:10,256 | INFO | ==================================Rhap-M2004.wav=========================================
2026-01-29 17:42:10,437 | INFO | Using rVAD model
2026-01-29 17:43:03,772 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,772 | INFO | Chunk: 1 | WER=16.666667 | S=0 D=0 I=1
2026-01-29 17:43:03,773 | INFO | Chunk: 2 | WER=9.523810 | S=0 D=0 I=2
2026-01-29 17:43:03,773 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,773 | INFO | Chunk: 4 | WER=30.000000 | S=0 D=1 I=2
2026-01-29 17:43:03,773 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,774 | INFO | Chunk: 6 | WER=12.500000 | S=0 D=0 I=2
2026-01-29 17:43:03,774 | INFO | Chunk: 7 | WER=6.666667 | S=1 D=0 I=0
2026-01-29 17:43:03,774 | INFO | Chunk: 8 | WER=25.000000 | S=0 D=0 I=1
2026-01-29 17:43:03,775 | INFO | Chunk: 9 | WER=13.793103 | S=3 D=0 I=1
2026-01-29 17:43:03,775 | INFO | Chunk: 10 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,775 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,776 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,776 | INFO | Chunk: 13 | WER=27.272727 | S=1 D=0 I=2
2026-01-29 17:43:03,777 | INFO | Chunk: 14 | WER=12.500000 | S=1 D=0 I=0
2026-01-29 17:43:03,777 | INFO | Chunk: 15 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,777 | INFO | Chunk: 16 | WER=12.500000 | S=0 D=0 I=2
2026-01-29 17:43:03,777 | INFO | Chunk: 17 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,778 | INFO | Chunk: 18 | WER=16.666667 | S=0 D=0 I=1
2026-01-29 17:43:03,778 | INFO | Chunk: 19 | WER=60.000000 | S=0 D=0 I=3
2026-01-29 17:43:03,778 | INFO | Chunk: 20 | WER=23.076923 | S=1 D=2 I=0
2026-01-29 17:43:03,778 | INFO | Chunk: 21 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,779 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,779 | INFO | Chunk: 23 | WER=37.500000 | S=1 D=1 I=1
2026-01-29 17:43:03,779 | INFO | Chunk: 24 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,779 | INFO | Chunk: 25 | WER=7.692308 | S=0 D=0 I=1
2026-01-29 17:43:03,779 | INFO | Chunk: 26 | WER=60.000000 | S=0 D=0 I=3
2026-01-29 17:43:03,780 | INFO | Chunk: 27 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,780 | INFO | Chunk: 28 | WER=21.428571 | S=2 D=0 I=1
2026-01-29 17:43:03,780 | INFO | Chunk: 29 | WER=66.666667 | S=2 D=0 I=0
2026-01-29 17:43:03,780 | INFO | Chunk: 30 | WER=0.000000 | S=1 D=0 I=0
2026-01-29 17:43:03,781 | INFO | Chunk: 31 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,781 | INFO | Chunk: 32 | WER=12.500000 | S=2 D=0 I=1
2026-01-29 17:43:03,782 | INFO | Chunk: 33 | WER=40.000000 | S=0 D=0 I=2
2026-01-29 17:43:03,782 | INFO | Chunk: 34 | WER=300.000000 | S=0 D=0 I=3
2026-01-29 17:43:03,782 | INFO | Chunk: 35 | WER=6.250000 | S=1 D=0 I=0
2026-01-29 17:43:03,782 | INFO | Chunk: 36 | WER=33.333333 | S=0 D=0 I=2
2026-01-29 17:43:03,783 | INFO | Chunk: 37 | WER=6.060606 | S=0 D=0 I=2
2026-01-29 17:43:03,783 | INFO | Chunk: 38 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,784 | INFO | Chunk: 39 | WER=22.222222 | S=2 D=0 I=0
2026-01-29 17:43:03,784 | INFO | Chunk: 40 | WER=9.090909 | S=1 D=0 I=0
2026-01-29 17:43:03,784 | INFO | Chunk: 41 | WER=100.000000 | S=3 D=0 I=0
2026-01-29 17:43:03,784 | INFO | Chunk: 42 | WER=20.000000 | S=0 D=0 I=2
2026-01-29 17:43:03,784 | INFO | Chunk: 43 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,785 | INFO | Chunk: 44 | WER=12.500000 | S=1 D=0 I=0
2026-01-29 17:43:03,785 | INFO | Chunk: 45 | WER=5.882353 | S=1 D=0 I=0
2026-01-29 17:43:03,785 | INFO | Chunk: 46 | WER=33.333333 | S=0 D=0 I=1
2026-01-29 17:43:03,785 | INFO | Chunk: 47 | WER=33.333333 | S=1 D=0 I=1
2026-01-29 17:43:03,786 | INFO | Chunk: 48 | WER=50.000000 | S=1 D=0 I=1
2026-01-29 17:43:03,786 | INFO | Chunk: 49 | WER=6.666667 | S=0 D=0 I=1
2026-01-29 17:43:03,786 | INFO | Chunk: 50 | WER=100.000000 | S=1 D=0 I=1
2026-01-29 17:43:03,786 | INFO | Chunk: 51 | WER=50.000000 | S=1 D=0 I=1
2026-01-29 17:43:03,786 | INFO | Chunk: 52 | WER=83.333333 | S=2 D=1 I=2
2026-01-29 17:43:03,787 | INFO | Chunk: 53 | WER=300.000000 | S=0 D=0 I=3
2026-01-29 17:43:03,787 | INFO | Chunk: 54 | WER=41.666667 | S=2 D=3 I=0
2026-01-29 17:43:03,787 | INFO | Chunk: 55 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,788 | INFO | Chunk: 56 | WER=5.555556 | S=1 D=0 I=0
2026-01-29 17:43:03,788 | INFO | Chunk: 57 | WER=14.285714 | S=0 D=0 I=1
2026-01-29 17:43:03,788 | INFO | Chunk: 58 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,788 | INFO | Chunk: 59 | WER=15.384615 | S=0 D=0 I=2
2026-01-29 17:43:03,788 | INFO | Chunk: 60 | WER=60.000000 | S=2 D=0 I=1
2026-01-29 17:43:03,789 | INFO | Chunk: 61 | WER=66.666667 | S=0 D=1 I=1
2026-01-29 17:43:03,789 | INFO | Chunk: 62 | WER=25.000000 | S=0 D=0 I=2
2026-01-29 17:43:03,789 | INFO | Chunk: 63 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,789 | INFO | Chunk: 64 | WER=25.000000 | S=0 D=0 I=1
2026-01-29 17:43:03,789 | INFO | Chunk: 65 | WER=66.666667 | S=0 D=1 I=1
2026-01-29 17:43:03,790 | INFO | Chunk: 66 | WER=14.285714 | S=2 D=0 I=1
2026-01-29 17:43:03,790 | INFO | Chunk: 67 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,790 | INFO | Chunk: 68 | WER=20.000000 | S=0 D=0 I=1
2026-01-29 17:43:03,790 | INFO | Chunk: 69 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,790 | INFO | Chunk: 70 | WER=7.692308 | S=0 D=0 I=1
2026-01-29 17:43:03,791 | INFO | Chunk: 71 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,791 | INFO | Chunk: 72 | WER=40.000000 | S=2 D=0 I=0
2026-01-29 17:43:03,791 | INFO | Chunk: 73 | WER=30.000000 | S=1 D=0 I=2
2026-01-29 17:43:03,791 | INFO | Chunk: 74 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,792 | INFO | Chunk: 75 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,792 | INFO | Chunk: 76 | WER=25.000000 | S=0 D=0 I=1
2026-01-29 17:43:03,792 | INFO | Chunk: 77 | WER=12.500000 | S=0 D=0 I=2
2026-01-29 17:43:03,792 | INFO | Chunk: 78 | WER=20.000000 | S=0 D=0 I=2
2026-01-29 17:43:03,793 | INFO | Chunk: 79 | WER=20.000000 | S=0 D=0 I=3
2026-01-29 17:43:03,793 | INFO | Chunk: 80 | WER=6.666667 | S=0 D=0 I=1
2026-01-29 17:43:03,793 | INFO | Chunk: 81 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,794 | INFO | Chunk: 82 | WER=30.000000 | S=0 D=2 I=1
2026-01-29 17:43:03,794 | INFO | Chunk: 83 | WER=50.000000 | S=1 D=0 I=2
2026-01-29 17:43:03,794 | INFO | Chunk: 84 | WER=37.500000 | S=1 D=0 I=2
2026-01-29 17:43:03,794 | INFO | Chunk: 85 | WER=20.000000 | S=0 D=1 I=0
2026-01-29 17:43:03,794 | INFO | Chunk: 86 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,795 | INFO | Chunk: 87 | WER=20.000000 | S=0 D=0 I=1
2026-01-29 17:43:03,795 | INFO | Chunk: 88 | WER=11.111111 | S=1 D=0 I=1
2026-01-29 17:43:03,795 | INFO | Chunk: 89 | WER=5.882353 | S=0 D=0 I=1
2026-01-29 17:43:03,796 | INFO | Chunk: 90 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,796 | INFO | Chunk: 91 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,796 | INFO | Chunk: 92 | WER=25.000000 | S=0 D=0 I=1
2026-01-29 17:43:03,796 | INFO | Chunk: 93 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,796 | INFO | Chunk: 94 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,797 | INFO | Chunk: 95 | WER=50.000000 | S=0 D=0 I=3
2026-01-29 17:43:03,797 | INFO | Chunk: 96 | WER=150.000000 | S=0 D=0 I=3
2026-01-29 17:43:03,797 | INFO | Chunk: 97 | WER=33.333333 | S=0 D=1 I=0
2026-01-29 17:43:03,797 | INFO | Chunk: 98 | WER=6.666667 | S=0 D=0 I=1
2026-01-29 17:43:03,797 | INFO | Chunk: 99 | WER=25.000000 | S=1 D=0 I=1
2026-01-29 17:43:03,797 | INFO | Chunk: 100 | WER=200.000000 | S=1 D=0 I=1
2026-01-29 17:43:03,798 | INFO | Chunk: 101 | WER=50.000000 | S=0 D=0 I=1
2026-01-29 17:43:03,798 | INFO | Chunk: 102 | WER=50.000000 | S=0 D=0 I=1
2026-01-29 17:43:03,798 | INFO | Chunk: 103 | WER=7.692308 | S=0 D=0 I=1
2026-01-29 17:43:03,798 | INFO | Chunk: 104 | WER=50.000000 | S=0 D=0 I=1
2026-01-29 17:43:03,798 | INFO | Chunk: 105 | WER=33.333333 | S=0 D=0 I=2
2026-01-29 17:43:03,799 | INFO | Chunk: 106 | WER=12.500000 | S=2 D=0 I=0
2026-01-29 17:43:03,799 | INFO | Chunk: 107 | WER=7.692308 | S=0 D=0 I=1
2026-01-29 17:43:03,799 | INFO | Chunk: 108 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,799 | INFO | Chunk: 109 | WER=6.250000 | S=0 D=0 I=1
2026-01-29 17:43:03,800 | INFO | Chunk: 110 | WER=22.222222 | S=0 D=1 I=1
2026-01-29 17:43:03,800 | INFO | Chunk: 111 | WER=14.285714 | S=2 D=0 I=0
2026-01-29 17:43:03,800 | INFO | Chunk: 112 | WER=18.181818 | S=0 D=0 I=2
2026-01-29 17:43:03,800 | INFO | Chunk: 113 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,801 | INFO | Chunk: 114 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,801 | INFO | Chunk: 115 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,801 | INFO | Chunk: 116 | WER=10.000000 | S=1 D=0 I=0
2026-01-29 17:43:03,801 | INFO | Chunk: 117 | WER=20.000000 | S=0 D=0 I=1
2026-01-29 17:43:03,801 | INFO | Chunk: 118 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:43:03,802 | INFO | Chunk: 119 | WER=16.666667 | S=0 D=0 I=2
2026-01-29 17:43:03,802 | INFO | Chunk: 120 | WER=5.555556 | S=0 D=0 I=1
2026-01-29 17:43:03,802 | INFO | Chunk: 121 | WER=60.000000 | S=0 D=0 I=3
2026-01-29 17:43:03,802 | INFO | Chunk: 122 | WER=9.090909 | S=1 D=0 I=0
2026-01-29 17:43:03,803 | INFO | Chunk: 123 | WER=33.333333 | S=0 D=0 I=1
2026-01-29 17:43:03,803 | INFO | Chunk: 124 | WER=33.333333 | S=0 D=0 I=2
2026-01-29 17:43:04,427 | INFO | File: Rhap-M2004.wav | WER=13.787086 | S=53 D=4 I=101
2026-01-29 17:43:04,428 | INFO | ------------------------------
2026-01-29 17:43:04,428 | INFO | w2vec vad chunk Done!
2026-01-29 17:44:17,323 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,324 | INFO | Chunk: 1 | WER=16.666667 | S=0 D=0 I=1
2026-01-29 17:44:17,324 | INFO | Chunk: 2 | WER=28.571429 | S=3 D=3 I=0
2026-01-29 17:44:17,324 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,325 | INFO | Chunk: 4 | WER=30.000000 | S=0 D=1 I=2
2026-01-29 17:44:17,325 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,325 | INFO | Chunk: 6 | WER=12.500000 | S=0 D=0 I=2
2026-01-29 17:44:17,326 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,326 | INFO | Chunk: 8 | WER=25.000000 | S=0 D=0 I=1
2026-01-29 17:44:17,327 | INFO | Chunk: 9 | WER=13.793103 | S=3 D=0 I=1
2026-01-29 17:44:17,327 | INFO | Chunk: 10 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,327 | INFO | Chunk: 11 | WER=12.500000 | S=0 D=0 I=1
2026-01-29 17:44:17,328 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,328 | INFO | Chunk: 13 | WER=27.272727 | S=1 D=0 I=2
2026-01-29 17:44:17,328 | INFO | Chunk: 14 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,328 | INFO | Chunk: 15 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,329 | INFO | Chunk: 16 | WER=12.500000 | S=0 D=0 I=2
2026-01-29 17:44:17,329 | INFO | Chunk: 17 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,329 | INFO | Chunk: 18 | WER=33.333333 | S=1 D=0 I=1
2026-01-29 17:44:17,329 | INFO | Chunk: 19 | WER=60.000000 | S=0 D=0 I=3
2026-01-29 17:44:17,330 | INFO | Chunk: 20 | WER=23.076923 | S=1 D=2 I=0
2026-01-29 17:44:17,330 | INFO | Chunk: 21 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,330 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,330 | INFO | Chunk: 23 | WER=25.000000 | S=0 D=1 I=1
2026-01-29 17:44:17,330 | INFO | Chunk: 24 | WER=11.111111 | S=1 D=0 I=0
2026-01-29 17:44:17,331 | INFO | Chunk: 25 | WER=15.384615 | S=0 D=0 I=2
2026-01-29 17:44:17,331 | INFO | Chunk: 26 | WER=60.000000 | S=0 D=0 I=3
2026-01-29 17:44:17,331 | INFO | Chunk: 27 | WER=20.000000 | S=1 D=0 I=1
2026-01-29 17:44:17,331 | INFO | Chunk: 28 | WER=21.428571 | S=1 D=0 I=2
2026-01-29 17:44:17,332 | INFO | Chunk: 29 | WER=33.333333 | S=0 D=0 I=1
2026-01-29 17:44:17,332 | INFO | Chunk: 30 | WER=0.000000 | S=1 D=0 I=0
2026-01-29 17:44:17,332 | INFO | Chunk: 31 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,333 | INFO | Chunk: 32 | WER=29.166667 | S=6 D=0 I=1
2026-01-29 17:44:17,333 | INFO | Chunk: 33 | WER=40.000000 | S=0 D=0 I=2
2026-01-29 17:44:17,333 | INFO | Chunk: 34 | WER=300.000000 | S=0 D=0 I=3
2026-01-29 17:44:17,333 | INFO | Chunk: 35 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,334 | INFO | Chunk: 36 | WER=33.333333 | S=0 D=0 I=2
2026-01-29 17:44:17,334 | INFO | Chunk: 37 | WER=12.121212 | S=0 D=0 I=4
2026-01-29 17:44:17,335 | INFO | Chunk: 38 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,335 | INFO | Chunk: 39 | WER=11.111111 | S=1 D=0 I=0
2026-01-29 17:44:17,335 | INFO | Chunk: 40 | WER=9.090909 | S=1 D=0 I=0
2026-01-29 17:44:17,335 | INFO | Chunk: 41 | WER=66.666667 | S=0 D=1 I=1
2026-01-29 17:44:17,336 | INFO | Chunk: 42 | WER=20.000000 | S=0 D=0 I=2
2026-01-29 17:44:17,336 | INFO | Chunk: 43 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,336 | INFO | Chunk: 44 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,336 | INFO | Chunk: 45 | WER=5.882353 | S=1 D=0 I=0
2026-01-29 17:44:17,337 | INFO | Chunk: 46 | WER=33.333333 | S=0 D=0 I=1
2026-01-29 17:44:17,337 | INFO | Chunk: 47 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,337 | INFO | Chunk: 48 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,337 | INFO | Chunk: 49 | WER=6.666667 | S=0 D=0 I=1
2026-01-29 17:44:17,337 | INFO | Chunk: 50 | WER=50.000000 | S=1 D=0 I=0
2026-01-29 17:44:17,338 | INFO | Chunk: 51 | WER=25.000000 | S=0 D=0 I=1
2026-01-29 17:44:17,338 | INFO | Chunk: 52 | WER=83.333333 | S=2 D=1 I=2
2026-01-29 17:44:17,338 | INFO | Chunk: 53 | WER=300.000000 | S=0 D=0 I=3
2026-01-29 17:44:17,338 | INFO | Chunk: 54 | WER=8.333333 | S=1 D=0 I=0
2026-01-29 17:44:17,338 | INFO | Chunk: 55 | WER=7.692308 | S=1 D=0 I=0
2026-01-29 17:44:17,339 | INFO | Chunk: 56 | WER=5.555556 | S=1 D=0 I=0
2026-01-29 17:44:17,339 | INFO | Chunk: 57 | WER=14.285714 | S=0 D=0 I=1
2026-01-29 17:44:17,339 | INFO | Chunk: 58 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,340 | INFO | Chunk: 59 | WER=15.384615 | S=0 D=0 I=2
2026-01-29 17:44:17,340 | INFO | Chunk: 60 | WER=20.000000 | S=0 D=0 I=1
2026-01-29 17:44:17,340 | INFO | Chunk: 61 | WER=66.666667 | S=0 D=1 I=1
2026-01-29 17:44:17,340 | INFO | Chunk: 62 | WER=25.000000 | S=0 D=0 I=2
2026-01-29 17:44:17,340 | INFO | Chunk: 63 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,340 | INFO | Chunk: 64 | WER=25.000000 | S=0 D=0 I=1
2026-01-29 17:44:17,341 | INFO | Chunk: 65 | WER=66.666667 | S=0 D=1 I=1
2026-01-29 17:44:17,341 | INFO | Chunk: 66 | WER=23.809524 | S=4 D=0 I=1
2026-01-29 17:44:17,341 | INFO | Chunk: 67 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,341 | INFO | Chunk: 68 | WER=60.000000 | S=2 D=0 I=1
2026-01-29 17:44:17,342 | INFO | Chunk: 69 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,342 | INFO | Chunk: 70 | WER=7.692308 | S=0 D=0 I=1
2026-01-29 17:44:17,342 | INFO | Chunk: 71 | WER=7.692308 | S=1 D=0 I=0
2026-01-29 17:44:17,342 | INFO | Chunk: 72 | WER=40.000000 | S=2 D=0 I=0
2026-01-29 17:44:17,343 | INFO | Chunk: 73 | WER=30.000000 | S=1 D=0 I=2
2026-01-29 17:44:17,343 | INFO | Chunk: 74 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,343 | INFO | Chunk: 75 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,343 | INFO | Chunk: 76 | WER=25.000000 | S=0 D=0 I=1
2026-01-29 17:44:17,344 | INFO | Chunk: 77 | WER=18.750000 | S=1 D=0 I=2
2026-01-29 17:44:17,344 | INFO | Chunk: 78 | WER=20.000000 | S=0 D=0 I=2
2026-01-29 17:44:17,344 | INFO | Chunk: 79 | WER=20.000000 | S=0 D=0 I=3
2026-01-29 17:44:17,345 | INFO | Chunk: 80 | WER=6.666667 | S=0 D=0 I=1
2026-01-29 17:44:17,345 | INFO | Chunk: 81 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,345 | INFO | Chunk: 82 | WER=30.000000 | S=0 D=2 I=1
2026-01-29 17:44:17,345 | INFO | Chunk: 83 | WER=16.666667 | S=0 D=0 I=1
2026-01-29 17:44:17,345 | INFO | Chunk: 84 | WER=37.500000 | S=1 D=0 I=2
2026-01-29 17:44:17,346 | INFO | Chunk: 85 | WER=40.000000 | S=0 D=1 I=1
2026-01-29 17:44:17,346 | INFO | Chunk: 86 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,346 | INFO | Chunk: 87 | WER=20.000000 | S=0 D=0 I=1
2026-01-29 17:44:17,346 | INFO | Chunk: 88 | WER=16.666667 | S=1 D=0 I=2
2026-01-29 17:44:17,347 | INFO | Chunk: 89 | WER=5.882353 | S=0 D=0 I=1
2026-01-29 17:44:17,347 | INFO | Chunk: 90 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,347 | INFO | Chunk: 91 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,347 | INFO | Chunk: 92 | WER=50.000000 | S=1 D=0 I=1
2026-01-29 17:44:17,348 | INFO | Chunk: 93 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,348 | INFO | Chunk: 94 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,348 | INFO | Chunk: 95 | WER=50.000000 | S=0 D=0 I=3
2026-01-29 17:44:17,348 | INFO | Chunk: 96 | WER=250.000000 | S=2 D=0 I=3
2026-01-29 17:44:17,348 | INFO | Chunk: 97 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,348 | INFO | Chunk: 98 | WER=6.666667 | S=0 D=0 I=1
2026-01-29 17:44:17,349 | INFO | Chunk: 99 | WER=25.000000 | S=1 D=0 I=1
2026-01-29 17:44:17,349 | INFO | Chunk: 100 | WER=200.000000 | S=1 D=0 I=1
2026-01-29 17:44:17,349 | INFO | Chunk: 101 | WER=50.000000 | S=0 D=0 I=1
2026-01-29 17:44:17,349 | INFO | Chunk: 102 | WER=100.000000 | S=1 D=0 I=1
2026-01-29 17:44:17,349 | INFO | Chunk: 103 | WER=7.692308 | S=0 D=0 I=1
2026-01-29 17:44:17,350 | INFO | Chunk: 104 | WER=50.000000 | S=0 D=0 I=1
2026-01-29 17:44:17,350 | INFO | Chunk: 105 | WER=33.333333 | S=0 D=0 I=2
2026-01-29 17:44:17,350 | INFO | Chunk: 106 | WER=12.500000 | S=2 D=0 I=0
2026-01-29 17:44:17,350 | INFO | Chunk: 107 | WER=7.692308 | S=0 D=0 I=1
2026-01-29 17:44:17,351 | INFO | Chunk: 108 | WER=33.333333 | S=1 D=0 I=0
2026-01-29 17:44:17,351 | INFO | Chunk: 109 | WER=6.250000 | S=0 D=0 I=1
2026-01-29 17:44:17,351 | INFO | Chunk: 110 | WER=22.222222 | S=0 D=1 I=1
2026-01-29 17:44:17,351 | INFO | Chunk: 111 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,352 | INFO | Chunk: 112 | WER=18.181818 | S=0 D=0 I=2
2026-01-29 17:44:17,352 | INFO | Chunk: 113 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,352 | INFO | Chunk: 114 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,352 | INFO | Chunk: 115 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,352 | INFO | Chunk: 116 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,353 | INFO | Chunk: 117 | WER=20.000000 | S=0 D=0 I=1
2026-01-29 17:44:17,353 | INFO | Chunk: 118 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:44:17,353 | INFO | Chunk: 119 | WER=16.666667 | S=0 D=0 I=2
2026-01-29 17:44:17,353 | INFO | Chunk: 120 | WER=5.555556 | S=0 D=0 I=1
2026-01-29 17:44:17,354 | INFO | Chunk: 121 | WER=60.000000 | S=0 D=0 I=3
2026-01-29 17:44:17,354 | INFO | Chunk: 122 | WER=9.090909 | S=1 D=0 I=0
2026-01-29 17:44:17,354 | INFO | Chunk: 123 | WER=33.333333 | S=0 D=0 I=1
2026-01-29 17:44:17,354 | INFO | Chunk: 124 | WER=16.666667 | S=0 D=0 I=1
2026-01-29 17:44:17,972 | INFO | File: Rhap-M2004.wav | WER=14.310646 | S=50 D=7 I=107
2026-01-29 17:44:17,973 | INFO | ------------------------------
2026-01-29 17:44:17,973 | INFO | whisper med Done!
2026-01-29 17:45:54,178 | INFO | Chunk: 0 | WER=33.333333 | S=1 D=0 I=0
2026-01-29 17:45:54,179 | INFO | Chunk: 1 | WER=16.666667 | S=0 D=0 I=1
2026-01-29 17:45:54,179 | INFO | Chunk: 2 | WER=9.523810 | S=0 D=0 I=2
2026-01-29 17:45:54,179 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,180 | INFO | Chunk: 4 | WER=30.000000 | S=0 D=1 I=2
2026-01-29 17:45:54,180 | INFO | Chunk: 5 | WER=27.272727 | S=3 D=0 I=0
2026-01-29 17:45:54,180 | INFO | Chunk: 6 | WER=25.000000 | S=2 D=0 I=2
2026-01-29 17:45:54,181 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,181 | INFO | Chunk: 8 | WER=25.000000 | S=0 D=0 I=1
2026-01-29 17:45:54,182 | INFO | Chunk: 9 | WER=6.896552 | S=1 D=0 I=1
2026-01-29 17:45:54,182 | INFO | Chunk: 10 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,182 | INFO | Chunk: 11 | WER=12.500000 | S=0 D=0 I=1
2026-01-29 17:45:54,183 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,183 | INFO | Chunk: 13 | WER=27.272727 | S=1 D=0 I=2
2026-01-29 17:45:54,183 | INFO | Chunk: 14 | WER=12.500000 | S=1 D=0 I=0
2026-01-29 17:45:54,183 | INFO | Chunk: 15 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,184 | INFO | Chunk: 16 | WER=12.500000 | S=0 D=0 I=2
2026-01-29 17:45:54,184 | INFO | Chunk: 17 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,184 | INFO | Chunk: 18 | WER=16.666667 | S=0 D=0 I=1
2026-01-29 17:45:54,184 | INFO | Chunk: 19 | WER=60.000000 | S=0 D=0 I=3
2026-01-29 17:45:54,185 | INFO | Chunk: 20 | WER=7.692308 | S=0 D=1 I=0
2026-01-29 17:45:54,185 | INFO | Chunk: 21 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,185 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,185 | INFO | Chunk: 23 | WER=25.000000 | S=0 D=1 I=1
2026-01-29 17:45:54,185 | INFO | Chunk: 24 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,186 | INFO | Chunk: 25 | WER=7.692308 | S=0 D=0 I=1
2026-01-29 17:45:54,186 | INFO | Chunk: 26 | WER=60.000000 | S=0 D=0 I=3
2026-01-29 17:45:54,186 | INFO | Chunk: 27 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,187 | INFO | Chunk: 28 | WER=28.571429 | S=2 D=0 I=2
2026-01-29 17:45:54,187 | INFO | Chunk: 29 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,187 | INFO | Chunk: 30 | WER=0.000000 | S=1 D=0 I=0
2026-01-29 17:45:54,187 | INFO | Chunk: 31 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,188 | INFO | Chunk: 32 | WER=8.333333 | S=1 D=0 I=1
2026-01-29 17:45:54,188 | INFO | Chunk: 33 | WER=40.000000 | S=0 D=0 I=2
2026-01-29 17:45:54,188 | INFO | Chunk: 34 | WER=300.000000 | S=0 D=0 I=3
2026-01-29 17:45:54,188 | INFO | Chunk: 35 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,189 | INFO | Chunk: 36 | WER=33.333333 | S=0 D=0 I=2
2026-01-29 17:45:54,189 | INFO | Chunk: 37 | WER=12.121212 | S=2 D=0 I=2
2026-01-29 17:45:54,190 | INFO | Chunk: 38 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,190 | INFO | Chunk: 39 | WER=11.111111 | S=1 D=0 I=0
2026-01-29 17:45:54,190 | INFO | Chunk: 40 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,190 | INFO | Chunk: 41 | WER=100.000000 | S=3 D=0 I=0
2026-01-29 17:45:54,191 | INFO | Chunk: 42 | WER=20.000000 | S=0 D=0 I=2
2026-01-29 17:45:54,191 | INFO | Chunk: 43 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,191 | INFO | Chunk: 44 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,191 | INFO | Chunk: 45 | WER=5.882353 | S=1 D=0 I=0
2026-01-29 17:45:54,191 | INFO | Chunk: 46 | WER=33.333333 | S=0 D=0 I=1
2026-01-29 17:45:54,192 | INFO | Chunk: 47 | WER=33.333333 | S=1 D=0 I=1
2026-01-29 17:45:54,192 | INFO | Chunk: 48 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,192 | INFO | Chunk: 49 | WER=13.333333 | S=1 D=0 I=1
2026-01-29 17:45:54,192 | INFO | Chunk: 50 | WER=50.000000 | S=1 D=0 I=0
2026-01-29 17:45:54,192 | INFO | Chunk: 51 | WER=25.000000 | S=0 D=0 I=1
2026-01-29 17:45:54,193 | INFO | Chunk: 52 | WER=83.333333 | S=2 D=1 I=2
2026-01-29 17:45:54,193 | INFO | Chunk: 53 | WER=300.000000 | S=0 D=0 I=3
2026-01-29 17:45:54,193 | INFO | Chunk: 54 | WER=8.333333 | S=1 D=0 I=0
2026-01-29 17:45:54,193 | INFO | Chunk: 55 | WER=23.076923 | S=3 D=0 I=0
2026-01-29 17:45:54,194 | INFO | Chunk: 56 | WER=5.555556 | S=1 D=0 I=0
2026-01-29 17:45:54,194 | INFO | Chunk: 57 | WER=14.285714 | S=0 D=0 I=1
2026-01-29 17:45:54,194 | INFO | Chunk: 58 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,194 | INFO | Chunk: 59 | WER=15.384615 | S=0 D=0 I=2
2026-01-29 17:45:54,194 | INFO | Chunk: 60 | WER=40.000000 | S=1 D=0 I=1
2026-01-29 17:45:54,195 | INFO | Chunk: 61 | WER=66.666667 | S=0 D=1 I=1
2026-01-29 17:45:54,195 | INFO | Chunk: 62 | WER=25.000000 | S=0 D=0 I=2
2026-01-29 17:45:54,195 | INFO | Chunk: 63 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,195 | INFO | Chunk: 64 | WER=25.000000 | S=0 D=0 I=1
2026-01-29 17:45:54,195 | INFO | Chunk: 65 | WER=66.666667 | S=0 D=1 I=1
2026-01-29 17:45:54,196 | INFO | Chunk: 66 | WER=23.809524 | S=4 D=0 I=1
2026-01-29 17:45:54,196 | INFO | Chunk: 67 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,196 | INFO | Chunk: 68 | WER=60.000000 | S=2 D=0 I=1
2026-01-29 17:45:54,196 | INFO | Chunk: 69 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,197 | INFO | Chunk: 70 | WER=7.692308 | S=0 D=0 I=1
2026-01-29 17:45:54,197 | INFO | Chunk: 71 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,197 | INFO | Chunk: 72 | WER=40.000000 | S=2 D=0 I=0
2026-01-29 17:45:54,197 | INFO | Chunk: 73 | WER=30.000000 | S=1 D=0 I=2
2026-01-29 17:45:54,197 | INFO | Chunk: 74 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,198 | INFO | Chunk: 75 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,198 | INFO | Chunk: 76 | WER=25.000000 | S=0 D=0 I=1
2026-01-29 17:45:54,198 | INFO | Chunk: 77 | WER=12.500000 | S=0 D=0 I=2
2026-01-29 17:45:54,198 | INFO | Chunk: 78 | WER=20.000000 | S=0 D=0 I=2
2026-01-29 17:45:54,199 | INFO | Chunk: 79 | WER=20.000000 | S=0 D=0 I=3
2026-01-29 17:45:54,199 | INFO | Chunk: 80 | WER=6.666667 | S=0 D=0 I=1
2026-01-29 17:45:54,199 | INFO | Chunk: 81 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,200 | INFO | Chunk: 82 | WER=40.000000 | S=1 D=2 I=1
2026-01-29 17:45:54,200 | INFO | Chunk: 83 | WER=16.666667 | S=0 D=0 I=1
2026-01-29 17:45:54,200 | INFO | Chunk: 84 | WER=37.500000 | S=1 D=0 I=2
2026-01-29 17:45:54,200 | INFO | Chunk: 85 | WER=40.000000 | S=1 D=1 I=0
2026-01-29 17:45:54,200 | INFO | Chunk: 86 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,200 | INFO | Chunk: 87 | WER=20.000000 | S=0 D=0 I=1
2026-01-29 17:45:54,201 | INFO | Chunk: 88 | WER=5.555556 | S=0 D=0 I=1
2026-01-29 17:45:54,201 | INFO | Chunk: 89 | WER=5.882353 | S=0 D=0 I=1
2026-01-29 17:45:54,202 | INFO | Chunk: 90 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,202 | INFO | Chunk: 91 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,202 | INFO | Chunk: 92 | WER=50.000000 | S=1 D=0 I=1
2026-01-29 17:45:54,202 | INFO | Chunk: 93 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,202 | INFO | Chunk: 94 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,202 | INFO | Chunk: 95 | WER=50.000000 | S=0 D=0 I=3
2026-01-29 17:45:54,203 | INFO | Chunk: 96 | WER=250.000000 | S=2 D=0 I=3
2026-01-29 17:45:54,203 | INFO | Chunk: 97 | WER=33.333333 | S=0 D=1 I=0
2026-01-29 17:45:54,203 | INFO | Chunk: 98 | WER=6.666667 | S=0 D=0 I=1
2026-01-29 17:45:54,203 | INFO | Chunk: 99 | WER=12.500000 | S=0 D=0 I=1
2026-01-29 17:45:54,203 | INFO | Chunk: 100 | WER=200.000000 | S=1 D=0 I=1
2026-01-29 17:45:54,203 | INFO | Chunk: 101 | WER=50.000000 | S=0 D=0 I=1
2026-01-29 17:45:54,204 | INFO | Chunk: 102 | WER=100.000000 | S=1 D=0 I=1
2026-01-29 17:45:54,204 | INFO | Chunk: 103 | WER=7.692308 | S=0 D=0 I=1
2026-01-29 17:45:54,204 | INFO | Chunk: 104 | WER=50.000000 | S=0 D=0 I=1
2026-01-29 17:45:54,204 | INFO | Chunk: 105 | WER=33.333333 | S=0 D=0 I=2
2026-01-29 17:45:54,204 | INFO | Chunk: 106 | WER=12.500000 | S=2 D=0 I=0
2026-01-29 17:45:54,205 | INFO | Chunk: 107 | WER=15.384615 | S=1 D=0 I=1
2026-01-29 17:45:54,205 | INFO | Chunk: 108 | WER=33.333333 | S=1 D=0 I=0
2026-01-29 17:45:54,205 | INFO | Chunk: 109 | WER=12.500000 | S=1 D=0 I=1
2026-01-29 17:45:54,205 | INFO | Chunk: 110 | WER=22.222222 | S=0 D=1 I=1
2026-01-29 17:45:54,206 | INFO | Chunk: 111 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,206 | INFO | Chunk: 112 | WER=18.181818 | S=0 D=0 I=2
2026-01-29 17:45:54,206 | INFO | Chunk: 113 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,206 | INFO | Chunk: 114 | WER=25.000000 | S=1 D=0 I=0
2026-01-29 17:45:54,207 | INFO | Chunk: 115 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,207 | INFO | Chunk: 116 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,207 | INFO | Chunk: 117 | WER=20.000000 | S=0 D=0 I=1
2026-01-29 17:45:54,207 | INFO | Chunk: 118 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:45:54,207 | INFO | Chunk: 119 | WER=16.666667 | S=0 D=0 I=2
2026-01-29 17:45:54,208 | INFO | Chunk: 120 | WER=5.555556 | S=0 D=0 I=1
2026-01-29 17:45:54,208 | INFO | Chunk: 121 | WER=40.000000 | S=0 D=0 I=2
2026-01-29 17:45:54,208 | INFO | Chunk: 122 | WER=9.090909 | S=1 D=0 I=0
2026-01-29 17:45:54,208 | INFO | Chunk: 123 | WER=33.333333 | S=0 D=0 I=1
2026-01-29 17:45:54,209 | INFO | Chunk: 124 | WER=16.666667 | S=0 D=0 I=1
2026-01-29 17:45:54,823 | INFO | File: Rhap-M2004.wav | WER=13.787086 | S=54 D=3 I=101
2026-01-29 17:45:54,824 | INFO | ------------------------------
2026-01-29 17:45:54,824 | INFO | whisper large Done!
2026-01-29 17:45:54,983 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-29 17:45:55,013 | INFO | Vocabulary size: 350
2026-01-29 17:45:55,568 | INFO | Gradient checkpoint layers: []
2026-01-29 17:45:56,208 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 17:45:56,211 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 17:45:56,211 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 17:45:56,212 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-29 17:45:56,212 | INFO | speech length: 15520
2026-01-29 17:45:56,248 | INFO | decoder input length: 23
2026-01-29 17:45:56,248 | INFO | max output length: 23
2026-01-29 17:45:56,248 | INFO | min output length: 2
2026-01-29 17:45:56,768 | INFO | end detected at 16
2026-01-29 17:45:56,770 | INFO |  -2.10 * 0.5 =  -1.05 for decoder
2026-01-29 17:45:56,770 | INFO |  -1.03 * 0.5 =  -0.51 for ctc
2026-01-29 17:45:56,770 | INFO | total log probability: -1.56
2026-01-29 17:45:56,770 | INFO | normalized log probability: -0.12
2026-01-29 17:45:56,770 | INFO | total number of ended hypotheses: 141
2026-01-29 17:45:56,770 | INFO | best hypo: ▁mes▁chers▁compatriotes

2026-01-29 17:45:56,773 | INFO | speech length: 44960
2026-01-29 17:45:56,803 | INFO | decoder input length: 69
2026-01-29 17:45:56,803 | INFO | max output length: 69
2026-01-29 17:45:56,803 | INFO | min output length: 6
2026-01-29 17:45:57,863 | INFO | end detected at 29
2026-01-29 17:45:57,864 | INFO |  -2.48 * 0.5 =  -1.24 for decoder
2026-01-29 17:45:57,864 | INFO |  -6.68 * 0.5 =  -3.34 for ctc
2026-01-29 17:45:57,864 | INFO | total log probability: -4.58
2026-01-29 17:45:57,864 | INFO | normalized log probability: -0.21
2026-01-29 17:45:57,864 | INFO | total number of ended hypotheses: 184
2026-01-29 17:45:57,864 | INFO | best hypo: ▁je▁voudrais▁d'abord▁exprimer▁ma▁sablette

2026-01-29 17:45:57,866 | INFO | speech length: 107360
2026-01-29 17:45:57,893 | INFO | decoder input length: 167
2026-01-29 17:45:57,893 | INFO | max output length: 167
2026-01-29 17:45:57,893 | INFO | min output length: 16
2026-01-29 17:46:00,143 | INFO | end detected at 49
2026-01-29 17:46:00,144 | INFO |  -4.19 * 0.5 =  -2.10 for decoder
2026-01-29 17:46:00,144 | INFO |  -1.73 * 0.5 =  -0.87 for ctc
2026-01-29 17:46:00,144 | INFO | total log probability: -2.96
2026-01-29 17:46:00,144 | INFO | normalized log probability: -0.07
2026-01-29 17:46:00,144 | INFO | total number of ended hypotheses: 144
2026-01-29 17:46:00,144 | INFO | best hypo: ▁a▁toute▁celle▁et▁à▁tous▁ceux▁qui▁vivent▁ces▁derniers▁jours▁de▁mille▁neuf▁cent▁quatre▁vingt▁dix▁neuf▁dans▁l'épreuve

2026-01-29 17:46:00,146 | INFO | speech length: 48800
2026-01-29 17:46:00,176 | INFO | decoder input length: 75
2026-01-29 17:46:00,176 | INFO | max output length: 75
2026-01-29 17:46:00,176 | INFO | min output length: 7
2026-01-29 17:46:01,199 | INFO | end detected at 28
2026-01-29 17:46:01,199 | INFO |  -1.59 * 0.5 =  -0.79 for decoder
2026-01-29 17:46:01,199 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-29 17:46:01,200 | INFO | total log probability: -0.80
2026-01-29 17:46:01,200 | INFO | normalized log probability: -0.03
2026-01-29 17:46:01,200 | INFO | total number of ended hypotheses: 141
2026-01-29 17:46:01,200 | INFO | best hypo: ▁je▁pense▁aux▁nombreuses▁victimes▁de▁la▁tempête

2026-01-29 17:46:01,201 | INFO | speech length: 59520
2026-01-29 17:46:01,234 | INFO | decoder input length: 92
2026-01-29 17:46:01,234 | INFO | max output length: 92
2026-01-29 17:46:01,234 | INFO | min output length: 9
2026-01-29 17:46:02,538 | INFO | end detected at 34
2026-01-29 17:46:02,538 | INFO |  -1.95 * 0.5 =  -0.98 for decoder
2026-01-29 17:46:02,539 | INFO |  -6.53 * 0.5 =  -3.26 for ctc
2026-01-29 17:46:02,539 | INFO | total log probability: -4.24
2026-01-29 17:46:02,539 | INFO | normalized log probability: -0.14
2026-01-29 17:46:02,539 | INFO | total number of ended hypotheses: 154
2026-01-29 17:46:02,539 | INFO | best hypo: ▁et▁à▁toutes▁les▁familles▁endeuillées▁dont▁nous▁partageons▁la▁paix

2026-01-29 17:46:02,541 | INFO | speech length: 75520
2026-01-29 17:46:02,572 | INFO | decoder input length: 117
2026-01-29 17:46:02,572 | INFO | max output length: 117
2026-01-29 17:46:02,572 | INFO | min output length: 11
2026-01-29 17:46:04,203 | INFO | end detected at 40
2026-01-29 17:46:04,205 | INFO |  -4.92 * 0.5 =  -2.46 for decoder
2026-01-29 17:46:04,205 | INFO |  -6.26 * 0.5 =  -3.13 for ctc
2026-01-29 17:46:04,205 | INFO | total log probability: -5.59
2026-01-29 17:46:04,205 | INFO | normalized log probability: -0.16
2026-01-29 17:46:04,205 | INFO | total number of ended hypotheses: 199
2026-01-29 17:46:04,206 | INFO | best hypo: ▁je▁pense▁à▁nos▁concitoyens▁cruellement▁touchés▁dans▁leur▁vie▁quotidienne

2026-01-29 17:46:04,207 | INFO | speech length: 92160
2026-01-29 17:46:04,239 | INFO | decoder input length: 143
2026-01-29 17:46:04,240 | INFO | max output length: 143
2026-01-29 17:46:04,240 | INFO | min output length: 14
2026-01-29 17:46:06,332 | INFO | end detected at 49
2026-01-29 17:46:06,334 | INFO |  -4.36 * 0.5 =  -2.18 for decoder
2026-01-29 17:46:06,334 | INFO |  -1.71 * 0.5 =  -0.86 for ctc
2026-01-29 17:46:06,334 | INFO | total log probability: -3.04
2026-01-29 17:46:06,334 | INFO | normalized log probability: -0.07
2026-01-29 17:46:06,334 | INFO | total number of ended hypotheses: 180
2026-01-29 17:46:06,334 | INFO | best hypo: ▁a▁ceux▁dont▁les▁biens▁ont▁été▁détruits▁à▁ceux▁qui▁craignent▁pour▁leur▁activité▁et▁leurs▁emplois

2026-01-29 17:46:06,336 | INFO | speech length: 122720
2026-01-29 17:46:06,368 | INFO | decoder input length: 191
2026-01-29 17:46:06,368 | INFO | max output length: 191
2026-01-29 17:46:06,368 | INFO | min output length: 19
2026-01-29 17:46:08,710 | INFO | end detected at 49
2026-01-29 17:46:08,711 | INFO |  -3.57 * 0.5 =  -1.78 for decoder
2026-01-29 17:46:08,711 | INFO |  -0.49 * 0.5 =  -0.24 for ctc
2026-01-29 17:46:08,711 | INFO | total log probability: -2.03
2026-01-29 17:46:08,712 | INFO | normalized log probability: -0.05
2026-01-29 17:46:08,712 | INFO | total number of ended hypotheses: 160
2026-01-29 17:46:08,712 | INFO | best hypo: ▁a▁ceux▁qui▁souffrent▁de▁voir▁notre▁patrimoine▁notre▁littoral▁nos▁forêts▁nos▁monuments▁défigurés

2026-01-29 17:46:08,714 | INFO | speech length: 20800
2026-01-29 17:46:08,740 | INFO | decoder input length: 32
2026-01-29 17:46:08,740 | INFO | max output length: 32
2026-01-29 17:46:08,740 | INFO | min output length: 3
2026-01-29 17:46:09,227 | INFO | end detected at 15
2026-01-29 17:46:09,228 | INFO |  -0.70 * 0.5 =  -0.35 for decoder
2026-01-29 17:46:09,228 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-29 17:46:09,228 | INFO | total log probability: -0.36
2026-01-29 17:46:09,228 | INFO | normalized log probability: -0.03
2026-01-29 17:46:09,228 | INFO | total number of ended hypotheses: 147
2026-01-29 17:46:09,228 | INFO | best hypo: ▁je▁vous▁redis▁mon▁émotion

2026-01-29 17:46:09,230 | INFO | speech length: 217280
2026-01-29 17:46:09,262 | INFO | decoder input length: 339
2026-01-29 17:46:09,262 | INFO | max output length: 339
2026-01-29 17:46:09,262 | INFO | min output length: 33
2026-01-29 17:46:15,213 | INFO | end detected at 96
2026-01-29 17:46:15,215 | INFO | -71.67 * 0.5 = -35.84 for decoder
2026-01-29 17:46:15,215 | INFO | -38.69 * 0.5 = -19.35 for ctc
2026-01-29 17:46:15,215 | INFO | total log probability: -55.18
2026-01-29 17:46:15,215 | INFO | normalized log probability: -0.61
2026-01-29 17:46:15,215 | INFO | total number of ended hypotheses: 157
2026-01-29 17:46:15,216 | INFO | best hypo: ▁mais▁aussi▁ma▁fierté▁devant▁l'exceptionnel▁élan▁de▁solidarité▁qui▁anime▁tant▁de▁bénévoles▁et▁d'associations▁mobilisés▁aux▁côtés▁et▁des▁services▁à▁public▁civils▁et▁militaires▁et▁des▁élus

2026-01-29 17:46:15,218 | INFO | speech length: 91360
2026-01-29 17:46:15,250 | INFO | decoder input length: 142
2026-01-29 17:46:15,250 | INFO | max output length: 142
2026-01-29 17:46:15,250 | INFO | min output length: 14
2026-01-29 17:46:16,951 | INFO | end detected at 40
2026-01-29 17:46:16,952 | INFO |  -3.46 * 0.5 =  -1.73 for decoder
2026-01-29 17:46:16,952 | INFO |  -0.69 * 0.5 =  -0.35 for ctc
2026-01-29 17:46:16,953 | INFO | total log probability: -2.08
2026-01-29 17:46:16,953 | INFO | normalized log probability: -0.06
2026-01-29 17:46:16,953 | INFO | total number of ended hypotheses: 154
2026-01-29 17:46:16,953 | INFO | best hypo: ▁en▁ses▁heures▁difficiles▁nous▁ressentons▁profondément▁la▁fragilité▁des▁choses

2026-01-29 17:46:16,955 | INFO | speech length: 56960
2026-01-29 17:46:16,983 | INFO | decoder input length: 88
2026-01-29 17:46:16,983 | INFO | max output length: 88
2026-01-29 17:46:16,983 | INFO | min output length: 8
2026-01-29 17:46:17,868 | INFO | end detected at 23
2026-01-29 17:46:17,869 | INFO |  -1.46 * 0.5 =  -0.73 for decoder
2026-01-29 17:46:17,869 | INFO |  -0.10 * 0.5 =  -0.05 for ctc
2026-01-29 17:46:17,869 | INFO | total log probability: -0.78
2026-01-29 17:46:17,869 | INFO | normalized log probability: -0.04
2026-01-29 17:46:17,869 | INFO | total number of ended hypotheses: 166
2026-01-29 17:46:17,869 | INFO | best hypo: ▁la▁précarité▁de▁ce▁qui▁nous▁semblait▁acquis

2026-01-29 17:46:17,871 | INFO | speech length: 140800
2026-01-29 17:46:17,902 | INFO | decoder input length: 219
2026-01-29 17:46:17,902 | INFO | max output length: 219
2026-01-29 17:46:17,902 | INFO | min output length: 21
2026-01-29 17:46:21,161 | INFO | end detected at 64
2026-01-29 17:46:21,162 | INFO |  -4.91 * 0.5 =  -2.46 for decoder
2026-01-29 17:46:21,162 | INFO |  -0.08 * 0.5 =  -0.04 for ctc
2026-01-29 17:46:21,162 | INFO | total log probability: -2.50
2026-01-29 17:46:21,162 | INFO | normalized log probability: -0.04
2026-01-29 17:46:21,162 | INFO | total number of ended hypotheses: 158
2026-01-29 17:46:21,163 | INFO | best hypo: ▁nous▁voyons▁combien▁tout▁peut▁être▁parfois▁remis▁en▁cause▁du▁fait▁de▁l'inconscience▁des▁hommes▁ou▁du▁déchaînement▁des▁éléments▁naturels

2026-01-29 17:46:21,165 | INFO | speech length: 84320
2026-01-29 17:46:21,197 | INFO | decoder input length: 131
2026-01-29 17:46:21,197 | INFO | max output length: 131
2026-01-29 17:46:21,197 | INFO | min output length: 13
2026-01-29 17:46:22,751 | INFO | end detected at 37
2026-01-29 17:46:22,752 | INFO |  -2.34 * 0.5 =  -1.17 for decoder
2026-01-29 17:46:22,752 | INFO |  -0.87 * 0.5 =  -0.44 for ctc
2026-01-29 17:46:22,753 | INFO | total log probability: -1.61
2026-01-29 17:46:22,753 | INFO | normalized log probability: -0.05
2026-01-29 17:46:22,753 | INFO | total number of ended hypotheses: 147
2026-01-29 17:46:22,753 | INFO | best hypo: ▁nous▁mesurons▁aussi▁l'importance▁du▁rôle▁de▁l'état▁dans▁notre▁société

2026-01-29 17:46:22,755 | INFO | speech length: 65600
2026-01-29 17:46:22,785 | INFO | decoder input length: 102
2026-01-29 17:46:22,785 | INFO | max output length: 102
2026-01-29 17:46:22,785 | INFO | min output length: 10
2026-01-29 17:46:24,149 | INFO | end detected at 35
2026-01-29 17:46:24,150 | INFO |  -2.58 * 0.5 =  -1.29 for decoder
2026-01-29 17:46:24,150 | INFO |  -1.99 * 0.5 =  -1.00 for ctc
2026-01-29 17:46:24,150 | INFO | total log probability: -2.28
2026-01-29 17:46:24,150 | INFO | normalized log probability: -0.07
2026-01-29 17:46:24,150 | INFO | total number of ended hypotheses: 144
2026-01-29 17:46:24,151 | INFO | best hypo: ▁un▁état▁sur▁lequel▁pèsent▁des▁responsabilités▁essentielles

2026-01-29 17:46:24,152 | INFO | speech length: 57600
2026-01-29 17:46:24,185 | INFO | decoder input length: 89
2026-01-29 17:46:24,185 | INFO | max output length: 89
2026-01-29 17:46:24,185 | INFO | min output length: 8
2026-01-29 17:46:25,271 | INFO | end detected at 28
2026-01-29 17:46:25,271 | INFO |  -1.57 * 0.5 =  -0.79 for decoder
2026-01-29 17:46:25,271 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-29 17:46:25,271 | INFO | total log probability: -0.80
2026-01-29 17:46:25,271 | INFO | normalized log probability: -0.03
2026-01-29 17:46:25,271 | INFO | total number of ended hypotheses: 134
2026-01-29 17:46:25,272 | INFO | best hypo: ▁le▁service▁public▁la▁sécurité▁la▁solidarité

2026-01-29 17:46:25,273 | INFO | speech length: 122080
2026-01-29 17:46:25,305 | INFO | decoder input length: 190
2026-01-29 17:46:25,306 | INFO | max output length: 190
2026-01-29 17:46:25,306 | INFO | min output length: 19
2026-01-29 17:46:27,642 | INFO | end detected at 49
2026-01-29 17:46:27,643 | INFO |  -4.03 * 0.5 =  -2.02 for decoder
2026-01-29 17:46:27,643 | INFO |  -0.69 * 0.5 =  -0.34 for ctc
2026-01-29 17:46:27,643 | INFO | total log probability: -2.36
2026-01-29 17:46:27,643 | INFO | normalized log probability: -0.05
2026-01-29 17:46:27,643 | INFO | total number of ended hypotheses: 155
2026-01-29 17:46:27,644 | INFO | best hypo: ▁un▁état▁auquel▁il▁appartient▁de▁prévoir▁de▁faire▁face▁d'assurer▁la▁coordination▁des▁moyens▁du▁pays

2026-01-29 17:46:27,646 | INFO | speech length: 70400
2026-01-29 17:46:27,677 | INFO | decoder input length: 109
2026-01-29 17:46:27,677 | INFO | max output length: 109
2026-01-29 17:46:27,677 | INFO | min output length: 10
2026-01-29 17:46:28,913 | INFO | end detected at 31
2026-01-29 17:46:28,914 | INFO |  -2.67 * 0.5 =  -1.33 for decoder
2026-01-29 17:46:28,914 | INFO |  -0.56 * 0.5 =  -0.28 for ctc
2026-01-29 17:46:28,914 | INFO | total log probability: -1.61
2026-01-29 17:46:28,914 | INFO | normalized log probability: -0.06
2026-01-29 17:46:28,914 | INFO | total number of ended hypotheses: 155
2026-01-29 17:46:28,914 | INFO | best hypo: ▁nous▁mesurons▁surtout▁le▁prix▁de▁l'aide▁fraternel

2026-01-29 17:46:28,916 | INFO | speech length: 45120
2026-01-29 17:46:28,947 | INFO | decoder input length: 70
2026-01-29 17:46:28,948 | INFO | max output length: 70
2026-01-29 17:46:28,948 | INFO | min output length: 7
2026-01-29 17:46:29,754 | INFO | end detected at 22
2026-01-29 17:46:29,755 | INFO |  -1.31 * 0.5 =  -0.66 for decoder
2026-01-29 17:46:29,755 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-29 17:46:29,755 | INFO | total log probability: -0.66
2026-01-29 17:46:29,755 | INFO | normalized log probability: -0.04
2026-01-29 17:46:29,755 | INFO | total number of ended hypotheses: 146
2026-01-29 17:46:29,755 | INFO | best hypo: ▁du▁soutien▁spontané▁de▁la▁main▁tendue

2026-01-29 17:46:29,757 | INFO | speech length: 35200
2026-01-29 17:46:29,788 | INFO | decoder input length: 54
2026-01-29 17:46:29,788 | INFO | max output length: 54
2026-01-29 17:46:29,788 | INFO | min output length: 5
2026-01-29 17:46:30,624 | INFO | end detected at 18
2026-01-29 17:46:30,625 | INFO |  -4.19 * 0.5 =  -2.10 for decoder
2026-01-29 17:46:30,625 | INFO |  -0.55 * 0.5 =  -0.27 for ctc
2026-01-29 17:46:30,625 | INFO | total log probability: -2.37
2026-01-29 17:46:30,625 | INFO | normalized log probability: -0.18
2026-01-29 17:46:30,625 | INFO | total number of ended hypotheses: 158
2026-01-29 17:46:30,626 | INFO | best hypo: ▁qui▁sont▁le▁ciment▁même▁de▁la▁nation

2026-01-29 17:46:30,627 | INFO | speech length: 54720
2026-01-29 17:46:30,657 | INFO | decoder input length: 85
2026-01-29 17:46:30,657 | INFO | max output length: 85
2026-01-29 17:46:30,657 | INFO | min output length: 8
2026-01-29 17:46:31,986 | INFO | end detected at 33
2026-01-29 17:46:31,988 | INFO |  -8.38 * 0.5 =  -4.19 for decoder
2026-01-29 17:46:31,988 | INFO |  -6.60 * 0.5 =  -3.30 for ctc
2026-01-29 17:46:31,988 | INFO | total log probability: -7.49
2026-01-29 17:46:31,988 | INFO | normalized log probability: -0.31
2026-01-29 17:46:31,988 | INFO | total number of ended hypotheses: 203
2026-01-29 17:46:31,989 | INFO | best hypo: ▁au▁moment▁où▁nous▁touchons▁aux▁portes▁de▁leur▁demiille

2026-01-29 17:46:31,991 | INFO | speech length: 83200
2026-01-29 17:46:32,022 | INFO | decoder input length: 129
2026-01-29 17:46:32,022 | INFO | max output length: 129
2026-01-29 17:46:32,022 | INFO | min output length: 12
2026-01-29 17:46:33,432 | INFO | end detected at 33
2026-01-29 17:46:33,432 | INFO |  -2.09 * 0.5 =  -1.04 for decoder
2026-01-29 17:46:33,433 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-29 17:46:33,433 | INFO | total log probability: -1.05
2026-01-29 17:46:33,433 | INFO | normalized log probability: -0.04
2026-01-29 17:46:33,433 | INFO | total number of ended hypotheses: 138
2026-01-29 17:46:33,433 | INFO | best hypo: ▁rien▁n'est▁décidément▁plus▁moderne▁plus▁nécessaire▁plus▁solide

2026-01-29 17:46:33,435 | INFO | speech length: 56000
2026-01-29 17:46:33,467 | INFO | decoder input length: 87
2026-01-29 17:46:33,467 | INFO | max output length: 87
2026-01-29 17:46:33,467 | INFO | min output length: 8
2026-01-29 17:46:34,564 | INFO | end detected at 29
2026-01-29 17:46:34,565 | INFO |  -1.96 * 0.5 =  -0.98 for decoder
2026-01-29 17:46:34,565 | INFO |  -0.14 * 0.5 =  -0.07 for ctc
2026-01-29 17:46:34,565 | INFO | total log probability: -1.05
2026-01-29 17:46:34,565 | INFO | normalized log probability: -0.04
2026-01-29 17:46:34,565 | INFO | total number of ended hypotheses: 146
2026-01-29 17:46:34,566 | INFO | best hypo: ▁que▁le▁sentiment▁d'appartenir▁à▁une▁même▁communauté

2026-01-29 17:46:34,567 | INFO | speech length: 50720
2026-01-29 17:46:34,597 | INFO | decoder input length: 78
2026-01-29 17:46:34,597 | INFO | max output length: 78
2026-01-29 17:46:34,597 | INFO | min output length: 7
2026-01-29 17:46:35,537 | INFO | end detected at 25
2026-01-29 17:46:35,539 | INFO |  -1.50 * 0.5 =  -0.75 for decoder
2026-01-29 17:46:35,539 | INFO |  -0.10 * 0.5 =  -0.05 for ctc
2026-01-29 17:46:35,539 | INFO | total log probability: -0.80
2026-01-29 17:46:35,539 | INFO | normalized log probability: -0.04
2026-01-29 17:46:35,539 | INFO | total number of ended hypotheses: 150
2026-01-29 17:46:35,539 | INFO | best hypo: ▁et▁d'être▁responsables▁les▁uns▁des▁autres

2026-01-29 17:46:35,541 | INFO | speech length: 98560
2026-01-29 17:46:35,574 | INFO | decoder input length: 153
2026-01-29 17:46:35,574 | INFO | max output length: 153
2026-01-29 17:46:35,574 | INFO | min output length: 15
2026-01-29 17:46:37,102 | INFO | end detected at 33
2026-01-29 17:46:37,102 | INFO |  -2.24 * 0.5 =  -1.12 for decoder
2026-01-29 17:46:37,103 | INFO |  -0.13 * 0.5 =  -0.06 for ctc
2026-01-29 17:46:37,103 | INFO | total log probability: -1.18
2026-01-29 17:46:37,103 | INFO | normalized log probability: -0.04
2026-01-29 17:46:37,103 | INFO | total number of ended hypotheses: 154
2026-01-29 17:46:37,103 | INFO | best hypo: ▁la▁france▁blessée▁veut▁se▁retrouver▁rassemblée▁et▁fraternelle

2026-01-29 17:46:37,105 | INFO | speech length: 95040
2026-01-29 17:46:37,132 | INFO | decoder input length: 148
2026-01-29 17:46:37,132 | INFO | max output length: 148
2026-01-29 17:46:37,133 | INFO | min output length: 14
2026-01-29 17:46:39,150 | INFO | end detected at 46
2026-01-29 17:46:39,152 | INFO |  -6.73 * 0.5 =  -3.37 for decoder
2026-01-29 17:46:39,152 | INFO |  -4.69 * 0.5 =  -2.34 for ctc
2026-01-29 17:46:39,152 | INFO | total log probability: -5.71
2026-01-29 17:46:39,152 | INFO | normalized log probability: -0.15
2026-01-29 17:46:39,152 | INFO | total number of ended hypotheses: 190
2026-01-29 17:46:39,153 | INFO | best hypo: ▁parce▁que▁nos▁compatriotes▁ont▁toujours▁su▁dans▁les▁preuves▁faire▁parler▁leur▁coeur

2026-01-29 17:46:39,155 | INFO | speech length: 56960
2026-01-29 17:46:39,182 | INFO | decoder input length: 88
2026-01-29 17:46:39,182 | INFO | max output length: 88
2026-01-29 17:46:39,182 | INFO | min output length: 8
2026-01-29 17:46:40,105 | INFO | end detected at 24
2026-01-29 17:46:40,106 | INFO |  -1.36 * 0.5 =  -0.68 for decoder
2026-01-29 17:46:40,106 | INFO |  -1.09 * 0.5 =  -0.54 for ctc
2026-01-29 17:46:40,106 | INFO | total log probability: -1.22
2026-01-29 17:46:40,106 | INFO | normalized log probability: -0.06
2026-01-29 17:46:40,106 | INFO | total number of ended hypotheses: 146
2026-01-29 17:46:40,106 | INFO | best hypo: ▁je▁voudrais▁dire▁merci▁à▁tous▁les▁francs

2026-01-29 17:46:40,108 | INFO | speech length: 83680
2026-01-29 17:46:40,134 | INFO | decoder input length: 130
2026-01-29 17:46:40,134 | INFO | max output length: 130
2026-01-29 17:46:40,134 | INFO | min output length: 13
2026-01-29 17:46:41,503 | INFO | end detected at 32
2026-01-29 17:46:41,504 | INFO |  -2.16 * 0.5 =  -1.08 for decoder
2026-01-29 17:46:41,504 | INFO |  -0.08 * 0.5 =  -0.04 for ctc
2026-01-29 17:46:41,504 | INFO | total log probability: -1.12
2026-01-29 17:46:41,504 | INFO | normalized log probability: -0.04
2026-01-29 17:46:41,504 | INFO | total number of ended hypotheses: 167
2026-01-29 17:46:41,505 | INFO | best hypo: ▁ce▁soir▁nous▁vivons▁ensemble▁un▁moment▁fort▁et▁singulier

2026-01-29 17:46:41,506 | INFO | speech length: 92160
2026-01-29 17:46:41,533 | INFO | decoder input length: 143
2026-01-29 17:46:41,533 | INFO | max output length: 143
2026-01-29 17:46:41,533 | INFO | min output length: 14
2026-01-29 17:46:43,386 | INFO | end detected at 43
2026-01-29 17:46:43,387 | INFO |  -3.28 * 0.5 =  -1.64 for decoder
2026-01-29 17:46:43,387 | INFO |  -0.32 * 0.5 =  -0.16 for ctc
2026-01-29 17:46:43,388 | INFO | total log probability: -1.80
2026-01-29 17:46:43,388 | INFO | normalized log probability: -0.05
2026-01-29 17:46:43,388 | INFO | total number of ended hypotheses: 162
2026-01-29 17:46:43,388 | INFO | best hypo: ▁ce▁qui▁paraissait▁très▁lointain▁qui▁a▁longtemps▁symbolisé▁le▁futur▁l'en▁deux▁mille

2026-01-29 17:46:43,390 | INFO | speech length: 29760
2026-01-29 17:46:43,421 | INFO | decoder input length: 46
2026-01-29 17:46:43,421 | INFO | max output length: 46
2026-01-29 17:46:43,421 | INFO | min output length: 4
2026-01-29 17:46:44,007 | INFO | end detected at 17
2026-01-29 17:46:44,008 | INFO |  -1.87 * 0.5 =  -0.94 for decoder
2026-01-29 17:46:44,009 | INFO |  -0.92 * 0.5 =  -0.46 for ctc
2026-01-29 17:46:44,009 | INFO | total log probability: -1.40
2026-01-29 17:46:44,009 | INFO | normalized log probability: -0.12
2026-01-29 17:46:44,009 | INFO | total number of ended hypotheses: 160
2026-01-29 17:46:44,009 | INFO | best hypo: ▁est▁devenu▁contemporain

2026-01-29 17:46:44,010 | INFO | speech length: 9760
2026-01-29 17:46:44,034 | INFO | decoder input length: 14
2026-01-29 17:46:44,034 | INFO | max output length: 14
2026-01-29 17:46:44,034 | INFO | min output length: 1
2026-01-29 17:46:44,389 | INFO | end detected at 11
2026-01-29 17:46:44,390 | INFO |  -0.38 * 0.5 =  -0.19 for decoder
2026-01-29 17:46:44,390 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-29 17:46:44,390 | INFO | total log probability: -0.19
2026-01-29 17:46:44,390 | INFO | normalized log probability: -0.03
2026-01-29 17:46:44,390 | INFO | total number of ended hypotheses: 135
2026-01-29 17:46:44,390 | INFO | best hypo: ▁immédiat

2026-01-29 17:46:44,392 | INFO | speech length: 106240
2026-01-29 17:46:44,419 | INFO | decoder input length: 165
2026-01-29 17:46:44,420 | INFO | max output length: 165
2026-01-29 17:46:44,420 | INFO | min output length: 16
2026-01-29 17:46:46,792 | INFO | end detected at 53
2026-01-29 17:46:46,793 | INFO |  -6.29 * 0.5 =  -3.14 for decoder
2026-01-29 17:46:46,793 | INFO |  -4.65 * 0.5 =  -2.32 for ctc
2026-01-29 17:46:46,793 | INFO | total log probability: -5.47
2026-01-29 17:46:46,793 | INFO | normalized log probability: -0.11
2026-01-29 17:46:46,793 | INFO | total number of ended hypotheses: 163
2026-01-29 17:46:46,794 | INFO | best hypo: ▁je▁suis▁sûr▁que▁beaucoup▁d'entre▁vous▁vont▁vivre▁ces▁instants▁avec▁un▁peu▁d'émotion▁un▁peu▁d'étonnement

2026-01-29 17:46:46,796 | INFO | speech length: 164320
2026-01-29 17:46:46,825 | INFO | decoder input length: 256
2026-01-29 17:46:46,825 | INFO | max output length: 256
2026-01-29 17:46:46,825 | INFO | min output length: 25
2026-01-29 17:46:51,026 | INFO | end detected at 79
2026-01-29 17:46:51,027 | INFO | -21.56 * 0.5 = -10.78 for decoder
2026-01-29 17:46:51,027 | INFO |  -0.74 * 0.5 =  -0.37 for ctc
2026-01-29 17:46:51,027 | INFO | total log probability: -11.15
2026-01-29 17:46:51,027 | INFO | normalized log probability: -0.15
2026-01-29 17:46:51,027 | INFO | total number of ended hypotheses: 147
2026-01-29 17:46:51,028 | INFO | best hypo: ▁une▁certaine▁appréhension▁parfois▁n'est▁du▁sentiment▁que▁s'achève▁une▁époque▁dont▁on▁possédait▁les▁clés▁dont▁on▁maîtrisait▁les▁règles▁et▁les▁habitudes

2026-01-29 17:46:51,029 | INFO | speech length: 30560
2026-01-29 17:46:51,058 | INFO | decoder input length: 47
2026-01-29 17:46:51,058 | INFO | max output length: 47
2026-01-29 17:46:51,058 | INFO | min output length: 4
2026-01-29 17:46:51,762 | INFO | end detected at 21
2026-01-29 17:46:51,763 | INFO |  -1.87 * 0.5 =  -0.93 for decoder
2026-01-29 17:46:51,763 | INFO |  -1.83 * 0.5 =  -0.92 for ctc
2026-01-29 17:46:51,763 | INFO | total log probability: -1.85
2026-01-29 17:46:51,763 | INFO | normalized log probability: -0.11
2026-01-29 17:46:51,763 | INFO | total number of ended hypotheses: 157
2026-01-29 17:46:51,763 | INFO | best hypo: ▁je▁comprends▁ces▁mouvements▁de▁lames

2026-01-29 17:46:51,765 | INFO | speech length: 15040
2026-01-29 17:46:51,794 | INFO | decoder input length: 23
2026-01-29 17:46:51,794 | INFO | max output length: 23
2026-01-29 17:46:51,794 | INFO | min output length: 2
2026-01-29 17:46:52,363 | INFO | end detected at 18
2026-01-29 17:46:52,363 | INFO |  -3.43 * 0.5 =  -1.71 for decoder
2026-01-29 17:46:52,364 | INFO |  -1.12 * 0.5 =  -0.56 for ctc
2026-01-29 17:46:52,364 | INFO | total log probability: -2.28
2026-01-29 17:46:52,364 | INFO | normalized log probability: -0.16
2026-01-29 17:46:52,364 | INFO | total number of ended hypotheses: 159
2026-01-29 17:46:52,364 | INFO | best hypo: ▁pourtant▁j'ai▁confiant

2026-01-29 17:46:52,365 | INFO | speech length: 99200
2026-01-29 17:46:52,393 | INFO | decoder input length: 154
2026-01-29 17:46:52,393 | INFO | max output length: 154
2026-01-29 17:46:52,393 | INFO | min output length: 15
2026-01-29 17:46:54,421 | INFO | end detected at 45
2026-01-29 17:46:54,422 | INFO |  -3.46 * 0.5 =  -1.73 for decoder
2026-01-29 17:46:54,422 | INFO |  -0.99 * 0.5 =  -0.50 for ctc
2026-01-29 17:46:54,422 | INFO | total log probability: -2.23
2026-01-29 17:46:54,422 | INFO | normalized log probability: -0.05
2026-01-29 17:46:54,422 | INFO | total number of ended hypotheses: 151
2026-01-29 17:46:54,422 | INFO | best hypo: ▁la▁france▁franchira▁des▁obstacles▁comme▁elle▁l'a▁toujours▁fait▁au▁long▁de▁son▁histoire

2026-01-29 17:46:54,424 | INFO | speech length: 31840
2026-01-29 17:46:54,453 | INFO | decoder input length: 49
2026-01-29 17:46:54,453 | INFO | max output length: 49
2026-01-29 17:46:54,453 | INFO | min output length: 4
2026-01-29 17:46:55,181 | INFO | end detected at 21
2026-01-29 17:46:55,182 | INFO | -10.05 * 0.5 =  -5.03 for decoder
2026-01-29 17:46:55,182 | INFO |  -0.37 * 0.5 =  -0.18 for ctc
2026-01-29 17:46:55,182 | INFO | total log probability: -5.21
2026-01-29 17:46:55,182 | INFO | normalized log probability: -0.35
2026-01-29 17:46:55,182 | INFO | total number of ended hypotheses: 166
2026-01-29 17:46:55,183 | INFO | best hypo: ▁pour▁peu▁qu'elle▁soit▁fidèlele

2026-01-29 17:46:55,184 | INFO | speech length: 220640
2026-01-29 17:46:55,217 | INFO | decoder input length: 344
2026-01-29 17:46:55,217 | INFO | max output length: 344
2026-01-29 17:46:55,217 | INFO | min output length: 34
2026-01-29 17:47:00,725 | INFO | end detected at 88
2026-01-29 17:47:00,726 | INFO | -44.30 * 0.5 = -22.15 for decoder
2026-01-29 17:47:00,726 | INFO |  -9.57 * 0.5 =  -4.78 for ctc
2026-01-29 17:47:00,726 | INFO | total log probability: -26.93
2026-01-29 17:47:00,726 | INFO | normalized log probability: -0.32
2026-01-29 17:47:00,727 | INFO | total number of ended hypotheses: 157
2026-01-29 17:47:00,728 | INFO | best hypo: ▁même▁si▁le▁passé▁est▁bien▁présent▁dans▁notre▁mémoire▁je▁ne▁m'attarderai▁pas▁sur▁le▁siècle▁qui▁s'achève▁siècle▁de▁progrès▁sans▁précédent▁pour▁la▁santé▁d'éducation▁les▁conditions▁de▁vie

2026-01-29 17:47:00,730 | INFO | speech length: 81600
2026-01-29 17:47:00,763 | INFO | decoder input length: 127
2026-01-29 17:47:00,763 | INFO | max output length: 127
2026-01-29 17:47:00,763 | INFO | min output length: 12
2026-01-29 17:47:02,400 | INFO | end detected at 40
2026-01-29 17:47:02,401 | INFO |  -2.63 * 0.5 =  -1.32 for decoder
2026-01-29 17:47:02,402 | INFO |  -1.03 * 0.5 =  -0.51 for ctc
2026-01-29 17:47:02,402 | INFO | total log probability: -1.83
2026-01-29 17:47:02,402 | INFO | normalized log probability: -0.05
2026-01-29 17:47:02,402 | INFO | total number of ended hypotheses: 163
2026-01-29 17:47:02,402 | INFO | best hypo: ▁pour▁les▁libertés▁la▁vie▁démocratique▁la▁situation▁des▁femmes▁les▁solidarités

2026-01-29 17:47:02,404 | INFO | speech length: 71360
2026-01-29 17:47:02,432 | INFO | decoder input length: 111
2026-01-29 17:47:02,432 | INFO | max output length: 111
2026-01-29 17:47:02,432 | INFO | min output length: 11
2026-01-29 17:47:03,754 | INFO | end detected at 33
2026-01-29 17:47:03,755 | INFO |  -2.89 * 0.5 =  -1.45 for decoder
2026-01-29 17:47:03,756 | INFO |  -1.33 * 0.5 =  -0.66 for ctc
2026-01-29 17:47:03,756 | INFO | total log probability: -2.11
2026-01-29 17:47:03,756 | INFO | normalized log probability: -0.08
2026-01-29 17:47:03,756 | INFO | total number of ended hypotheses: 168
2026-01-29 17:47:03,756 | INFO | best hypo: ▁mais▁aussi▁siècle▁d'horreur▁de▁tragédie▁de▁convulsions

2026-01-29 17:47:03,758 | INFO | speech length: 83520
2026-01-29 17:47:03,786 | INFO | decoder input length: 130
2026-01-29 17:47:03,786 | INFO | max output length: 130
2026-01-29 17:47:03,786 | INFO | min output length: 13
2026-01-29 17:47:05,348 | INFO | end detected at 37
2026-01-29 17:47:05,350 | INFO |  -2.92 * 0.5 =  -1.46 for decoder
2026-01-29 17:47:05,350 | INFO |  -6.42 * 0.5 =  -3.21 for ctc
2026-01-29 17:47:05,350 | INFO | total log probability: -4.67
2026-01-29 17:47:05,351 | INFO | normalized log probability: -0.14
2026-01-29 17:47:05,351 | INFO | total number of ended hypotheses: 139
2026-01-29 17:47:05,351 | INFO | best hypo: ▁qui▁a▁vu▁deux▁guerres▁mondiales▁le▁boulag▁les▁dictatures▁totalitaires

2026-01-29 17:47:05,354 | INFO | speech length: 13920
2026-01-29 17:47:05,395 | INFO | decoder input length: 21
2026-01-29 17:47:05,395 | INFO | max output length: 21
2026-01-29 17:47:05,395 | INFO | min output length: 2
2026-01-29 17:47:05,794 | INFO | end detected at 12
2026-01-29 17:47:05,794 | INFO |  -3.48 * 0.5 =  -1.74 for decoder
2026-01-29 17:47:05,795 | INFO |  -1.36 * 0.5 =  -0.68 for ctc
2026-01-29 17:47:05,795 | INFO | total log probability: -2.42
2026-01-29 17:47:05,795 | INFO | normalized log probability: -0.35
2026-01-29 17:47:05,795 | INFO | total number of ended hypotheses: 163
2026-01-29 17:47:05,795 | INFO | best hypo: ▁et▁la▁choa

2026-01-29 17:47:05,796 | INFO | speech length: 65440
2026-01-29 17:47:05,825 | INFO | decoder input length: 101
2026-01-29 17:47:05,825 | INFO | max output length: 101
2026-01-29 17:47:05,825 | INFO | min output length: 10
2026-01-29 17:47:07,065 | INFO | end detected at 31
2026-01-29 17:47:07,066 | INFO |  -6.50 * 0.5 =  -3.25 for decoder
2026-01-29 17:47:07,066 | INFO |  -3.07 * 0.5 =  -1.54 for ctc
2026-01-29 17:47:07,066 | INFO | total log probability: -4.79
2026-01-29 17:47:07,066 | INFO | normalized log probability: -0.21
2026-01-29 17:47:07,066 | INFO | total number of ended hypotheses: 191
2026-01-29 17:47:07,067 | INFO | best hypo: ▁mais▁ce▁soir▁ce▁qui▁importe▁c'est▁l'avenir▁notre

2026-01-29 17:47:07,068 | INFO | speech length: 18240
2026-01-29 17:47:07,095 | INFO | decoder input length: 28
2026-01-29 17:47:07,095 | INFO | max output length: 28
2026-01-29 17:47:07,095 | INFO | min output length: 2
2026-01-29 17:47:07,624 | INFO | end detected at 16
2026-01-29 17:47:07,625 | INFO |  -0.83 * 0.5 =  -0.41 for decoder
2026-01-29 17:47:07,625 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:47:07,625 | INFO | total log probability: -0.42
2026-01-29 17:47:07,625 | INFO | normalized log probability: -0.03
2026-01-29 17:47:07,625 | INFO | total number of ended hypotheses: 129
2026-01-29 17:47:07,625 | INFO | best hypo: ▁celui▁de▁nos▁enfants

2026-01-29 17:47:07,626 | INFO | speech length: 50560
2026-01-29 17:47:07,654 | INFO | decoder input length: 78
2026-01-29 17:47:07,654 | INFO | max output length: 78
2026-01-29 17:47:07,654 | INFO | min output length: 7
2026-01-29 17:47:08,702 | INFO | end detected at 28
2026-01-29 17:47:08,704 | INFO |  -1.90 * 0.5 =  -0.95 for decoder
2026-01-29 17:47:08,704 | INFO |  -0.24 * 0.5 =  -0.12 for ctc
2026-01-29 17:47:08,704 | INFO | total log probability: -1.07
2026-01-29 17:47:08,704 | INFO | normalized log probability: -0.04
2026-01-29 17:47:08,704 | INFO | total number of ended hypotheses: 149
2026-01-29 17:47:08,705 | INFO | best hypo: ▁le▁progrès▁va▁se▁poursuivre▁avec▁ses▁hésitations

2026-01-29 17:47:08,706 | INFO | speech length: 106240
2026-01-29 17:47:08,733 | INFO | decoder input length: 165
2026-01-29 17:47:08,733 | INFO | max output length: 165
2026-01-29 17:47:08,733 | INFO | min output length: 16
2026-01-29 17:47:11,075 | INFO | end detected at 48
2026-01-29 17:47:11,075 | INFO |  -4.30 * 0.5 =  -2.15 for decoder
2026-01-29 17:47:11,075 | INFO |  -4.15 * 0.5 =  -2.08 for ctc
2026-01-29 17:47:11,075 | INFO | total log probability: -4.22
2026-01-29 17:47:11,075 | INFO | normalized log probability: -0.10
2026-01-29 17:47:11,075 | INFO | total number of ended hypotheses: 152
2026-01-29 17:47:11,076 | INFO | best hypo: ▁avec▁ses▁limites▁que▁nous▁mesurons▁bien▁face▁aux▁événements▁récents▁qui▁nous▁invitent▁à▁l'humilité

2026-01-29 17:47:11,077 | INFO | speech length: 24480
2026-01-29 17:47:11,109 | INFO | decoder input length: 37
2026-01-29 17:47:11,109 | INFO | max output length: 37
2026-01-29 17:47:11,109 | INFO | min output length: 3
2026-01-29 17:47:11,686 | INFO | end detected at 16
2026-01-29 17:47:11,687 | INFO |  -0.79 * 0.5 =  -0.39 for decoder
2026-01-29 17:47:11,687 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-29 17:47:11,687 | INFO | total log probability: -0.40
2026-01-29 17:47:11,687 | INFO | normalized log probability: -0.03
2026-01-29 17:47:11,687 | INFO | total number of ended hypotheses: 143
2026-01-29 17:47:11,687 | INFO | best hypo: ▁progrès▁de▁la▁science

2026-01-29 17:47:11,688 | INFO | speech length: 37280
2026-01-29 17:47:11,719 | INFO | decoder input length: 57
2026-01-29 17:47:11,719 | INFO | max output length: 57
2026-01-29 17:47:11,719 | INFO | min output length: 5
2026-01-29 17:47:12,591 | INFO | end detected at 25
2026-01-29 17:47:12,593 | INFO |  -8.27 * 0.5 =  -4.13 for decoder
2026-01-29 17:47:12,593 | INFO |  -0.31 * 0.5 =  -0.15 for ctc
2026-01-29 17:47:12,593 | INFO | total log probability: -4.29
2026-01-29 17:47:12,593 | INFO | normalized log probability: -0.20
2026-01-29 17:47:12,593 | INFO | total number of ended hypotheses: 143
2026-01-29 17:47:12,593 | INFO | best hypo: ▁pro▁créer▁des▁communications▁entre▁les▁hommes

2026-01-29 17:47:12,594 | INFO | speech length: 16960
2026-01-29 17:47:12,624 | INFO | decoder input length: 26
2026-01-29 17:47:12,625 | INFO | max output length: 26
2026-01-29 17:47:12,625 | INFO | min output length: 2
2026-01-29 17:47:13,182 | INFO | end detected at 17
2026-01-29 17:47:13,183 | INFO |  -2.60 * 0.5 =  -1.30 for decoder
2026-01-29 17:47:13,183 | INFO |  -6.84 * 0.5 =  -3.42 for ctc
2026-01-29 17:47:13,183 | INFO | total log probability: -4.72
2026-01-29 17:47:13,183 | INFO | normalized log probability: -0.34
2026-01-29 17:47:13,183 | INFO | total number of ended hypotheses: 145
2026-01-29 17:47:13,183 | INFO | best hypo: ▁on▁grève▁de▁la▁médecine

2026-01-29 17:47:13,184 | INFO | speech length: 97280
2026-01-29 17:47:13,217 | INFO | decoder input length: 151
2026-01-29 17:47:13,217 | INFO | max output length: 151
2026-01-29 17:47:13,218 | INFO | min output length: 15
2026-01-29 17:47:15,034 | INFO | end detected at 40
2026-01-29 17:47:15,035 | INFO |  -3.23 * 0.5 =  -1.62 for decoder
2026-01-29 17:47:15,035 | INFO |  -1.53 * 0.5 =  -0.77 for ctc
2026-01-29 17:47:15,036 | INFO | total log probability: -2.38
2026-01-29 17:47:15,036 | INFO | normalized log probability: -0.07
2026-01-29 17:47:15,036 | INFO | total number of ended hypotheses: 164
2026-01-29 17:47:15,036 | INFO | best hypo: ▁un▁grand▁nombre▁des▁enfants▁qui▁vont▁naître▁cette▁année▁verront▁l'an▁deux▁mille▁cent

2026-01-29 17:47:15,038 | INFO | speech length: 17920
2026-01-29 17:47:15,068 | INFO | decoder input length: 27
2026-01-29 17:47:15,069 | INFO | max output length: 27
2026-01-29 17:47:15,069 | INFO | min output length: 2
2026-01-29 17:47:15,524 | INFO | end detected at 14
2026-01-29 17:47:15,525 | INFO |  -0.84 * 0.5 =  -0.42 for decoder
2026-01-29 17:47:15,525 | INFO |  -0.23 * 0.5 =  -0.11 for ctc
2026-01-29 17:47:15,525 | INFO | total log probability: -0.53
2026-01-29 17:47:15,525 | INFO | normalized log probability: -0.05
2026-01-29 17:47:15,525 | INFO | total number of ended hypotheses: 146
2026-01-29 17:47:15,525 | INFO | best hypo: ▁c'est▁progrès

2026-01-29 17:47:15,526 | INFO | speech length: 26080
2026-01-29 17:47:15,554 | INFO | decoder input length: 40
2026-01-29 17:47:15,554 | INFO | max output length: 40
2026-01-29 17:47:15,554 | INFO | min output length: 4
2026-01-29 17:47:16,201 | INFO | end detected at 19
2026-01-29 17:47:16,202 | INFO |  -2.41 * 0.5 =  -1.21 for decoder
2026-01-29 17:47:16,202 | INFO |  -3.15 * 0.5 =  -1.58 for ctc
2026-01-29 17:47:16,202 | INFO | total log probability: -2.78
2026-01-29 17:47:16,202 | INFO | normalized log probability: -0.21
2026-01-29 17:47:16,202 | INFO | total number of ended hypotheses: 167
2026-01-29 17:47:16,203 | INFO | best hypo: ▁ne▁prendront▁tout▁leur▁sang

2026-01-29 17:47:16,204 | INFO | speech length: 27520
2026-01-29 17:47:16,236 | INFO | decoder input length: 42
2026-01-29 17:47:16,236 | INFO | max output length: 42
2026-01-29 17:47:16,236 | INFO | min output length: 4
2026-01-29 17:47:17,010 | INFO | end detected at 23
2026-01-29 17:47:17,011 | INFO |  -1.68 * 0.5 =  -0.84 for decoder
2026-01-29 17:47:17,011 | INFO |  -2.16 * 0.5 =  -1.08 for ctc
2026-01-29 17:47:17,011 | INFO | total log probability: -1.92
2026-01-29 17:47:17,011 | INFO | normalized log probability: -0.10
2026-01-29 17:47:17,011 | INFO | total number of ended hypotheses: 163
2026-01-29 17:47:17,012 | INFO | best hypo: ▁que▁s'ils▁bénéficient▁à▁l'homme

2026-01-29 17:47:17,013 | INFO | speech length: 14720
2026-01-29 17:47:17,047 | INFO | decoder input length: 22
2026-01-29 17:47:17,047 | INFO | max output length: 22
2026-01-29 17:47:17,047 | INFO | min output length: 2
2026-01-29 17:47:17,578 | INFO | end detected at 15
2026-01-29 17:47:17,580 | INFO |  -4.66 * 0.5 =  -2.33 for decoder
2026-01-29 17:47:17,580 | INFO |  -5.15 * 0.5 =  -2.57 for ctc
2026-01-29 17:47:17,580 | INFO | total log probability: -4.90
2026-01-29 17:47:17,580 | INFO | normalized log probability: -0.54
2026-01-29 17:47:17,580 | INFO | total number of ended hypotheses: 180
2026-01-29 17:47:17,580 | INFO | best hypo: ▁a▁tous▁les▁ans

2026-01-29 17:47:17,582 | INFO | speech length: 67520
2026-01-29 17:47:17,612 | INFO | decoder input length: 105
2026-01-29 17:47:17,612 | INFO | max output length: 105
2026-01-29 17:47:17,612 | INFO | min output length: 10
2026-01-29 17:47:18,866 | INFO | end detected at 31
2026-01-29 17:47:18,867 | INFO |  -1.91 * 0.5 =  -0.95 for decoder
2026-01-29 17:47:18,867 | INFO |  -1.28 * 0.5 =  -0.64 for ctc
2026-01-29 17:47:18,867 | INFO | total log probability: -1.59
2026-01-29 17:47:18,867 | INFO | normalized log probability: -0.06
2026-01-29 17:47:18,867 | INFO | total number of ended hypotheses: 143
2026-01-29 17:47:18,867 | INFO | best hypo: ▁le▁vingt▁et▁unième▁siècle▁doit▁être▁le▁siècle▁de▁l'éthique

2026-01-29 17:47:18,869 | INFO | speech length: 57440
2026-01-29 17:47:18,898 | INFO | decoder input length: 89
2026-01-29 17:47:18,898 | INFO | max output length: 89
2026-01-29 17:47:18,898 | INFO | min output length: 8
2026-01-29 17:47:20,423 | INFO | end detected at 41
2026-01-29 17:47:20,423 | INFO |  -4.26 * 0.5 =  -2.13 for decoder
2026-01-29 17:47:20,423 | INFO |  -1.09 * 0.5 =  -0.54 for ctc
2026-01-29 17:47:20,424 | INFO | total log probability: -2.67
2026-01-29 17:47:20,424 | INFO | normalized log probability: -0.07
2026-01-29 17:47:20,424 | INFO | total number of ended hypotheses: 155
2026-01-29 17:47:20,424 | INFO | best hypo: ▁je▁sais▁que▁bien▁les▁tragédies▁aujourd'hui▁font▁douter▁de▁cet▁espérance

2026-01-29 17:47:20,426 | INFO | speech length: 108960
2026-01-29 17:47:20,458 | INFO | decoder input length: 169
2026-01-29 17:47:20,459 | INFO | max output length: 169
2026-01-29 17:47:20,459 | INFO | min output length: 16
2026-01-29 17:47:22,517 | INFO | end detected at 45
2026-01-29 17:47:22,518 | INFO |  -3.03 * 0.5 =  -1.52 for decoder
2026-01-29 17:47:22,518 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:47:22,518 | INFO | total log probability: -1.52
2026-01-29 17:47:22,518 | INFO | normalized log probability: -0.04
2026-01-29 17:47:22,518 | INFO | total number of ended hypotheses: 149
2026-01-29 17:47:22,519 | INFO | best hypo: ▁pourtant▁de▁plus▁en▁plus▁les▁nations▁s'accordent▁pour▁mieux▁faire▁respecter▁les▁droits▁de▁l'homme

2026-01-29 17:47:22,520 | INFO | speech length: 59520
2026-01-29 17:47:22,549 | INFO | decoder input length: 92
2026-01-29 17:47:22,550 | INFO | max output length: 92
2026-01-29 17:47:22,550 | INFO | min output length: 9
2026-01-29 17:47:23,589 | INFO | end detected at 27
2026-01-29 17:47:23,590 | INFO |  -1.55 * 0.5 =  -0.77 for decoder
2026-01-29 17:47:23,590 | INFO |  -0.13 * 0.5 =  -0.06 for ctc
2026-01-29 17:47:23,590 | INFO | total log probability: -0.84
2026-01-29 17:47:23,590 | INFO | normalized log probability: -0.04
2026-01-29 17:47:23,590 | INFO | total number of ended hypotheses: 144
2026-01-29 17:47:23,590 | INFO | best hypo: ▁pour▁défendre▁la▁liberté▁et▁la▁dignité▁humaine

2026-01-29 17:47:23,592 | INFO | speech length: 50720
2026-01-29 17:47:23,618 | INFO | decoder input length: 78
2026-01-29 17:47:23,618 | INFO | max output length: 78
2026-01-29 17:47:23,618 | INFO | min output length: 7
2026-01-29 17:47:24,624 | INFO | end detected at 27
2026-01-29 17:47:24,625 | INFO |  -1.73 * 0.5 =  -0.87 for decoder
2026-01-29 17:47:24,625 | INFO |  -0.17 * 0.5 =  -0.09 for ctc
2026-01-29 17:47:24,625 | INFO | total log probability: -0.95
2026-01-29 17:47:24,625 | INFO | normalized log probability: -0.04
2026-01-29 17:47:24,625 | INFO | total number of ended hypotheses: 147
2026-01-29 17:47:24,625 | INFO | best hypo: ▁un▁nouvel▁ordre▁international▁s'affirme▁peu▁à▁peu

2026-01-29 17:47:24,627 | INFO | speech length: 100000
2026-01-29 17:47:24,660 | INFO | decoder input length: 155
2026-01-29 17:47:24,660 | INFO | max output length: 155
2026-01-29 17:47:24,660 | INFO | min output length: 15
2026-01-29 17:47:26,285 | INFO | end detected at 36
2026-01-29 17:47:26,286 | INFO |  -3.73 * 0.5 =  -1.87 for decoder
2026-01-29 17:47:26,286 | INFO |  -0.45 * 0.5 =  -0.23 for ctc
2026-01-29 17:47:26,286 | INFO | total log probability: -2.09
2026-01-29 17:47:26,286 | INFO | normalized log probability: -0.07
2026-01-29 17:47:26,286 | INFO | total number of ended hypotheses: 149
2026-01-29 17:47:26,286 | INFO | best hypo: ▁demain▁il▁ne▁devra▁plus▁y▁avoir▁de▁repos▁pour▁les▁criminels▁contre▁l'humanite

2026-01-29 17:47:26,288 | INFO | speech length: 16800
2026-01-29 17:47:26,320 | INFO | decoder input length: 25
2026-01-29 17:47:26,320 | INFO | max output length: 25
2026-01-29 17:47:26,320 | INFO | min output length: 2
2026-01-29 17:47:26,810 | INFO | end detected at 15
2026-01-29 17:47:26,811 | INFO |  -3.45 * 0.5 =  -1.72 for decoder
2026-01-29 17:47:26,811 | INFO |  -1.76 * 0.5 =  -0.88 for ctc
2026-01-29 17:47:26,812 | INFO | total log probability: -2.61
2026-01-29 17:47:26,812 | INFO | normalized log probability: -0.29
2026-01-29 17:47:26,812 | INFO | total number of ended hypotheses: 182
2026-01-29 17:47:26,812 | INFO | best hypo: ▁sur▁au▁nom▁de▁la▁france

2026-01-29 17:47:26,813 | INFO | speech length: 14720
2026-01-29 17:47:26,839 | INFO | decoder input length: 22
2026-01-29 17:47:26,839 | INFO | max output length: 22
2026-01-29 17:47:26,839 | INFO | min output length: 2
2026-01-29 17:47:27,177 | INFO | end detected at 10
2026-01-29 17:47:27,178 | INFO |  -1.93 * 0.5 =  -0.96 for decoder
2026-01-29 17:47:27,178 | INFO |  -1.01 * 0.5 =  -0.50 for ctc
2026-01-29 17:47:27,178 | INFO | total log probability: -1.47
2026-01-29 17:47:27,178 | INFO | normalized log probability: -0.24
2026-01-29 17:47:27,178 | INFO | total number of ended hypotheses: 151
2026-01-29 17:47:27,178 | INFO | best hypo: ▁en▁votre▁nom

2026-01-29 17:47:27,180 | INFO | speech length: 44160
2026-01-29 17:47:27,206 | INFO | decoder input length: 68
2026-01-29 17:47:27,206 | INFO | max output length: 68
2026-01-29 17:47:27,206 | INFO | min output length: 6
2026-01-29 17:47:28,157 | INFO | end detected at 26
2026-01-29 17:47:28,158 | INFO |  -1.50 * 0.5 =  -0.75 for decoder
2026-01-29 17:47:28,158 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-29 17:47:28,158 | INFO | total log probability: -0.75
2026-01-29 17:47:28,158 | INFO | normalized log probability: -0.03
2026-01-29 17:47:28,158 | INFO | total number of ended hypotheses: 148
2026-01-29 17:47:28,159 | INFO | best hypo: ▁c'est▁le▁combat▁difficile▁que▁je▁mène▁chaque▁jour

2026-01-29 17:47:28,160 | INFO | speech length: 32160
2026-01-29 17:47:28,187 | INFO | decoder input length: 49
2026-01-29 17:47:28,188 | INFO | max output length: 49
2026-01-29 17:47:28,188 | INFO | min output length: 4
2026-01-29 17:47:28,863 | INFO | end detected at 18
2026-01-29 17:47:28,864 | INFO |  -1.04 * 0.5 =  -0.52 for decoder
2026-01-29 17:47:28,864 | INFO |  -0.10 * 0.5 =  -0.05 for ctc
2026-01-29 17:47:28,864 | INFO | total log probability: -0.57
2026-01-29 17:47:28,864 | INFO | normalized log probability: -0.04
2026-01-29 17:47:28,864 | INFO | total number of ended hypotheses: 141
2026-01-29 17:47:28,865 | INFO | best hypo: ▁a▁l'intérieur▁de▁chaque▁nation

2026-01-29 17:47:28,866 | INFO | speech length: 32480
2026-01-29 17:47:28,896 | INFO | decoder input length: 50
2026-01-29 17:47:28,896 | INFO | max output length: 50
2026-01-29 17:47:28,896 | INFO | min output length: 5
2026-01-29 17:47:29,503 | INFO | end detected at 17
2026-01-29 17:47:29,504 | INFO |  -5.76 * 0.5 =  -2.88 for decoder
2026-01-29 17:47:29,504 | INFO |  -3.50 * 0.5 =  -1.75 for ctc
2026-01-29 17:47:29,504 | INFO | total log probability: -4.63
2026-01-29 17:47:29,504 | INFO | normalized log probability: -0.42
2026-01-29 17:47:29,504 | INFO | total number of ended hypotheses: 164
2026-01-29 17:47:29,505 | INFO | best hypo: ▁une▁exigence▁se▁fait▁entre

2026-01-29 17:47:29,506 | INFO | speech length: 15360
2026-01-29 17:47:29,533 | INFO | decoder input length: 23
2026-01-29 17:47:29,534 | INFO | max output length: 23
2026-01-29 17:47:29,534 | INFO | min output length: 2
2026-01-29 17:47:30,025 | INFO | end detected at 15
2026-01-29 17:47:30,026 | INFO |  -0.84 * 0.5 =  -0.42 for decoder
2026-01-29 17:47:30,026 | INFO |  -1.10 * 0.5 =  -0.55 for ctc
2026-01-29 17:47:30,026 | INFO | total log probability: -0.97
2026-01-29 17:47:30,026 | INFO | normalized log probability: -0.09
2026-01-29 17:47:30,026 | INFO | total number of ended hypotheses: 154
2026-01-29 17:47:30,026 | INFO | best hypo: ▁toujours▁plus▁fort

2026-01-29 17:47:30,028 | INFO | speech length: 117280
2026-01-29 17:47:30,060 | INFO | decoder input length: 182
2026-01-29 17:47:30,060 | INFO | max output length: 182
2026-01-29 17:47:30,060 | INFO | min output length: 18
2026-01-29 17:47:32,619 | INFO | end detected at 55
2026-01-29 17:47:32,620 | INFO |  -3.88 * 0.5 =  -1.94 for decoder
2026-01-29 17:47:32,620 | INFO |  -0.80 * 0.5 =  -0.40 for ctc
2026-01-29 17:47:32,620 | INFO | total log probability: -2.34
2026-01-29 17:47:32,620 | INFO | normalized log probability: -0.05
2026-01-29 17:47:32,620 | INFO | total number of ended hypotheses: 150
2026-01-29 17:47:32,621 | INFO | best hypo: ▁pour▁que▁les▁avancées▁de▁la▁science▁soient▁orientées▁vers▁le▁bien▁de▁l'homme▁et▁ne▁se▁retournent▁jamais▁contre▁lui

2026-01-29 17:47:32,622 | INFO | speech length: 29920
2026-01-29 17:47:32,650 | INFO | decoder input length: 46
2026-01-29 17:47:32,650 | INFO | max output length: 46
2026-01-29 17:47:32,650 | INFO | min output length: 4
2026-01-29 17:47:33,142 | INFO | end detected at 14
2026-01-29 17:47:33,142 | INFO |  -0.89 * 0.5 =  -0.44 for decoder
2026-01-29 17:47:33,142 | INFO |  -1.87 * 0.5 =  -0.94 for ctc
2026-01-29 17:47:33,143 | INFO | total log probability: -1.38
2026-01-29 17:47:33,143 | INFO | normalized log probability: -0.14
2026-01-29 17:47:33,143 | INFO | total number of ended hypotheses: 144
2026-01-29 17:47:33,143 | INFO | best hypo: ▁je▁pense▁par▁exemple

2026-01-29 17:47:33,144 | INFO | speech length: 46080
2026-01-29 17:47:33,170 | INFO | decoder input length: 71
2026-01-29 17:47:33,170 | INFO | max output length: 71
2026-01-29 17:47:33,170 | INFO | min output length: 7
2026-01-29 17:47:34,080 | INFO | end detected at 25
2026-01-29 17:47:34,082 | INFO |  -3.26 * 0.5 =  -1.63 for decoder
2026-01-29 17:47:34,082 | INFO |  -5.70 * 0.5 =  -2.85 for ctc
2026-01-29 17:47:34,082 | INFO | total log probability: -4.48
2026-01-29 17:47:34,082 | INFO | normalized log probability: -0.22
2026-01-29 17:47:34,082 | INFO | total number of ended hypotheses: 167
2026-01-29 17:47:34,082 | INFO | best hypo: ▁aux▁manipulations▁génétiques▁au▁clonage

2026-01-29 17:47:34,084 | INFO | speech length: 27840
2026-01-29 17:47:34,112 | INFO | decoder input length: 43
2026-01-29 17:47:34,112 | INFO | max output length: 43
2026-01-29 17:47:34,112 | INFO | min output length: 4
2026-01-29 17:47:34,862 | INFO | end detected at 21
2026-01-29 17:47:34,863 | INFO |  -1.16 * 0.5 =  -0.58 for decoder
2026-01-29 17:47:34,863 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-29 17:47:34,863 | INFO | total log probability: -0.58
2026-01-29 17:47:34,863 | INFO | normalized log probability: -0.03
2026-01-29 17:47:34,863 | INFO | total number of ended hypotheses: 138
2026-01-29 17:47:34,863 | INFO | best hypo: ▁de▁même▁dans▁le▁domaine▁de▁l'environnement

2026-01-29 17:47:34,864 | INFO | speech length: 96320
2026-01-29 17:47:34,896 | INFO | decoder input length: 150
2026-01-29 17:47:34,896 | INFO | max output length: 150
2026-01-29 17:47:34,896 | INFO | min output length: 15
2026-01-29 17:47:36,747 | INFO | end detected at 41
2026-01-29 17:47:36,748 | INFO |  -2.91 * 0.5 =  -1.45 for decoder
2026-01-29 17:47:36,748 | INFO |  -0.16 * 0.5 =  -0.08 for ctc
2026-01-29 17:47:36,748 | INFO | total log probability: -1.53
2026-01-29 17:47:36,748 | INFO | normalized log probability: -0.04
2026-01-29 17:47:36,748 | INFO | total number of ended hypotheses: 152
2026-01-29 17:47:36,748 | INFO | best hypo: ▁les▁peuples▁ne▁veulent▁plus▁que▁la▁course▁à▁la▁productivité▁épuise▁la▁planète

2026-01-29 17:47:36,750 | INFO | speech length: 92800
2026-01-29 17:47:36,776 | INFO | decoder input length: 144
2026-01-29 17:47:36,776 | INFO | max output length: 144
2026-01-29 17:47:36,776 | INFO | min output length: 14
2026-01-29 17:47:38,510 | INFO | end detected at 40
2026-01-29 17:47:38,511 | INFO |  -2.70 * 0.5 =  -1.35 for decoder
2026-01-29 17:47:38,511 | INFO |  -0.17 * 0.5 =  -0.09 for ctc
2026-01-29 17:47:38,511 | INFO | total log probability: -1.44
2026-01-29 17:47:38,511 | INFO | normalized log probability: -0.04
2026-01-29 17:47:38,511 | INFO | total number of ended hypotheses: 149
2026-01-29 17:47:38,512 | INFO | best hypo: ▁la▁responsabilité▁de▁tous▁ceux▁qui▁dans▁le▁monde▁dégradent▁le▁patrimoine▁naturel

2026-01-29 17:47:38,513 | INFO | speech length: 41280
2026-01-29 17:47:38,545 | INFO | decoder input length: 64
2026-01-29 17:47:38,545 | INFO | max output length: 64
2026-01-29 17:47:38,545 | INFO | min output length: 6
2026-01-29 17:47:39,345 | INFO | end detected at 21
2026-01-29 17:47:39,346 | INFO |  -1.15 * 0.5 =  -0.57 for decoder
2026-01-29 17:47:39,346 | INFO |  -0.29 * 0.5 =  -0.14 for ctc
2026-01-29 17:47:39,346 | INFO | total log probability: -0.72
2026-01-29 17:47:39,346 | INFO | normalized log probability: -0.04
2026-01-29 17:47:39,346 | INFO | total number of ended hypotheses: 162
2026-01-29 17:47:39,347 | INFO | best hypo: ▁doit▁être▁recherché▁et▁sanctionné

2026-01-29 17:47:39,348 | INFO | speech length: 68800
2026-01-29 17:47:39,379 | INFO | decoder input length: 107
2026-01-29 17:47:39,380 | INFO | max output length: 107
2026-01-29 17:47:39,380 | INFO | min output length: 10
2026-01-29 17:47:40,708 | INFO | end detected at 33
2026-01-29 17:47:40,709 | INFO |  -9.24 * 0.5 =  -4.62 for decoder
2026-01-29 17:47:40,709 | INFO |  -0.81 * 0.5 =  -0.41 for ctc
2026-01-29 17:47:40,709 | INFO | total log probability: -5.03
2026-01-29 17:47:40,709 | INFO | normalized log probability: -0.19
2026-01-29 17:47:40,709 | INFO | total number of ended hypotheses: 179
2026-01-29 17:47:40,710 | INFO | best hypo: ▁car▁il▁s'agit▁du▁patrimoine▁que▁nous▁lèguerons▁à▁nos▁ens

2026-01-29 17:47:40,711 | INFO | speech length: 54720
2026-01-29 17:47:40,738 | INFO | decoder input length: 85
2026-01-29 17:47:40,738 | INFO | max output length: 85
2026-01-29 17:47:40,738 | INFO | min output length: 8
2026-01-29 17:47:41,739 | INFO | end detected at 26
2026-01-29 17:47:41,739 | INFO |  -1.57 * 0.5 =  -0.78 for decoder
2026-01-29 17:47:41,739 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:47:41,739 | INFO | total log probability: -0.79
2026-01-29 17:47:41,739 | INFO | normalized log probability: -0.04
2026-01-29 17:47:41,739 | INFO | total number of ended hypotheses: 138
2026-01-29 17:47:41,740 | INFO | best hypo: ▁même▁si▁le▁monde▁change▁comme▁il▁n'a▁jamais▁changé

2026-01-29 17:47:41,741 | INFO | speech length: 42560
2026-01-29 17:47:41,770 | INFO | decoder input length: 66
2026-01-29 17:47:41,771 | INFO | max output length: 66
2026-01-29 17:47:41,771 | INFO | min output length: 6
2026-01-29 17:47:42,717 | INFO | end detected at 21
2026-01-29 17:47:42,718 | INFO |  -1.19 * 0.5 =  -0.59 for decoder
2026-01-29 17:47:42,718 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:47:42,718 | INFO | total log probability: -0.59
2026-01-29 17:47:42,718 | INFO | normalized log probability: -0.03
2026-01-29 17:47:42,718 | INFO | total number of ended hypotheses: 152
2026-01-29 17:47:42,718 | INFO | best hypo: ▁la▁modernité▁ne▁doit▁pas▁nous▁diviser

2026-01-29 17:47:42,720 | INFO | speech length: 36800
2026-01-29 17:47:42,749 | INFO | decoder input length: 57
2026-01-29 17:47:42,749 | INFO | max output length: 57
2026-01-29 17:47:42,749 | INFO | min output length: 5
2026-01-29 17:47:43,368 | INFO | end detected at 17
2026-01-29 17:47:43,369 | INFO |  -0.92 * 0.5 =  -0.46 for decoder
2026-01-29 17:47:43,369 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-29 17:47:43,369 | INFO | total log probability: -0.46
2026-01-29 17:47:43,369 | INFO | normalized log probability: -0.04
2026-01-29 17:47:43,369 | INFO | total number of ended hypotheses: 141
2026-01-29 17:47:43,369 | INFO | best hypo: ▁elle▁doit▁profiter▁à▁chacun

2026-01-29 17:47:43,371 | INFO | speech length: 113440
2026-01-29 17:47:43,403 | INFO | decoder input length: 176
2026-01-29 17:47:43,403 | INFO | max output length: 176
2026-01-29 17:47:43,403 | INFO | min output length: 17
2026-01-29 17:47:45,884 | INFO | end detected at 54
2026-01-29 17:47:45,885 | INFO |  -7.23 * 0.5 =  -3.61 for decoder
2026-01-29 17:47:45,885 | INFO |  -6.98 * 0.5 =  -3.49 for ctc
2026-01-29 17:47:45,885 | INFO | total log probability: -7.11
2026-01-29 17:47:45,885 | INFO | normalized log probability: -0.15
2026-01-29 17:47:45,885 | INFO | total number of ended hypotheses: 173
2026-01-29 17:47:45,886 | INFO | best hypo: ▁nous▁réussirons▁nous▁réussirons▁parce▁que▁nous▁avons▁pris▁des▁décisions▁qui▁engagent▁et▁qui▁garantissent▁nos▁travail

2026-01-29 17:47:45,888 | INFO | speech length: 56960
2026-01-29 17:47:45,915 | INFO | decoder input length: 88
2026-01-29 17:47:45,915 | INFO | max output length: 88
2026-01-29 17:47:45,915 | INFO | min output length: 8
2026-01-29 17:47:47,187 | INFO | end detected at 34
2026-01-29 17:47:47,188 | INFO |  -2.08 * 0.5 =  -1.04 for decoder
2026-01-29 17:47:47,188 | INFO |  -0.04 * 0.5 =  -0.02 for ctc
2026-01-29 17:47:47,188 | INFO | total log probability: -1.06
2026-01-29 17:47:47,188 | INFO | normalized log probability: -0.04
2026-01-29 17:47:47,188 | INFO | total number of ended hypotheses: 140
2026-01-29 17:47:47,189 | INFO | best hypo: ▁nous▁avons▁choisi▁ensemble▁de▁faire▁grandir▁la▁france▁dans▁l'europe

2026-01-29 17:47:47,190 | INFO | speech length: 101280
2026-01-29 17:47:47,219 | INFO | decoder input length: 157
2026-01-29 17:47:47,219 | INFO | max output length: 157
2026-01-29 17:47:47,219 | INFO | min output length: 15
2026-01-29 17:47:49,302 | INFO | end detected at 47
2026-01-29 17:47:49,303 | INFO |  -3.85 * 0.5 =  -1.93 for decoder
2026-01-29 17:47:49,303 | INFO |  -0.87 * 0.5 =  -0.44 for ctc
2026-01-29 17:47:49,303 | INFO | total log probability: -2.36
2026-01-29 17:47:49,303 | INFO | normalized log probability: -0.05
2026-01-29 17:47:49,303 | INFO | total number of ended hypotheses: 152
2026-01-29 17:47:49,304 | INFO | best hypo: ▁une▁europe▁qui▁nous▁garantit▁la▁paix▁une▁europe▁qui▁nous▁permet▁de▁peser▁davantage▁dans▁le▁monde

2026-01-29 17:47:49,305 | INFO | speech length: 105600
2026-01-29 17:47:49,338 | INFO | decoder input length: 164
2026-01-29 17:47:49,338 | INFO | max output length: 164
2026-01-29 17:47:49,338 | INFO | min output length: 16
2026-01-29 17:47:51,264 | INFO | end detected at 42
2026-01-29 17:47:51,264 | INFO |  -2.57 * 0.5 =  -1.29 for decoder
2026-01-29 17:47:51,264 | INFO |  -0.06 * 0.5 =  -0.03 for ctc
2026-01-29 17:47:51,264 | INFO | total log probability: -1.32
2026-01-29 17:47:51,264 | INFO | normalized log probability: -0.03
2026-01-29 17:47:51,264 | INFO | total number of ended hypotheses: 145
2026-01-29 17:47:51,265 | INFO | best hypo: ▁nous▁avons▁choisi▁aussi▁de▁prendre▁part▁à▁la▁mondialisation▁d'en▁prendre▁toute▁notre▁part

2026-01-29 17:47:51,266 | INFO | speech length: 92640
2026-01-29 17:47:51,297 | INFO | decoder input length: 144
2026-01-29 17:47:51,297 | INFO | max output length: 144
2026-01-29 17:47:51,297 | INFO | min output length: 14
2026-01-29 17:47:53,066 | INFO | end detected at 41
2026-01-29 17:47:53,066 | INFO |  -2.65 * 0.5 =  -1.32 for decoder
2026-01-29 17:47:53,067 | INFO |  -0.09 * 0.5 =  -0.05 for ctc
2026-01-29 17:47:53,067 | INFO | total log probability: -1.37
2026-01-29 17:47:53,067 | INFO | normalized log probability: -0.04
2026-01-29 17:47:53,067 | INFO | total number of ended hypotheses: 143
2026-01-29 17:47:53,067 | INFO | best hypo: ▁mais▁une▁mondialisation▁maîtrisée▁organisée▁respectueuse▁de▁l'environnement

2026-01-29 17:47:53,069 | INFO | speech length: 67840
2026-01-29 17:47:53,098 | INFO | decoder input length: 105
2026-01-29 17:47:53,098 | INFO | max output length: 105
2026-01-29 17:47:53,098 | INFO | min output length: 10
2026-01-29 17:47:54,343 | INFO | end detected at 31
2026-01-29 17:47:54,344 | INFO |  -1.88 * 0.5 =  -0.94 for decoder
2026-01-29 17:47:54,344 | INFO |  -0.27 * 0.5 =  -0.14 for ctc
2026-01-29 17:47:54,344 | INFO | total log probability: -1.07
2026-01-29 17:47:54,345 | INFO | normalized log probability: -0.04
2026-01-29 17:47:54,345 | INFO | total number of ended hypotheses: 160
2026-01-29 17:47:54,345 | INFO | best hypo: ▁capable▁de▁prendre▁en▁compte▁les▁aspirations▁des▁hommes

2026-01-29 17:47:54,347 | INFO | speech length: 34560
2026-01-29 17:47:54,376 | INFO | decoder input length: 53
2026-01-29 17:47:54,376 | INFO | max output length: 53
2026-01-29 17:47:54,376 | INFO | min output length: 5
2026-01-29 17:47:55,148 | INFO | end detected at 22
2026-01-29 17:47:55,149 | INFO |  -5.97 * 0.5 =  -2.98 for decoder
2026-01-29 17:47:55,149 | INFO |  -0.55 * 0.5 =  -0.27 for ctc
2026-01-29 17:47:55,149 | INFO | total log probability: -3.26
2026-01-29 17:47:55,149 | INFO | normalized log probability: -0.19
2026-01-29 17:47:55,149 | INFO | total number of ended hypotheses: 145
2026-01-29 17:47:55,149 | INFO | best hypo: ▁il▁est▁capable▁de▁faire▁reculer▁la▁pauvre

2026-01-29 17:47:55,150 | INFO | speech length: 52000
2026-01-29 17:47:55,183 | INFO | decoder input length: 80
2026-01-29 17:47:55,183 | INFO | max output length: 80
2026-01-29 17:47:55,183 | INFO | min output length: 8
2026-01-29 17:47:55,994 | INFO | end detected at 21
2026-01-29 17:47:55,994 | INFO |  -1.16 * 0.5 =  -0.58 for decoder
2026-01-29 17:47:55,994 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-29 17:47:55,994 | INFO | total log probability: -0.58
2026-01-29 17:47:55,994 | INFO | normalized log probability: -0.03
2026-01-29 17:47:55,994 | INFO | total number of ended hypotheses: 137
2026-01-29 17:47:55,995 | INFO | best hypo: ▁ce▁sera▁tout▁le▁sens▁du▁combat▁de▁la▁france

2026-01-29 17:47:55,996 | INFO | speech length: 21600
2026-01-29 17:47:56,025 | INFO | decoder input length: 33
2026-01-29 17:47:56,026 | INFO | max output length: 33
2026-01-29 17:47:56,026 | INFO | min output length: 3
2026-01-29 17:47:56,724 | INFO | end detected at 21
2026-01-29 17:47:56,725 | INFO |  -3.52 * 0.5 =  -1.76 for decoder
2026-01-29 17:47:56,725 | INFO |  -5.38 * 0.5 =  -2.69 for ctc
2026-01-29 17:47:56,725 | INFO | total log probability: -4.45
2026-01-29 17:47:56,725 | INFO | normalized log probability: -0.34
2026-01-29 17:47:56,725 | INFO | total number of ended hypotheses: 179
2026-01-29 17:47:56,726 | INFO | best hypo: ▁dans▁les▁grandes▁négociations

2026-01-29 17:47:56,727 | INFO | speech length: 20160
2026-01-29 17:47:56,755 | INFO | decoder input length: 31
2026-01-29 17:47:56,755 | INFO | max output length: 31
2026-01-29 17:47:56,755 | INFO | min output length: 3
2026-01-29 17:47:57,320 | INFO | end detected at 17
2026-01-29 17:47:57,321 | INFO |  -0.90 * 0.5 =  -0.45 for decoder
2026-01-29 17:47:57,322 | INFO |  -0.23 * 0.5 =  -0.12 for ctc
2026-01-29 17:47:57,322 | INFO | total log probability: -0.57
2026-01-29 17:47:57,322 | INFO | normalized log probability: -0.04
2026-01-29 17:47:57,322 | INFO | total number of ended hypotheses: 135
2026-01-29 17:47:57,322 | INFO | best hypo: ▁mes▁chers▁compatriotes

2026-01-29 17:47:57,323 | INFO | speech length: 37440
2026-01-29 17:47:57,349 | INFO | decoder input length: 58
2026-01-29 17:47:57,349 | INFO | max output length: 58
2026-01-29 17:47:57,349 | INFO | min output length: 5
2026-01-29 17:47:58,105 | INFO | end detected at 21
2026-01-29 17:47:58,106 | INFO |  -1.17 * 0.5 =  -0.58 for decoder
2026-01-29 17:47:58,106 | INFO |  -0.33 * 0.5 =  -0.17 for ctc
2026-01-29 17:47:58,106 | INFO | total log probability: -0.75
2026-01-29 17:47:58,106 | INFO | normalized log probability: -0.04
2026-01-29 17:47:58,106 | INFO | total number of ended hypotheses: 151
2026-01-29 17:47:58,107 | INFO | best hypo: ▁nous▁avons▁en▁commun▁certaines▁valeurs

2026-01-29 17:47:58,108 | INFO | speech length: 116960
2026-01-29 17:47:58,138 | INFO | decoder input length: 182
2026-01-29 17:47:58,138 | INFO | max output length: 182
2026-01-29 17:47:58,138 | INFO | min output length: 18
2026-01-29 17:48:00,612 | INFO | end detected at 52
2026-01-29 17:48:00,613 | INFO |  -5.18 * 0.5 =  -2.59 for decoder
2026-01-29 17:48:00,613 | INFO |  -4.54 * 0.5 =  -2.27 for ctc
2026-01-29 17:48:00,613 | INFO | total log probability: -4.86
2026-01-29 17:48:00,613 | INFO | normalized log probability: -0.10
2026-01-29 17:48:00,613 | INFO | total number of ended hypotheses: 161
2026-01-29 17:48:00,614 | INFO | best hypo: ▁la▁volonté▁de▁donner▁à▁chacun▁sa▁chance▁pour▁que▁notre▁société▁soit▁plus▁alente▁plus▁mobile▁plus▁optimiste

2026-01-29 17:48:00,616 | INFO | speech length: 118400
2026-01-29 17:48:00,649 | INFO | decoder input length: 184
2026-01-29 17:48:00,649 | INFO | max output length: 184
2026-01-29 17:48:00,650 | INFO | min output length: 18
2026-01-29 17:48:03,347 | INFO | end detected at 58
2026-01-29 17:48:03,348 | INFO |  -3.96 * 0.5 =  -1.98 for decoder
2026-01-29 17:48:03,348 | INFO |  -0.40 * 0.5 =  -0.20 for ctc
2026-01-29 17:48:03,348 | INFO | total log probability: -2.18
2026-01-29 17:48:03,348 | INFO | normalized log probability: -0.04
2026-01-29 17:48:03,348 | INFO | total number of ended hypotheses: 143
2026-01-29 17:48:03,348 | INFO | best hypo: ▁l'exigence▁de▁solidarité▁une▁solidarité▁plus▁responsable▁où▁chacun▁s'efforcerait▁de▁prendre▁sa▁part▁du▁contrat

2026-01-29 17:48:03,350 | INFO | speech length: 83520
2026-01-29 17:48:03,377 | INFO | decoder input length: 130
2026-01-29 17:48:03,377 | INFO | max output length: 130
2026-01-29 17:48:03,377 | INFO | min output length: 13
2026-01-29 17:48:04,974 | INFO | end detected at 38
2026-01-29 17:48:04,974 | INFO |  -2.99 * 0.5 =  -1.50 for decoder
2026-01-29 17:48:04,975 | INFO |  -5.03 * 0.5 =  -2.51 for ctc
2026-01-29 17:48:04,975 | INFO | total log probability: -4.01
2026-01-29 17:48:04,975 | INFO | normalized log probability: -0.13
2026-01-29 17:48:04,975 | INFO | total number of ended hypotheses: 171
2026-01-29 17:48:04,975 | INFO | best hypo: ▁l'attachement▁à▁la▁famille▁parce▁qu'elle▁est▁chaleur▁entre▁aide▁sécurité

2026-01-29 17:48:04,977 | INFO | speech length: 61120
2026-01-29 17:48:05,009 | INFO | decoder input length: 95
2026-01-29 17:48:05,009 | INFO | max output length: 95
2026-01-29 17:48:05,009 | INFO | min output length: 9
2026-01-29 17:48:06,236 | INFO | end detected at 32
2026-01-29 17:48:06,237 | INFO |  -2.07 * 0.5 =  -1.03 for decoder
2026-01-29 17:48:06,237 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-29 17:48:06,237 | INFO | total log probability: -1.04
2026-01-29 17:48:06,237 | INFO | normalized log probability: -0.04
2026-01-29 17:48:06,237 | INFO | total number of ended hypotheses: 147
2026-01-29 17:48:06,237 | INFO | best hypo: ▁le▁désir▁d'être▁utile▁de▁trouver▁sa▁place▁dans▁la▁société

2026-01-29 17:48:06,239 | INFO | speech length: 29280
2026-01-29 17:48:06,267 | INFO | decoder input length: 45
2026-01-29 17:48:06,267 | INFO | max output length: 45
2026-01-29 17:48:06,267 | INFO | min output length: 4
2026-01-29 17:48:06,860 | INFO | end detected at 17
2026-01-29 17:48:06,861 | INFO |  -1.03 * 0.5 =  -0.51 for decoder
2026-01-29 17:48:06,861 | INFO |  -0.38 * 0.5 =  -0.19 for ctc
2026-01-29 17:48:06,861 | INFO | total log probability: -0.70
2026-01-29 17:48:06,861 | INFO | normalized log probability: -0.05
2026-01-29 17:48:06,861 | INFO | total number of ended hypotheses: 152
2026-01-29 17:48:06,861 | INFO | best hypo: ▁de▁donner▁autour▁de▁soi

2026-01-29 17:48:06,862 | INFO | speech length: 13600
2026-01-29 17:48:06,888 | INFO | decoder input length: 20
2026-01-29 17:48:06,888 | INFO | max output length: 20
2026-01-29 17:48:06,888 | INFO | min output length: 2
2026-01-29 17:48:07,287 | INFO | end detected at 12
2026-01-29 17:48:07,287 | INFO |  -1.15 * 0.5 =  -0.57 for decoder
2026-01-29 17:48:07,288 | INFO |  -1.93 * 0.5 =  -0.97 for ctc
2026-01-29 17:48:07,288 | INFO | total log probability: -1.54
2026-01-29 17:48:07,288 | INFO | normalized log probability: -0.19
2026-01-29 17:48:07,288 | INFO | total number of ended hypotheses: 139
2026-01-29 17:48:07,288 | INFO | best hypo: ▁elle▁se▁réalise

2026-01-29 17:48:07,289 | INFO | speech length: 13600
2026-01-29 17:48:07,314 | INFO | decoder input length: 20
2026-01-29 17:48:07,315 | INFO | max output length: 20
2026-01-29 17:48:07,315 | INFO | min output length: 2
2026-01-29 17:48:07,710 | INFO | end detected at 12
2026-01-29 17:48:07,711 | INFO |  -0.52 * 0.5 =  -0.26 for decoder
2026-01-29 17:48:07,711 | INFO |  -0.06 * 0.5 =  -0.03 for ctc
2026-01-29 17:48:07,711 | INFO | total log probability: -0.29
2026-01-29 17:48:07,711 | INFO | normalized log probability: -0.04
2026-01-29 17:48:07,711 | INFO | total number of ended hypotheses: 139
2026-01-29 17:48:07,711 | INFO | best hypo: ▁la▁tolérance

2026-01-29 17:48:07,712 | INFO | speech length: 50720
2026-01-29 17:48:07,738 | INFO | decoder input length: 78
2026-01-29 17:48:07,738 | INFO | max output length: 78
2026-01-29 17:48:07,738 | INFO | min output length: 7
2026-01-29 17:48:08,621 | INFO | end detected at 23
2026-01-29 17:48:08,622 | INFO |  -9.95 * 0.5 =  -4.98 for decoder
2026-01-29 17:48:08,622 | INFO |  -1.64 * 0.5 =  -0.82 for ctc
2026-01-29 17:48:08,622 | INFO | total log probability: -5.80
2026-01-29 17:48:08,622 | INFO | normalized log probability: -0.32
2026-01-29 17:48:08,622 | INFO | total number of ended hypotheses: 163
2026-01-29 17:48:08,622 | INFO | best hypo: ▁qui▁ne▁doit▁pas▁être▁renoncement▁à▁ses▁condé

2026-01-29 17:48:08,624 | INFO | speech length: 16480
2026-01-29 17:48:08,654 | INFO | decoder input length: 25
2026-01-29 17:48:08,654 | INFO | max output length: 25
2026-01-29 17:48:08,654 | INFO | min output length: 2
2026-01-29 17:48:09,307 | INFO | end detected at 20
2026-01-29 17:48:09,308 | INFO |  -1.42 * 0.5 =  -0.71 for decoder
2026-01-29 17:48:09,308 | INFO |  -2.15 * 0.5 =  -1.07 for ctc
2026-01-29 17:48:09,308 | INFO | total log probability: -1.78
2026-01-29 17:48:09,308 | INFO | normalized log probability: -0.11
2026-01-29 17:48:09,308 | INFO | total number of ended hypotheses: 173
2026-01-29 17:48:09,308 | INFO | best hypo: ▁mes▁respects▁de▁l'eau

2026-01-29 17:48:09,310 | INFO | speech length: 18720
2026-01-29 17:48:09,339 | INFO | decoder input length: 28
2026-01-29 17:48:09,340 | INFO | max output length: 28
2026-01-29 17:48:09,340 | INFO | min output length: 2
2026-01-29 17:48:09,933 | INFO | end detected at 18
2026-01-29 17:48:09,935 | INFO |  -6.99 * 0.5 =  -3.50 for decoder
2026-01-29 17:48:09,935 | INFO |  -3.23 * 0.5 =  -1.62 for ctc
2026-01-29 17:48:09,935 | INFO | total log probability: -5.11
2026-01-29 17:48:09,935 | INFO | normalized log probability: -0.39
2026-01-29 17:48:09,935 | INFO | total number of ended hypotheses: 168
2026-01-29 17:48:09,936 | INFO | best hypo: ▁sprit▁républicain

2026-01-29 17:48:09,937 | INFO | speech length: 80800
2026-01-29 17:48:09,967 | INFO | decoder input length: 125
2026-01-29 17:48:09,967 | INFO | max output length: 125
2026-01-29 17:48:09,967 | INFO | min output length: 12
2026-01-29 17:48:11,674 | INFO | end detected at 41
2026-01-29 17:48:11,675 | INFO |  -2.80 * 0.5 =  -1.40 for decoder
2026-01-29 17:48:11,675 | INFO |  -0.82 * 0.5 =  -0.41 for ctc
2026-01-29 17:48:11,675 | INFO | total log probability: -1.81
2026-01-29 17:48:11,675 | INFO | normalized log probability: -0.05
2026-01-29 17:48:11,675 | INFO | total number of ended hypotheses: 166
2026-01-29 17:48:11,676 | INFO | best hypo: ▁et▁le▁sens▁de▁l'intérêt▁général▁qui▁impose▁que▁l'état▁conserve▁toute▁sa▁place

2026-01-29 17:48:11,677 | INFO | speech length: 64160
2026-01-29 17:48:11,710 | INFO | decoder input length: 99
2026-01-29 17:48:11,710 | INFO | max output length: 99
2026-01-29 17:48:11,710 | INFO | min output length: 9
2026-01-29 17:48:12,820 | INFO | end detected at 27
2026-01-29 17:48:12,820 | INFO |  -1.73 * 0.5 =  -0.86 for decoder
2026-01-29 17:48:12,820 | INFO |  -0.06 * 0.5 =  -0.03 for ctc
2026-01-29 17:48:12,820 | INFO | total log probability: -0.90
2026-01-29 17:48:12,820 | INFO | normalized log probability: -0.04
2026-01-29 17:48:12,820 | INFO | total number of ended hypotheses: 145
2026-01-29 17:48:12,821 | INFO | best hypo: ▁pour▁dire▁le▁droit▁le▁faire▁respecter▁avec▁autorité

2026-01-29 17:48:12,822 | INFO | speech length: 8640
2026-01-29 17:48:12,850 | INFO | decoder input length: 13
2026-01-29 17:48:12,850 | INFO | max output length: 13
2026-01-29 17:48:12,850 | INFO | min output length: 1
2026-01-29 17:48:13,212 | INFO | end detected at 11
2026-01-29 17:48:13,213 | INFO |  -7.02 * 0.5 =  -3.51 for decoder
2026-01-29 17:48:13,213 | INFO |  -1.94 * 0.5 =  -0.97 for ctc
2026-01-29 17:48:13,213 | INFO | total log probability: -4.48
2026-01-29 17:48:13,213 | INFO | normalized log probability: -0.90
2026-01-29 17:48:13,213 | INFO | total number of ended hypotheses: 170
2026-01-29 17:48:13,213 | INFO | best hypo: ▁avec▁juis

2026-01-29 17:48:13,215 | INFO | speech length: 19521
2026-01-29 17:48:13,244 | INFO | decoder input length: 30
2026-01-29 17:48:13,244 | INFO | max output length: 30
2026-01-29 17:48:13,245 | INFO | min output length: 3
2026-01-29 17:48:13,782 | INFO | end detected at 16
2026-01-29 17:48:13,783 | INFO |  -1.87 * 0.5 =  -0.94 for decoder
2026-01-29 17:48:13,783 | INFO |  -2.39 * 0.5 =  -1.19 for ctc
2026-01-29 17:48:13,783 | INFO | total log probability: -2.13
2026-01-29 17:48:13,783 | INFO | normalized log probability: -0.19
2026-01-29 17:48:13,783 | INFO | total number of ended hypotheses: 159
2026-01-29 17:48:13,784 | INFO | best hypo: ▁gardons▁ses▁exiges

2026-01-29 17:48:13,785 | INFO | speech length: 19520
2026-01-29 17:48:13,811 | INFO | decoder input length: 30
2026-01-29 17:48:13,811 | INFO | max output length: 30
2026-01-29 17:48:13,811 | INFO | min output length: 3
2026-01-29 17:48:14,349 | INFO | end detected at 16
2026-01-29 17:48:14,351 | INFO |  -1.04 * 0.5 =  -0.52 for decoder
2026-01-29 17:48:14,351 | INFO |  -0.24 * 0.5 =  -0.12 for ctc
2026-01-29 17:48:14,351 | INFO | total log probability: -0.64
2026-01-29 17:48:14,351 | INFO | normalized log probability: -0.05
2026-01-29 17:48:14,351 | INFO | total number of ended hypotheses: 132
2026-01-29 17:48:14,351 | INFO | best hypo: ▁gardons▁ces▁valeurs

2026-01-29 17:48:14,353 | INFO | speech length: 81600
2026-01-29 17:48:14,379 | INFO | decoder input length: 127
2026-01-29 17:48:14,379 | INFO | max output length: 127
2026-01-29 17:48:14,379 | INFO | min output length: 12
2026-01-29 17:48:16,062 | INFO | end detected at 36
2026-01-29 17:48:16,062 | INFO |  -2.32 * 0.5 =  -1.16 for decoder
2026-01-29 17:48:16,063 | INFO |  -0.06 * 0.5 =  -0.03 for ctc
2026-01-29 17:48:16,063 | INFO | total log probability: -1.19
2026-01-29 17:48:16,063 | INFO | normalized log probability: -0.04
2026-01-29 17:48:16,063 | INFO | total number of ended hypotheses: 157
2026-01-29 17:48:16,063 | INFO | best hypo: ▁en▁les▁faisant▁vivre▁nous▁serons▁plus▁forts▁pour▁aborder▁les▁temps▁qui▁viennent

2026-01-29 17:48:16,065 | INFO | speech length: 17440
2026-01-29 17:48:16,093 | INFO | decoder input length: 26
2026-01-29 17:48:16,093 | INFO | max output length: 26
2026-01-29 17:48:16,093 | INFO | min output length: 2
2026-01-29 17:48:16,491 | INFO | end detected at 12
2026-01-29 17:48:16,492 | INFO |  -0.72 * 0.5 =  -0.36 for decoder
2026-01-29 17:48:16,492 | INFO |  -0.12 * 0.5 =  -0.06 for ctc
2026-01-29 17:48:16,492 | INFO | total log probability: -0.42
2026-01-29 17:48:16,492 | INFO | normalized log probability: -0.05
2026-01-29 17:48:16,492 | INFO | total number of ended hypotheses: 142
2026-01-29 17:48:16,492 | INFO | best hypo: ▁la▁france▁chante

2026-01-29 17:48:16,493 | INFO | speech length: 26400
2026-01-29 17:48:16,523 | INFO | decoder input length: 40
2026-01-29 17:48:16,524 | INFO | max output length: 40
2026-01-29 17:48:16,524 | INFO | min output length: 4
2026-01-29 17:48:17,230 | INFO | end detected at 21
2026-01-29 17:48:17,231 | INFO |  -1.21 * 0.5 =  -0.61 for decoder
2026-01-29 17:48:17,231 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-29 17:48:17,231 | INFO | total log probability: -0.61
2026-01-29 17:48:17,231 | INFO | normalized log probability: -0.04
2026-01-29 17:48:17,231 | INFO | total number of ended hypotheses: 146
2026-01-29 17:48:17,231 | INFO | best hypo: ▁elle▁doit▁le▁faire▁au▁rythme▁du▁monde

2026-01-29 17:48:17,233 | INFO | speech length: 104960
2026-01-29 17:48:17,259 | INFO | decoder input length: 163
2026-01-29 17:48:17,259 | INFO | max output length: 163
2026-01-29 17:48:17,259 | INFO | min output length: 16
2026-01-29 17:48:19,366 | INFO | end detected at 47
2026-01-29 17:48:19,367 | INFO |  -3.23 * 0.5 =  -1.62 for decoder
2026-01-29 17:48:19,367 | INFO |  -0.91 * 0.5 =  -0.46 for ctc
2026-01-29 17:48:19,367 | INFO | total log probability: -2.07
2026-01-29 17:48:19,367 | INFO | normalized log probability: -0.05
2026-01-29 17:48:19,367 | INFO | total number of ended hypotheses: 148
2026-01-29 17:48:19,367 | INFO | best hypo: ▁en▁étant▁fidèle▁à▁son▁génie▁propre▁elle▁saura▁conjuguer▁le▁changement▁et▁la▁cohésion▁sociale

2026-01-29 17:48:19,369 | INFO | speech length: 92000
2026-01-29 17:48:19,400 | INFO | decoder input length: 143
2026-01-29 17:48:19,400 | INFO | max output length: 143
2026-01-29 17:48:19,400 | INFO | min output length: 14
2026-01-29 17:48:21,293 | INFO | end detected at 43
2026-01-29 17:48:21,294 | INFO |  -3.12 * 0.5 =  -1.56 for decoder
2026-01-29 17:48:21,294 | INFO |  -0.36 * 0.5 =  -0.18 for ctc
2026-01-29 17:48:21,294 | INFO | total log probability: -1.74
2026-01-29 17:48:21,295 | INFO | normalized log probability: -0.04
2026-01-29 17:48:21,295 | INFO | total number of ended hypotheses: 151
2026-01-29 17:48:21,295 | INFO | best hypo: ▁l'esprit▁d'initiative▁est▁la▁sécurité▁la▁modernité▁et▁le▁bien▁vivre▁ensemble

2026-01-29 17:48:21,297 | INFO | speech length: 20160
2026-01-29 17:48:21,322 | INFO | decoder input length: 31
2026-01-29 17:48:21,322 | INFO | max output length: 31
2026-01-29 17:48:21,322 | INFO | min output length: 3
2026-01-29 17:48:21,883 | INFO | end detected at 17
2026-01-29 17:48:21,884 | INFO |  -1.01 * 0.5 =  -0.51 for decoder
2026-01-29 17:48:21,885 | INFO |  -3.41 * 0.5 =  -1.71 for ctc
2026-01-29 17:48:21,885 | INFO | total log probability: -2.21
2026-01-29 17:48:21,885 | INFO | normalized log probability: -0.17
2026-01-29 17:48:21,885 | INFO | total number of ended hypotheses: 164
2026-01-29 17:48:21,885 | INFO | best hypo: ▁mes▁chers▁compatriotes

2026-01-29 17:48:21,886 | INFO | speech length: 106240
2026-01-29 17:48:21,913 | INFO | decoder input length: 165
2026-01-29 17:48:21,913 | INFO | max output length: 165
2026-01-29 17:48:21,913 | INFO | min output length: 16
2026-01-29 17:48:24,080 | INFO | end detected at 48
2026-01-29 17:48:24,081 | INFO |  -3.73 * 0.5 =  -1.87 for decoder
2026-01-29 17:48:24,081 | INFO |  -2.22 * 0.5 =  -1.11 for ctc
2026-01-29 17:48:24,081 | INFO | total log probability: -2.98
2026-01-29 17:48:24,081 | INFO | normalized log probability: -0.07
2026-01-29 17:48:24,081 | INFO | total number of ended hypotheses: 156
2026-01-29 17:48:24,082 | INFO | best hypo: ▁je▁mesure▁l'honneur▁et▁la▁responsabilité▁qui▁m'échouent▁de▁m'adresser▁à▁vous▁ce▁soir

2026-01-29 17:48:24,083 | INFO | speech length: 55040
2026-01-29 17:48:24,113 | INFO | decoder input length: 85
2026-01-29 17:48:24,113 | INFO | max output length: 85
2026-01-29 17:48:24,113 | INFO | min output length: 8
2026-01-29 17:48:25,027 | INFO | end detected at 24
2026-01-29 17:48:25,028 | INFO |  -1.36 * 0.5 =  -0.68 for decoder
2026-01-29 17:48:25,028 | INFO |  -0.07 * 0.5 =  -0.03 for ctc
2026-01-29 17:48:25,028 | INFO | total log probability: -0.71
2026-01-29 17:48:25,028 | INFO | normalized log probability: -0.04
2026-01-29 17:48:25,028 | INFO | total number of ended hypotheses: 151
2026-01-29 17:48:25,028 | INFO | best hypo: ▁alors▁que▁notre▁nation▁franchit▁le▁cap▁du▁siècle

2026-01-29 17:48:25,030 | INFO | speech length: 80480
2026-01-29 17:48:25,058 | INFO | decoder input length: 125
2026-01-29 17:48:25,058 | INFO | max output length: 125
2026-01-29 17:48:25,058 | INFO | min output length: 12
2026-01-29 17:48:26,801 | INFO | end detected at 42
2026-01-29 17:48:26,802 | INFO |  -5.91 * 0.5 =  -2.96 for decoder
2026-01-29 17:48:26,802 | INFO |  -2.80 * 0.5 =  -1.40 for ctc
2026-01-29 17:48:26,802 | INFO | total log probability: -4.36
2026-01-29 17:48:26,802 | INFO | normalized log probability: -0.12
2026-01-29 17:48:26,802 | INFO | total number of ended hypotheses: 189
2026-01-29 17:48:26,803 | INFO | best hypo: ▁la▁france▁a▁plus▁de▁mille▁ans▁riches▁de▁fièvres▁de▁passion▁d'enthousiasme

2026-01-29 17:48:26,804 | INFO | speech length: 88000
2026-01-29 17:48:26,833 | INFO | decoder input length: 137
2026-01-29 17:48:26,833 | INFO | max output length: 137
2026-01-29 17:48:26,833 | INFO | min output length: 13
2026-01-29 17:48:28,337 | INFO | end detected at 35
2026-01-29 17:48:28,338 | INFO |  -8.91 * 0.5 =  -4.45 for decoder
2026-01-29 17:48:28,338 | INFO |  -2.59 * 0.5 =  -1.30 for ctc
2026-01-29 17:48:28,338 | INFO | total log probability: -5.75
2026-01-29 17:48:28,338 | INFO | normalized log probability: -0.19
2026-01-29 17:48:28,338 | INFO | total number of ended hypotheses: 156
2026-01-29 17:48:28,338 | INFO | best hypo: ▁elle▁continue▁comme▁ière▁à▁ouvrir▁et▁à▁défricher▁les▁chemins▁du▁monde

2026-01-29 17:48:28,340 | INFO | speech length: 42560
2026-01-29 17:48:28,366 | INFO | decoder input length: 66
2026-01-29 17:48:28,366 | INFO | max output length: 66
2026-01-29 17:48:28,366 | INFO | min output length: 6
2026-01-29 17:48:29,124 | INFO | end detected at 21
2026-01-29 17:48:29,125 | INFO |  -1.27 * 0.5 =  -0.63 for decoder
2026-01-29 17:48:29,125 | INFO |  -0.12 * 0.5 =  -0.06 for ctc
2026-01-29 17:48:29,125 | INFO | total log probability: -0.70
2026-01-29 17:48:29,125 | INFO | normalized log probability: -0.04
2026-01-29 17:48:29,125 | INFO | total number of ended hypotheses: 143
2026-01-29 17:48:29,125 | INFO | best hypo: ▁le▁nouveau▁siècle▁est▁à▁inventer

2026-01-29 17:48:29,126 | INFO | speech length: 37120
2026-01-29 17:48:29,154 | INFO | decoder input length: 57
2026-01-29 17:48:29,154 | INFO | max output length: 57
2026-01-29 17:48:29,154 | INFO | min output length: 5
2026-01-29 17:48:29,799 | INFO | end detected at 18
2026-01-29 17:48:29,801 | INFO |  -1.05 * 0.5 =  -0.52 for decoder
2026-01-29 17:48:29,801 | INFO |  -1.05 * 0.5 =  -0.52 for ctc
2026-01-29 17:48:29,801 | INFO | total log probability: -1.05
2026-01-29 17:48:29,801 | INFO | normalized log probability: -0.08
2026-01-29 17:48:29,801 | INFO | total number of ended hypotheses: 154
2026-01-29 17:48:29,802 | INFO | best hypo: ▁plus▁fraternel▁plus▁volontaire

2026-01-29 17:48:29,803 | INFO | speech length: 41440
2026-01-29 17:48:29,831 | INFO | decoder input length: 64
2026-01-29 17:48:29,831 | INFO | max output length: 64
2026-01-29 17:48:29,831 | INFO | min output length: 6
2026-01-29 17:48:30,639 | INFO | end detected at 22
2026-01-29 17:48:30,640 | INFO |  -1.31 * 0.5 =  -0.65 for decoder
2026-01-29 17:48:30,640 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-29 17:48:30,640 | INFO | total log probability: -0.66
2026-01-29 17:48:30,640 | INFO | normalized log probability: -0.04
2026-01-29 17:48:30,640 | INFO | total number of ended hypotheses: 142
2026-01-29 17:48:30,641 | INFO | best hypo: ▁il▁aura▁les▁couleurs▁que▁nous▁lui▁donnerons

2026-01-29 17:48:30,642 | INFO | speech length: 48480
2026-01-29 17:48:30,669 | INFO | decoder input length: 75
2026-01-29 17:48:30,669 | INFO | max output length: 75
2026-01-29 17:48:30,669 | INFO | min output length: 7
2026-01-29 17:48:31,682 | INFO | end detected at 27
2026-01-29 17:48:31,683 | INFO |  -1.61 * 0.5 =  -0.81 for decoder
2026-01-29 17:48:31,683 | INFO |  -0.23 * 0.5 =  -0.11 for ctc
2026-01-29 17:48:31,683 | INFO | total log probability: -0.92
2026-01-29 17:48:31,683 | INFO | normalized log probability: -0.04
2026-01-29 17:48:31,683 | INFO | total number of ended hypotheses: 166
2026-01-29 17:48:31,684 | INFO | best hypo: ▁la▁france▁sera▁ce▁que▁nous▁voudrons▁qu'elle▁soit

2026-01-29 17:48:31,685 | INFO | speech length: 59840
2026-01-29 17:48:31,717 | INFO | decoder input length: 93
2026-01-29 17:48:31,717 | INFO | max output length: 93
2026-01-29 17:48:31,717 | INFO | min output length: 9
2026-01-29 17:48:32,599 | INFO | end detected at 22
2026-01-29 17:48:32,600 | INFO |  -1.22 * 0.5 =  -0.61 for decoder
2026-01-29 17:48:32,600 | INFO |  -0.03 * 0.5 =  -0.01 for ctc
2026-01-29 17:48:32,600 | INFO | total log probability: -0.62
2026-01-29 17:48:32,600 | INFO | normalized log probability: -0.03
2026-01-29 17:48:32,600 | INFO | total number of ended hypotheses: 144
2026-01-29 17:48:32,601 | INFO | best hypo: ▁une▁nation▁unie▁vivante▁solidaire▁ouverte

2026-01-29 17:48:32,602 | INFO | speech length: 31040
2026-01-29 17:48:32,633 | INFO | decoder input length: 48
2026-01-29 17:48:32,633 | INFO | max output length: 48
2026-01-29 17:48:32,633 | INFO | min output length: 4
2026-01-29 17:48:33,323 | INFO | end detected at 20
2026-01-29 17:48:33,324 | INFO |  -1.61 * 0.5 =  -0.80 for decoder
2026-01-29 17:48:33,324 | INFO |  -0.22 * 0.5 =  -0.11 for ctc
2026-01-29 17:48:33,324 | INFO | total log probability: -0.91
2026-01-29 17:48:33,324 | INFO | normalized log probability: -0.06
2026-01-29 17:48:33,325 | INFO | total number of ended hypotheses: 142
2026-01-29 17:48:33,325 | INFO | best hypo: ▁il▁n'accepte▁aucune▁fatalité

2026-01-29 17:48:33,326 | INFO | speech length: 72960
2026-01-29 17:48:33,353 | INFO | decoder input length: 113
2026-01-29 17:48:33,353 | INFO | max output length: 113
2026-01-29 17:48:33,353 | INFO | min output length: 11
2026-01-29 17:48:34,784 | INFO | end detected at 35
2026-01-29 17:48:34,785 | INFO |  -2.49 * 0.5 =  -1.25 for decoder
2026-01-29 17:48:34,785 | INFO |  -2.69 * 0.5 =  -1.34 for ctc
2026-01-29 17:48:34,785 | INFO | total log probability: -2.59
2026-01-29 17:48:34,785 | INFO | normalized log probability: -0.08
2026-01-29 17:48:34,785 | INFO | total number of ended hypotheses: 151
2026-01-29 17:48:34,786 | INFO | best hypo: ▁car▁dans▁un▁monde▁où▁rien▁n'est▁figé▁l'avenir▁dépend▁de▁nous

2026-01-29 17:48:34,787 | INFO | speech length: 132960
2026-01-29 17:48:34,821 | INFO | decoder input length: 207
2026-01-29 17:48:34,821 | INFO | max output length: 207
2026-01-29 17:48:34,821 | INFO | min output length: 20
2026-01-29 17:48:37,575 | INFO | end detected at 56
2026-01-29 17:48:37,577 | INFO |  -5.37 * 0.5 =  -2.68 for decoder
2026-01-29 17:48:37,577 | INFO |  -3.93 * 0.5 =  -1.96 for ctc
2026-01-29 17:48:37,577 | INFO | total log probability: -4.65
2026-01-29 17:48:37,577 | INFO | normalized log probability: -0.09
2026-01-29 17:48:37,577 | INFO | total number of ended hypotheses: 187
2026-01-29 17:48:37,578 | INFO | best hypo: ▁l'avenir▁dépend▁de▁notre▁capacité▁à▁construire▁à▁créer▁à▁réver▁ensemble▁les▁voies▁de▁l'aventure▁humaine

2026-01-29 17:48:37,579 | INFO | speech length: 24480
2026-01-29 17:48:37,606 | INFO | decoder input length: 37
2026-01-29 17:48:37,606 | INFO | max output length: 37
2026-01-29 17:48:37,606 | INFO | min output length: 3
2026-01-29 17:48:38,257 | INFO | end detected at 20
2026-01-29 17:48:38,258 | INFO |  -3.60 * 0.5 =  -1.80 for decoder
2026-01-29 17:48:38,258 | INFO |  -1.37 * 0.5 =  -0.68 for ctc
2026-01-29 17:48:38,258 | INFO | total log probability: -2.48
2026-01-29 17:48:38,258 | INFO | normalized log probability: -0.16
2026-01-29 17:48:38,258 | INFO | total number of ended hypotheses: 151
2026-01-29 17:48:38,259 | INFO | best hypo: ▁chacune▁est▁à▁chacun▁d'entre▁eux

2026-01-29 17:48:38,260 | INFO | speech length: 77120
2026-01-29 17:48:38,293 | INFO | decoder input length: 120
2026-01-29 17:48:38,293 | INFO | max output length: 120
2026-01-29 17:48:38,293 | INFO | min output length: 12
2026-01-29 17:48:39,532 | INFO | end detected at 30
2026-01-29 17:48:39,533 | INFO |  -2.20 * 0.5 =  -1.10 for decoder
2026-01-29 17:48:39,533 | INFO |  -1.05 * 0.5 =  -0.52 for ctc
2026-01-29 17:48:39,533 | INFO | total log probability: -1.62
2026-01-29 17:48:39,533 | INFO | normalized log probability: -0.06
2026-01-29 17:48:39,533 | INFO | total number of ended hypotheses: 152
2026-01-29 17:48:39,534 | INFO | best hypo: ▁françaises▁et▁français▁de▁métropole▁d'outre▁mer▁de▁l'étranger

2026-01-29 17:48:39,535 | INFO | speech length: 34240
2026-01-29 17:48:39,561 | INFO | decoder input length: 53
2026-01-29 17:48:39,562 | INFO | max output length: 53
2026-01-29 17:48:39,562 | INFO | min output length: 5
2026-01-29 17:48:40,236 | INFO | end detected at 19
2026-01-29 17:48:40,237 | INFO |  -0.89 * 0.5 =  -0.45 for decoder
2026-01-29 17:48:40,237 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-29 17:48:40,237 | INFO | total log probability: -0.45
2026-01-29 17:48:40,237 | INFO | normalized log probability: -0.03
2026-01-29 17:48:40,237 | INFO | total number of ended hypotheses: 135
2026-01-29 17:48:40,237 | INFO | best hypo: ▁je▁souhaite▁très▁chaleureuse

2026-01-29 17:48:40,239 | INFO | speech length: 50880
2026-01-29 17:48:40,268 | INFO | decoder input length: 79
2026-01-29 17:48:40,268 | INFO | max output length: 79
2026-01-29 17:48:40,268 | INFO | min output length: 7
2026-01-29 17:48:41,113 | INFO | end detected at 22
2026-01-29 17:48:41,115 | INFO |  -1.61 * 0.5 =  -0.81 for decoder
2026-01-29 17:48:41,115 | INFO |  -0.41 * 0.5 =  -0.21 for ctc
2026-01-29 17:48:41,115 | INFO | total log probability: -1.01
2026-01-29 17:48:41,115 | INFO | normalized log probability: -0.06
2026-01-29 17:48:41,115 | INFO | total number of ended hypotheses: 151
2026-01-29 17:48:41,115 | INFO | best hypo: ▁une▁bonne▁et▁une▁heureuse▁année▁demie

2026-01-29 17:48:41,126 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,126 | INFO | Chunk: 1 | WER=16.666667 | S=0 D=0 I=1
2026-01-29 17:48:41,127 | INFO | Chunk: 2 | WER=19.047619 | S=2 D=0 I=2
2026-01-29 17:48:41,127 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,127 | INFO | Chunk: 4 | WER=30.000000 | S=0 D=1 I=2
2026-01-29 17:48:41,128 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,128 | INFO | Chunk: 6 | WER=12.500000 | S=0 D=0 I=2
2026-01-29 17:48:41,128 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,129 | INFO | Chunk: 8 | WER=25.000000 | S=0 D=0 I=1
2026-01-29 17:48:41,129 | INFO | Chunk: 9 | WER=13.793103 | S=1 D=0 I=3
2026-01-29 17:48:41,130 | INFO | Chunk: 10 | WER=9.090909 | S=1 D=0 I=0
2026-01-29 17:48:41,130 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,130 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,131 | INFO | Chunk: 13 | WER=27.272727 | S=1 D=0 I=2
2026-01-29 17:48:41,131 | INFO | Chunk: 14 | WER=12.500000 | S=1 D=0 I=0
2026-01-29 17:48:41,131 | INFO | Chunk: 15 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,132 | INFO | Chunk: 16 | WER=12.500000 | S=0 D=0 I=2
2026-01-29 17:48:41,132 | INFO | Chunk: 17 | WER=11.111111 | S=1 D=0 I=0
2026-01-29 17:48:41,132 | INFO | Chunk: 18 | WER=16.666667 | S=0 D=0 I=1
2026-01-29 17:48:41,132 | INFO | Chunk: 19 | WER=60.000000 | S=0 D=0 I=3
2026-01-29 17:48:41,132 | INFO | Chunk: 20 | WER=38.461538 | S=2 D=3 I=0
2026-01-29 17:48:41,133 | INFO | Chunk: 21 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,133 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,133 | INFO | Chunk: 23 | WER=25.000000 | S=0 D=1 I=1
2026-01-29 17:48:41,133 | INFO | Chunk: 24 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,134 | INFO | Chunk: 25 | WER=23.076923 | S=2 D=0 I=1
2026-01-29 17:48:41,134 | INFO | Chunk: 26 | WER=60.000000 | S=0 D=0 I=3
2026-01-29 17:48:41,134 | INFO | Chunk: 27 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,134 | INFO | Chunk: 28 | WER=28.571429 | S=1 D=1 I=2
2026-01-29 17:48:41,135 | INFO | Chunk: 29 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,135 | INFO | Chunk: 30 | WER=0.000000 | S=1 D=0 I=0
2026-01-29 17:48:41,135 | INFO | Chunk: 31 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,136 | INFO | Chunk: 32 | WER=12.500000 | S=1 D=0 I=2
2026-01-29 17:48:41,136 | INFO | Chunk: 33 | WER=20.000000 | S=0 D=0 I=1
2026-01-29 17:48:41,136 | INFO | Chunk: 34 | WER=300.000000 | S=0 D=0 I=3
2026-01-29 17:48:41,136 | INFO | Chunk: 35 | WER=6.250000 | S=1 D=0 I=0
2026-01-29 17:48:41,137 | INFO | Chunk: 36 | WER=16.666667 | S=1 D=0 I=0
2026-01-29 17:48:41,137 | INFO | Chunk: 37 | WER=9.090909 | S=1 D=0 I=2
2026-01-29 17:48:41,138 | INFO | Chunk: 38 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,138 | INFO | Chunk: 39 | WER=11.111111 | S=1 D=0 I=0
2026-01-29 17:48:41,138 | INFO | Chunk: 40 | WER=9.090909 | S=1 D=0 I=0
2026-01-29 17:48:41,138 | INFO | Chunk: 41 | WER=66.666667 | S=0 D=1 I=1
2026-01-29 17:48:41,139 | INFO | Chunk: 42 | WER=10.000000 | S=0 D=0 I=1
2026-01-29 17:48:41,139 | INFO | Chunk: 43 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,139 | INFO | Chunk: 44 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,139 | INFO | Chunk: 45 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,139 | INFO | Chunk: 46 | WER=33.333333 | S=0 D=0 I=1
2026-01-29 17:48:41,140 | INFO | Chunk: 47 | WER=33.333333 | S=1 D=0 I=1
2026-01-29 17:48:41,140 | INFO | Chunk: 48 | WER=50.000000 | S=1 D=0 I=1
2026-01-29 17:48:41,140 | INFO | Chunk: 49 | WER=6.666667 | S=0 D=0 I=1
2026-01-29 17:48:41,140 | INFO | Chunk: 50 | WER=100.000000 | S=1 D=0 I=1
2026-01-29 17:48:41,140 | INFO | Chunk: 51 | WER=25.000000 | S=0 D=0 I=1
2026-01-29 17:48:41,141 | INFO | Chunk: 52 | WER=50.000000 | S=0 D=1 I=2
2026-01-29 17:48:41,141 | INFO | Chunk: 53 | WER=300.000000 | S=0 D=0 I=3
2026-01-29 17:48:41,141 | INFO | Chunk: 54 | WER=8.333333 | S=1 D=0 I=0
2026-01-29 17:48:41,141 | INFO | Chunk: 55 | WER=15.384615 | S=2 D=0 I=0
2026-01-29 17:48:41,142 | INFO | Chunk: 56 | WER=5.555556 | S=1 D=0 I=0
2026-01-29 17:48:41,142 | INFO | Chunk: 57 | WER=14.285714 | S=0 D=0 I=1
2026-01-29 17:48:41,142 | INFO | Chunk: 58 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,142 | INFO | Chunk: 59 | WER=15.384615 | S=0 D=0 I=2
2026-01-29 17:48:41,143 | INFO | Chunk: 60 | WER=40.000000 | S=1 D=0 I=1
2026-01-29 17:48:41,143 | INFO | Chunk: 61 | WER=66.666667 | S=0 D=1 I=1
2026-01-29 17:48:41,143 | INFO | Chunk: 62 | WER=25.000000 | S=0 D=0 I=2
2026-01-29 17:48:41,143 | INFO | Chunk: 63 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,143 | INFO | Chunk: 64 | WER=25.000000 | S=0 D=0 I=1
2026-01-29 17:48:41,143 | INFO | Chunk: 65 | WER=66.666667 | S=0 D=1 I=1
2026-01-29 17:48:41,144 | INFO | Chunk: 66 | WER=19.047619 | S=3 D=0 I=1
2026-01-29 17:48:41,144 | INFO | Chunk: 67 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,144 | INFO | Chunk: 68 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,144 | INFO | Chunk: 69 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,145 | INFO | Chunk: 70 | WER=7.692308 | S=0 D=0 I=1
2026-01-29 17:48:41,145 | INFO | Chunk: 71 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,145 | INFO | Chunk: 72 | WER=40.000000 | S=2 D=0 I=0
2026-01-29 17:48:41,145 | INFO | Chunk: 73 | WER=20.000000 | S=0 D=0 I=2
2026-01-29 17:48:41,146 | INFO | Chunk: 74 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,146 | INFO | Chunk: 75 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,146 | INFO | Chunk: 76 | WER=25.000000 | S=0 D=0 I=1
2026-01-29 17:48:41,146 | INFO | Chunk: 77 | WER=12.500000 | S=0 D=0 I=2
2026-01-29 17:48:41,147 | INFO | Chunk: 78 | WER=20.000000 | S=0 D=0 I=2
2026-01-29 17:48:41,147 | INFO | Chunk: 79 | WER=20.000000 | S=0 D=0 I=3
2026-01-29 17:48:41,147 | INFO | Chunk: 80 | WER=6.666667 | S=0 D=0 I=1
2026-01-29 17:48:41,148 | INFO | Chunk: 81 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,148 | INFO | Chunk: 82 | WER=30.000000 | S=0 D=2 I=1
2026-01-29 17:48:41,148 | INFO | Chunk: 83 | WER=50.000000 | S=1 D=0 I=2
2026-01-29 17:48:41,148 | INFO | Chunk: 84 | WER=37.500000 | S=1 D=0 I=2
2026-01-29 17:48:41,148 | INFO | Chunk: 85 | WER=20.000000 | S=0 D=1 I=0
2026-01-29 17:48:41,149 | INFO | Chunk: 86 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,149 | INFO | Chunk: 87 | WER=20.000000 | S=0 D=0 I=1
2026-01-29 17:48:41,149 | INFO | Chunk: 88 | WER=11.111111 | S=1 D=0 I=1
2026-01-29 17:48:41,150 | INFO | Chunk: 89 | WER=5.882353 | S=0 D=0 I=1
2026-01-29 17:48:41,150 | INFO | Chunk: 90 | WER=16.666667 | S=1 D=0 I=1
2026-01-29 17:48:41,150 | INFO | Chunk: 91 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,150 | INFO | Chunk: 92 | WER=25.000000 | S=0 D=0 I=1
2026-01-29 17:48:41,150 | INFO | Chunk: 93 | WER=66.666667 | S=2 D=0 I=0
2026-01-29 17:48:41,150 | INFO | Chunk: 94 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,151 | INFO | Chunk: 95 | WER=50.000000 | S=0 D=0 I=3
2026-01-29 17:48:41,151 | INFO | Chunk: 96 | WER=250.000000 | S=2 D=0 I=3
2026-01-29 17:48:41,151 | INFO | Chunk: 97 | WER=66.666667 | S=1 D=1 I=0
2026-01-29 17:48:41,151 | INFO | Chunk: 98 | WER=6.666667 | S=0 D=0 I=1
2026-01-29 17:48:41,151 | INFO | Chunk: 99 | WER=12.500000 | S=0 D=0 I=1
2026-01-29 17:48:41,152 | INFO | Chunk: 100 | WER=200.000000 | S=1 D=0 I=1
2026-01-29 17:48:41,152 | INFO | Chunk: 101 | WER=100.000000 | S=1 D=0 I=1
2026-01-29 17:48:41,152 | INFO | Chunk: 102 | WER=50.000000 | S=0 D=0 I=1
2026-01-29 17:48:41,152 | INFO | Chunk: 103 | WER=7.692308 | S=0 D=0 I=1
2026-01-29 17:48:41,152 | INFO | Chunk: 104 | WER=50.000000 | S=0 D=0 I=1
2026-01-29 17:48:41,152 | INFO | Chunk: 105 | WER=33.333333 | S=0 D=0 I=2
2026-01-29 17:48:41,153 | INFO | Chunk: 106 | WER=6.250000 | S=1 D=0 I=0
2026-01-29 17:48:41,153 | INFO | Chunk: 107 | WER=15.384615 | S=1 D=0 I=1
2026-01-29 17:48:41,153 | INFO | Chunk: 108 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,154 | INFO | Chunk: 109 | WER=12.500000 | S=1 D=0 I=1
2026-01-29 17:48:41,154 | INFO | Chunk: 110 | WER=22.222222 | S=0 D=1 I=1
2026-01-29 17:48:41,154 | INFO | Chunk: 111 | WER=14.285714 | S=2 D=0 I=0
2026-01-29 17:48:41,154 | INFO | Chunk: 112 | WER=27.272727 | S=1 D=0 I=2
2026-01-29 17:48:41,155 | INFO | Chunk: 113 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,155 | INFO | Chunk: 114 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,155 | INFO | Chunk: 115 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,155 | INFO | Chunk: 116 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:48:41,155 | INFO | Chunk: 117 | WER=20.000000 | S=0 D=0 I=1
2026-01-29 17:48:41,156 | INFO | Chunk: 118 | WER=20.000000 | S=1 D=0 I=0
2026-01-29 17:48:41,156 | INFO | Chunk: 119 | WER=16.666667 | S=0 D=0 I=2
2026-01-29 17:48:41,156 | INFO | Chunk: 120 | WER=5.555556 | S=0 D=0 I=1
2026-01-29 17:48:41,156 | INFO | Chunk: 121 | WER=100.000000 | S=1 D=1 I=3
2026-01-29 17:48:41,157 | INFO | Chunk: 122 | WER=9.090909 | S=1 D=0 I=0
2026-01-29 17:48:41,157 | INFO | Chunk: 123 | WER=33.333333 | S=0 D=0 I=1
2026-01-29 17:48:41,157 | INFO | Chunk: 124 | WER=16.666667 | S=0 D=0 I=1
2026-01-29 17:48:41,795 | INFO | File: Rhap-M2004.wav | WER=14.048866 | S=53 D=6 I=102
2026-01-29 17:48:41,796 | INFO | ------------------------------
2026-01-29 17:48:41,796 | INFO | Conf cv Done!
2026-01-29 17:48:41,925 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-29 17:48:41,943 | INFO | Vocabulary size: 47
2026-01-29 17:48:42,466 | INFO | Gradient checkpoint layers: []
2026-01-29 17:48:43,092 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 17:48:43,096 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 17:48:43,096 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 17:48:43,096 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-29 17:48:43,099 | INFO | speech length: 15520
2026-01-29 17:48:43,127 | INFO | decoder input length: 23
2026-01-29 17:48:43,127 | INFO | max output length: 23
2026-01-29 17:48:43,127 | INFO | min output length: 2
2026-01-29 17:48:43,748 | INFO | adding <eos> in the last position in the loop
2026-01-29 17:48:43,754 | INFO | no hypothesis. Finish decoding.
2026-01-29 17:48:43,754 | INFO |  -2.84 * 0.5 =  -1.42 for decoder
2026-01-29 17:48:43,755 | INFO |  -2.30 * 0.5 =  -1.15 for ctc
2026-01-29 17:48:43,755 | INFO | total log probability: -2.57
2026-01-29 17:48:43,755 | INFO | normalized log probability: -0.10
2026-01-29 17:48:43,755 | INFO | total number of ended hypotheses: 53
2026-01-29 17:48:43,755 | INFO | best hypo: mes<space>chers<space>compatriotes<sos/eos>

2026-01-29 17:48:43,755 | WARNING | best hypo length: 23 == max output length: 23
2026-01-29 17:48:43,755 | WARNING | decoding may be stopped by the max output length limitation, please consider to increase the maxlenratio.
2026-01-29 17:48:43,756 | INFO | speech length: 44960
2026-01-29 17:48:43,781 | INFO | decoder input length: 69
2026-01-29 17:48:43,781 | INFO | max output length: 69
2026-01-29 17:48:43,781 | INFO | min output length: 6
2026-01-29 17:48:45,390 | INFO | end detected at 50
2026-01-29 17:48:45,391 | INFO |  -4.48 * 0.5 =  -2.24 for decoder
2026-01-29 17:48:45,391 | INFO |  -4.31 * 0.5 =  -2.15 for ctc
2026-01-29 17:48:45,392 | INFO | total log probability: -4.39
2026-01-29 17:48:45,392 | INFO | normalized log probability: -0.10
2026-01-29 17:48:45,392 | INFO | total number of ended hypotheses: 184
2026-01-29 17:48:45,392 | INFO | best hypo: je<space>voudrais<space>d'abord<space>euh<space>exprimer<space>ma<space>sadette

2026-01-29 17:48:45,394 | INFO | speech length: 107360
2026-01-29 17:48:45,421 | INFO | decoder input length: 167
2026-01-29 17:48:45,422 | INFO | max output length: 167
2026-01-29 17:48:45,422 | INFO | min output length: 16
2026-01-29 17:48:50,138 | INFO | end detected at 122
2026-01-29 17:48:50,139 | INFO |  -9.49 * 0.5 =  -4.75 for decoder
2026-01-29 17:48:50,139 | INFO |  -0.90 * 0.5 =  -0.45 for ctc
2026-01-29 17:48:50,139 | INFO | total log probability: -5.20
2026-01-29 17:48:50,139 | INFO | normalized log probability: -0.04
2026-01-29 17:48:50,139 | INFO | total number of ended hypotheses: 177
2026-01-29 17:48:50,141 | INFO | best hypo: à<space>toute<space>seule<space>et<space>à<space>tous<space>ceux<space>qui<space>vivent<space>ces<space>derniers<space>jours<space>de<space>mille<space>neuf<space>cent<space>quatre<space>vingt<space>dix<space>neuf<space>dans<space>l'épreuve

2026-01-29 17:48:50,142 | INFO | speech length: 48800
2026-01-29 17:48:50,170 | INFO | decoder input length: 75
2026-01-29 17:48:50,170 | INFO | max output length: 75
2026-01-29 17:48:50,170 | INFO | min output length: 7
2026-01-29 17:48:51,928 | INFO | end detected at 55
2026-01-29 17:48:51,930 | INFO |  -3.78 * 0.5 =  -1.89 for decoder
2026-01-29 17:48:51,930 | INFO |  -0.63 * 0.5 =  -0.31 for ctc
2026-01-29 17:48:51,930 | INFO | total log probability: -2.20
2026-01-29 17:48:51,930 | INFO | normalized log probability: -0.05
2026-01-29 17:48:51,930 | INFO | total number of ended hypotheses: 180
2026-01-29 17:48:51,931 | INFO | best hypo: je<space>pense<space>aux<space>nombreuses<space>victimes<space>de<space>la<space>tempête

2026-01-29 17:48:51,932 | INFO | speech length: 59520
2026-01-29 17:48:51,959 | INFO | decoder input length: 92
2026-01-29 17:48:51,959 | INFO | max output length: 92
2026-01-29 17:48:51,959 | INFO | min output length: 9
2026-01-29 17:48:54,341 | INFO | end detected at 72
2026-01-29 17:48:54,342 | INFO |  -5.25 * 0.5 =  -2.62 for decoder
2026-01-29 17:48:54,342 | INFO |  -0.03 * 0.5 =  -0.02 for ctc
2026-01-29 17:48:54,342 | INFO | total log probability: -2.64
2026-01-29 17:48:54,342 | INFO | normalized log probability: -0.04
2026-01-29 17:48:54,342 | INFO | total number of ended hypotheses: 174
2026-01-29 17:48:54,343 | INFO | best hypo: et<space>à<space>toutes<space>les<space>familles<space>endeuillées<space>dont<space>nous<space>partageons<space>la<space>paix

2026-01-29 17:48:54,345 | INFO | speech length: 75520
2026-01-29 17:48:54,372 | INFO | decoder input length: 117
2026-01-29 17:48:54,372 | INFO | max output length: 117
2026-01-29 17:48:54,372 | INFO | min output length: 11
2026-01-29 17:48:57,175 | INFO | end detected at 79
2026-01-29 17:48:57,176 | INFO |  -5.83 * 0.5 =  -2.91 for decoder
2026-01-29 17:48:57,176 | INFO |  -0.03 * 0.5 =  -0.01 for ctc
2026-01-29 17:48:57,176 | INFO | total log probability: -2.93
2026-01-29 17:48:57,176 | INFO | normalized log probability: -0.04
2026-01-29 17:48:57,176 | INFO | total number of ended hypotheses: 171
2026-01-29 17:48:57,177 | INFO | best hypo: je<space>pense<space>à<space>nos<space>concitoyens<space>cruellement<space>touchés<space>dans<space>leur<space>vie<space>quotidienne

2026-01-29 17:48:57,179 | INFO | speech length: 92160
2026-01-29 17:48:57,206 | INFO | decoder input length: 143
2026-01-29 17:48:57,206 | INFO | max output length: 143
2026-01-29 17:48:57,206 | INFO | min output length: 14
2026-01-29 17:49:00,899 | INFO | end detected at 100
2026-01-29 17:49:00,900 | INFO |  -7.51 * 0.5 =  -3.75 for decoder
2026-01-29 17:49:00,900 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-29 17:49:00,900 | INFO | total log probability: -3.76
2026-01-29 17:49:00,900 | INFO | normalized log probability: -0.04
2026-01-29 17:49:00,900 | INFO | total number of ended hypotheses: 178
2026-01-29 17:49:00,902 | INFO | best hypo: à<space>ceux<space>dont<space>les<space>biens<space>ont<space>été<space>détruits<space>à<space>ceux<space>qui<space>craignent<space>pour<space>leur<space>activité<space>et<space>leur<space>emploi

2026-01-29 17:49:00,903 | INFO | speech length: 122720
2026-01-29 17:49:00,930 | INFO | decoder input length: 191
2026-01-29 17:49:00,930 | INFO | max output length: 191
2026-01-29 17:49:00,930 | INFO | min output length: 19
2026-01-29 17:49:05,230 | INFO | end detected at 102
2026-01-29 17:49:05,232 | INFO |  -8.50 * 0.5 =  -4.25 for decoder
2026-01-29 17:49:05,232 | INFO |  -2.59 * 0.5 =  -1.29 for ctc
2026-01-29 17:49:05,232 | INFO | total log probability: -5.55
2026-01-29 17:49:05,232 | INFO | normalized log probability: -0.06
2026-01-29 17:49:05,232 | INFO | total number of ended hypotheses: 169
2026-01-29 17:49:05,233 | INFO | best hypo: à<space>ceux<space>qui<space>souffrent<space>de<space>voir<space>notre<space>patrimoine<space>notre<space>littoral<space>nos<space>forêts<space>nos<space>monuments<space>défigurés

2026-01-29 17:49:05,235 | INFO | speech length: 20800
2026-01-29 17:49:05,261 | INFO | decoder input length: 32
2026-01-29 17:49:05,261 | INFO | max output length: 32
2026-01-29 17:49:05,261 | INFO | min output length: 3
2026-01-29 17:49:06,130 | INFO | adding <eos> in the last position in the loop
2026-01-29 17:49:06,138 | INFO | no hypothesis. Finish decoding.
2026-01-29 17:49:06,139 | INFO |  -3.97 * 0.5 =  -1.99 for decoder
2026-01-29 17:49:06,140 | INFO |  -2.28 * 0.5 =  -1.14 for ctc
2026-01-29 17:49:06,140 | INFO | total log probability: -3.12
2026-01-29 17:49:06,140 | INFO | normalized log probability: -0.12
2026-01-29 17:49:06,140 | INFO | total number of ended hypotheses: 159
2026-01-29 17:49:06,140 | INFO | best hypo: je<space>vous<space>redit<space>mon<space>émotion

2026-01-29 17:49:06,141 | INFO | speech length: 217280
2026-01-29 17:49:06,168 | INFO | decoder input length: 339
2026-01-29 17:49:06,168 | INFO | max output length: 339
2026-01-29 17:49:06,168 | INFO | min output length: 33
2026-01-29 17:49:16,622 | INFO | end detected at 188
2026-01-29 17:49:16,623 | INFO | -14.99 * 0.5 =  -7.50 for decoder
2026-01-29 17:49:16,624 | INFO |  -2.57 * 0.5 =  -1.29 for ctc
2026-01-29 17:49:16,624 | INFO | total log probability: -8.78
2026-01-29 17:49:16,624 | INFO | normalized log probability: -0.05
2026-01-29 17:49:16,624 | INFO | total number of ended hypotheses: 167
2026-01-29 17:49:16,626 | INFO | best hypo: mais<space>aussi<space>ma<space>fierté<space>devant<space>l'exceptionnel<space>élan<space>de<space>solidarité<space>qui<space>anime<space>tant<space>de<space>bénévoles<space>et<space>d'associations<space>mobilisées<space>aux<space>côtés<space>des<space>services<space>publics<space>civils<space>et<space>militaires<space>et<space>des<space>élus

2026-01-29 17:49:16,628 | INFO | speech length: 91360
2026-01-29 17:49:16,657 | INFO | decoder input length: 142
2026-01-29 17:49:16,657 | INFO | max output length: 142
2026-01-29 17:49:16,657 | INFO | min output length: 14
2026-01-29 17:49:19,956 | INFO | end detected at 86
2026-01-29 17:49:19,958 | INFO |  -6.77 * 0.5 =  -3.39 for decoder
2026-01-29 17:49:19,958 | INFO |  -1.70 * 0.5 =  -0.85 for ctc
2026-01-29 17:49:19,958 | INFO | total log probability: -4.24
2026-01-29 17:49:19,959 | INFO | normalized log probability: -0.05
2026-01-29 17:49:19,959 | INFO | total number of ended hypotheses: 211
2026-01-29 17:49:19,960 | INFO | best hypo: en<space>ces<space>heures<space>difficiles<space>nous<space>ressentons<space>profondément<space>la<space>fragilité<space>des<space>choses

2026-01-29 17:49:19,963 | INFO | speech length: 56960
2026-01-29 17:49:20,000 | INFO | decoder input length: 88
2026-01-29 17:49:20,000 | INFO | max output length: 88
2026-01-29 17:49:20,000 | INFO | min output length: 8
2026-01-29 17:49:21,688 | INFO | end detected at 50
2026-01-29 17:49:21,689 | INFO |  -3.58 * 0.5 =  -1.79 for decoder
2026-01-29 17:49:21,689 | INFO |  -0.18 * 0.5 =  -0.09 for ctc
2026-01-29 17:49:21,689 | INFO | total log probability: -1.88
2026-01-29 17:49:21,689 | INFO | normalized log probability: -0.04
2026-01-29 17:49:21,689 | INFO | total number of ended hypotheses: 165
2026-01-29 17:49:21,690 | INFO | best hypo: la<space>précarité<space>de<space>ce<space>qui<space>nous<space>semblait<space>acquis

2026-01-29 17:49:21,691 | INFO | speech length: 140800
2026-01-29 17:49:21,718 | INFO | decoder input length: 219
2026-01-29 17:49:21,718 | INFO | max output length: 219
2026-01-29 17:49:21,718 | INFO | min output length: 21
2026-01-29 17:49:27,827 | INFO | end detected at 142
2026-01-29 17:49:27,828 | INFO | -10.86 * 0.5 =  -5.43 for decoder
2026-01-29 17:49:27,828 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-29 17:49:27,828 | INFO | total log probability: -5.43
2026-01-29 17:49:27,828 | INFO | normalized log probability: -0.04
2026-01-29 17:49:27,828 | INFO | total number of ended hypotheses: 174
2026-01-29 17:49:27,830 | INFO | best hypo: nous<space>voyons<space>combien<space>tout<space>peut<space>être<space>parfois<space>remis<space>en<space>cause<space>du<space>fait<space>de<space>l'inconscience<space>des<space>hommes<space>ou<space>du<space>déchaînement<space>des<space>éléments<space>naturels

2026-01-29 17:49:27,832 | INFO | speech length: 84320
2026-01-29 17:49:27,867 | INFO | decoder input length: 131
2026-01-29 17:49:27,867 | INFO | max output length: 131
2026-01-29 17:49:27,867 | INFO | min output length: 13
2026-01-29 17:49:30,690 | INFO | end detected at 77
2026-01-29 17:49:30,692 | INFO |  -6.62 * 0.5 =  -3.31 for decoder
2026-01-29 17:49:30,692 | INFO |  -5.11 * 0.5 =  -2.55 for ctc
2026-01-29 17:49:30,692 | INFO | total log probability: -5.86
2026-01-29 17:49:30,692 | INFO | normalized log probability: -0.08
2026-01-29 17:49:30,692 | INFO | total number of ended hypotheses: 179
2026-01-29 17:49:30,693 | INFO | best hypo: nous<space>mesurons<space>aussi<space>l'importance<space>du<space>rôle<space>de<space>l'état<space>dans<space>notre<space>société

2026-01-29 17:49:30,694 | INFO | speech length: 65600
2026-01-29 17:49:30,724 | INFO | decoder input length: 102
2026-01-29 17:49:30,724 | INFO | max output length: 102
2026-01-29 17:49:30,724 | INFO | min output length: 10
2026-01-29 17:49:32,951 | INFO | end detected at 65
2026-01-29 17:49:32,953 | INFO |  -4.89 * 0.5 =  -2.44 for decoder
2026-01-29 17:49:32,953 | INFO |  -0.32 * 0.5 =  -0.16 for ctc
2026-01-29 17:49:32,953 | INFO | total log probability: -2.60
2026-01-29 17:49:32,953 | INFO | normalized log probability: -0.04
2026-01-29 17:49:32,953 | INFO | total number of ended hypotheses: 173
2026-01-29 17:49:32,954 | INFO | best hypo: un<space>état<space>sur<space>lequel<space>pèsent<space>des<space>responsabilités<space>essentielles

2026-01-29 17:49:32,956 | INFO | speech length: 57600
2026-01-29 17:49:32,995 | INFO | decoder input length: 89
2026-01-29 17:49:32,995 | INFO | max output length: 89
2026-01-29 17:49:32,995 | INFO | min output length: 8
2026-01-29 17:49:34,693 | INFO | end detected at 50
2026-01-29 17:49:34,694 | INFO |  -3.59 * 0.5 =  -1.80 for decoder
2026-01-29 17:49:34,695 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-29 17:49:34,695 | INFO | total log probability: -1.80
2026-01-29 17:49:34,695 | INFO | normalized log probability: -0.04
2026-01-29 17:49:34,695 | INFO | total number of ended hypotheses: 160
2026-01-29 17:49:34,695 | INFO | best hypo: le<space>service<space>public<space>la<space>sécurité<space>la<space>solidarité

2026-01-29 17:49:34,697 | INFO | speech length: 122080
2026-01-29 17:49:34,723 | INFO | decoder input length: 190
2026-01-29 17:49:34,723 | INFO | max output length: 190
2026-01-29 17:49:34,723 | INFO | min output length: 19
2026-01-29 17:49:39,088 | INFO | end detected at 105
2026-01-29 17:49:39,089 | INFO |  -7.96 * 0.5 =  -3.98 for decoder
2026-01-29 17:49:39,089 | INFO |  -0.03 * 0.5 =  -0.02 for ctc
2026-01-29 17:49:39,089 | INFO | total log probability: -4.00
2026-01-29 17:49:39,089 | INFO | normalized log probability: -0.04
2026-01-29 17:49:39,089 | INFO | total number of ended hypotheses: 177
2026-01-29 17:49:39,090 | INFO | best hypo: un<space>état<space>auquel<space>il<space>appartient<space>de<space>prévoir<space>de<space>faire<space>face<space>d'assurer<space>la<space>coordination<space>des<space>moyens<space>du<space>pays

2026-01-29 17:49:39,092 | INFO | speech length: 70400
2026-01-29 17:49:39,117 | INFO | decoder input length: 109
2026-01-29 17:49:39,117 | INFO | max output length: 109
2026-01-29 17:49:39,117 | INFO | min output length: 10
2026-01-29 17:49:41,152 | INFO | end detected at 58
2026-01-29 17:49:41,153 | INFO |  -4.15 * 0.5 =  -2.08 for decoder
2026-01-29 17:49:41,153 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:49:41,153 | INFO | total log probability: -2.08
2026-01-29 17:49:41,153 | INFO | normalized log probability: -0.04
2026-01-29 17:49:41,153 | INFO | total number of ended hypotheses: 172
2026-01-29 17:49:41,154 | INFO | best hypo: nous<space>mesurons<space>surtout<space>le<space>prix<space>de<space>l'aide<space>fraternelle

2026-01-29 17:49:41,155 | INFO | speech length: 45120
2026-01-29 17:49:41,180 | INFO | decoder input length: 70
2026-01-29 17:49:41,181 | INFO | max output length: 70
2026-01-29 17:49:41,181 | INFO | min output length: 7
2026-01-29 17:49:42,585 | INFO | end detected at 44
2026-01-29 17:49:42,586 | INFO |  -3.11 * 0.5 =  -1.55 for decoder
2026-01-29 17:49:42,586 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-29 17:49:42,586 | INFO | total log probability: -1.57
2026-01-29 17:49:42,586 | INFO | normalized log probability: -0.04
2026-01-29 17:49:42,586 | INFO | total number of ended hypotheses: 166
2026-01-29 17:49:42,587 | INFO | best hypo: du<space>soutien<space>spontané<space>de<space>la<space>main<space>tendue

2026-01-29 17:49:42,588 | INFO | speech length: 35200
2026-01-29 17:49:42,613 | INFO | decoder input length: 54
2026-01-29 17:49:42,613 | INFO | max output length: 54
2026-01-29 17:49:42,613 | INFO | min output length: 5
2026-01-29 17:49:43,908 | INFO | end detected at 42
2026-01-29 17:49:43,909 | INFO |  -2.98 * 0.5 =  -1.49 for decoder
2026-01-29 17:49:43,909 | INFO |  -0.10 * 0.5 =  -0.05 for ctc
2026-01-29 17:49:43,909 | INFO | total log probability: -1.54
2026-01-29 17:49:43,909 | INFO | normalized log probability: -0.04
2026-01-29 17:49:43,909 | INFO | total number of ended hypotheses: 156
2026-01-29 17:49:43,910 | INFO | best hypo: qui<space>sont<space>le<space>ciment<space>même<space>de<space>la<space>nation

2026-01-29 17:49:43,911 | INFO | speech length: 54720
2026-01-29 17:49:43,936 | INFO | decoder input length: 85
2026-01-29 17:49:43,936 | INFO | max output length: 85
2026-01-29 17:49:43,936 | INFO | min output length: 8
2026-01-29 17:49:46,121 | INFO | end detected at 66
2026-01-29 17:49:46,123 | INFO |  -4.81 * 0.5 =  -2.40 for decoder
2026-01-29 17:49:46,123 | INFO |  -0.22 * 0.5 =  -0.11 for ctc
2026-01-29 17:49:46,123 | INFO | total log probability: -2.51
2026-01-29 17:49:46,123 | INFO | normalized log probability: -0.04
2026-01-29 17:49:46,123 | INFO | total number of ended hypotheses: 179
2026-01-29 17:49:46,124 | INFO | best hypo: au<space>moment<space>où<space>où<space>nous<space>touchons<space>aux<space>portes<space>de<space>l'an<space>deux<space>mille

2026-01-29 17:49:46,125 | INFO | speech length: 83200
2026-01-29 17:49:46,150 | INFO | decoder input length: 129
2026-01-29 17:49:46,150 | INFO | max output length: 129
2026-01-29 17:49:46,151 | INFO | min output length: 12
2026-01-29 17:49:48,986 | INFO | end detected at 69
2026-01-29 17:49:48,987 | INFO |  -5.08 * 0.5 =  -2.54 for decoder
2026-01-29 17:49:48,987 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-29 17:49:48,987 | INFO | total log probability: -2.55
2026-01-29 17:49:48,987 | INFO | normalized log probability: -0.04
2026-01-29 17:49:48,987 | INFO | total number of ended hypotheses: 168
2026-01-29 17:49:48,988 | INFO | best hypo: rien<space>n'est<space>décidément<space>plus<space>moderne<space>plus<space>nécessaire<space>plus<space>solide

2026-01-29 17:49:48,989 | INFO | speech length: 56000
2026-01-29 17:49:49,014 | INFO | decoder input length: 87
2026-01-29 17:49:49,014 | INFO | max output length: 87
2026-01-29 17:49:49,014 | INFO | min output length: 8
2026-01-29 17:49:50,922 | INFO | end detected at 58
2026-01-29 17:49:50,923 | INFO |  -4.19 * 0.5 =  -2.09 for decoder
2026-01-29 17:49:50,923 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:49:50,923 | INFO | total log probability: -2.09
2026-01-29 17:49:50,923 | INFO | normalized log probability: -0.04
2026-01-29 17:49:50,923 | INFO | total number of ended hypotheses: 174
2026-01-29 17:49:50,924 | INFO | best hypo: que<space>le<space>sentiment<space>d'appartenir<space>à<space>une<space>même<space>communauté

2026-01-29 17:49:50,926 | INFO | speech length: 50720
2026-01-29 17:49:50,951 | INFO | decoder input length: 78
2026-01-29 17:49:50,951 | INFO | max output length: 78
2026-01-29 17:49:50,951 | INFO | min output length: 7
2026-01-29 17:49:52,526 | INFO | end detected at 48
2026-01-29 17:49:52,527 | INFO |  -3.85 * 0.5 =  -1.93 for decoder
2026-01-29 17:49:52,527 | INFO |  -1.97 * 0.5 =  -0.99 for ctc
2026-01-29 17:49:52,527 | INFO | total log probability: -2.91
2026-01-29 17:49:52,527 | INFO | normalized log probability: -0.07
2026-01-29 17:49:52,527 | INFO | total number of ended hypotheses: 164
2026-01-29 17:49:52,528 | INFO | best hypo: et<space>d'être<space>responsables<space>les<space>uns<space>des<space>autres

2026-01-29 17:49:52,529 | INFO | speech length: 98560
2026-01-29 17:49:52,560 | INFO | decoder input length: 153
2026-01-29 17:49:52,561 | INFO | max output length: 153
2026-01-29 17:49:52,561 | INFO | min output length: 15
2026-01-29 17:49:55,411 | INFO | end detected at 69
2026-01-29 17:49:55,413 | INFO |  -6.28 * 0.5 =  -3.14 for decoder
2026-01-29 17:49:55,413 | INFO |  -1.28 * 0.5 =  -0.64 for ctc
2026-01-29 17:49:55,413 | INFO | total log probability: -3.78
2026-01-29 17:49:55,413 | INFO | normalized log probability: -0.06
2026-01-29 17:49:55,413 | INFO | total number of ended hypotheses: 192
2026-01-29 17:49:55,414 | INFO | best hypo: la<space>france<space>blessée<space>veut<space>se<space>retrouver<space>rassembler<space>et<space>fraternelle

2026-01-29 17:49:55,415 | INFO | speech length: 95040
2026-01-29 17:49:55,442 | INFO | decoder input length: 148
2026-01-29 17:49:55,442 | INFO | max output length: 148
2026-01-29 17:49:55,442 | INFO | min output length: 14
2026-01-29 17:49:58,781 | INFO | end detected at 88
2026-01-29 17:49:58,782 | INFO |  -6.61 * 0.5 =  -3.30 for decoder
2026-01-29 17:49:58,782 | INFO |  -0.52 * 0.5 =  -0.26 for ctc
2026-01-29 17:49:58,782 | INFO | total log probability: -3.56
2026-01-29 17:49:58,782 | INFO | normalized log probability: -0.04
2026-01-29 17:49:58,782 | INFO | total number of ended hypotheses: 169
2026-01-29 17:49:58,783 | INFO | best hypo: parce<space>que<space>nos<space>compatriotes<space>ont<space>toujours<space>su<space>dans<space>l'épreuve<space>faire<space>parler<space>leur<space>coeur

2026-01-29 17:49:58,785 | INFO | speech length: 56960
2026-01-29 17:49:58,813 | INFO | decoder input length: 88
2026-01-29 17:49:58,813 | INFO | max output length: 88
2026-01-29 17:49:58,814 | INFO | min output length: 8
2026-01-29 17:50:00,488 | INFO | end detected at 49
2026-01-29 17:50:00,489 | INFO |  -3.45 * 0.5 =  -1.72 for decoder
2026-01-29 17:50:00,489 | INFO |  -3.25 * 0.5 =  -1.62 for ctc
2026-01-29 17:50:00,489 | INFO | total log probability: -3.35
2026-01-29 17:50:00,490 | INFO | normalized log probability: -0.08
2026-01-29 17:50:00,490 | INFO | total number of ended hypotheses: 176
2026-01-29 17:50:00,490 | INFO | best hypo: je<space>voudrais<space>dire<space>merci<space>à<space>tous<space>les<space>français

2026-01-29 17:50:00,492 | INFO | speech length: 83680
2026-01-29 17:50:00,517 | INFO | decoder input length: 130
2026-01-29 17:50:00,517 | INFO | max output length: 130
2026-01-29 17:50:00,517 | INFO | min output length: 13
2026-01-29 17:50:03,003 | INFO | end detected at 67
2026-01-29 17:50:03,004 | INFO |  -5.20 * 0.5 =  -2.60 for decoder
2026-01-29 17:50:03,004 | INFO |  -0.69 * 0.5 =  -0.35 for ctc
2026-01-29 17:50:03,004 | INFO | total log probability: -2.94
2026-01-29 17:50:03,004 | INFO | normalized log probability: -0.05
2026-01-29 17:50:03,004 | INFO | total number of ended hypotheses: 174
2026-01-29 17:50:03,005 | INFO | best hypo: ce<space>soir<space>euh<space>nous<space>vivons<space>ensemble<space>un<space>moment<space>fort<space>et<space>singulier

2026-01-29 17:50:03,007 | INFO | speech length: 92160
2026-01-29 17:50:03,035 | INFO | decoder input length: 143
2026-01-29 17:50:03,035 | INFO | max output length: 143
2026-01-29 17:50:03,035 | INFO | min output length: 14
2026-01-29 17:50:06,511 | INFO | end detected at 92
2026-01-29 17:50:06,512 | INFO |  -6.81 * 0.5 =  -3.41 for decoder
2026-01-29 17:50:06,512 | INFO |  -0.03 * 0.5 =  -0.02 for ctc
2026-01-29 17:50:06,512 | INFO | total log probability: -3.42
2026-01-29 17:50:06,512 | INFO | normalized log probability: -0.04
2026-01-29 17:50:06,512 | INFO | total number of ended hypotheses: 166
2026-01-29 17:50:06,514 | INFO | best hypo: ce<space>qui<space>paraissait<space>très<space>lointain<space>et<space>qui<space>a<space>longtemps<space>symbolisé<space>le<space>futur<space>l'an<space>deux<space>mille

2026-01-29 17:50:06,515 | INFO | speech length: 29760
2026-01-29 17:50:06,540 | INFO | decoder input length: 46
2026-01-29 17:50:06,540 | INFO | max output length: 46
2026-01-29 17:50:06,540 | INFO | min output length: 4
2026-01-29 17:50:07,538 | INFO | end detected at 33
2026-01-29 17:50:07,539 | INFO |  -3.05 * 0.5 =  -1.52 for decoder
2026-01-29 17:50:07,539 | INFO |  -0.62 * 0.5 =  -0.31 for ctc
2026-01-29 17:50:07,539 | INFO | total log probability: -1.84
2026-01-29 17:50:07,539 | INFO | normalized log probability: -0.07
2026-01-29 17:50:07,539 | INFO | total number of ended hypotheses: 198
2026-01-29 17:50:07,540 | INFO | best hypo: est<space>devenu<space>contemporain

2026-01-29 17:50:07,542 | INFO | speech length: 9760
2026-01-29 17:50:07,563 | INFO | decoder input length: 14
2026-01-29 17:50:07,563 | INFO | max output length: 14
2026-01-29 17:50:07,563 | INFO | min output length: 1
2026-01-29 17:50:07,932 | INFO | adding <eos> in the last position in the loop
2026-01-29 17:50:07,940 | INFO | no hypothesis. Finish decoding.
2026-01-29 17:50:07,940 | INFO |  -1.63 * 0.5 =  -0.82 for decoder
2026-01-29 17:50:07,941 | INFO |  -0.64 * 0.5 =  -0.32 for ctc
2026-01-29 17:50:07,941 | INFO | total log probability: -1.13
2026-01-29 17:50:07,941 | INFO | normalized log probability: -0.11
2026-01-29 17:50:07,941 | INFO | total number of ended hypotheses: 131
2026-01-29 17:50:07,941 | INFO | best hypo: immédiat

2026-01-29 17:50:07,942 | INFO | speech length: 106240
2026-01-29 17:50:07,969 | INFO | decoder input length: 165
2026-01-29 17:50:07,969 | INFO | max output length: 165
2026-01-29 17:50:07,969 | INFO | min output length: 16
2026-01-29 17:50:12,249 | INFO | end detected at 112
2026-01-29 17:50:12,251 | INFO | -11.58 * 0.5 =  -5.79 for decoder
2026-01-29 17:50:12,251 | INFO |  -1.11 * 0.5 =  -0.55 for ctc
2026-01-29 17:50:12,251 | INFO | total log probability: -6.34
2026-01-29 17:50:12,251 | INFO | normalized log probability: -0.06
2026-01-29 17:50:12,251 | INFO | total number of ended hypotheses: 191
2026-01-29 17:50:12,252 | INFO | best hypo: je<space>suis<space>sûr<space>que<space>beaucoup<space>d'entre<space>vous<space>vont<space>vivre<space>ces<space>instants<space>avec<space>un<space>peu<space>des<space>motions<space>un<space>peu<space>d'étonnement

2026-01-29 17:50:12,254 | INFO | speech length: 164320
2026-01-29 17:50:12,280 | INFO | decoder input length: 256
2026-01-29 17:50:12,280 | INFO | max output length: 256
2026-01-29 17:50:12,280 | INFO | min output length: 25
2026-01-29 17:50:19,484 | INFO | end detected at 154
2026-01-29 17:50:19,485 | INFO | -11.94 * 0.5 =  -5.97 for decoder
2026-01-29 17:50:19,485 | INFO |  -7.73 * 0.5 =  -3.87 for ctc
2026-01-29 17:50:19,485 | INFO | total log probability: -9.84
2026-01-29 17:50:19,485 | INFO | normalized log probability: -0.07
2026-01-29 17:50:19,485 | INFO | total number of ended hypotheses: 160
2026-01-29 17:50:19,487 | INFO | best hypo: une<space>certaine<space>appréhension<space>parfois<space>née<space>du<space>sentiment<space>que<space>s'achève<space>une<space>époque<space>dont<space>on<space>possédait<space>les<space>clés<space>dont<space>on<space>maîtrisait<space>les<space>règles<space>et<space>les<space>habitudes

2026-01-29 17:50:19,488 | INFO | speech length: 30560
2026-01-29 17:50:19,516 | INFO | decoder input length: 47
2026-01-29 17:50:19,516 | INFO | max output length: 47
2026-01-29 17:50:19,516 | INFO | min output length: 4
2026-01-29 17:50:20,766 | INFO | end detected at 43
2026-01-29 17:50:20,767 | INFO |  -3.10 * 0.5 =  -1.55 for decoder
2026-01-29 17:50:20,767 | INFO |  -1.85 * 0.5 =  -0.92 for ctc
2026-01-29 17:50:20,767 | INFO | total log probability: -2.47
2026-01-29 17:50:20,767 | INFO | normalized log probability: -0.07
2026-01-29 17:50:20,767 | INFO | total number of ended hypotheses: 181
2026-01-29 17:50:20,767 | INFO | best hypo: je<space>comprends<space>ces<space>mouvements<space>de<space>l'âme

2026-01-29 17:50:20,769 | INFO | speech length: 15040
2026-01-29 17:50:20,795 | INFO | decoder input length: 23
2026-01-29 17:50:20,795 | INFO | max output length: 23
2026-01-29 17:50:20,795 | INFO | min output length: 2
2026-01-29 17:50:21,574 | INFO | adding <eos> in the last position in the loop
2026-01-29 17:50:21,580 | INFO | no hypothesis. Finish decoding.
2026-01-29 17:50:21,581 | INFO |  -6.73 * 0.5 =  -3.37 for decoder
2026-01-29 17:50:21,581 | INFO |  -2.46 * 0.5 =  -1.23 for ctc
2026-01-29 17:50:21,581 | INFO | total log probability: -4.60
2026-01-29 17:50:21,581 | INFO | normalized log probability: -0.18
2026-01-29 17:50:21,581 | INFO | total number of ended hypotheses: 57
2026-01-29 17:50:21,581 | INFO | best hypo: pourtant<space>j'ai<space>confirme<sos/eos>

2026-01-29 17:50:21,581 | WARNING | best hypo length: 23 == max output length: 23
2026-01-29 17:50:21,581 | WARNING | decoding may be stopped by the max output length limitation, please consider to increase the maxlenratio.
2026-01-29 17:50:21,582 | INFO | speech length: 99200
2026-01-29 17:50:21,609 | INFO | decoder input length: 154
2026-01-29 17:50:21,609 | INFO | max output length: 154
2026-01-29 17:50:21,609 | INFO | min output length: 15
2026-01-29 17:50:25,189 | INFO | end detected at 93
2026-01-29 17:50:25,190 | INFO |  -7.85 * 0.5 =  -3.92 for decoder
2026-01-29 17:50:25,191 | INFO |  -1.85 * 0.5 =  -0.92 for ctc
2026-01-29 17:50:25,191 | INFO | total log probability: -4.85
2026-01-29 17:50:25,191 | INFO | normalized log probability: -0.06
2026-01-29 17:50:25,191 | INFO | total number of ended hypotheses: 164
2026-01-29 17:50:25,192 | INFO | best hypo: la<space>france<space>franchir<space>les<space>obstacles<space>comme<space>elle<space>l'a<space>toujours<space>fait<space>au<space>long<space>de<space>son<space>histoire

2026-01-29 17:50:25,193 | INFO | speech length: 31840
2026-01-29 17:50:25,221 | INFO | decoder input length: 49
2026-01-29 17:50:25,221 | INFO | max output length: 49
2026-01-29 17:50:25,221 | INFO | min output length: 4
2026-01-29 17:50:26,517 | INFO | end detected at 41
2026-01-29 17:50:26,519 | INFO |  -3.72 * 0.5 =  -1.86 for decoder
2026-01-29 17:50:26,519 | INFO |  -6.51 * 0.5 =  -3.25 for ctc
2026-01-29 17:50:26,519 | INFO | total log probability: -5.11
2026-01-29 17:50:26,519 | INFO | normalized log probability: -0.16
2026-01-29 17:50:26,519 | INFO | total number of ended hypotheses: 219
2026-01-29 17:50:26,520 | INFO | best hypo: pour<space>quoi<space>qu'elle<space>soit<space>fidèle

2026-01-29 17:50:26,522 | INFO | speech length: 220640
2026-01-29 17:50:26,551 | INFO | decoder input length: 344
2026-01-29 17:50:26,551 | INFO | max output length: 344
2026-01-29 17:50:26,551 | INFO | min output length: 34
2026-01-29 17:50:37,021 | INFO | end detected at 190
2026-01-29 17:50:37,021 | INFO | -18.86 * 0.5 =  -9.43 for decoder
2026-01-29 17:50:37,021 | INFO |  -0.21 * 0.5 =  -0.10 for ctc
2026-01-29 17:50:37,022 | INFO | total log probability: -9.53
2026-01-29 17:50:37,022 | INFO | normalized log probability: -0.05
2026-01-29 17:50:37,022 | INFO | total number of ended hypotheses: 164
2026-01-29 17:50:37,024 | INFO | best hypo: même<space>si<space>le<space>passé<space>est<space>bien<space>présent<space>dans<space>notre<space>mémoire<space>je<space>ne<space>m'attarderai<space>pas<space>sur<space>le<space>siècle<space>qui<space>s'achève<space>siècle<space>de<space>progrès<space>sans<space>précédent<space>pour<space>la<space>santé<space>l'éducation<space>les<space>conditions<space>de<space>vie

2026-01-29 17:50:37,025 | INFO | speech length: 81600
2026-01-29 17:50:37,052 | INFO | decoder input length: 127
2026-01-29 17:50:37,052 | INFO | max output length: 127
2026-01-29 17:50:37,052 | INFO | min output length: 12
2026-01-29 17:50:40,068 | INFO | end detected at 85
2026-01-29 17:50:40,069 | INFO |  -6.31 * 0.5 =  -3.16 for decoder
2026-01-29 17:50:40,069 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-29 17:50:40,069 | INFO | total log probability: -3.17
2026-01-29 17:50:40,069 | INFO | normalized log probability: -0.04
2026-01-29 17:50:40,069 | INFO | total number of ended hypotheses: 183
2026-01-29 17:50:40,070 | INFO | best hypo: pour<space>les<space>libertés<space>la<space>vie<space>démocratique<space>la<space>situation<space>des<space>femmes<space>les<space>solidarités

2026-01-29 17:50:40,072 | INFO | speech length: 71360
2026-01-29 17:50:40,098 | INFO | decoder input length: 111
2026-01-29 17:50:40,098 | INFO | max output length: 111
2026-01-29 17:50:40,098 | INFO | min output length: 11
2026-01-29 17:50:42,490 | INFO | end detected at 68
2026-01-29 17:50:42,492 | INFO |  -6.33 * 0.5 =  -3.17 for decoder
2026-01-29 17:50:42,492 | INFO |  -8.26 * 0.5 =  -4.13 for ctc
2026-01-29 17:50:42,492 | INFO | total log probability: -7.29
2026-01-29 17:50:42,492 | INFO | normalized log probability: -0.12
2026-01-29 17:50:42,492 | INFO | total number of ended hypotheses: 209
2026-01-29 17:50:42,493 | INFO | best hypo: mais<space>aussi<space>euh<space>siècle<space>d'horreur<space>de<space>tragédie<space>de<space>convulsion

2026-01-29 17:50:42,495 | INFO | speech length: 83520
2026-01-29 17:50:42,522 | INFO | decoder input length: 130
2026-01-29 17:50:42,522 | INFO | max output length: 130
2026-01-29 17:50:42,522 | INFO | min output length: 13
2026-01-29 17:50:45,363 | INFO | end detected at 78
2026-01-29 17:50:45,364 | INFO |  -8.84 * 0.5 =  -4.42 for decoder
2026-01-29 17:50:45,364 | INFO |  -2.63 * 0.5 =  -1.31 for ctc
2026-01-29 17:50:45,364 | INFO | total log probability: -5.73
2026-01-29 17:50:45,365 | INFO | normalized log probability: -0.08
2026-01-29 17:50:45,365 | INFO | total number of ended hypotheses: 157
2026-01-29 17:50:45,366 | INFO | best hypo: qui<space>a<space>vu<space>deux<space>guerres<space>mondiales<space>le<space>goulingue<space>les<space>dictatures<space>totalitaires

2026-01-29 17:50:45,367 | INFO | speech length: 13920
2026-01-29 17:50:45,393 | INFO | decoder input length: 21
2026-01-29 17:50:45,393 | INFO | max output length: 21
2026-01-29 17:50:45,393 | INFO | min output length: 2
2026-01-29 17:50:45,913 | INFO | end detected at 18
2026-01-29 17:50:45,914 | INFO |  -1.00 * 0.5 =  -0.50 for decoder
2026-01-29 17:50:45,914 | INFO |  -0.29 * 0.5 =  -0.15 for ctc
2026-01-29 17:50:45,914 | INFO | total log probability: -0.65
2026-01-29 17:50:45,914 | INFO | normalized log probability: -0.05
2026-01-29 17:50:45,914 | INFO | total number of ended hypotheses: 170
2026-01-29 17:50:45,914 | INFO | best hypo: et<space>la<space>choix

2026-01-29 17:50:45,915 | INFO | speech length: 65440
2026-01-29 17:50:45,942 | INFO | decoder input length: 101
2026-01-29 17:50:45,942 | INFO | max output length: 101
2026-01-29 17:50:45,942 | INFO | min output length: 10
2026-01-29 17:50:47,873 | INFO | end detected at 56
2026-01-29 17:50:47,874 | INFO |  -4.02 * 0.5 =  -2.01 for decoder
2026-01-29 17:50:47,874 | INFO |  -0.32 * 0.5 =  -0.16 for ctc
2026-01-29 17:50:47,874 | INFO | total log probability: -2.17
2026-01-29 17:50:47,875 | INFO | normalized log probability: -0.04
2026-01-29 17:50:47,875 | INFO | total number of ended hypotheses: 172
2026-01-29 17:50:47,875 | INFO | best hypo: mais<space>ce<space>soir<space>ce<space>qui<space>importe<space>c'est<space>l'avenir<space>notre

2026-01-29 17:50:47,877 | INFO | speech length: 18240
2026-01-29 17:50:47,904 | INFO | decoder input length: 28
2026-01-29 17:50:47,904 | INFO | max output length: 28
2026-01-29 17:50:47,904 | INFO | min output length: 2
2026-01-29 17:50:48,662 | INFO | end detected at 26
2026-01-29 17:50:48,663 | INFO |  -1.69 * 0.5 =  -0.84 for decoder
2026-01-29 17:50:48,663 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:50:48,663 | INFO | total log probability: -0.84
2026-01-29 17:50:48,663 | INFO | normalized log probability: -0.04
2026-01-29 17:50:48,663 | INFO | total number of ended hypotheses: 142
2026-01-29 17:50:48,664 | INFO | best hypo: celui<space>de<space>nos<space>enfants

2026-01-29 17:50:48,665 | INFO | speech length: 50560
2026-01-29 17:50:48,697 | INFO | decoder input length: 78
2026-01-29 17:50:48,697 | INFO | max output length: 78
2026-01-29 17:50:48,697 | INFO | min output length: 7
2026-01-29 17:50:50,477 | INFO | end detected at 55
2026-01-29 17:50:50,478 | INFO |  -4.07 * 0.5 =  -2.03 for decoder
2026-01-29 17:50:50,478 | INFO |  -0.11 * 0.5 =  -0.06 for ctc
2026-01-29 17:50:50,478 | INFO | total log probability: -2.09
2026-01-29 17:50:50,478 | INFO | normalized log probability: -0.04
2026-01-29 17:50:50,478 | INFO | total number of ended hypotheses: 166
2026-01-29 17:50:50,479 | INFO | best hypo: le<space>progrès<space>va<space>se<space>poursuivre<space>avec<space>ces<space>hésitations

2026-01-29 17:50:50,480 | INFO | speech length: 106240
2026-01-29 17:50:50,509 | INFO | decoder input length: 165
2026-01-29 17:50:50,509 | INFO | max output length: 165
2026-01-29 17:50:50,509 | INFO | min output length: 16
2026-01-29 17:50:54,562 | INFO | end detected at 104
2026-01-29 17:50:54,563 | INFO |  -8.07 * 0.5 =  -4.04 for decoder
2026-01-29 17:50:54,563 | INFO |  -0.50 * 0.5 =  -0.25 for ctc
2026-01-29 17:50:54,563 | INFO | total log probability: -4.29
2026-01-29 17:50:54,563 | INFO | normalized log probability: -0.04
2026-01-29 17:50:54,563 | INFO | total number of ended hypotheses: 151
2026-01-29 17:50:54,564 | INFO | best hypo: avec<space>ses<space>limites<space>que<space>nous<space>mesurons<space>bien<space>face<space>aux<space>événements<space>récents<space>qui<space>nous<space>invitent<space>à<space>l'humilité

2026-01-29 17:50:54,566 | INFO | speech length: 24480
2026-01-29 17:50:54,593 | INFO | decoder input length: 37
2026-01-29 17:50:54,593 | INFO | max output length: 37
2026-01-29 17:50:54,593 | INFO | min output length: 3
2026-01-29 17:50:55,406 | INFO | end detected at 28
2026-01-29 17:50:55,407 | INFO |  -1.78 * 0.5 =  -0.89 for decoder
2026-01-29 17:50:55,407 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:50:55,407 | INFO | total log probability: -0.89
2026-01-29 17:50:55,407 | INFO | normalized log probability: -0.04
2026-01-29 17:50:55,407 | INFO | total number of ended hypotheses: 170
2026-01-29 17:50:55,408 | INFO | best hypo: progrès<space>de<space>la<space>science

2026-01-29 17:50:55,409 | INFO | speech length: 37280
2026-01-29 17:50:55,436 | INFO | decoder input length: 57
2026-01-29 17:50:55,436 | INFO | max output length: 57
2026-01-29 17:50:55,436 | INFO | min output length: 5
2026-01-29 17:50:57,104 | INFO | end detected at 49
2026-01-29 17:50:57,105 | INFO |  -3.59 * 0.5 =  -1.79 for decoder
2026-01-29 17:50:57,105 | INFO |  -0.46 * 0.5 =  -0.23 for ctc
2026-01-29 17:50:57,105 | INFO | total log probability: -2.02
2026-01-29 17:50:57,105 | INFO | normalized log probability: -0.04
2026-01-29 17:50:57,105 | INFO | total number of ended hypotheses: 158
2026-01-29 17:50:57,106 | INFO | best hypo: progrès<space>des<space>communications<space>entre<space>les<space>hommes

2026-01-29 17:50:57,108 | INFO | speech length: 16960
2026-01-29 17:50:57,134 | INFO | decoder input length: 26
2026-01-29 17:50:57,134 | INFO | max output length: 26
2026-01-29 17:50:57,134 | INFO | min output length: 2
2026-01-29 17:50:57,832 | INFO | adding <eos> in the last position in the loop
2026-01-29 17:50:57,839 | INFO | no hypothesis. Finish decoding.
2026-01-29 17:50:57,840 | INFO |  -2.17 * 0.5 =  -1.09 for decoder
2026-01-29 17:50:57,840 | INFO |  -0.80 * 0.5 =  -0.40 for ctc
2026-01-29 17:50:57,840 | INFO | total log probability: -1.48
2026-01-29 17:50:57,840 | INFO | normalized log probability: -0.06
2026-01-29 17:50:57,840 | INFO | total number of ended hypotheses: 93
2026-01-29 17:50:57,841 | INFO | best hypo: progrès<space>de<space>la<space>médecine

2026-01-29 17:50:57,842 | INFO | speech length: 97280
2026-01-29 17:50:57,868 | INFO | decoder input length: 151
2026-01-29 17:50:57,868 | INFO | max output length: 151
2026-01-29 17:50:57,868 | INFO | min output length: 15
2026-01-29 17:51:01,372 | INFO | end detected at 91
2026-01-29 17:51:01,373 | INFO |  -9.05 * 0.5 =  -4.52 for decoder
2026-01-29 17:51:01,373 | INFO |  -0.74 * 0.5 =  -0.37 for ctc
2026-01-29 17:51:01,373 | INFO | total log probability: -4.89
2026-01-29 17:51:01,373 | INFO | normalized log probability: -0.06
2026-01-29 17:51:01,373 | INFO | total number of ended hypotheses: 173
2026-01-29 17:51:01,374 | INFO | best hypo: un<space>grand<space>nombre<space>des<space>enfants<space>qui<space>vont<space>naître<space>cette<space>année<space>versont<space>l'an<space>deux<space>mille<space>cent

2026-01-29 17:51:01,376 | INFO | speech length: 17920
2026-01-29 17:51:01,403 | INFO | decoder input length: 27
2026-01-29 17:51:01,403 | INFO | max output length: 27
2026-01-29 17:51:01,403 | INFO | min output length: 2
2026-01-29 17:51:01,931 | INFO | end detected at 18
2026-01-29 17:51:01,932 | INFO |  -0.94 * 0.5 =  -0.47 for decoder
2026-01-29 17:51:01,932 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-29 17:51:01,932 | INFO | total log probability: -0.48
2026-01-29 17:51:01,932 | INFO | normalized log probability: -0.04
2026-01-29 17:51:01,932 | INFO | total number of ended hypotheses: 164
2026-01-29 17:51:01,932 | INFO | best hypo: ces<space>progrès

2026-01-29 17:51:01,933 | INFO | speech length: 26080
2026-01-29 17:51:01,960 | INFO | decoder input length: 40
2026-01-29 17:51:01,960 | INFO | max output length: 40
2026-01-29 17:51:01,960 | INFO | min output length: 4
2026-01-29 17:51:02,983 | INFO | end detected at 35
2026-01-29 17:51:02,984 | INFO |  -3.17 * 0.5 =  -1.59 for decoder
2026-01-29 17:51:02,984 | INFO |  -1.47 * 0.5 =  -0.74 for ctc
2026-01-29 17:51:02,984 | INFO | total log probability: -2.32
2026-01-29 17:51:02,984 | INFO | normalized log probability: -0.08
2026-01-29 17:51:02,984 | INFO | total number of ended hypotheses: 173
2026-01-29 17:51:02,985 | INFO | best hypo: ne<space>prendront<space>tous<space>leur<space>sang

2026-01-29 17:51:02,986 | INFO | speech length: 27520
2026-01-29 17:51:03,012 | INFO | decoder input length: 42
2026-01-29 17:51:03,012 | INFO | max output length: 42
2026-01-29 17:51:03,012 | INFO | min output length: 4
2026-01-29 17:51:04,105 | INFO | end detected at 37
2026-01-29 17:51:04,107 | INFO |  -4.33 * 0.5 =  -2.16 for decoder
2026-01-29 17:51:04,107 | INFO |  -2.71 * 0.5 =  -1.35 for ctc
2026-01-29 17:51:04,107 | INFO | total log probability: -3.52
2026-01-29 17:51:04,107 | INFO | normalized log probability: -0.11
2026-01-29 17:51:04,107 | INFO | total number of ended hypotheses: 181
2026-01-29 17:51:04,107 | INFO | best hypo: que<space>s'ils<space>bénéficient<space>à<space>l'homme

2026-01-29 17:51:04,109 | INFO | speech length: 14720
2026-01-29 17:51:04,135 | INFO | decoder input length: 22
2026-01-29 17:51:04,135 | INFO | max output length: 22
2026-01-29 17:51:04,135 | INFO | min output length: 2
2026-01-29 17:51:04,723 | INFO | adding <eos> in the last position in the loop
2026-01-29 17:51:04,731 | INFO | no hypothesis. Finish decoding.
2026-01-29 17:51:04,732 | INFO |  -1.45 * 0.5 =  -0.73 for decoder
2026-01-29 17:51:04,732 | INFO |  -0.03 * 0.5 =  -0.01 for ctc
2026-01-29 17:51:04,732 | INFO | total log probability: -0.74
2026-01-29 17:51:04,732 | INFO | normalized log probability: -0.04
2026-01-29 17:51:04,732 | INFO | total number of ended hypotheses: 129
2026-01-29 17:51:04,732 | INFO | best hypo: à<space>tous<space>les<space>hommes

2026-01-29 17:51:04,733 | INFO | speech length: 67520
2026-01-29 17:51:04,760 | INFO | decoder input length: 105
2026-01-29 17:51:04,760 | INFO | max output length: 105
2026-01-29 17:51:04,760 | INFO | min output length: 10
2026-01-29 17:51:07,000 | INFO | end detected at 64
2026-01-29 17:51:07,001 | INFO |  -4.71 * 0.5 =  -2.36 for decoder
2026-01-29 17:51:07,001 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:51:07,001 | INFO | total log probability: -2.36
2026-01-29 17:51:07,001 | INFO | normalized log probability: -0.04
2026-01-29 17:51:07,001 | INFO | total number of ended hypotheses: 146
2026-01-29 17:51:07,002 | INFO | best hypo: le<space>vingt<space>et<space>unième<space>siècle<space>doit<space>être<space>le<space>siècle<space>de<space>l'éthique

2026-01-29 17:51:07,003 | INFO | speech length: 57440
2026-01-29 17:51:07,029 | INFO | decoder input length: 89
2026-01-29 17:51:07,029 | INFO | max output length: 89
2026-01-29 17:51:07,029 | INFO | min output length: 8
2026-01-29 17:51:09,549 | INFO | end detected at 79
2026-01-29 17:51:09,550 | INFO |  -6.01 * 0.5 =  -3.00 for decoder
2026-01-29 17:51:09,550 | INFO |  -0.19 * 0.5 =  -0.10 for ctc
2026-01-29 17:51:09,550 | INFO | total log probability: -3.10
2026-01-29 17:51:09,550 | INFO | normalized log probability: -0.04
2026-01-29 17:51:09,550 | INFO | total number of ended hypotheses: 160
2026-01-29 17:51:09,551 | INFO | best hypo: je<space>sais<space>que<space>bien<space>les<space>tragédies<space>aujourd'hui<space>font<space>douter<space>de<space>cette<space>espérance

2026-01-29 17:51:09,552 | INFO | speech length: 108960
2026-01-29 17:51:09,579 | INFO | decoder input length: 169
2026-01-29 17:51:09,579 | INFO | max output length: 169
2026-01-29 17:51:09,579 | INFO | min output length: 16
2026-01-29 17:51:13,696 | INFO | end detected at 104
2026-01-29 17:51:13,697 | INFO |  -7.81 * 0.5 =  -3.91 for decoder
2026-01-29 17:51:13,697 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:51:13,697 | INFO | total log probability: -3.91
2026-01-29 17:51:13,697 | INFO | normalized log probability: -0.04
2026-01-29 17:51:13,697 | INFO | total number of ended hypotheses: 173
2026-01-29 17:51:13,698 | INFO | best hypo: pourtant<space>de<space>plus<space>en<space>plus<space>les<space>nations<space>s'accordent<space>pour<space>mieux<space>faire<space>respecter<space>les<space>droits<space>de<space>l'homme

2026-01-29 17:51:13,700 | INFO | speech length: 59520
2026-01-29 17:51:13,726 | INFO | decoder input length: 92
2026-01-29 17:51:13,726 | INFO | max output length: 92
2026-01-29 17:51:13,726 | INFO | min output length: 9
2026-01-29 17:51:15,504 | INFO | end detected at 53
2026-01-29 17:51:15,505 | INFO |  -3.80 * 0.5 =  -1.90 for decoder
2026-01-29 17:51:15,505 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:51:15,505 | INFO | total log probability: -1.90
2026-01-29 17:51:15,505 | INFO | normalized log probability: -0.04
2026-01-29 17:51:15,505 | INFO | total number of ended hypotheses: 167
2026-01-29 17:51:15,506 | INFO | best hypo: pour<space>défendre<space>la<space>liberté<space>et<space>la<space>dignité<space>humaine

2026-01-29 17:51:15,507 | INFO | speech length: 50720
2026-01-29 17:51:15,534 | INFO | decoder input length: 78
2026-01-29 17:51:15,534 | INFO | max output length: 78
2026-01-29 17:51:15,534 | INFO | min output length: 7
2026-01-29 17:51:17,331 | INFO | end detected at 56
2026-01-29 17:51:17,332 | INFO |  -4.00 * 0.5 =  -2.00 for decoder
2026-01-29 17:51:17,332 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-29 17:51:17,332 | INFO | total log probability: -2.01
2026-01-29 17:51:17,332 | INFO | normalized log probability: -0.04
2026-01-29 17:51:17,332 | INFO | total number of ended hypotheses: 172
2026-01-29 17:51:17,333 | INFO | best hypo: un<space>nouvel<space>ordre<space>international<space>s'affirme<space>peu<space>à<space>peu

2026-01-29 17:51:17,335 | INFO | speech length: 100000
2026-01-29 17:51:17,362 | INFO | decoder input length: 155
2026-01-29 17:51:17,362 | INFO | max output length: 155
2026-01-29 17:51:17,362 | INFO | min output length: 15
2026-01-29 17:51:20,622 | INFO | end detected at 84
2026-01-29 17:51:20,623 | INFO |  -6.25 * 0.5 =  -3.13 for decoder
2026-01-29 17:51:20,623 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-29 17:51:20,623 | INFO | total log probability: -3.13
2026-01-29 17:51:20,623 | INFO | normalized log probability: -0.04
2026-01-29 17:51:20,623 | INFO | total number of ended hypotheses: 168
2026-01-29 17:51:20,624 | INFO | best hypo: demain<space>il<space>ne<space>devra<space>plus<space>y<space>avoir<space>de<space>repos<space>pour<space>les<space>criminels<space>contre<space>l'humanité

2026-01-29 17:51:20,625 | INFO | speech length: 16800
2026-01-29 17:51:20,652 | INFO | decoder input length: 25
2026-01-29 17:51:20,652 | INFO | max output length: 25
2026-01-29 17:51:20,652 | INFO | min output length: 2
2026-01-29 17:51:21,314 | INFO | adding <eos> in the last position in the loop
2026-01-29 17:51:21,322 | INFO | no hypothesis. Finish decoding.
2026-01-29 17:51:21,323 | INFO |  -1.93 * 0.5 =  -0.97 for decoder
2026-01-29 17:51:21,323 | INFO |  -0.96 * 0.5 =  -0.48 for ctc
2026-01-29 17:51:21,323 | INFO | total log probability: -1.45
2026-01-29 17:51:21,323 | INFO | normalized log probability: -0.07
2026-01-29 17:51:21,323 | INFO | total number of ended hypotheses: 133
2026-01-29 17:51:21,324 | INFO | best hypo: au<space>nom<space>de<space>la<space>france

2026-01-29 17:51:21,325 | INFO | speech length: 14720
2026-01-29 17:51:21,352 | INFO | decoder input length: 22
2026-01-29 17:51:21,352 | INFO | max output length: 22
2026-01-29 17:51:21,352 | INFO | min output length: 2
2026-01-29 17:51:21,954 | INFO | adding <eos> in the last position in the loop
2026-01-29 17:51:21,963 | INFO | end detected at 21
2026-01-29 17:51:21,964 | INFO |  -2.24 * 0.5 =  -1.12 for decoder
2026-01-29 17:51:21,965 | INFO |  -3.49 * 0.5 =  -1.75 for ctc
2026-01-29 17:51:21,965 | INFO | total log probability: -2.87
2026-01-29 17:51:21,965 | INFO | normalized log probability: -0.20
2026-01-29 17:51:21,965 | INFO | total number of ended hypotheses: 201
2026-01-29 17:51:21,965 | INFO | best hypo: on<space>votre<space>nom

2026-01-29 17:51:21,966 | INFO | speech length: 44160
2026-01-29 17:51:21,993 | INFO | decoder input length: 68
2026-01-29 17:51:21,993 | INFO | max output length: 68
2026-01-29 17:51:21,993 | INFO | min output length: 6
2026-01-29 17:51:23,697 | INFO | end detected at 55
2026-01-29 17:51:23,698 | INFO |  -4.00 * 0.5 =  -2.00 for decoder
2026-01-29 17:51:23,698 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:51:23,698 | INFO | total log probability: -2.00
2026-01-29 17:51:23,698 | INFO | normalized log probability: -0.04
2026-01-29 17:51:23,698 | INFO | total number of ended hypotheses: 144
2026-01-29 17:51:23,699 | INFO | best hypo: c'est<space>le<space>combat<space>difficile<space>que<space>je<space>mène<space>chaque<space>jour

2026-01-29 17:51:23,700 | INFO | speech length: 32160
2026-01-29 17:51:23,726 | INFO | decoder input length: 49
2026-01-29 17:51:23,727 | INFO | max output length: 49
2026-01-29 17:51:23,727 | INFO | min output length: 4
2026-01-29 17:51:24,813 | INFO | end detected at 36
2026-01-29 17:51:24,814 | INFO |  -2.50 * 0.5 =  -1.25 for decoder
2026-01-29 17:51:24,814 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:51:24,814 | INFO | total log probability: -1.25
2026-01-29 17:51:24,814 | INFO | normalized log probability: -0.04
2026-01-29 17:51:24,814 | INFO | total number of ended hypotheses: 139
2026-01-29 17:51:24,814 | INFO | best hypo: à<space>l'intérieur<space>de<space>chaque<space>nation

2026-01-29 17:51:24,816 | INFO | speech length: 32480
2026-01-29 17:51:24,842 | INFO | decoder input length: 50
2026-01-29 17:51:24,842 | INFO | max output length: 50
2026-01-29 17:51:24,842 | INFO | min output length: 5
2026-01-29 17:51:25,832 | INFO | end detected at 32
2026-01-29 17:51:25,833 | INFO |  -5.67 * 0.5 =  -2.83 for decoder
2026-01-29 17:51:25,834 | INFO |  -3.30 * 0.5 =  -1.65 for ctc
2026-01-29 17:51:25,834 | INFO | total log probability: -4.48
2026-01-29 17:51:25,834 | INFO | normalized log probability: -0.17
2026-01-29 17:51:25,834 | INFO | total number of ended hypotheses: 185
2026-01-29 17:51:25,834 | INFO | best hypo: une<space>exigence<space>se<space>faitante

2026-01-29 17:51:25,836 | INFO | speech length: 15360
2026-01-29 17:51:25,862 | INFO | decoder input length: 23
2026-01-29 17:51:25,862 | INFO | max output length: 23
2026-01-29 17:51:25,862 | INFO | min output length: 2
2026-01-29 17:51:26,644 | INFO | adding <eos> in the last position in the loop
2026-01-29 17:51:26,652 | INFO | no hypothesis. Finish decoding.
2026-01-29 17:51:26,653 | INFO |  -1.63 * 0.5 =  -0.81 for decoder
2026-01-29 17:51:26,653 | INFO |  -0.26 * 0.5 =  -0.13 for ctc
2026-01-29 17:51:26,653 | INFO | total log probability: -0.95
2026-01-29 17:51:26,653 | INFO | normalized log probability: -0.05
2026-01-29 17:51:26,653 | INFO | total number of ended hypotheses: 129
2026-01-29 17:51:26,654 | INFO | best hypo: toujours<space>plus<space>fort

2026-01-29 17:51:26,655 | INFO | speech length: 117280
2026-01-29 17:51:26,687 | INFO | decoder input length: 182
2026-01-29 17:51:26,687 | INFO | max output length: 182
2026-01-29 17:51:26,687 | INFO | min output length: 18
2026-01-29 17:51:31,450 | INFO | end detected at 120
2026-01-29 17:51:31,451 | INFO |  -9.18 * 0.5 =  -4.59 for decoder
2026-01-29 17:51:31,451 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-29 17:51:31,451 | INFO | total log probability: -4.60
2026-01-29 17:51:31,451 | INFO | normalized log probability: -0.04
2026-01-29 17:51:31,451 | INFO | total number of ended hypotheses: 151
2026-01-29 17:51:31,453 | INFO | best hypo: pour<space>que<space>les<space>avancées<space>de<space>la<space>science<space>soient<space>orientées<space>vers<space>le<space>bien<space>de<space>l'homme<space>et<space>ne<space>se<space>retournent<space>jamais<space>contre<space>lui

2026-01-29 17:51:31,455 | INFO | speech length: 29920
2026-01-29 17:51:31,479 | INFO | decoder input length: 46
2026-01-29 17:51:31,479 | INFO | max output length: 46
2026-01-29 17:51:31,479 | INFO | min output length: 4
2026-01-29 17:51:32,293 | INFO | end detected at 27
2026-01-29 17:51:32,294 | INFO |  -1.72 * 0.5 =  -0.86 for decoder
2026-01-29 17:51:32,294 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:51:32,294 | INFO | total log probability: -0.86
2026-01-29 17:51:32,294 | INFO | normalized log probability: -0.04
2026-01-29 17:51:32,294 | INFO | total number of ended hypotheses: 165
2026-01-29 17:51:32,294 | INFO | best hypo: je<space>pense<space>par<space>exemple

2026-01-29 17:51:32,295 | INFO | speech length: 46080
2026-01-29 17:51:32,322 | INFO | decoder input length: 71
2026-01-29 17:51:32,322 | INFO | max output length: 71
2026-01-29 17:51:32,322 | INFO | min output length: 7
2026-01-29 17:51:33,870 | INFO | end detected at 47
2026-01-29 17:51:33,871 | INFO |  -4.66 * 0.5 =  -2.33 for decoder
2026-01-29 17:51:33,872 | INFO |  -2.61 * 0.5 =  -1.30 for ctc
2026-01-29 17:51:33,872 | INFO | total log probability: -3.63
2026-01-29 17:51:33,872 | INFO | normalized log probability: -0.09
2026-01-29 17:51:33,872 | INFO | total number of ended hypotheses: 173
2026-01-29 17:51:33,872 | INFO | best hypo: aux<space>manipulations<space>génétiques<space>aux<space>clonaes

2026-01-29 17:51:33,874 | INFO | speech length: 27840
2026-01-29 17:51:33,900 | INFO | decoder input length: 43
2026-01-29 17:51:33,900 | INFO | max output length: 43
2026-01-29 17:51:33,900 | INFO | min output length: 4
2026-01-29 17:51:35,125 | INFO | adding <eos> in the last position in the loop
2026-01-29 17:51:35,132 | INFO | no hypothesis. Finish decoding.
2026-01-29 17:51:35,132 | INFO |  -3.52 * 0.5 =  -1.76 for decoder
2026-01-29 17:51:35,132 | INFO |  -0.04 * 0.5 =  -0.02 for ctc
2026-01-29 17:51:35,132 | INFO | total log probability: -1.78
2026-01-29 17:51:35,132 | INFO | normalized log probability: -0.04
2026-01-29 17:51:35,132 | INFO | total number of ended hypotheses: 72
2026-01-29 17:51:35,133 | INFO | best hypo: de<space>même<space>dans<space>le<space>domaine<space>de<space>l'environnement<sos/eos>

2026-01-29 17:51:35,133 | WARNING | best hypo length: 43 == max output length: 43
2026-01-29 17:51:35,133 | WARNING | decoding may be stopped by the max output length limitation, please consider to increase the maxlenratio.
2026-01-29 17:51:35,134 | INFO | speech length: 96320
2026-01-29 17:51:35,160 | INFO | decoder input length: 150
2026-01-29 17:51:35,160 | INFO | max output length: 150
2026-01-29 17:51:35,160 | INFO | min output length: 15
2026-01-29 17:51:38,366 | INFO | end detected at 84
2026-01-29 17:51:38,367 | INFO |  -6.31 * 0.5 =  -3.16 for decoder
2026-01-29 17:51:38,367 | INFO |  -0.25 * 0.5 =  -0.12 for ctc
2026-01-29 17:51:38,367 | INFO | total log probability: -3.28
2026-01-29 17:51:38,367 | INFO | normalized log probability: -0.04
2026-01-29 17:51:38,367 | INFO | total number of ended hypotheses: 163
2026-01-29 17:51:38,368 | INFO | best hypo: les<space>peuples<space>ne<space>veulent<space>plus<space>que<space>la<space>course<space>à<space>la<space>productivité<space>épuise<space>la<space>planète

2026-01-29 17:51:38,370 | INFO | speech length: 92800
2026-01-29 17:51:38,397 | INFO | decoder input length: 144
2026-01-29 17:51:38,397 | INFO | max output length: 144
2026-01-29 17:51:38,397 | INFO | min output length: 14
2026-01-29 17:51:41,670 | INFO | end detected at 87
2026-01-29 17:51:41,671 | INFO |  -6.49 * 0.5 =  -3.25 for decoder
2026-01-29 17:51:41,671 | INFO |  -1.17 * 0.5 =  -0.58 for ctc
2026-01-29 17:51:41,671 | INFO | total log probability: -3.83
2026-01-29 17:51:41,671 | INFO | normalized log probability: -0.05
2026-01-29 17:51:41,671 | INFO | total number of ended hypotheses: 164
2026-01-29 17:51:41,672 | INFO | best hypo: la<space>responsabilité<space>de<space>tous<space>ceux<space>qui<space>dans<space>le<space>monde<space>dégradent<space>le<space>patrimoine<space>naturel

2026-01-29 17:51:41,674 | INFO | speech length: 41280
2026-01-29 17:51:41,700 | INFO | decoder input length: 64
2026-01-29 17:51:41,701 | INFO | max output length: 64
2026-01-29 17:51:41,701 | INFO | min output length: 6
2026-01-29 17:51:43,042 | INFO | end detected at 42
2026-01-29 17:51:43,044 | INFO |  -4.18 * 0.5 =  -2.09 for decoder
2026-01-29 17:51:43,044 | INFO |  -1.27 * 0.5 =  -0.63 for ctc
2026-01-29 17:51:43,044 | INFO | total log probability: -2.73
2026-01-29 17:51:43,044 | INFO | normalized log probability: -0.08
2026-01-29 17:51:43,044 | INFO | total number of ended hypotheses: 180
2026-01-29 17:51:43,045 | INFO | best hypo: doit<space>être<space>recherché<space>et<space>sanctionné

2026-01-29 17:51:43,046 | INFO | speech length: 68800
2026-01-29 17:51:43,073 | INFO | decoder input length: 107
2026-01-29 17:51:43,073 | INFO | max output length: 107
2026-01-29 17:51:43,073 | INFO | min output length: 10
2026-01-29 17:51:45,391 | INFO | end detected at 67
2026-01-29 17:51:45,392 | INFO |  -4.93 * 0.5 =  -2.46 for decoder
2026-01-29 17:51:45,392 | INFO |  -1.62 * 0.5 =  -0.81 for ctc
2026-01-29 17:51:45,392 | INFO | total log probability: -3.27
2026-01-29 17:51:45,392 | INFO | normalized log probability: -0.05
2026-01-29 17:51:45,392 | INFO | total number of ended hypotheses: 188
2026-01-29 17:51:45,393 | INFO | best hypo: car<space>il<space>s'agit<space>du<space>patrimoine<space>que<space>nous<space>lèverons<space>à<space>nos<space>enfants

2026-01-29 17:51:45,395 | INFO | speech length: 54720
2026-01-29 17:51:45,421 | INFO | decoder input length: 85
2026-01-29 17:51:45,422 | INFO | max output length: 85
2026-01-29 17:51:45,422 | INFO | min output length: 8
2026-01-29 17:51:47,300 | INFO | end detected at 57
2026-01-29 17:51:47,301 | INFO |  -4.09 * 0.5 =  -2.04 for decoder
2026-01-29 17:51:47,301 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:51:47,301 | INFO | total log probability: -2.05
2026-01-29 17:51:47,301 | INFO | normalized log probability: -0.04
2026-01-29 17:51:47,301 | INFO | total number of ended hypotheses: 173
2026-01-29 17:51:47,302 | INFO | best hypo: même<space>si<space>le<space>monde<space>change<space>comme<space>il<space>n'a<space>jamais<space>changé

2026-01-29 17:51:47,303 | INFO | speech length: 42560
2026-01-29 17:51:47,330 | INFO | decoder input length: 66
2026-01-29 17:51:47,330 | INFO | max output length: 66
2026-01-29 17:51:47,330 | INFO | min output length: 6
2026-01-29 17:51:48,694 | INFO | end detected at 43
2026-01-29 17:51:48,695 | INFO |  -3.06 * 0.5 =  -1.53 for decoder
2026-01-29 17:51:48,695 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:51:48,695 | INFO | total log probability: -1.53
2026-01-29 17:51:48,695 | INFO | normalized log probability: -0.04
2026-01-29 17:51:48,695 | INFO | total number of ended hypotheses: 141
2026-01-29 17:51:48,695 | INFO | best hypo: la<space>modernité<space>ne<space>doit<space>pas<space>nous<space>diviser

2026-01-29 17:51:48,697 | INFO | speech length: 36800
2026-01-29 17:51:48,723 | INFO | decoder input length: 57
2026-01-29 17:51:48,723 | INFO | max output length: 57
2026-01-29 17:51:48,723 | INFO | min output length: 5
2026-01-29 17:51:49,796 | INFO | end detected at 34
2026-01-29 17:51:49,797 | INFO |  -2.25 * 0.5 =  -1.13 for decoder
2026-01-29 17:51:49,797 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:51:49,797 | INFO | total log probability: -1.13
2026-01-29 17:51:49,797 | INFO | normalized log probability: -0.04
2026-01-29 17:51:49,797 | INFO | total number of ended hypotheses: 169
2026-01-29 17:51:49,797 | INFO | best hypo: elle<space>doit<space>profiter<space>à<space>chacun

2026-01-29 17:51:49,799 | INFO | speech length: 113440
2026-01-29 17:51:49,826 | INFO | decoder input length: 176
2026-01-29 17:51:49,826 | INFO | max output length: 176
2026-01-29 17:51:49,826 | INFO | min output length: 17
2026-01-29 17:51:54,744 | INFO | end detected at 124
2026-01-29 17:51:54,746 | INFO | -10.15 * 0.5 =  -5.07 for decoder
2026-01-29 17:51:54,746 | INFO |  -4.99 * 0.5 =  -2.49 for ctc
2026-01-29 17:51:54,746 | INFO | total log probability: -7.57
2026-01-29 17:51:54,746 | INFO | normalized log probability: -0.06
2026-01-29 17:51:54,746 | INFO | total number of ended hypotheses: 192
2026-01-29 17:51:54,747 | INFO | best hypo: nous<space>réussirons<space>nous<space>réussirons<space>parce<space>que<space>nous<space>avons<space>pris<space>les<space>décisions<space>qui<space>engagent<space>et<space>qui<space>garantissent<space>notre<space>oeuvre

2026-01-29 17:51:54,750 | INFO | speech length: 56960
2026-01-29 17:51:54,777 | INFO | decoder input length: 88
2026-01-29 17:51:54,777 | INFO | max output length: 88
2026-01-29 17:51:54,777 | INFO | min output length: 8
2026-01-29 17:51:57,200 | INFO | end detected at 74
2026-01-29 17:51:57,201 | INFO |  -5.41 * 0.5 =  -2.71 for decoder
2026-01-29 17:51:57,201 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:51:57,201 | INFO | total log probability: -2.71
2026-01-29 17:51:57,201 | INFO | normalized log probability: -0.04
2026-01-29 17:51:57,201 | INFO | total number of ended hypotheses: 171
2026-01-29 17:51:57,202 | INFO | best hypo: nous<space>avons<space>choisi<space>ensemble<space>de<space>faire<space>grandir<space>la<space>france<space>dans<space>l'europe

2026-01-29 17:51:57,205 | INFO | speech length: 101280
2026-01-29 17:51:57,232 | INFO | decoder input length: 157
2026-01-29 17:51:57,232 | INFO | max output length: 157
2026-01-29 17:51:57,233 | INFO | min output length: 15
2026-01-29 17:52:01,432 | INFO | end detected at 103
2026-01-29 17:52:01,434 | INFO |  -7.87 * 0.5 =  -3.93 for decoder
2026-01-29 17:52:01,434 | INFO |  -0.90 * 0.5 =  -0.45 for ctc
2026-01-29 17:52:01,434 | INFO | total log probability: -4.38
2026-01-29 17:52:01,434 | INFO | normalized log probability: -0.04
2026-01-29 17:52:01,434 | INFO | total number of ended hypotheses: 170
2026-01-29 17:52:01,435 | INFO | best hypo: une<space>europe<space>qui<space>nous<space>garantit<space>la<space>paix<space>une<space>europe<space>qui<space>nous<space>permet<space>de<space>peser<space>davantage<space>dans<space>le<space>monde

2026-01-29 17:52:01,437 | INFO | speech length: 105600
2026-01-29 17:52:01,469 | INFO | decoder input length: 164
2026-01-29 17:52:01,469 | INFO | max output length: 164
2026-01-29 17:52:01,469 | INFO | min output length: 16
2026-01-29 17:52:05,304 | INFO | end detected at 96
2026-01-29 17:52:05,306 | INFO |  -7.23 * 0.5 =  -3.62 for decoder
2026-01-29 17:52:05,306 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-29 17:52:05,306 | INFO | total log probability: -3.62
2026-01-29 17:52:05,306 | INFO | normalized log probability: -0.04
2026-01-29 17:52:05,306 | INFO | total number of ended hypotheses: 177
2026-01-29 17:52:05,307 | INFO | best hypo: nous<space>avons<space>choisi<space>aussi<space>de<space>prendre<space>part<space>à<space>la<space>mondialisation<space>d'en<space>prendre<space>toute<space>notre<space>part

2026-01-29 17:52:05,310 | INFO | speech length: 92640
2026-01-29 17:52:05,352 | INFO | decoder input length: 144
2026-01-29 17:52:05,352 | INFO | max output length: 144
2026-01-29 17:52:05,352 | INFO | min output length: 14
2026-01-29 17:52:08,495 | INFO | end detected at 82
2026-01-29 17:52:08,496 | INFO |  -6.13 * 0.5 =  -3.06 for decoder
2026-01-29 17:52:08,496 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-29 17:52:08,496 | INFO | total log probability: -3.07
2026-01-29 17:52:08,496 | INFO | normalized log probability: -0.04
2026-01-29 17:52:08,496 | INFO | total number of ended hypotheses: 168
2026-01-29 17:52:08,497 | INFO | best hypo: mais<space>une<space>mondialisation<space>maîtrisée<space>organisée<space>respectueuse<space>de<space>l'environnement

2026-01-29 17:52:08,499 | INFO | speech length: 67840
2026-01-29 17:52:08,527 | INFO | decoder input length: 105
2026-01-29 17:52:08,527 | INFO | max output length: 105
2026-01-29 17:52:08,527 | INFO | min output length: 10
2026-01-29 17:52:10,701 | INFO | end detected at 62
2026-01-29 17:52:10,702 | INFO |  -5.27 * 0.5 =  -2.63 for decoder
2026-01-29 17:52:10,702 | INFO |  -0.32 * 0.5 =  -0.16 for ctc
2026-01-29 17:52:10,702 | INFO | total log probability: -2.79
2026-01-29 17:52:10,702 | INFO | normalized log probability: -0.05
2026-01-29 17:52:10,702 | INFO | total number of ended hypotheses: 154
2026-01-29 17:52:10,703 | INFO | best hypo: capable<space>de<space>prendre<space>en<space>compte<space>les<space>aspirations<space>des<space>hommes

2026-01-29 17:52:10,704 | INFO | speech length: 34560
2026-01-29 17:52:10,732 | INFO | decoder input length: 53
2026-01-29 17:52:10,732 | INFO | max output length: 53
2026-01-29 17:52:10,732 | INFO | min output length: 5
2026-01-29 17:52:12,129 | INFO | end detected at 46
2026-01-29 17:52:12,130 | INFO |  -3.37 * 0.5 =  -1.69 for decoder
2026-01-29 17:52:12,130 | INFO |  -1.78 * 0.5 =  -0.89 for ctc
2026-01-29 17:52:12,130 | INFO | total log probability: -2.57
2026-01-29 17:52:12,130 | INFO | normalized log probability: -0.06
2026-01-29 17:52:12,130 | INFO | total number of ended hypotheses: 163
2026-01-29 17:52:12,131 | INFO | best hypo: est<space>capable<space>de<space>faire<space>reculer<space>la<space>pauvreté

2026-01-29 17:52:12,132 | INFO | speech length: 52000
2026-01-29 17:52:12,159 | INFO | decoder input length: 80
2026-01-29 17:52:12,159 | INFO | max output length: 80
2026-01-29 17:52:12,159 | INFO | min output length: 8
2026-01-29 17:52:13,832 | INFO | end detected at 50
2026-01-29 17:52:13,833 | INFO |  -3.57 * 0.5 =  -1.79 for decoder
2026-01-29 17:52:13,834 | INFO |  -0.03 * 0.5 =  -0.01 for ctc
2026-01-29 17:52:13,834 | INFO | total log probability: -1.80
2026-01-29 17:52:13,834 | INFO | normalized log probability: -0.04
2026-01-29 17:52:13,834 | INFO | total number of ended hypotheses: 169
2026-01-29 17:52:13,834 | INFO | best hypo: ce<space>sera<space>tout<space>le<space>sens<space>du<space>combat<space>de<space>la<space>france

2026-01-29 17:52:13,836 | INFO | speech length: 21600
2026-01-29 17:52:13,863 | INFO | decoder input length: 33
2026-01-29 17:52:13,863 | INFO | max output length: 33
2026-01-29 17:52:13,863 | INFO | min output length: 3
2026-01-29 17:52:14,772 | INFO | adding <eos> in the last position in the loop
2026-01-29 17:52:14,779 | INFO | no hypothesis. Finish decoding.
2026-01-29 17:52:14,780 | INFO |  -3.38 * 0.5 =  -1.69 for decoder
2026-01-29 17:52:14,780 | INFO | -12.61 * 0.5 =  -6.30 for ctc
2026-01-29 17:52:14,780 | INFO | total log probability: -7.99
2026-01-29 17:52:14,780 | INFO | normalized log probability: -0.26
2026-01-29 17:52:14,780 | INFO | total number of ended hypotheses: 78
2026-01-29 17:52:14,781 | INFO | best hypo: dans<space>les<space>grandes<space>négociations

2026-01-29 17:52:14,782 | INFO | speech length: 20160
2026-01-29 17:52:14,807 | INFO | decoder input length: 31
2026-01-29 17:52:14,807 | INFO | max output length: 31
2026-01-29 17:52:14,807 | INFO | min output length: 3
2026-01-29 17:52:15,650 | INFO | end detected at 29
2026-01-29 17:52:15,651 | INFO |  -3.05 * 0.5 =  -1.52 for decoder
2026-01-29 17:52:15,651 | INFO |  -1.19 * 0.5 =  -0.60 for ctc
2026-01-29 17:52:15,651 | INFO | total log probability: -2.12
2026-01-29 17:52:15,651 | INFO | normalized log probability: -0.08
2026-01-29 17:52:15,651 | INFO | total number of ended hypotheses: 164
2026-01-29 17:52:15,651 | INFO | best hypo: mais<space>chers<space>compatriotes

2026-01-29 17:52:15,653 | INFO | speech length: 37440
2026-01-29 17:52:15,679 | INFO | decoder input length: 58
2026-01-29 17:52:15,679 | INFO | max output length: 58
2026-01-29 17:52:15,679 | INFO | min output length: 5
2026-01-29 17:52:17,109 | INFO | end detected at 45
2026-01-29 17:52:17,110 | INFO |  -3.20 * 0.5 =  -1.60 for decoder
2026-01-29 17:52:17,110 | INFO |  -0.06 * 0.5 =  -0.03 for ctc
2026-01-29 17:52:17,110 | INFO | total log probability: -1.63
2026-01-29 17:52:17,110 | INFO | normalized log probability: -0.04
2026-01-29 17:52:17,111 | INFO | total number of ended hypotheses: 171
2026-01-29 17:52:17,111 | INFO | best hypo: nous<space>avons<space>en<space>commun<space>certaines<space>valeurs

2026-01-29 17:52:17,113 | INFO | speech length: 116960
2026-01-29 17:52:17,140 | INFO | decoder input length: 182
2026-01-29 17:52:17,141 | INFO | max output length: 182
2026-01-29 17:52:17,141 | INFO | min output length: 18
2026-01-29 17:52:21,725 | INFO | end detected at 114
2026-01-29 17:52:21,726 | INFO |  -8.84 * 0.5 =  -4.42 for decoder
2026-01-29 17:52:21,726 | INFO |  -1.10 * 0.5 =  -0.55 for ctc
2026-01-29 17:52:21,726 | INFO | total log probability: -4.97
2026-01-29 17:52:21,726 | INFO | normalized log probability: -0.05
2026-01-29 17:52:21,726 | INFO | total number of ended hypotheses: 172
2026-01-29 17:52:21,728 | INFO | best hypo: la<space>volonté<space>de<space>donner<space>à<space>chacun<space>sa<space>chance<space>pour<space>que<space>notre<space>société<space>soit<space>plus<space>alente<space>plus<space>mobile<space>plus<space>optimiste

2026-01-29 17:52:21,729 | INFO | speech length: 118400
2026-01-29 17:52:21,758 | INFO | decoder input length: 184
2026-01-29 17:52:21,758 | INFO | max output length: 184
2026-01-29 17:52:21,758 | INFO | min output length: 18
2026-01-29 17:52:26,447 | INFO | end detected at 117
2026-01-29 17:52:26,448 | INFO |  -8.90 * 0.5 =  -4.45 for decoder
2026-01-29 17:52:26,448 | INFO |  -0.15 * 0.5 =  -0.08 for ctc
2026-01-29 17:52:26,448 | INFO | total log probability: -4.53
2026-01-29 17:52:26,448 | INFO | normalized log probability: -0.04
2026-01-29 17:52:26,448 | INFO | total number of ended hypotheses: 167
2026-01-29 17:52:26,450 | INFO | best hypo: l'exigence<space>de<space>solidarité<space>une<space>solidarité<space>plus<space>responsable<space>où<space>chacun<space>s'efforcerait<space>de<space>prendre<space>sa<space>part<space>du<space>contrat

2026-01-29 17:52:26,452 | INFO | speech length: 83520
2026-01-29 17:52:26,482 | INFO | decoder input length: 130
2026-01-29 17:52:26,482 | INFO | max output length: 130
2026-01-29 17:52:26,482 | INFO | min output length: 13
2026-01-29 17:52:29,385 | INFO | end detected at 80
2026-01-29 17:52:29,387 | INFO |  -6.11 * 0.5 =  -3.05 for decoder
2026-01-29 17:52:29,387 | INFO |  -0.41 * 0.5 =  -0.21 for ctc
2026-01-29 17:52:29,387 | INFO | total log probability: -3.26
2026-01-29 17:52:29,387 | INFO | normalized log probability: -0.04
2026-01-29 17:52:29,387 | INFO | total number of ended hypotheses: 174
2026-01-29 17:52:29,388 | INFO | best hypo: l'attachement<space>à<space>la<space>famille<space>parce<space>qu'elle<space>est<space>chaleur<space>entre<space>aides<space>sécurité

2026-01-29 17:52:29,389 | INFO | speech length: 61120
2026-01-29 17:52:29,416 | INFO | decoder input length: 95
2026-01-29 17:52:29,416 | INFO | max output length: 95
2026-01-29 17:52:29,416 | INFO | min output length: 9
2026-01-29 17:52:31,623 | INFO | end detected at 64
2026-01-29 17:52:31,624 | INFO |  -4.67 * 0.5 =  -2.34 for decoder
2026-01-29 17:52:31,624 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:52:31,624 | INFO | total log probability: -2.34
2026-01-29 17:52:31,624 | INFO | normalized log probability: -0.04
2026-01-29 17:52:31,624 | INFO | total number of ended hypotheses: 177
2026-01-29 17:52:31,625 | INFO | best hypo: le<space>désir<space>d'être<space>utile<space>de<space>trouver<space>sa<space>place<space>dans<space>la<space>société

2026-01-29 17:52:31,627 | INFO | speech length: 29280
2026-01-29 17:52:31,653 | INFO | decoder input length: 45
2026-01-29 17:52:31,653 | INFO | max output length: 45
2026-01-29 17:52:31,653 | INFO | min output length: 4
2026-01-29 17:52:32,846 | INFO | end detected at 29
2026-01-29 17:52:32,847 | INFO |  -1.97 * 0.5 =  -0.99 for decoder
2026-01-29 17:52:32,847 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:52:32,847 | INFO | total log probability: -0.99
2026-01-29 17:52:32,847 | INFO | normalized log probability: -0.04
2026-01-29 17:52:32,847 | INFO | total number of ended hypotheses: 134
2026-01-29 17:52:32,848 | INFO | best hypo: de<space>donner<space>autour<space>de<space>soi

2026-01-29 17:52:32,849 | INFO | speech length: 13600
2026-01-29 17:52:32,875 | INFO | decoder input length: 20
2026-01-29 17:52:32,875 | INFO | max output length: 20
2026-01-29 17:52:32,875 | INFO | min output length: 2
2026-01-29 17:52:33,438 | INFO | adding <eos> in the last position in the loop
2026-01-29 17:52:33,446 | INFO | no hypothesis. Finish decoding.
2026-01-29 17:52:33,447 | INFO |  -1.20 * 0.5 =  -0.60 for decoder
2026-01-29 17:52:33,447 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:52:33,447 | INFO | total log probability: -0.60
2026-01-29 17:52:33,447 | INFO | normalized log probability: -0.04
2026-01-29 17:52:33,447 | INFO | total number of ended hypotheses: 150
2026-01-29 17:52:33,447 | INFO | best hypo: de<space>se<space>réaliser

2026-01-29 17:52:33,448 | INFO | speech length: 13600
2026-01-29 17:52:33,473 | INFO | decoder input length: 20
2026-01-29 17:52:33,473 | INFO | max output length: 20
2026-01-29 17:52:33,473 | INFO | min output length: 2
2026-01-29 17:52:33,995 | INFO | end detected at 18
2026-01-29 17:52:33,995 | INFO |  -1.07 * 0.5 =  -0.53 for decoder
2026-01-29 17:52:33,996 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:52:33,996 | INFO | total log probability: -0.53
2026-01-29 17:52:33,996 | INFO | normalized log probability: -0.04
2026-01-29 17:52:33,996 | INFO | total number of ended hypotheses: 139
2026-01-29 17:52:33,996 | INFO | best hypo: la<space>tolérance

2026-01-29 17:52:33,997 | INFO | speech length: 50720
2026-01-29 17:52:34,023 | INFO | decoder input length: 78
2026-01-29 17:52:34,023 | INFO | max output length: 78
2026-01-29 17:52:34,023 | INFO | min output length: 7
2026-01-29 17:52:35,686 | INFO | end detected at 49
2026-01-29 17:52:35,688 | INFO |  -3.30 * 0.5 =  -1.65 for decoder
2026-01-29 17:52:35,688 | INFO |  -1.83 * 0.5 =  -0.92 for ctc
2026-01-29 17:52:35,688 | INFO | total log probability: -2.57
2026-01-29 17:52:35,688 | INFO | normalized log probability: -0.06
2026-01-29 17:52:35,688 | INFO | total number of ended hypotheses: 210
2026-01-29 17:52:35,689 | INFO | best hypo: qui<space>ne<space>doit<space>pas<space>être<space>renoncement<space>à<space>ces

2026-01-29 17:52:35,691 | INFO | speech length: 16480
2026-01-29 17:52:35,716 | INFO | decoder input length: 25
2026-01-29 17:52:35,716 | INFO | max output length: 25
2026-01-29 17:52:35,716 | INFO | min output length: 2
2026-01-29 17:52:36,407 | INFO | adding <eos> in the last position in the loop
2026-01-29 17:52:36,416 | INFO | no hypothesis. Finish decoding.
2026-01-29 17:52:36,418 | INFO |  -2.58 * 0.5 =  -1.29 for decoder
2026-01-29 17:52:36,418 | INFO |  -7.08 * 0.5 =  -3.54 for ctc
2026-01-29 17:52:36,418 | INFO | total log probability: -4.83
2026-01-29 17:52:36,418 | INFO | normalized log probability: -0.19
2026-01-29 17:52:36,418 | INFO | total number of ended hypotheses: 92
2026-01-29 17:52:36,419 | INFO | best hypo: mais<space>respect<space>de<space>l'autre

2026-01-29 17:52:36,420 | INFO | speech length: 18720
2026-01-29 17:52:36,472 | INFO | decoder input length: 28
2026-01-29 17:52:36,472 | INFO | max output length: 28
2026-01-29 17:52:36,472 | INFO | min output length: 2
2026-01-29 17:52:37,239 | INFO | end detected at 26
2026-01-29 17:52:37,240 | INFO |  -3.27 * 0.5 =  -1.63 for decoder
2026-01-29 17:52:37,240 | INFO |  -7.52 * 0.5 =  -3.76 for ctc
2026-01-29 17:52:37,240 | INFO | total log probability: -5.39
2026-01-29 17:52:37,240 | INFO | normalized log probability: -0.27
2026-01-29 17:52:37,240 | INFO | total number of ended hypotheses: 194
2026-01-29 17:52:37,240 | INFO | best hypo: esprit<space>républicain

2026-01-29 17:52:37,242 | INFO | speech length: 80800
2026-01-29 17:52:37,267 | INFO | decoder input length: 125
2026-01-29 17:52:37,267 | INFO | max output length: 125
2026-01-29 17:52:37,267 | INFO | min output length: 12
2026-01-29 17:52:40,300 | INFO | end detected at 83
2026-01-29 17:52:40,301 | INFO |  -6.30 * 0.5 =  -3.15 for decoder
2026-01-29 17:52:40,301 | INFO |  -0.06 * 0.5 =  -0.03 for ctc
2026-01-29 17:52:40,301 | INFO | total log probability: -3.18
2026-01-29 17:52:40,301 | INFO | normalized log probability: -0.04
2026-01-29 17:52:40,301 | INFO | total number of ended hypotheses: 137
2026-01-29 17:52:40,302 | INFO | best hypo: et<space>le<space>sens<space>de<space>l'intérêt<space>général<space>qui<space>impose<space>que<space>l'état<space>conserve<space>toute<space>sa<space>place

2026-01-29 17:52:40,304 | INFO | speech length: 64160
2026-01-29 17:52:40,330 | INFO | decoder input length: 99
2026-01-29 17:52:40,330 | INFO | max output length: 99
2026-01-29 17:52:40,330 | INFO | min output length: 9
2026-01-29 17:52:42,405 | INFO | end detected at 60
2026-01-29 17:52:42,406 | INFO |  -4.42 * 0.5 =  -2.21 for decoder
2026-01-29 17:52:42,406 | INFO |  -0.27 * 0.5 =  -0.14 for ctc
2026-01-29 17:52:42,406 | INFO | total log probability: -2.35
2026-01-29 17:52:42,406 | INFO | normalized log probability: -0.04
2026-01-29 17:52:42,406 | INFO | total number of ended hypotheses: 191
2026-01-29 17:52:42,407 | INFO | best hypo: pour<space>dire<space>le<space>droit<space>le<space>faire<space>respecter<space>avec<space>autorité

2026-01-29 17:52:42,409 | INFO | speech length: 8640
2026-01-29 17:52:42,431 | INFO | decoder input length: 13
2026-01-29 17:52:42,431 | INFO | max output length: 13
2026-01-29 17:52:42,431 | INFO | min output length: 1
2026-01-29 17:52:42,769 | INFO | adding <eos> in the last position in the loop
2026-01-29 17:52:42,775 | INFO | no hypothesis. Finish decoding.
2026-01-29 17:52:42,776 | INFO |  -1.02 * 0.5 =  -0.51 for decoder
2026-01-29 17:52:42,776 | INFO |  -0.39 * 0.5 =  -0.20 for ctc
2026-01-29 17:52:42,776 | INFO | total log probability: -0.71
2026-01-29 17:52:42,776 | INFO | normalized log probability: -0.06
2026-01-29 17:52:42,776 | INFO | total number of ended hypotheses: 83
2026-01-29 17:52:42,776 | INFO | best hypo: avec<space>juste

2026-01-29 17:52:42,777 | INFO | speech length: 19521
2026-01-29 17:52:42,802 | INFO | decoder input length: 30
2026-01-29 17:52:42,802 | INFO | max output length: 30
2026-01-29 17:52:42,802 | INFO | min output length: 3
2026-01-29 17:52:43,590 | INFO | end detected at 27
2026-01-29 17:52:43,591 | INFO |  -3.74 * 0.5 =  -1.87 for decoder
2026-01-29 17:52:43,591 | INFO |  -1.46 * 0.5 =  -0.73 for ctc
2026-01-29 17:52:43,591 | INFO | total log probability: -2.60
2026-01-29 17:52:43,591 | INFO | normalized log probability: -0.12
2026-01-29 17:52:43,591 | INFO | total number of ended hypotheses: 172
2026-01-29 17:52:43,592 | INFO | best hypo: gardons<space>ses<space>exigents

2026-01-29 17:52:43,593 | INFO | speech length: 19520
2026-01-29 17:52:43,619 | INFO | decoder input length: 30
2026-01-29 17:52:43,619 | INFO | max output length: 30
2026-01-29 17:52:43,619 | INFO | min output length: 3
2026-01-29 17:52:44,347 | INFO | end detected at 25
2026-01-29 17:52:44,348 | INFO |  -2.16 * 0.5 =  -1.08 for decoder
2026-01-29 17:52:44,348 | INFO |  -0.75 * 0.5 =  -0.38 for ctc
2026-01-29 17:52:44,348 | INFO | total log probability: -1.46
2026-01-29 17:52:44,348 | INFO | normalized log probability: -0.07
2026-01-29 17:52:44,348 | INFO | total number of ended hypotheses: 145
2026-01-29 17:52:44,349 | INFO | best hypo: gardant<space>ses<space>valeurs

2026-01-29 17:52:44,350 | INFO | speech length: 81600
2026-01-29 17:52:44,376 | INFO | decoder input length: 127
2026-01-29 17:52:44,376 | INFO | max output length: 127
2026-01-29 17:52:44,376 | INFO | min output length: 12
2026-01-29 17:52:47,443 | INFO | end detected at 86
2026-01-29 17:52:47,445 | INFO |  -6.37 * 0.5 =  -3.18 for decoder
2026-01-29 17:52:47,445 | INFO |  -0.11 * 0.5 =  -0.05 for ctc
2026-01-29 17:52:47,445 | INFO | total log probability: -3.24
2026-01-29 17:52:47,445 | INFO | normalized log probability: -0.04
2026-01-29 17:52:47,445 | INFO | total number of ended hypotheses: 174
2026-01-29 17:52:47,446 | INFO | best hypo: en<space>les<space>faisant<space>vivre<space>nous<space>serons<space>plus<space>forts<space>pour<space>aborder<space>les<space>temps<space>qui<space>viennent

2026-01-29 17:52:47,447 | INFO | speech length: 17440
2026-01-29 17:52:47,473 | INFO | decoder input length: 26
2026-01-29 17:52:47,474 | INFO | max output length: 26
2026-01-29 17:52:47,474 | INFO | min output length: 2
2026-01-29 17:52:48,116 | INFO | end detected at 22
2026-01-29 17:52:48,117 | INFO |  -1.44 * 0.5 =  -0.72 for decoder
2026-01-29 17:52:48,117 | INFO |  -5.28 * 0.5 =  -2.64 for ctc
2026-01-29 17:52:48,117 | INFO | total log probability: -3.36
2026-01-29 17:52:48,117 | INFO | normalized log probability: -0.19
2026-01-29 17:52:48,117 | INFO | total number of ended hypotheses: 174
2026-01-29 17:52:48,117 | INFO | best hypo: la<space>france<space>chante

2026-01-29 17:52:48,119 | INFO | speech length: 26400
2026-01-29 17:52:48,145 | INFO | decoder input length: 40
2026-01-29 17:52:48,145 | INFO | max output length: 40
2026-01-29 17:52:48,145 | INFO | min output length: 4
2026-01-29 17:52:49,308 | INFO | adding <eos> in the last position in the loop
2026-01-29 17:52:49,316 | INFO | no hypothesis. Finish decoding.
2026-01-29 17:52:49,316 | INFO |  -3.07 * 0.5 =  -1.53 for decoder
2026-01-29 17:52:49,316 | INFO |  -0.49 * 0.5 =  -0.24 for ctc
2026-01-29 17:52:49,316 | INFO | total log probability: -1.78
2026-01-29 17:52:49,316 | INFO | normalized log probability: -0.05
2026-01-29 17:52:49,316 | INFO | total number of ended hypotheses: 102
2026-01-29 17:52:49,317 | INFO | best hypo: elle<space>doit<space>le<space>faire<space>au<space>rythme<space>du<space>monde

2026-01-29 17:52:49,318 | INFO | speech length: 104960
2026-01-29 17:52:49,344 | INFO | decoder input length: 163
2026-01-29 17:52:49,345 | INFO | max output length: 163
2026-01-29 17:52:49,345 | INFO | min output length: 16
2026-01-29 17:52:53,156 | INFO | end detected at 98
2026-01-29 17:52:53,156 | INFO |  -7.56 * 0.5 =  -3.78 for decoder
2026-01-29 17:52:53,157 | INFO |  -1.10 * 0.5 =  -0.55 for ctc
2026-01-29 17:52:53,157 | INFO | total log probability: -4.33
2026-01-29 17:52:53,157 | INFO | normalized log probability: -0.05
2026-01-29 17:52:53,157 | INFO | total number of ended hypotheses: 155
2026-01-29 17:52:53,158 | INFO | best hypo: en<space>étant<space>fidèle<space>à<space>son<space>génie<space>propre<space>elle<space>sera<space>conjuguée<space>le<space>changement<space>et<space>la<space>cohésion<space>sociale

2026-01-29 17:52:53,159 | INFO | speech length: 92000
2026-01-29 17:52:53,186 | INFO | decoder input length: 143
2026-01-29 17:52:53,186 | INFO | max output length: 143
2026-01-29 17:52:53,186 | INFO | min output length: 14
2026-01-29 17:52:56,355 | INFO | end detected at 83
2026-01-29 17:52:56,356 | INFO |  -6.42 * 0.5 =  -3.21 for decoder
2026-01-29 17:52:56,356 | INFO |  -0.27 * 0.5 =  -0.14 for ctc
2026-01-29 17:52:56,356 | INFO | total log probability: -3.35
2026-01-29 17:52:56,356 | INFO | normalized log probability: -0.04
2026-01-29 17:52:56,356 | INFO | total number of ended hypotheses: 179
2026-01-29 17:52:56,357 | INFO | best hypo: l'esprit<space>d'initiative<space>et<space>la<space>sécurité<space>la<space>modernité<space>et<space>le<space>bien<space>vivre<space>ensemble

2026-01-29 17:52:56,359 | INFO | speech length: 20160
2026-01-29 17:52:56,386 | INFO | decoder input length: 31
2026-01-29 17:52:56,386 | INFO | max output length: 31
2026-01-29 17:52:56,386 | INFO | min output length: 3
2026-01-29 17:52:57,233 | INFO | end detected at 28
2026-01-29 17:52:57,234 | INFO |  -2.45 * 0.5 =  -1.22 for decoder
2026-01-29 17:52:57,234 | INFO |  -1.39 * 0.5 =  -0.69 for ctc
2026-01-29 17:52:57,234 | INFO | total log probability: -1.92
2026-01-29 17:52:57,234 | INFO | normalized log probability: -0.08
2026-01-29 17:52:57,234 | INFO | total number of ended hypotheses: 169
2026-01-29 17:52:57,235 | INFO | best hypo: et<space>chers<space>compatriotes

2026-01-29 17:52:57,236 | INFO | speech length: 106240
2026-01-29 17:52:57,261 | INFO | decoder input length: 165
2026-01-29 17:52:57,261 | INFO | max output length: 165
2026-01-29 17:52:57,261 | INFO | min output length: 16
2026-01-29 17:53:00,874 | INFO | end detected at 91
2026-01-29 17:53:00,876 | INFO |  -7.12 * 0.5 =  -3.56 for decoder
2026-01-29 17:53:00,876 | INFO |  -0.99 * 0.5 =  -0.49 for ctc
2026-01-29 17:53:00,876 | INFO | total log probability: -4.05
2026-01-29 17:53:00,876 | INFO | normalized log probability: -0.05
2026-01-29 17:53:00,876 | INFO | total number of ended hypotheses: 200
2026-01-29 17:53:00,877 | INFO | best hypo: je<space>mesure<space>l'honneur<space>et<space>la<space>responsabilité<space>qui<space>m'échoit<space>de<space>m'adresser<space>à<space>vous<space>ce<space>soir

2026-01-29 17:53:00,879 | INFO | speech length: 55040
2026-01-29 17:53:00,905 | INFO | decoder input length: 85
2026-01-29 17:53:00,905 | INFO | max output length: 85
2026-01-29 17:53:00,905 | INFO | min output length: 8
2026-01-29 17:53:02,867 | INFO | end detected at 55
2026-01-29 17:53:02,868 | INFO |  -3.92 * 0.5 =  -1.96 for decoder
2026-01-29 17:53:02,868 | INFO |  -0.03 * 0.5 =  -0.01 for ctc
2026-01-29 17:53:02,868 | INFO | total log probability: -1.97
2026-01-29 17:53:02,868 | INFO | normalized log probability: -0.04
2026-01-29 17:53:02,868 | INFO | total number of ended hypotheses: 169
2026-01-29 17:53:02,869 | INFO | best hypo: alors<space>que<space>notre<space>nation<space>franchit<space>le<space>cap<space>du<space>siècle

2026-01-29 17:53:02,870 | INFO | speech length: 80480
2026-01-29 17:53:02,897 | INFO | decoder input length: 125
2026-01-29 17:53:02,897 | INFO | max output length: 125
2026-01-29 17:53:02,897 | INFO | min output length: 12
2026-01-29 17:53:05,807 | INFO | end detected at 81
2026-01-29 17:53:05,809 | INFO |  -6.80 * 0.5 =  -3.40 for decoder
2026-01-29 17:53:05,809 | INFO |  -0.83 * 0.5 =  -0.41 for ctc
2026-01-29 17:53:05,809 | INFO | total log probability: -3.81
2026-01-29 17:53:05,809 | INFO | normalized log probability: -0.05
2026-01-29 17:53:05,809 | INFO | total number of ended hypotheses: 172
2026-01-29 17:53:05,810 | INFO | best hypo: la<space>france<space>a<space>plus<space>de<space>mille<space>ans<space>riches<space>de<space>fièvres<space>de<space>passions<space>d'enthousiasme

2026-01-29 17:53:05,812 | INFO | speech length: 88000
2026-01-29 17:53:05,839 | INFO | decoder input length: 137
2026-01-29 17:53:05,839 | INFO | max output length: 137
2026-01-29 17:53:05,839 | INFO | min output length: 13
2026-01-29 17:53:08,796 | INFO | end detected at 78
2026-01-29 17:53:08,798 | INFO |  -5.76 * 0.5 =  -2.88 for decoder
2026-01-29 17:53:08,798 | INFO |  -5.74 * 0.5 =  -2.87 for ctc
2026-01-29 17:53:08,798 | INFO | total log probability: -5.75
2026-01-29 17:53:08,798 | INFO | normalized log probability: -0.08
2026-01-29 17:53:08,798 | INFO | total number of ended hypotheses: 182
2026-01-29 17:53:08,799 | INFO | best hypo: elle<space>continue<space>comme<space>hier<space>à<space>ouvrir<space>et<space>à<space>défricher<space>les<space>chemins<space>du<space>monde

2026-01-29 17:53:08,801 | INFO | speech length: 42560
2026-01-29 17:53:08,828 | INFO | decoder input length: 66
2026-01-29 17:53:08,828 | INFO | max output length: 66
2026-01-29 17:53:08,828 | INFO | min output length: 6
2026-01-29 17:53:10,083 | INFO | end detected at 39
2026-01-29 17:53:10,084 | INFO |  -2.75 * 0.5 =  -1.38 for decoder
2026-01-29 17:53:10,084 | INFO |  -0.83 * 0.5 =  -0.42 for ctc
2026-01-29 17:53:10,084 | INFO | total log probability: -1.79
2026-01-29 17:53:10,084 | INFO | normalized log probability: -0.05
2026-01-29 17:53:10,084 | INFO | total number of ended hypotheses: 170
2026-01-29 17:53:10,084 | INFO | best hypo: le<space>nouveau<space>siècle<space>est<space>à<space>inventer

2026-01-29 17:53:10,086 | INFO | speech length: 37120
2026-01-29 17:53:10,113 | INFO | decoder input length: 57
2026-01-29 17:53:10,113 | INFO | max output length: 57
2026-01-29 17:53:10,113 | INFO | min output length: 5
2026-01-29 17:53:11,337 | INFO | end detected at 39
2026-01-29 17:53:11,339 | INFO |  -2.75 * 0.5 =  -1.37 for decoder
2026-01-29 17:53:11,339 | INFO |  -0.54 * 0.5 =  -0.27 for ctc
2026-01-29 17:53:11,339 | INFO | total log probability: -1.64
2026-01-29 17:53:11,339 | INFO | normalized log probability: -0.05
2026-01-29 17:53:11,339 | INFO | total number of ended hypotheses: 196
2026-01-29 17:53:11,339 | INFO | best hypo: plus<space>fraternel<space>plus<space>volontaire

2026-01-29 17:53:11,341 | INFO | speech length: 41440
2026-01-29 17:53:11,368 | INFO | decoder input length: 64
2026-01-29 17:53:11,368 | INFO | max output length: 64
2026-01-29 17:53:11,368 | INFO | min output length: 6
2026-01-29 17:53:12,933 | INFO | end detected at 50
2026-01-29 17:53:12,934 | INFO |  -3.52 * 0.5 =  -1.76 for decoder
2026-01-29 17:53:12,934 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-29 17:53:12,934 | INFO | total log probability: -1.77
2026-01-29 17:53:12,934 | INFO | normalized log probability: -0.04
2026-01-29 17:53:12,934 | INFO | total number of ended hypotheses: 177
2026-01-29 17:53:12,935 | INFO | best hypo: il<space>aura<space>les<space>couleurs<space>que<space>nous<space>lui<space>donnerons

2026-01-29 17:53:12,936 | INFO | speech length: 48480
2026-01-29 17:53:12,962 | INFO | decoder input length: 75
2026-01-29 17:53:12,963 | INFO | max output length: 75
2026-01-29 17:53:12,963 | INFO | min output length: 7
2026-01-29 17:53:14,803 | INFO | end detected at 55
2026-01-29 17:53:14,804 | INFO |  -3.94 * 0.5 =  -1.97 for decoder
2026-01-29 17:53:14,804 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-29 17:53:14,804 | INFO | total log probability: -1.98
2026-01-29 17:53:14,804 | INFO | normalized log probability: -0.04
2026-01-29 17:53:14,804 | INFO | total number of ended hypotheses: 171
2026-01-29 17:53:14,805 | INFO | best hypo: la<space>france<space>sera<space>ce<space>que<space>nous<space>voudrons<space>qu'elle<space>soit

2026-01-29 17:53:14,806 | INFO | speech length: 59840
2026-01-29 17:53:14,832 | INFO | decoder input length: 93
2026-01-29 17:53:14,832 | INFO | max output length: 93
2026-01-29 17:53:14,832 | INFO | min output length: 9
2026-01-29 17:53:16,490 | INFO | end detected at 48
2026-01-29 17:53:16,491 | INFO |  -3.53 * 0.5 =  -1.76 for decoder
2026-01-29 17:53:16,491 | INFO |  -0.08 * 0.5 =  -0.04 for ctc
2026-01-29 17:53:16,491 | INFO | total log probability: -1.81
2026-01-29 17:53:16,491 | INFO | normalized log probability: -0.04
2026-01-29 17:53:16,491 | INFO | total number of ended hypotheses: 164
2026-01-29 17:53:16,492 | INFO | best hypo: une<space>nation<space>unie<space>vivante<space>solidaire<space>ouverte

2026-01-29 17:53:16,493 | INFO | speech length: 31040
2026-01-29 17:53:16,519 | INFO | decoder input length: 48
2026-01-29 17:53:16,519 | INFO | max output length: 48
2026-01-29 17:53:16,520 | INFO | min output length: 4
2026-01-29 17:53:17,635 | INFO | end detected at 37
2026-01-29 17:53:17,636 | INFO |  -3.36 * 0.5 =  -1.68 for decoder
2026-01-29 17:53:17,636 | INFO |  -1.24 * 0.5 =  -0.62 for ctc
2026-01-29 17:53:17,636 | INFO | total log probability: -2.30
2026-01-29 17:53:17,637 | INFO | normalized log probability: -0.07
2026-01-29 17:53:17,637 | INFO | total number of ended hypotheses: 160
2026-01-29 17:53:17,637 | INFO | best hypo: qui<space>n'acceptent<space>aucune<space>fatalité

2026-01-29 17:53:17,639 | INFO | speech length: 72960
2026-01-29 17:53:17,665 | INFO | decoder input length: 113
2026-01-29 17:53:17,665 | INFO | max output length: 113
2026-01-29 17:53:17,665 | INFO | min output length: 11
2026-01-29 17:53:20,043 | INFO | end detected at 67
2026-01-29 17:53:20,044 | INFO |  -4.84 * 0.5 =  -2.42 for decoder
2026-01-29 17:53:20,044 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-29 17:53:20,044 | INFO | total log probability: -2.42
2026-01-29 17:53:20,044 | INFO | normalized log probability: -0.04
2026-01-29 17:53:20,044 | INFO | total number of ended hypotheses: 165
2026-01-29 17:53:20,045 | INFO | best hypo: car<space>dans<space>un<space>monde<space>où<space>rien<space>n'est<space>figé<space>l'avenir<space>dépend<space>de<space>nous

2026-01-29 17:53:20,046 | INFO | speech length: 132960
2026-01-29 17:53:20,074 | INFO | decoder input length: 207
2026-01-29 17:53:20,074 | INFO | max output length: 207
2026-01-29 17:53:20,074 | INFO | min output length: 20
2026-01-29 17:53:24,814 | INFO | end detected at 109
2026-01-29 17:53:24,815 | INFO |  -8.56 * 0.5 =  -4.28 for decoder
2026-01-29 17:53:24,816 | INFO |  -0.64 * 0.5 =  -0.32 for ctc
2026-01-29 17:53:24,816 | INFO | total log probability: -4.60
2026-01-29 17:53:24,816 | INFO | normalized log probability: -0.04
2026-01-29 17:53:24,816 | INFO | total number of ended hypotheses: 156
2026-01-29 17:53:24,817 | INFO | best hypo: l'avenir<space>dépend<space>de<space>notre<space>capacité<space>à<space>construire<space>à<space>créer<space>à<space>rêver<space>ensemble<space>les<space>voix<space>de<space>l'aventure<space>humaine

2026-01-29 17:53:24,818 | INFO | speech length: 24480
2026-01-29 17:53:24,844 | INFO | decoder input length: 37
2026-01-29 17:53:24,844 | INFO | max output length: 37
2026-01-29 17:53:24,844 | INFO | min output length: 3
2026-01-29 17:53:25,847 | INFO | adding <eos> in the last position in the loop
2026-01-29 17:53:25,855 | INFO | no hypothesis. Finish decoding.
2026-01-29 17:53:25,856 | INFO |  -2.73 * 0.5 =  -1.36 for decoder
2026-01-29 17:53:25,856 | INFO |  -0.88 * 0.5 =  -0.44 for ctc
2026-01-29 17:53:25,856 | INFO | total log probability: -1.80
2026-01-29 17:53:25,856 | INFO | normalized log probability: -0.05
2026-01-29 17:53:25,856 | INFO | total number of ended hypotheses: 118
2026-01-29 17:53:25,857 | INFO | best hypo: à<space>chacune<space>et<space>à<space>chacun<space>d'entre<space>eux

2026-01-29 17:53:25,858 | INFO | speech length: 77120
2026-01-29 17:53:25,885 | INFO | decoder input length: 120
2026-01-29 17:53:25,885 | INFO | max output length: 120
2026-01-29 17:53:25,885 | INFO | min output length: 12
2026-01-29 17:53:28,337 | INFO | end detected at 68
2026-01-29 17:53:28,337 | INFO |  -5.25 * 0.5 =  -2.62 for decoder
2026-01-29 17:53:28,338 | INFO |  -0.18 * 0.5 =  -0.09 for ctc
2026-01-29 17:53:28,338 | INFO | total log probability: -2.71
2026-01-29 17:53:28,338 | INFO | normalized log probability: -0.04
2026-01-29 17:53:28,338 | INFO | total number of ended hypotheses: 174
2026-01-29 17:53:28,339 | INFO | best hypo: françaises<space>et<space>français<space>de<space>métropole<space>d'outre<space>mer<space>de<space>l'étranger

2026-01-29 17:53:28,340 | INFO | speech length: 34240
2026-01-29 17:53:28,367 | INFO | decoder input length: 53
2026-01-29 17:53:28,367 | INFO | max output length: 53
2026-01-29 17:53:28,367 | INFO | min output length: 5
2026-01-29 17:53:29,547 | INFO | end detected at 38
2026-01-29 17:53:29,549 | INFO |  -2.94 * 0.5 =  -1.47 for decoder
2026-01-29 17:53:29,549 | INFO |  -1.55 * 0.5 =  -0.78 for ctc
2026-01-29 17:53:29,549 | INFO | total log probability: -2.25
2026-01-29 17:53:29,549 | INFO | normalized log probability: -0.07
2026-01-29 17:53:29,549 | INFO | total number of ended hypotheses: 218
2026-01-29 17:53:29,549 | INFO | best hypo: je<space>souhaite<space>très<space>chaleureuse

2026-01-29 17:53:29,551 | INFO | speech length: 50880
2026-01-29 17:53:29,577 | INFO | decoder input length: 79
2026-01-29 17:53:29,578 | INFO | max output length: 79
2026-01-29 17:53:29,578 | INFO | min output length: 7
2026-01-29 17:53:31,163 | INFO | end detected at 48
2026-01-29 17:53:31,164 | INFO |  -3.45 * 0.5 =  -1.73 for decoder
2026-01-29 17:53:31,164 | INFO |  -0.05 * 0.5 =  -0.02 for ctc
2026-01-29 17:53:31,164 | INFO | total log probability: -1.75
2026-01-29 17:53:31,164 | INFO | normalized log probability: -0.04
2026-01-29 17:53:31,164 | INFO | total number of ended hypotheses: 152
2026-01-29 17:53:31,165 | INFO | best hypo: une<space>bonne<space>et<space>une<space>heureuse<space>année<space>deux<space>mille

2026-01-29 17:53:31,176 | INFO | Chunk: 0 | WER=66.666667 | S=0 D=0 I=2
2026-01-29 17:53:31,177 | INFO | Chunk: 1 | WER=33.333333 | S=0 D=0 I=2
2026-01-29 17:53:31,177 | INFO | Chunk: 2 | WER=19.047619 | S=2 D=0 I=2
2026-01-29 17:53:31,178 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:53:31,178 | INFO | Chunk: 4 | WER=30.000000 | S=0 D=1 I=2
2026-01-29 17:53:31,178 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:53:31,178 | INFO | Chunk: 6 | WER=12.500000 | S=0 D=0 I=2
2026-01-29 17:53:31,179 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:53:31,179 | INFO | Chunk: 8 | WER=50.000000 | S=1 D=0 I=1
2026-01-29 17:53:31,180 | INFO | Chunk: 9 | WER=6.896552 | S=1 D=0 I=1
2026-01-29 17:53:31,180 | INFO | Chunk: 10 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:53:31,180 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:53:31,181 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:53:31,181 | INFO | Chunk: 13 | WER=27.272727 | S=1 D=0 I=2
2026-01-29 17:53:31,181 | INFO | Chunk: 14 | WER=12.500000 | S=1 D=0 I=0
2026-01-29 17:53:31,182 | INFO | Chunk: 15 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:53:31,182 | INFO | Chunk: 16 | WER=12.500000 | S=0 D=0 I=2
2026-01-29 17:53:31,182 | INFO | Chunk: 17 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:53:31,182 | INFO | Chunk: 18 | WER=16.666667 | S=0 D=0 I=1
2026-01-29 17:53:31,183 | INFO | Chunk: 19 | WER=60.000000 | S=0 D=0 I=3
2026-01-29 17:53:31,183 | INFO | Chunk: 20 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:53:31,183 | INFO | Chunk: 21 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:53:31,183 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:53:31,183 | INFO | Chunk: 23 | WER=25.000000 | S=0 D=1 I=1
2026-01-29 17:53:31,184 | INFO | Chunk: 24 | WER=11.111111 | S=1 D=0 I=0
2026-01-29 17:53:31,184 | INFO | Chunk: 25 | WER=7.692308 | S=0 D=0 I=1
2026-01-29 17:53:31,184 | INFO | Chunk: 26 | WER=60.000000 | S=0 D=0 I=3
2026-01-29 17:53:31,184 | INFO | Chunk: 27 | WER=10.000000 | S=0 D=0 I=1
2026-01-29 17:53:31,185 | INFO | Chunk: 28 | WER=14.285714 | S=0 D=0 I=2
2026-01-29 17:53:31,185 | INFO | Chunk: 29 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:53:31,185 | INFO | Chunk: 30 | WER=0.000000 | S=1 D=0 I=0
2026-01-29 17:53:31,186 | INFO | Chunk: 31 | WER=9.523810 | S=2 D=0 I=0
2026-01-29 17:53:31,186 | INFO | Chunk: 32 | WER=4.166667 | S=0 D=0 I=1
2026-01-29 17:53:31,186 | INFO | Chunk: 33 | WER=40.000000 | S=0 D=0 I=2
2026-01-29 17:53:31,186 | INFO | Chunk: 34 | WER=500.000000 | S=0 D=0 I=5
2026-01-29 17:53:31,187 | INFO | Chunk: 35 | WER=6.250000 | S=1 D=0 I=0
2026-01-29 17:53:31,187 | INFO | Chunk: 36 | WER=16.666667 | S=1 D=0 I=0
2026-01-29 17:53:31,188 | INFO | Chunk: 37 | WER=6.060606 | S=0 D=0 I=2
2026-01-29 17:53:31,188 | INFO | Chunk: 38 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:53:31,188 | INFO | Chunk: 39 | WER=33.333333 | S=2 D=0 I=1
2026-01-29 17:53:31,189 | INFO | Chunk: 40 | WER=9.090909 | S=1 D=0 I=0
2026-01-29 17:53:31,189 | INFO | Chunk: 41 | WER=66.666667 | S=0 D=1 I=1
2026-01-29 17:53:31,189 | INFO | Chunk: 42 | WER=10.000000 | S=0 D=0 I=1
2026-01-29 17:53:31,189 | INFO | Chunk: 43 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:53:31,189 | INFO | Chunk: 44 | WER=12.500000 | S=1 D=0 I=0
2026-01-29 17:53:31,190 | INFO | Chunk: 45 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:53:31,190 | INFO | Chunk: 46 | WER=33.333333 | S=0 D=0 I=1
2026-01-29 17:53:31,190 | INFO | Chunk: 47 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:53:31,190 | INFO | Chunk: 48 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:53:31,191 | INFO | Chunk: 49 | WER=13.333333 | S=1 D=0 I=1
2026-01-29 17:53:31,191 | INFO | Chunk: 50 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:53:31,191 | INFO | Chunk: 51 | WER=50.000000 | S=1 D=0 I=1
2026-01-29 17:53:31,191 | INFO | Chunk: 52 | WER=50.000000 | S=0 D=1 I=2
2026-01-29 17:53:31,191 | INFO | Chunk: 53 | WER=300.000000 | S=0 D=0 I=3
2026-01-29 17:53:31,192 | INFO | Chunk: 54 | WER=8.333333 | S=1 D=0 I=0
2026-01-29 17:53:31,192 | INFO | Chunk: 55 | WER=7.692308 | S=1 D=0 I=0
2026-01-29 17:53:31,192 | INFO | Chunk: 56 | WER=5.555556 | S=1 D=0 I=0
2026-01-29 17:53:31,192 | INFO | Chunk: 57 | WER=14.285714 | S=0 D=0 I=1
2026-01-29 17:53:31,193 | INFO | Chunk: 58 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:53:31,193 | INFO | Chunk: 59 | WER=15.384615 | S=0 D=0 I=2
2026-01-29 17:53:31,193 | INFO | Chunk: 60 | WER=40.000000 | S=0 D=1 I=1
2026-01-29 17:53:31,193 | INFO | Chunk: 61 | WER=100.000000 | S=1 D=1 I=1
2026-01-29 17:53:31,193 | INFO | Chunk: 62 | WER=25.000000 | S=0 D=0 I=2
2026-01-29 17:53:31,194 | INFO | Chunk: 63 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:53:31,194 | INFO | Chunk: 64 | WER=25.000000 | S=1 D=0 I=0
2026-01-29 17:53:31,194 | INFO | Chunk: 65 | WER=66.666667 | S=0 D=1 I=1
2026-01-29 17:53:31,194 | INFO | Chunk: 66 | WER=19.047619 | S=3 D=0 I=1
2026-01-29 17:53:31,195 | INFO | Chunk: 67 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:53:31,195 | INFO | Chunk: 68 | WER=40.000000 | S=2 D=0 I=0
2026-01-29 17:53:31,195 | INFO | Chunk: 69 | WER=25.000000 | S=0 D=0 I=2
2026-01-29 17:53:31,195 | INFO | Chunk: 70 | WER=7.692308 | S=0 D=0 I=1
2026-01-29 17:53:31,196 | INFO | Chunk: 71 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:53:31,196 | INFO | Chunk: 72 | WER=40.000000 | S=2 D=0 I=0
2026-01-29 17:53:31,196 | INFO | Chunk: 73 | WER=30.000000 | S=1 D=0 I=2
2026-01-29 17:53:31,196 | INFO | Chunk: 74 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:53:31,196 | INFO | Chunk: 75 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:53:31,197 | INFO | Chunk: 76 | WER=25.000000 | S=0 D=0 I=1
2026-01-29 17:53:31,197 | INFO | Chunk: 77 | WER=18.750000 | S=1 D=0 I=2
2026-01-29 17:53:31,197 | INFO | Chunk: 78 | WER=20.000000 | S=0 D=0 I=2
2026-01-29 17:53:31,198 | INFO | Chunk: 79 | WER=20.000000 | S=0 D=0 I=3
2026-01-29 17:53:31,198 | INFO | Chunk: 80 | WER=6.666667 | S=0 D=0 I=1
2026-01-29 17:53:31,198 | INFO | Chunk: 81 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:53:31,199 | INFO | Chunk: 82 | WER=30.000000 | S=0 D=2 I=1
2026-01-29 17:53:31,199 | INFO | Chunk: 83 | WER=33.333333 | S=1 D=0 I=1
2026-01-29 17:53:31,199 | INFO | Chunk: 84 | WER=37.500000 | S=1 D=0 I=2
2026-01-29 17:53:31,199 | INFO | Chunk: 85 | WER=20.000000 | S=0 D=1 I=0
2026-01-29 17:53:31,199 | INFO | Chunk: 86 | WER=33.333333 | S=1 D=0 I=0
2026-01-29 17:53:31,199 | INFO | Chunk: 87 | WER=20.000000 | S=0 D=0 I=1
2026-01-29 17:53:31,200 | INFO | Chunk: 88 | WER=11.111111 | S=1 D=0 I=1
2026-01-29 17:53:31,200 | INFO | Chunk: 89 | WER=5.882353 | S=0 D=0 I=1
2026-01-29 17:53:31,201 | INFO | Chunk: 90 | WER=16.666667 | S=1 D=0 I=1
2026-01-29 17:53:31,201 | INFO | Chunk: 91 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:53:31,201 | INFO | Chunk: 92 | WER=25.000000 | S=0 D=0 I=1
2026-01-29 17:53:31,201 | INFO | Chunk: 93 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:53:31,201 | INFO | Chunk: 94 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:53:31,201 | INFO | Chunk: 95 | WER=33.333333 | S=0 D=0 I=2
2026-01-29 17:53:31,202 | INFO | Chunk: 96 | WER=150.000000 | S=0 D=0 I=3
2026-01-29 17:53:31,202 | INFO | Chunk: 97 | WER=33.333333 | S=0 D=1 I=0
2026-01-29 17:53:31,202 | INFO | Chunk: 98 | WER=6.666667 | S=0 D=0 I=1
2026-01-29 17:53:31,202 | INFO | Chunk: 99 | WER=12.500000 | S=0 D=0 I=1
2026-01-29 17:53:31,202 | INFO | Chunk: 100 | WER=200.000000 | S=1 D=0 I=1
2026-01-29 17:53:31,203 | INFO | Chunk: 101 | WER=100.000000 | S=1 D=0 I=1
2026-01-29 17:53:31,203 | INFO | Chunk: 102 | WER=150.000000 | S=2 D=0 I=1
2026-01-29 17:53:31,203 | INFO | Chunk: 103 | WER=7.692308 | S=0 D=0 I=1
2026-01-29 17:53:31,203 | INFO | Chunk: 104 | WER=50.000000 | S=0 D=0 I=1
2026-01-29 17:53:31,203 | INFO | Chunk: 105 | WER=33.333333 | S=0 D=0 I=2
2026-01-29 17:53:31,204 | INFO | Chunk: 106 | WER=12.500000 | S=2 D=0 I=0
2026-01-29 17:53:31,204 | INFO | Chunk: 107 | WER=7.692308 | S=0 D=0 I=1
2026-01-29 17:53:31,204 | INFO | Chunk: 108 | WER=33.333333 | S=1 D=0 I=0
2026-01-29 17:53:31,204 | INFO | Chunk: 109 | WER=12.500000 | S=1 D=0 I=1
2026-01-29 17:53:31,205 | INFO | Chunk: 110 | WER=22.222222 | S=0 D=1 I=1
2026-01-29 17:53:31,205 | INFO | Chunk: 111 | WER=21.428571 | S=3 D=0 I=0
2026-01-29 17:53:31,205 | INFO | Chunk: 112 | WER=18.181818 | S=0 D=0 I=2
2026-01-29 17:53:31,205 | INFO | Chunk: 113 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:53:31,206 | INFO | Chunk: 114 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:53:31,206 | INFO | Chunk: 115 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:53:31,206 | INFO | Chunk: 116 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 17:53:31,206 | INFO | Chunk: 117 | WER=20.000000 | S=0 D=0 I=1
2026-01-29 17:53:31,206 | INFO | Chunk: 118 | WER=20.000000 | S=1 D=0 I=0
2026-01-29 17:53:31,207 | INFO | Chunk: 119 | WER=16.666667 | S=0 D=0 I=2
2026-01-29 17:53:31,207 | INFO | Chunk: 120 | WER=11.111111 | S=1 D=0 I=1
2026-01-29 17:53:31,207 | INFO | Chunk: 121 | WER=60.000000 | S=0 D=0 I=3
2026-01-29 17:53:31,208 | INFO | Chunk: 122 | WER=9.090909 | S=1 D=0 I=0
2026-01-29 17:53:31,208 | INFO | Chunk: 123 | WER=33.333333 | S=0 D=0 I=1
2026-01-29 17:53:31,208 | INFO | Chunk: 124 | WER=33.333333 | S=0 D=0 I=2
2026-01-29 17:53:31,825 | INFO | File: Rhap-M2004.wav | WER=13.874346 | S=51 D=3 I=105
2026-01-29 17:53:31,825 | INFO | ------------------------------
2026-01-29 17:53:31,825 | INFO | Conf ester Done!
2026-01-29 18:02:16,885 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,885 | INFO | Chunk: 1 | WER=16.666667 | S=0 D=0 I=1
2026-01-29 18:02:16,886 | INFO | Chunk: 2 | WER=9.523810 | S=0 D=0 I=2
2026-01-29 18:02:16,886 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,886 | INFO | Chunk: 4 | WER=30.000000 | S=0 D=1 I=2
2026-01-29 18:02:16,887 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,887 | INFO | Chunk: 6 | WER=25.000000 | S=2 D=0 I=2
2026-01-29 18:02:16,888 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,888 | INFO | Chunk: 8 | WER=25.000000 | S=0 D=0 I=1
2026-01-29 18:02:16,889 | INFO | Chunk: 9 | WER=17.241379 | S=4 D=0 I=1
2026-01-29 18:02:16,889 | INFO | Chunk: 10 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,889 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,890 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,890 | INFO | Chunk: 13 | WER=27.272727 | S=1 D=0 I=2
2026-01-29 18:02:16,890 | INFO | Chunk: 14 | WER=12.500000 | S=1 D=0 I=0
2026-01-29 18:02:16,890 | INFO | Chunk: 15 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,891 | INFO | Chunk: 16 | WER=12.500000 | S=0 D=0 I=2
2026-01-29 18:02:16,891 | INFO | Chunk: 17 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,891 | INFO | Chunk: 18 | WER=16.666667 | S=0 D=0 I=1
2026-01-29 18:02:16,891 | INFO | Chunk: 19 | WER=60.000000 | S=0 D=0 I=3
2026-01-29 18:02:16,892 | INFO | Chunk: 20 | WER=7.692308 | S=0 D=1 I=0
2026-01-29 18:02:16,892 | INFO | Chunk: 21 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,892 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,892 | INFO | Chunk: 23 | WER=25.000000 | S=0 D=1 I=1
2026-01-29 18:02:16,893 | INFO | Chunk: 24 | WER=22.222222 | S=2 D=0 I=0
2026-01-29 18:02:16,893 | INFO | Chunk: 25 | WER=7.692308 | S=0 D=0 I=1
2026-01-29 18:02:16,893 | INFO | Chunk: 26 | WER=60.000000 | S=0 D=0 I=3
2026-01-29 18:02:16,893 | INFO | Chunk: 27 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,894 | INFO | Chunk: 28 | WER=14.285714 | S=0 D=0 I=2
2026-01-29 18:02:16,894 | INFO | Chunk: 29 | WER=100.000000 | S=2 D=0 I=1
2026-01-29 18:02:16,894 | INFO | Chunk: 30 | WER=0.000000 | S=1 D=0 I=0
2026-01-29 18:02:16,894 | INFO | Chunk: 31 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,895 | INFO | Chunk: 32 | WER=8.333333 | S=1 D=0 I=1
2026-01-29 18:02:16,895 | INFO | Chunk: 33 | WER=20.000000 | S=0 D=0 I=1
2026-01-29 18:02:16,895 | INFO | Chunk: 34 | WER=300.000000 | S=0 D=0 I=3
2026-01-29 18:02:16,896 | INFO | Chunk: 35 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,896 | INFO | Chunk: 36 | WER=16.666667 | S=0 D=1 I=0
2026-01-29 18:02:16,897 | INFO | Chunk: 37 | WER=15.151515 | S=3 D=0 I=2
2026-01-29 18:02:16,897 | INFO | Chunk: 38 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,897 | INFO | Chunk: 39 | WER=33.333333 | S=2 D=0 I=1
2026-01-29 18:02:16,898 | INFO | Chunk: 40 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,898 | INFO | Chunk: 41 | WER=66.666667 | S=0 D=1 I=1
2026-01-29 18:02:16,898 | INFO | Chunk: 42 | WER=10.000000 | S=0 D=0 I=1
2026-01-29 18:02:16,898 | INFO | Chunk: 43 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,898 | INFO | Chunk: 44 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,899 | INFO | Chunk: 45 | WER=5.882353 | S=1 D=0 I=0
2026-01-29 18:02:16,899 | INFO | Chunk: 46 | WER=33.333333 | S=0 D=0 I=1
2026-01-29 18:02:16,899 | INFO | Chunk: 47 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,899 | INFO | Chunk: 48 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,900 | INFO | Chunk: 49 | WER=6.666667 | S=0 D=0 I=1
2026-01-29 18:02:16,900 | INFO | Chunk: 50 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,900 | INFO | Chunk: 51 | WER=25.000000 | S=0 D=0 I=1
2026-01-29 18:02:16,900 | INFO | Chunk: 52 | WER=66.666667 | S=3 D=1 I=0
2026-01-29 18:02:16,900 | INFO | Chunk: 53 | WER=300.000000 | S=0 D=0 I=3
2026-01-29 18:02:16,900 | INFO | Chunk: 54 | WER=8.333333 | S=1 D=0 I=0
2026-01-29 18:02:16,901 | INFO | Chunk: 55 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,901 | INFO | Chunk: 56 | WER=5.555556 | S=1 D=0 I=0
2026-01-29 18:02:16,901 | INFO | Chunk: 57 | WER=14.285714 | S=0 D=0 I=1
2026-01-29 18:02:16,902 | INFO | Chunk: 58 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,902 | INFO | Chunk: 59 | WER=15.384615 | S=0 D=0 I=2
2026-01-29 18:02:16,902 | INFO | Chunk: 60 | WER=40.000000 | S=0 D=1 I=1
2026-01-29 18:02:16,902 | INFO | Chunk: 61 | WER=66.666667 | S=0 D=1 I=1
2026-01-29 18:02:16,902 | INFO | Chunk: 62 | WER=25.000000 | S=0 D=0 I=2
2026-01-29 18:02:16,903 | INFO | Chunk: 63 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,903 | INFO | Chunk: 64 | WER=50.000000 | S=2 D=0 I=0
2026-01-29 18:02:16,903 | INFO | Chunk: 65 | WER=66.666667 | S=0 D=1 I=1
2026-01-29 18:02:16,903 | INFO | Chunk: 66 | WER=23.809524 | S=4 D=0 I=1
2026-01-29 18:02:16,903 | INFO | Chunk: 67 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,904 | INFO | Chunk: 68 | WER=20.000000 | S=0 D=1 I=0
2026-01-29 18:02:16,904 | INFO | Chunk: 69 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,904 | INFO | Chunk: 70 | WER=7.692308 | S=0 D=0 I=1
2026-01-29 18:02:16,904 | INFO | Chunk: 71 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,905 | INFO | Chunk: 72 | WER=20.000000 | S=1 D=0 I=0
2026-01-29 18:02:16,905 | INFO | Chunk: 73 | WER=30.000000 | S=1 D=0 I=2
2026-01-29 18:02:16,905 | INFO | Chunk: 74 | WER=9.090909 | S=1 D=0 I=0
2026-01-29 18:02:16,905 | INFO | Chunk: 75 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,906 | INFO | Chunk: 76 | WER=25.000000 | S=0 D=0 I=1
2026-01-29 18:02:16,906 | INFO | Chunk: 77 | WER=6.250000 | S=0 D=0 I=1
2026-01-29 18:02:16,906 | INFO | Chunk: 78 | WER=20.000000 | S=0 D=0 I=2
2026-01-29 18:02:16,907 | INFO | Chunk: 79 | WER=20.000000 | S=0 D=0 I=3
2026-01-29 18:02:16,907 | INFO | Chunk: 80 | WER=6.666667 | S=0 D=0 I=1
2026-01-29 18:02:16,907 | INFO | Chunk: 81 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,907 | INFO | Chunk: 82 | WER=30.000000 | S=0 D=2 I=1
2026-01-29 18:02:16,908 | INFO | Chunk: 83 | WER=16.666667 | S=0 D=0 I=1
2026-01-29 18:02:16,908 | INFO | Chunk: 84 | WER=37.500000 | S=1 D=0 I=2
2026-01-29 18:02:16,908 | INFO | Chunk: 85 | WER=20.000000 | S=0 D=1 I=0
2026-01-29 18:02:16,908 | INFO | Chunk: 86 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,908 | INFO | Chunk: 87 | WER=20.000000 | S=0 D=0 I=1
2026-01-29 18:02:16,909 | INFO | Chunk: 88 | WER=11.111111 | S=1 D=0 I=1
2026-01-29 18:02:16,909 | INFO | Chunk: 89 | WER=11.764706 | S=1 D=0 I=1
2026-01-29 18:02:16,909 | INFO | Chunk: 90 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,910 | INFO | Chunk: 91 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,910 | INFO | Chunk: 92 | WER=25.000000 | S=0 D=0 I=1
2026-01-29 18:02:16,910 | INFO | Chunk: 93 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,910 | INFO | Chunk: 94 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,910 | INFO | Chunk: 95 | WER=50.000000 | S=0 D=0 I=3
2026-01-29 18:02:16,910 | INFO | Chunk: 96 | WER=150.000000 | S=0 D=0 I=3
2026-01-29 18:02:16,911 | INFO | Chunk: 97 | WER=33.333333 | S=0 D=1 I=0
2026-01-29 18:02:16,911 | INFO | Chunk: 98 | WER=6.666667 | S=0 D=0 I=1
2026-01-29 18:02:16,911 | INFO | Chunk: 99 | WER=25.000000 | S=1 D=0 I=1
2026-01-29 18:02:16,911 | INFO | Chunk: 100 | WER=200.000000 | S=1 D=0 I=1
2026-01-29 18:02:16,911 | INFO | Chunk: 101 | WER=50.000000 | S=0 D=0 I=1
2026-01-29 18:02:16,911 | INFO | Chunk: 102 | WER=50.000000 | S=0 D=0 I=1
2026-01-29 18:02:16,912 | INFO | Chunk: 103 | WER=7.692308 | S=0 D=0 I=1
2026-01-29 18:02:16,912 | INFO | Chunk: 104 | WER=50.000000 | S=0 D=0 I=1
2026-01-29 18:02:16,912 | INFO | Chunk: 105 | WER=33.333333 | S=0 D=0 I=2
2026-01-29 18:02:16,912 | INFO | Chunk: 106 | WER=6.250000 | S=1 D=0 I=0
2026-01-29 18:02:16,913 | INFO | Chunk: 107 | WER=7.692308 | S=0 D=0 I=1
2026-01-29 18:02:16,913 | INFO | Chunk: 108 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,913 | INFO | Chunk: 109 | WER=18.750000 | S=2 D=0 I=1
2026-01-29 18:02:16,914 | INFO | Chunk: 110 | WER=22.222222 | S=0 D=1 I=1
2026-01-29 18:02:16,914 | INFO | Chunk: 111 | WER=14.285714 | S=2 D=0 I=0
2026-01-29 18:02:16,914 | INFO | Chunk: 112 | WER=18.181818 | S=0 D=0 I=2
2026-01-29 18:02:16,914 | INFO | Chunk: 113 | WER=16.666667 | S=1 D=0 I=0
2026-01-29 18:02:16,914 | INFO | Chunk: 114 | WER=25.000000 | S=1 D=0 I=0
2026-01-29 18:02:16,915 | INFO | Chunk: 115 | WER=12.500000 | S=1 D=0 I=0
2026-01-29 18:02:16,915 | INFO | Chunk: 116 | WER=10.000000 | S=1 D=0 I=0
2026-01-29 18:02:16,915 | INFO | Chunk: 117 | WER=20.000000 | S=0 D=0 I=1
2026-01-29 18:02:16,915 | INFO | Chunk: 118 | WER=0.000000 | S=0 D=0 I=0
2026-01-29 18:02:16,916 | INFO | Chunk: 119 | WER=16.666667 | S=0 D=0 I=2
2026-01-29 18:02:16,916 | INFO | Chunk: 120 | WER=16.666667 | S=2 D=0 I=1
2026-01-29 18:02:16,916 | INFO | Chunk: 121 | WER=80.000000 | S=0 D=1 I=3
2026-01-29 18:02:16,916 | INFO | Chunk: 122 | WER=9.090909 | S=1 D=0 I=0
2026-01-29 18:02:16,917 | INFO | Chunk: 123 | WER=33.333333 | S=0 D=0 I=1
2026-01-29 18:02:16,917 | INFO | Chunk: 124 | WER=100.000000 | S=4 D=0 I=2
2026-01-29 18:02:17,531 | INFO | File: Rhap-M2004.wav | WER=13.525305 | S=54 D=7 I=94
2026-01-29 18:02:17,532 | INFO | ------------------------------
2026-01-29 18:02:17,533 | INFO | hmm_tdnn Done!
2026-01-29 18:02:17,705 | INFO | ==================================Rhap-M2005.wav=========================================
2026-01-29 18:02:17,750 | INFO | Using rVAD model
2026-01-29 18:02:22,945 | INFO | Chunk: 0 | WER=10.227273 | S=6 D=3 I=0
2026-01-29 18:02:22,948 | INFO | Chunk: 1 | WER=17.948718 | S=8 D=6 I=0
2026-01-29 18:02:22,952 | INFO | Chunk: 2 | WER=19.780220 | S=9 D=9 I=0
2026-01-29 18:02:22,956 | INFO | Chunk: 3 | WER=8.695652 | S=4 D=4 I=0
2026-01-29 18:02:23,002 | INFO | File: Rhap-M2005.wav | WER=13.333333 | S=26 D=19 I=1
2026-01-29 18:02:23,002 | INFO | ------------------------------
2026-01-29 18:02:23,003 | INFO | w2vec vad chunk Done!
2026-01-29 18:02:30,823 | INFO | Chunk: 0 | WER=63.636364 | S=2 D=54 I=0
2026-01-29 18:02:30,828 | INFO | Chunk: 1 | WER=64.102564 | S=10 D=40 I=0
2026-01-29 18:02:30,833 | INFO | Chunk: 2 | WER=65.934066 | S=1 D=59 I=0
2026-01-29 18:02:30,838 | INFO | Chunk: 3 | WER=61.956522 | S=3 D=54 I=0
2026-01-29 18:02:30,894 | INFO | File: Rhap-M2005.wav | WER=63.478261 | S=16 D=203 I=0
2026-01-29 18:02:30,894 | INFO | ------------------------------
2026-01-29 18:02:30,894 | INFO | whisper med Done!
2026-01-29 18:02:44,959 | INFO | Chunk: 0 | WER=53.409091 | S=16 D=31 I=0
2026-01-29 18:02:44,962 | INFO | Chunk: 1 | WER=50.000000 | S=12 D=27 I=0
2026-01-29 18:02:44,963 | INFO | Chunk: 2 | WER=64.835165 | S=1 D=58 I=0
2026-01-29 18:02:44,966 | INFO | Chunk: 3 | WER=46.739130 | S=4 D=39 I=0
2026-01-29 18:02:44,995 | INFO | File: Rhap-M2005.wav | WER=53.333333 | S=33 D=151 I=0
2026-01-29 18:02:44,995 | INFO | ------------------------------
2026-01-29 18:02:44,995 | INFO | whisper large Done!
2026-01-29 18:02:45,226 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-29 18:02:45,257 | INFO | Vocabulary size: 350
2026-01-29 18:02:45,842 | INFO | Gradient checkpoint layers: []
2026-01-29 18:02:46,482 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 18:02:46,485 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 18:02:46,485 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 18:02:46,486 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-29 18:02:46,486 | INFO | speech length: 446400
2026-01-29 18:02:46,524 | INFO | decoder input length: 697
2026-01-29 18:02:46,524 | INFO | max output length: 697
2026-01-29 18:02:46,524 | INFO | min output length: 69
2026-01-29 18:03:09,028 | INFO | end detected at 234
2026-01-29 18:03:09,030 | INFO | -654.88 * 0.5 = -327.44 for decoder
2026-01-29 18:03:09,030 | INFO | -176.98 * 0.5 = -88.49 for ctc
2026-01-29 18:03:09,031 | INFO | total log probability: -415.93
2026-01-29 18:03:09,031 | INFO | normalized log probability: -1.83
2026-01-29 18:03:09,031 | INFO | total number of ended hypotheses: 165
2026-01-29 18:03:09,034 | INFO | best hypo: ▁le▁succès▁des▁réseaux▁sociaux▁sur▁internet▁n'est▁plus▁à▁démontrer▁il▁n'y▁a▁qu'à▁voir▁les▁soixante▁cinq▁millions▁d'utilisateurs▁de▁facebook▁mais▁connaissez▁vous▁les▁sites▁sociaux▁spécialisés▁pour▁l'ombre▁de▁facebooke▁il▁existe▁en▁ejet▁des▁sites▁communautistes▁et▁qui▁permettent▁de▁communiquer▁avec▁des▁y▁amies▁ou▁de▁faire▁des▁rencontres'▁dans▁des▁cercles▁très▁fermés▁on▁peut▁cit▁par▁exemple▁le▁site▁small▁world▁set▁serveur▁américains▁est▁une▁sorte▁de▁clube▁viay▁per▁résoré▁à▁la▁jet▁set

2026-01-29 18:03:09,042 | INFO | speech length: 390080
2026-01-29 18:03:09,119 | INFO | decoder input length: 609
2026-01-29 18:03:09,119 | INFO | max output length: 609
2026-01-29 18:03:09,119 | INFO | min output length: 60
2026-01-29 18:03:28,187 | INFO | end detected at 221
2026-01-29 18:03:28,188 | INFO | -492.85 * 0.5 = -246.43 for decoder
2026-01-29 18:03:28,188 | INFO | -219.23 * 0.5 = -109.62 for ctc
2026-01-29 18:03:28,188 | INFO | total log probability: -356.04
2026-01-29 18:03:28,188 | INFO | normalized log probability: -1.67
2026-01-29 18:03:28,188 | INFO | total number of ended hypotheses: 165
2026-01-29 18:03:28,191 | INFO | best hypo: ▁smallworld▁compte▁parmi▁ses▁membres▁des▁gens▁comme▁quentin▁tarantino▁ou▁l'incontournable▁paris▁silton▁ou▁encore▁l'un▁déboulonnable▁massimo▁gargia▁bref▁que▁du▁beau▁monde▁version▁cibère▁n'espérait▁pas▁y▁entrerait▁pour▁pouvoir▁tather▁ou▁échanger▁vos▁phottours▁de▁vacnance▁avec▁ces▁gens▁làens▁carand▁même▁si▁internette▁rapproche▁le▁monde▁smallward▁est▁acces▁cible▁uniquement▁sur▁invitonation▁il▁faut▁ou▁déjà▁menaître▁à▁pippple▁pour▁avoir▁le▁droit▁fréquenter'é▁pipe

2026-01-29 18:03:28,193 | INFO | speech length: 474880
2026-01-29 18:03:28,232 | INFO | decoder input length: 741
2026-01-29 18:03:28,232 | INFO | max output length: 741
2026-01-29 18:03:28,233 | INFO | min output length: 74
2026-01-29 18:03:51,749 | INFO | end detected at 230
2026-01-29 18:03:51,751 | INFO | -670.78 * 0.5 = -335.39 for decoder
2026-01-29 18:03:51,751 | INFO | -175.06 * 0.5 = -87.53 for ctc
2026-01-29 18:03:51,751 | INFO | total log probability: -422.92
2026-01-29 18:03:51,751 | INFO | normalized log probability: -1.88
2026-01-29 18:03:51,751 | INFO | total number of ended hypotheses: 181
2026-01-29 18:03:51,754 | INFO | best hypo: ▁en▁france▁moins▁branché▁mais▁plus▁accessible▁au▁commun▁des▁mortels▁on▁peut▁citer▁une▁zickicom▁zeddyki▁un▁site▁communautaire▁dont▁le▁but▁est▁de▁vous▁aider▁et▁à▁créer▁une▁identité▁numérique▁et▁oui▁à▁éviter▁à▁qu'on▁dise▁'importe▁ou▁quo▁à▁propos▁de▁vous▁sur▁internet▁mieux▁vaut▁de▁prendre▁les▁devants▁et▁diffuser▁vous▁même▁les▁infts▁et▁qui▁vous▁me▁concernentre▁enfin▁le▁tout▁récemment▁vient▁de▁s'ouvrire▁le▁site▁famiboocom▁famieboo▁est▁une▁sorte▁de▁facebook▁cent▁pourcent▁français▁réservé▁aux▁familles

2026-01-29 18:03:51,756 | INFO | speech length: 450400
2026-01-29 18:03:51,801 | INFO | decoder input length: 703
2026-01-29 18:03:51,801 | INFO | max output length: 703
2026-01-29 18:03:51,801 | INFO | min output length: 70
2026-01-29 18:04:14,933 | INFO | end detected at 236
2026-01-29 18:04:14,934 | INFO | -585.99 * 0.5 = -293.00 for decoder
2026-01-29 18:04:14,934 | INFO | -171.32 * 0.5 = -85.66 for ctc
2026-01-29 18:04:14,934 | INFO | total log probability: -378.66
2026-01-29 18:04:14,934 | INFO | normalized log probability: -1.65
2026-01-29 18:04:14,934 | INFO | total number of ended hypotheses: 141
2026-01-29 18:04:14,937 | INFO | best hypo: ▁pour▁partager▁vos▁photos▁vos▁vidéos▁ou▁encore▁agenda▁avec▁vos▁parents▁vos▁cousins▁ou▁vos▁tontons▁chaque▁cercle▁familial▁est▁accessible▁uniquement▁à▁ceux▁qui▁en▁sont▁membres▁par▁exemple▁pouror▁organgiser'aniversaire▁de▁votre▁arrière▁grand▁mère▁et▁suffit▁donner▁le▁code▁d'accès▁à▁tous▁les▁membres▁de▁votre▁familles▁et▁de▁vous▁retrouver▁tous▁ou▁ens▁femble▁sur▁famibooscom▁d'autres▁rés'eaux▁sociaux▁thématique▁devraient▁voir▁le▁jour▁prochainement▁notamment▁un▁site▁pour▁les▁amateurs▁sport▁et▁un▁autre▁pour▁les▁clubles▁branch

2026-01-29 18:04:14,946 | INFO | Chunk: 0 | WER=18.181818 | S=11 D=2 I=3
2026-01-29 18:04:14,949 | INFO | Chunk: 1 | WER=37.179487 | S=20 D=6 I=3
2026-01-29 18:04:14,953 | INFO | Chunk: 2 | WER=29.670330 | S=12 D=8 I=7
2026-01-29 18:04:14,957 | INFO | Chunk: 3 | WER=21.739130 | S=14 D=5 I=1
2026-01-29 18:04:15,006 | INFO | File: Rhap-M2005.wav | WER=25.797101 | S=56 D=18 I=15
2026-01-29 18:04:15,006 | INFO | ------------------------------
2026-01-29 18:04:15,006 | INFO | Conf cv Done!
2026-01-29 18:04:15,137 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-29 18:04:15,155 | INFO | Vocabulary size: 47
2026-01-29 18:04:15,949 | INFO | Gradient checkpoint layers: []
2026-01-29 18:04:16,596 | INFO | BatchBeamSearch implementation is selected.
2026-01-29 18:04:16,600 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-29 18:04:16,600 | INFO | Decoding device=cuda, dtype=float32
2026-01-29 18:04:16,600 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-29 18:04:16,604 | INFO | speech length: 446400
2026-01-29 18:04:16,640 | INFO | decoder input length: 697
2026-01-29 18:04:16,641 | INFO | max output length: 697
2026-01-29 18:04:16,641 | INFO | min output length: 69
2026-01-29 18:04:56,669 | INFO | end detected at 497
2026-01-29 18:04:56,670 | INFO | -293.14 * 0.5 = -146.57 for decoder
2026-01-29 18:04:56,670 | INFO | -61.02 * 0.5 = -30.51 for ctc
2026-01-29 18:04:56,670 | INFO | total log probability: -177.08
2026-01-29 18:04:56,670 | INFO | normalized log probability: -0.36
2026-01-29 18:04:56,670 | INFO | total number of ended hypotheses: 158
2026-01-29 18:04:56,676 | INFO | best hypo: le<space>succès<space>des<space>réseaux<space>sociaux<space>sur<space>internet<space>n'est<space>plus<space>à<space>démontrer<space>il<space>n'y<space>a<space>qu'à<space>voir<space>les<space>soixante<space>cinq<space>millions<space>d'utilisateurs<space>de<space>feilles<space>book<space>mais<space>connaissez<space>vous<space>les<space>sites<space>euh<space>sociaux<space>spécialisés<space>dans<space>l'ombre<space>de<space>feize<space>book<space>il<space>existe<space>en<space>effet<space>des<space>sites<space>communautaires<space>qui<space>permettent<space>de<space>communiquer<space>avec<space>des<space>amis<space>ou<space>de<space>faire<space>des<space>rencontres<space>dans<space>des<space>cercles<space>très<space>fermés<space>on<space>peut<space>citer<space>par<space>exemple<space>l<space>le<space>site<space>smort<space>worde<space>ce<space>serveur<space>américain<space>est<space>une<space>sorte<space>de<space>club<space>vie<space>pie<space>réservé<space>à<space>ladjet<space>cet

2026-01-29 18:04:56,678 | INFO | speech length: 390080
2026-01-29 18:04:56,708 | INFO | decoder input length: 609
2026-01-29 18:04:56,708 | INFO | max output length: 609
2026-01-29 18:04:56,708 | INFO | min output length: 60
2026-01-29 18:05:29,555 | INFO | end detected at 457
2026-01-29 18:05:29,555 | INFO | -208.99 * 0.5 = -104.49 for decoder
2026-01-29 18:05:29,555 | INFO | -32.69 * 0.5 = -16.35 for ctc
2026-01-29 18:05:29,555 | INFO | total log probability: -120.84
2026-01-29 18:05:29,555 | INFO | normalized log probability: -0.27
2026-01-29 18:05:29,555 | INFO | total number of ended hypotheses: 146
2026-01-29 18:05:29,560 | INFO | best hypo: smol<space>ward<space>compte<space>parmi<space>ses<space>membres<space>des<space>gens<space>comme<space>quantin<space>ta<space>antino<space>ou<space>l'incontournable<space>paris<space>silton<space>ou<space>encore<space>l'un<space>des<space>boulonnables<space>massimo<space>gardia<space>bref<space>que<space>du<space>beau<space>monde<space>version<space>cybere<space>n'espérait<space>pas<space>y<space>entrer<space>pour<space>pouvoir<space>châter<space>ou<space>échanger<space>vos<space>photos<space>de<space>vacances<space>avec<space>ces<space>gens<space>là<space>car<space>même<space>si<space>internet<space>rapproche<space>le<space>monde<space>ce<space>mollward<space>est<space>accessible<space>uniquement<space>sur<space>invitation<space>il<space>faut<space>déjà<space>connaître<space>un<space>peop<space>le<space>pour<space>avoir<space>le<space>droit<space>de<space>fréquenter<space>le<space>pip

2026-01-29 18:05:29,562 | INFO | speech length: 474880
2026-01-29 18:05:29,593 | INFO | decoder input length: 741
2026-01-29 18:05:29,593 | INFO | max output length: 741
2026-01-29 18:05:29,593 | INFO | min output length: 74
2026-01-29 18:06:12,712 | INFO | end detected at 506
2026-01-29 18:06:12,713 | INFO | -426.35 * 0.5 = -213.17 for decoder
2026-01-29 18:06:12,714 | INFO | -33.79 * 0.5 = -16.89 for ctc
2026-01-29 18:06:12,714 | INFO | total log probability: -230.07
2026-01-29 18:06:12,714 | INFO | normalized log probability: -0.46
2026-01-29 18:06:12,714 | INFO | total number of ended hypotheses: 160
2026-01-29 18:06:12,720 | INFO | best hypo: en<space>france<space>moins<space>branché<space>mais<space>plus<space>accessible<space>comme<space>un<space>des<space>mortels<space>on<space>peut<space>citer<space>zicky<space>point<space>com<space>zedick<space>i<space>un<space>site<space>communautaire<space>dont<space>le<space>but<space>est<space>de<space>vous<space>aider<space>à<space>créer<space>une<space>identité<space>numérique<space>et<space>oui<space>pour<space>éviter<space>qu'on<space>dise<space>n'importe<space>quoi<space>à<space>propos<space>de<space>vous<space>sur<space>internet<space>mieux<space>vous<space>prendre<space>les<space>devants<space>et<space>diffuser<space>vous<space>même<space>les<space>infos<space>qui<space>vous<space>concernent<space>enfint<space>tout<space>récemment<space>vient<space>de<space>s'ouvrir<space>le<space>site<space>fami<space>boock<space>point<space>com<space>famil<space>bouqu<space>est<space>une<space>sorte<space>de<space>face<space>book<space>cent<space>pour<space>cent<space>français<space>réservés<space>aux<space>familles

2026-01-29 18:06:12,722 | INFO | speech length: 450400
2026-01-29 18:06:12,765 | INFO | decoder input length: 703
2026-01-29 18:06:12,766 | INFO | max output length: 703
2026-01-29 18:06:12,766 | INFO | min output length: 70
2026-01-29 18:06:56,124 | INFO | end detected at 545
2026-01-29 18:06:56,125 | INFO | -579.70 * 0.5 = -289.85 for decoder
2026-01-29 18:06:56,125 | INFO | -18.36 * 0.5 =  -9.18 for ctc
2026-01-29 18:06:56,125 | INFO | total log probability: -299.03
2026-01-29 18:06:56,125 | INFO | normalized log probability: -0.55
2026-01-29 18:06:56,125 | INFO | total number of ended hypotheses: 182
2026-01-29 18:06:56,131 | INFO | best hypo: pour<space>partager<space>vos<space>photos<space>vos<space>vidéos<space>ou<space>encore<space>un<space>un<space>agenda<space>avec<space>vos<space>parents<space>vos<space>cousins<space>ou<space>vos<space>tontons<space>chaque<space>cercle<space>familial<space>est<space>accessible<space>uniquement<space>à<space>ceux<space>qui<space>en<space>sont<space>membres<space>par<space>exemple<space>pour<space>organiser<space>l'anniversaire<space>de<space>votre<space>arrière<space>grand<space>mère<space>il<space>suffit<space>de<space>donner<space>le<space>code<space>d'accès<space>à<space>tous<space>les<space>membres<space>de<space>votre<space>famille<space>et<space>de<space>vous<space>retrouver<space>tous<space>ensemble<space>sur<space>famie<space>book<space>point<space>com<space>d'autres<space>réseaux<space>sociaux<space>thématiques<space>devrient<space>voir<space>le<space>jour<space>prochainement<space>notamment<space>un<space>site<space>pour<space>les<space>amateurs<space>de<space>sport<space>et<space>un<space>autre<space>pour<space>les<space>clubeurs<space>branches

2026-01-29 18:06:56,139 | INFO | Chunk: 0 | WER=14.772727 | S=7 D=2 I=4
2026-01-29 18:06:56,143 | INFO | Chunk: 1 | WER=28.205128 | S=15 D=3 I=4
2026-01-29 18:06:56,147 | INFO | Chunk: 2 | WER=17.582418 | S=11 D=2 I=3
2026-01-29 18:06:56,151 | INFO | Chunk: 3 | WER=4.347826 | S=3 D=0 I=1
2026-01-29 18:06:56,201 | INFO | File: Rhap-M2005.wav | WER=16.231884 | S=35 D=6 I=15
2026-01-29 18:06:56,201 | INFO | ------------------------------
2026-01-29 18:06:56,202 | INFO | Conf ester Done!
2026-01-29 18:07:35,297 | INFO | Chunk: 0 | WER=9.090909 | S=3 D=2 I=3
2026-01-29 18:07:35,302 | INFO | Chunk: 1 | WER=17.948718 | S=10 D=2 I=2
2026-01-29 18:07:35,306 | INFO | Chunk: 2 | WER=16.483516 | S=10 D=2 I=3
2026-01-29 18:07:35,310 | INFO | Chunk: 3 | WER=13.043478 | S=9 D=1 I=2
2026-01-29 18:07:35,362 | INFO | File: Rhap-M2005.wav | WER=13.913043 | S=31 D=5 I=12
2026-01-29 18:07:35,362 | INFO | ------------------------------
2026-01-29 18:07:35,362 | INFO | hmm_tdnn Done!
2026-01-29 18:07:35,524 | INFO | ==================================Rhap-M2006.wav=========================================
2026-01-29 18:07:35,525 | WARNING | The file /vol/corpora/Rhapsodie/wav16k_corrected/Rhap-M2006.wav does not exist
