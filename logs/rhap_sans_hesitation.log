2026-01-28 12:36:07,730 | INFO | Fetch hyperparams.yaml: Using file found at '/vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-wav2vec2-commonvoice-fr/hyperparams.yaml'
2026-01-28 12:36:07,730 | DEBUG | Fetch: Source and destination '/vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-wav2vec2-commonvoice-fr/custom.py' are identical, returning assuming this is intended
2026-01-28 12:36:07,763 | WARNING | This version of torchaudio is old. SpeechBrain no longer tries using the torchaudio global backend mechanism in recipes, so if you encounter issues, update torchaudio to >=2.1.0.
2026-01-28 12:36:08,334 | WARNING | speechbrain.lobes.models.huggingface_transformers.huggingface - Wav2Vec2Model is frozen.
2026-01-28 12:36:08,346 | DEBUG | Collecting files (or symlinks) for pretraining in /vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-wav2vec2-commonvoice-fr.
2026-01-28 12:36:08,346 | INFO | Fetch wav2vec2.ckpt: Using file found at '/vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-wav2vec2-commonvoice-fr/wav2vec2.ckpt'
2026-01-28 12:36:08,346 | DEBUG | Set local path in self.paths["wav2vec2"] = /vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-wav2vec2-commonvoice-fr/wav2vec2.ckpt
2026-01-28 12:36:08,346 | INFO | Fetch asr.ckpt: Using file found at '/vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-wav2vec2-commonvoice-fr/asr.ckpt'
2026-01-28 12:36:08,346 | DEBUG | Set local path in self.paths["asr"] = /vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-wav2vec2-commonvoice-fr/asr.ckpt
2026-01-28 12:36:08,346 | INFO | Fetch tokenizer.ckpt: Using file found at '/vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-wav2vec2-commonvoice-fr/tokenizer.ckpt'
2026-01-28 12:36:08,346 | DEBUG | Set local path in self.paths["tokenizer"] = /vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-wav2vec2-commonvoice-fr/tokenizer.ckpt
2026-01-28 12:36:08,346 | INFO | Loading pretrained files for: wav2vec2, asr, tokenizer
2026-01-28 12:36:08,346 | DEBUG | Redirecting (loading from local path): wav2vec2 -> /vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-wav2vec2-commonvoice-fr/wav2vec2.ckpt
2026-01-28 12:36:08,346 | DEBUG | Redirecting (loading from local path): asr -> /vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-wav2vec2-commonvoice-fr/asr.ckpt
2026-01-28 12:36:08,346 | DEBUG | Redirecting (loading from local path): tokenizer -> /vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-wav2vec2-commonvoice-fr/tokenizer.ckpt
2026-01-28 12:36:10,813 | INFO | Fetch hyperparams.yaml: Using file found at '/vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-whisper-medium-commonvoice-fr/hyperparams.yaml'
2026-01-28 12:36:10,813 | DEBUG | Fetch: Source and destination '/vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-whisper-medium-commonvoice-fr/custom.py' are identical, returning assuming this is intended
2026-01-28 12:36:11,692 | WARNING | speechbrain.lobes.models.huggingface_transformers.huggingface - WhisperModel is frozen.
2026-01-28 12:36:11,693 | WARNING | speechbrain.lobes.models.huggingface_transformers.whisper - whisper encoder-decoder is frozen.
2026-01-28 12:36:11,814 | DEBUG | Collecting files (or symlinks) for pretraining in /vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-whisper-medium-commonvoice-fr.
2026-01-28 12:36:11,814 | INFO | Fetch whisper.ckpt: Using file found at '/vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-whisper-medium-commonvoice-fr/whisper.ckpt'
2026-01-28 12:36:11,814 | DEBUG | Set local path in self.paths["whisper"] = /vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-whisper-medium-commonvoice-fr/whisper.ckpt
2026-01-28 12:36:11,814 | INFO | Loading pretrained files for: whisper
2026-01-28 12:36:11,814 | DEBUG | Redirecting (loading from local path): whisper -> /vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-whisper-medium-commonvoice-fr/whisper.ckpt
2026-01-28 12:36:14,896 | INFO | Fetch hyperparams.yaml: Using file found at '/vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-whisper-large-v2-commonvoice-fr/hyperparams.yaml'
2026-01-28 12:36:14,896 | DEBUG | Fetch: Source and destination '/vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-whisper-large-v2-commonvoice-fr/custom.py' are identical, returning assuming this is intended
2026-01-28 12:36:16,456 | WARNING | speechbrain.lobes.models.huggingface_transformers.huggingface - WhisperModel is frozen.
2026-01-28 12:36:16,457 | WARNING | speechbrain.lobes.models.huggingface_transformers.whisper - whisper encoder-decoder is frozen.
2026-01-28 12:36:16,843 | DEBUG | Collecting files (or symlinks) for pretraining in /vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-whisper-large-v2-commonvoice-fr.
2026-01-28 12:36:16,843 | INFO | Fetch whisper.ckpt: Using file found at '/vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-whisper-large-v2-commonvoice-fr/whisper.ckpt'
2026-01-28 12:36:16,843 | DEBUG | Set local path in self.paths["whisper"] = /vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-whisper-large-v2-commonvoice-fr/whisper.ckpt
2026-01-28 12:36:16,843 | INFO | Loading pretrained files for: whisper
2026-01-28 12:36:16,843 | DEBUG | Redirecting (loading from local path): whisper -> /vol/experiments3/imbenamor/TAPAS-FRAIS/models/asr-whisper-large-v2-commonvoice-fr/whisper.ckpt
2026-01-28 12:36:21,414 | INFO | ==================================Rhap-D0001.wav=========================================
2026-01-28 12:36:21,414 | WARNING | The file /vol/corpora/Rhapsodie/wav16k_corrected/Rhap-D0001.wav does not exist
2026-01-28 12:36:21,414 | INFO | ==================================Rhap-D0002.wav=========================================
2026-01-28 12:36:21,604 | INFO | Using rVAD model
2026-01-28 12:36:35,792 | INFO | Created a temporary directory at /tmp/tmpl43ti0h3
2026-01-28 12:36:35,793 | INFO | Writing /tmp/tmpl43ti0h3/_remote_module_non_scriptable.py
2026-01-28 12:36:35,872 | DEBUG | Registered checkpoint save hook for save
2026-01-28 12:36:35,872 | DEBUG | Registered checkpoint load hook for load_if_possible
2026-01-28 12:36:36,941 | INFO | Chunk: 0 | WER=20.000000 | S=0 D=3 I=0
2026-01-28 12:36:36,942 | INFO | Chunk: 1 | WER=34.375000 | S=6 D=5 I=0
2026-01-28 12:36:36,942 | INFO | Chunk: 2 | WER=100.000000 | S=2 D=3 I=0
2026-01-28 12:36:36,943 | INFO | Chunk: 3 | WER=16.666667 | S=2 D=1 I=1
2026-01-28 12:36:36,943 | INFO | Chunk: 4 | WER=50.000000 | S=2 D=0 I=0
2026-01-28 12:36:36,945 | INFO | Chunk: 5 | WER=22.033898 | S=8 D=5 I=0
2026-01-28 12:36:36,945 | INFO | Chunk: 6 | WER=66.666667 | S=2 D=0 I=0
2026-01-28 12:36:36,945 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 12:36:36,946 | INFO | Chunk: 8 | WER=87.500000 | S=4 D=3 I=0
2026-01-28 12:36:36,946 | INFO | Chunk: 9 | WER=37.500000 | S=1 D=2 I=0
2026-01-28 12:36:36,946 | INFO | Chunk: 10 | WER=100.000000 | S=2 D=1 I=0
2026-01-28 12:36:36,946 | INFO | Chunk: 11 | WER=66.666667 | S=2 D=0 I=0
2026-01-28 12:36:36,947 | INFO | Chunk: 12 | WER=100.000000 | S=1 D=0 I=0
2026-01-28 12:36:36,948 | INFO | Chunk: 13 | WER=24.561404 | S=8 D=6 I=0
2026-01-28 12:36:36,949 | INFO | Chunk: 14 | WER=22.222222 | S=1 D=1 I=0
2026-01-28 12:36:36,949 | INFO | Chunk: 15 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 12:36:36,952 | INFO | Chunk: 16 | WER=18.987342 | S=11 D=4 I=0
2026-01-28 12:36:36,953 | INFO | Chunk: 17 | WER=100.000000 | S=5 D=0 I=0
2026-01-28 12:36:36,953 | INFO | Chunk: 18 | WER=53.125000 | S=7 D=10 I=0
2026-01-28 12:36:36,957 | INFO | Chunk: 19 | WER=24.137931 | S=9 D=12 I=0
2026-01-28 12:36:36,957 | INFO | Chunk: 20 | WER=35.714286 | S=7 D=3 I=0
2026-01-28 12:36:36,958 | INFO | Chunk: 21 | WER=14.285714 | S=5 D=0 I=0
2026-01-28 12:36:36,959 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 12:36:36,959 | INFO | Chunk: 23 | WER=6.896552 | S=1 D=1 I=0
2026-01-28 12:36:36,960 | INFO | Chunk: 24 | WER=27.272727 | S=1 D=1 I=1
2026-01-28 12:36:36,960 | INFO | Chunk: 25 | WER=5.882353 | S=1 D=0 I=0
2026-01-28 12:36:36,960 | INFO | Chunk: 26 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 12:36:36,961 | INFO | Chunk: 27 | WER=12.500000 | S=0 D=1 I=0
2026-01-28 12:36:36,961 | INFO | Chunk: 28 | WER=26.666667 | S=3 D=1 I=0
2026-01-28 12:36:36,962 | INFO | Chunk: 29 | WER=33.333333 | S=3 D=6 I=0
2026-01-28 12:36:36,962 | INFO | Chunk: 30 | WER=35.000000 | S=1 D=6 I=0
2026-01-28 12:36:36,962 | INFO | Chunk: 31 | WER=33.333333 | S=1 D=0 I=0
2026-01-28 12:36:36,963 | INFO | Chunk: 32 | WER=33.333333 | S=1 D=1 I=0
2026-01-28 12:36:36,963 | INFO | Chunk: 33 | WER=42.857143 | S=1 D=2 I=0
2026-01-28 12:36:36,963 | INFO | Chunk: 34 | WER=50.000000 | S=1 D=0 I=0
2026-01-28 12:36:36,963 | INFO | Chunk: 35 | WER=100.000000 | S=1 D=2 I=0
2026-01-28 12:36:36,963 | INFO | Chunk: 36 | WER=100.000000 | S=3 D=0 I=0
2026-01-28 12:36:36,964 | INFO | Chunk: 37 | WER=47.826087 | S=5 D=6 I=0
2026-01-28 12:36:36,964 | INFO | Chunk: 38 | WER=33.333333 | S=1 D=2 I=0
2026-01-28 12:36:36,967 | INFO | Chunk: 39 | WER=30.303030 | S=11 D=8 I=1
2026-01-28 12:36:36,967 | INFO | Chunk: 40 | WER=25.000000 | S=0 D=1 I=0
2026-01-28 12:36:36,968 | INFO | Chunk: 41 | WER=19.354839 | S=2 D=3 I=1
2026-01-28 12:36:36,968 | INFO | Chunk: 42 | WER=15.384615 | S=1 D=1 I=0
2026-01-28 12:36:36,969 | INFO | Chunk: 43 | WER=6.451613 | S=1 D=1 I=0
2026-01-28 12:36:37,293 | INFO | File: Rhap-D0002.wav | WER=26.713124 | S=124 D=102 I=4
2026-01-28 12:36:37,293 | INFO | ------------------------------
2026-01-28 12:36:37,293 | INFO | w2vec vad chunk Done!
2026-01-28 12:37:08,383 | INFO | Chunk: 0 | WER=33.333333 | S=3 D=2 I=0
2026-01-28 12:37:08,384 | INFO | Chunk: 1 | WER=28.125000 | S=6 D=3 I=0
2026-01-28 12:37:08,384 | INFO | Chunk: 2 | WER=100.000000 | S=4 D=1 I=0
2026-01-28 12:37:08,384 | INFO | Chunk: 3 | WER=29.166667 | S=3 D=4 I=0
2026-01-28 12:37:08,385 | INFO | Chunk: 4 | WER=25.000000 | S=1 D=0 I=0
2026-01-28 12:37:08,386 | INFO | Chunk: 5 | WER=47.457627 | S=4 D=24 I=0
2026-01-28 12:37:08,386 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 12:37:08,386 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 12:37:08,387 | INFO | Chunk: 8 | WER=37.500000 | S=1 D=2 I=0
2026-01-28 12:37:08,387 | INFO | Chunk: 9 | WER=12.500000 | S=1 D=0 I=0
2026-01-28 12:37:08,387 | INFO | Chunk: 10 | WER=133.333333 | S=2 D=0 I=2
2026-01-28 12:37:08,387 | INFO | Chunk: 11 | WER=66.666667 | S=1 D=1 I=0
2026-01-28 12:37:08,387 | INFO | Chunk: 12 | WER=100.000000 | S=1 D=0 I=0
2026-01-28 12:37:08,388 | INFO | Chunk: 13 | WER=77.192982 | S=5 D=39 I=0
2026-01-28 12:37:08,388 | INFO | Chunk: 14 | WER=33.333333 | S=1 D=2 I=0
2026-01-28 12:37:08,389 | INFO | Chunk: 15 | WER=20.000000 | S=1 D=2 I=0
2026-01-28 12:37:08,390 | INFO | Chunk: 16 | WER=69.620253 | S=4 D=51 I=0
2026-01-28 12:37:08,390 | INFO | Chunk: 17 | WER=120.000000 | S=4 D=0 I=2
2026-01-28 12:37:08,391 | INFO | Chunk: 18 | WER=37.500000 | S=2 D=8 I=2
2026-01-28 12:37:08,394 | INFO | Chunk: 19 | WER=44.827586 | S=7 D=32 I=0
2026-01-28 12:37:08,394 | INFO | Chunk: 20 | WER=10.714286 | S=1 D=2 I=0
2026-01-28 12:37:08,395 | INFO | Chunk: 21 | WER=2.857143 | S=0 D=1 I=0
2026-01-28 12:37:08,395 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 12:37:08,396 | INFO | Chunk: 23 | WER=3.448276 | S=0 D=1 I=0
2026-01-28 12:37:08,396 | INFO | Chunk: 24 | WER=27.272727 | S=1 D=2 I=0
2026-01-28 12:37:08,397 | INFO | Chunk: 25 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 12:37:08,397 | INFO | Chunk: 26 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 12:37:08,397 | INFO | Chunk: 27 | WER=25.000000 | S=1 D=1 I=0
2026-01-28 12:37:08,397 | INFO | Chunk: 28 | WER=20.000000 | S=0 D=2 I=1
2026-01-28 12:37:08,398 | INFO | Chunk: 29 | WER=55.555556 | S=0 D=15 I=0
2026-01-28 12:37:08,398 | INFO | Chunk: 30 | WER=15.000000 | S=2 D=1 I=0
2026-01-28 12:37:08,398 | INFO | Chunk: 31 | WER=33.333333 | S=0 D=1 I=0
2026-01-28 12:37:08,398 | INFO | Chunk: 32 | WER=33.333333 | S=2 D=0 I=0
2026-01-28 12:37:08,399 | INFO | Chunk: 33 | WER=42.857143 | S=2 D=1 I=0
2026-01-28 12:37:08,399 | INFO | Chunk: 34 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 12:37:08,399 | INFO | Chunk: 35 | WER=133.333333 | S=3 D=0 I=1
2026-01-28 12:37:08,399 | INFO | Chunk: 36 | WER=100.000000 | S=2 D=1 I=0
2026-01-28 12:37:08,400 | INFO | Chunk: 37 | WER=21.739130 | S=4 D=1 I=0
2026-01-28 12:37:08,400 | INFO | Chunk: 38 | WER=11.111111 | S=1 D=0 I=0
2026-01-28 12:37:08,401 | INFO | Chunk: 39 | WER=87.878788 | S=3 D=54 I=1
2026-01-28 12:37:08,401 | INFO | Chunk: 40 | WER=50.000000 | S=1 D=1 I=0
2026-01-28 12:37:08,402 | INFO | Chunk: 41 | WER=22.580645 | S=1 D=6 I=0
2026-01-28 12:37:08,402 | INFO | Chunk: 42 | WER=7.692308 | S=0 D=0 I=1
2026-01-28 12:37:08,403 | INFO | Chunk: 43 | WER=29.032258 | S=5 D=2 I=2
2026-01-28 12:37:08,627 | INFO | File: Rhap-D0002.wav | WER=40.766551 | S=86 D=258 I=7
2026-01-28 12:37:08,628 | INFO | ------------------------------
2026-01-28 12:37:08,628 | INFO | whisper med Done!
2026-01-28 12:37:51,099 | INFO | Chunk: 0 | WER=26.666667 | S=2 D=2 I=0
2026-01-28 12:37:51,100 | INFO | Chunk: 1 | WER=46.875000 | S=5 D=10 I=0
2026-01-28 12:37:51,100 | INFO | Chunk: 2 | WER=40.000000 | S=1 D=1 I=0
2026-01-28 12:37:51,101 | INFO | Chunk: 3 | WER=8.333333 | S=0 D=2 I=0
2026-01-28 12:37:51,101 | INFO | Chunk: 4 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 12:37:51,102 | INFO | Chunk: 5 | WER=35.593220 | S=5 D=16 I=0
2026-01-28 12:37:51,103 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 12:37:51,103 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 12:37:51,103 | INFO | Chunk: 8 | WER=12.500000 | S=0 D=1 I=0
2026-01-28 12:37:51,103 | INFO | Chunk: 9 | WER=12.500000 | S=1 D=0 I=0
2026-01-28 12:37:51,103 | INFO | Chunk: 10 | WER=66.666667 | S=2 D=0 I=0
2026-01-28 12:37:51,104 | INFO | Chunk: 11 | WER=66.666667 | S=1 D=1 I=0
2026-01-28 12:37:51,104 | INFO | Chunk: 12 | WER=100.000000 | S=1 D=0 I=0
2026-01-28 12:37:51,105 | INFO | Chunk: 13 | WER=40.350877 | S=7 D=16 I=0
2026-01-28 12:37:51,106 | INFO | Chunk: 14 | WER=33.333333 | S=1 D=2 I=0
2026-01-28 12:37:51,106 | INFO | Chunk: 15 | WER=6.666667 | S=1 D=0 I=0
2026-01-28 12:37:51,108 | INFO | Chunk: 16 | WER=55.696203 | S=10 D=34 I=0
2026-01-28 12:37:51,108 | INFO | Chunk: 17 | WER=40.000000 | S=1 D=1 I=0
2026-01-28 12:37:51,109 | INFO | Chunk: 18 | WER=34.375000 | S=2 D=8 I=1
2026-01-28 12:37:51,110 | INFO | Chunk: 19 | WER=90.804598 | S=2 D=77 I=0
2026-01-28 12:37:51,111 | INFO | Chunk: 20 | WER=21.428571 | S=3 D=2 I=1
2026-01-28 12:37:51,111 | INFO | Chunk: 21 | WER=25.714286 | S=0 D=9 I=0
2026-01-28 12:37:51,112 | INFO | Chunk: 22 | WER=10.000000 | S=1 D=0 I=0
2026-01-28 12:37:51,112 | INFO | Chunk: 23 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 12:37:51,113 | INFO | Chunk: 24 | WER=18.181818 | S=1 D=1 I=0
2026-01-28 12:37:51,113 | INFO | Chunk: 25 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 12:37:51,113 | INFO | Chunk: 26 | WER=50.000000 | S=1 D=0 I=0
2026-01-28 12:37:51,113 | INFO | Chunk: 27 | WER=25.000000 | S=1 D=1 I=0
2026-01-28 12:37:51,114 | INFO | Chunk: 28 | WER=13.333333 | S=0 D=1 I=1
2026-01-28 12:37:51,114 | INFO | Chunk: 29 | WER=22.222222 | S=3 D=3 I=0
2026-01-28 12:37:51,115 | INFO | Chunk: 30 | WER=30.000000 | S=0 D=5 I=1
2026-01-28 12:37:51,115 | INFO | Chunk: 31 | WER=33.333333 | S=1 D=0 I=0
2026-01-28 12:37:51,115 | INFO | Chunk: 32 | WER=16.666667 | S=1 D=0 I=0
2026-01-28 12:37:51,115 | INFO | Chunk: 33 | WER=28.571429 | S=0 D=2 I=0
2026-01-28 12:37:51,115 | INFO | Chunk: 34 | WER=150.000000 | S=2 D=0 I=1
2026-01-28 12:37:51,115 | INFO | Chunk: 35 | WER=100.000000 | S=2 D=1 I=0
2026-01-28 12:37:51,116 | INFO | Chunk: 36 | WER=66.666667 | S=0 D=2 I=0
2026-01-28 12:37:51,116 | INFO | Chunk: 37 | WER=30.434783 | S=4 D=3 I=0
2026-01-28 12:37:51,116 | INFO | Chunk: 38 | WER=33.333333 | S=1 D=2 I=0
2026-01-28 12:37:51,117 | INFO | Chunk: 39 | WER=86.363636 | S=5 D=51 I=1
2026-01-28 12:37:51,117 | INFO | Chunk: 40 | WER=50.000000 | S=1 D=1 I=0
2026-01-28 12:37:51,118 | INFO | Chunk: 41 | WER=6.451613 | S=0 D=2 I=0
2026-01-28 12:37:51,118 | INFO | Chunk: 42 | WER=15.384615 | S=1 D=1 I=0
2026-01-28 12:37:51,119 | INFO | Chunk: 43 | WER=22.580645 | S=4 D=3 I=0
2026-01-28 12:37:51,340 | INFO | File: Rhap-D0002.wav | WER=39.488966 | S=74 D=260 I=6
2026-01-28 12:37:51,340 | INFO | ------------------------------
2026-01-28 12:37:51,340 | INFO | whisper large Done!
2026-01-28 12:37:51,566 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 12:37:51,596 | INFO | Vocabulary size: 350
2026-01-28 12:37:52,103 | INFO | Gradient checkpoint layers: []
2026-01-28 12:37:52,700 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 12:37:52,703 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 12:37:52,703 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 12:37:52,704 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 12:37:52,705 | INFO | speech length: 53600
2026-01-28 12:37:52,739 | INFO | decoder input length: 83
2026-01-28 12:37:52,739 | INFO | max output length: 83
2026-01-28 12:37:52,739 | INFO | min output length: 8
2026-01-28 12:37:53,934 | INFO | end detected at 34
2026-01-28 12:37:53,935 | INFO |  -5.84 * 0.5 =  -2.92 for decoder
2026-01-28 12:37:53,936 | INFO |  -4.22 * 0.5 =  -2.11 for ctc
2026-01-28 12:37:53,936 | INFO | total log probability: -5.03
2026-01-28 12:37:53,936 | INFO | normalized log probability: -0.17
2026-01-28 12:37:53,936 | INFO | total number of ended hypotheses: 167
2026-01-28 12:37:53,936 | INFO | best hypo: ▁j'avais▁la▁chance▁d'avoir▁des▁enfants▁qui▁travaillaient▁je▁du▁reste

2026-01-28 12:37:53,939 | INFO | speech length: 142880
2026-01-28 12:37:53,983 | INFO | decoder input length: 222
2026-01-28 12:37:53,983 | INFO | max output length: 222
2026-01-28 12:37:53,983 | INFO | min output length: 22
2026-01-28 12:37:58,660 | INFO | end detected at 78
2026-01-28 12:37:58,661 | INFO | -22.83 * 0.5 = -11.41 for decoder
2026-01-28 12:37:58,661 | INFO | -12.85 * 0.5 =  -6.42 for ctc
2026-01-28 12:37:58,661 | INFO | total log probability: -17.84
2026-01-28 12:37:58,661 | INFO | normalized log probability: -0.25
2026-01-28 12:37:58,661 | INFO | total number of ended hypotheses: 183
2026-01-28 12:37:58,662 | INFO | best hypo: ▁il▁préférait▁rigoler▁que▁travailler▁ils▁étaient▁tout▁à▁fait▁normaux▁mais▁je▁le▁diris▁sans▁travailler▁normalement▁et▁c'est▁vrai▁que▁bon▁habitants▁dans▁l'ensemble▁de▁paris

2026-01-28 12:37:58,664 | INFO | speech length: 9440
2026-01-28 12:37:58,689 | INFO | decoder input length: 14
2026-01-28 12:37:58,689 | INFO | max output length: 14
2026-01-28 12:37:58,689 | INFO | min output length: 1
2026-01-28 12:37:59,054 | INFO | adding <eos> in the last position in the loop
2026-01-28 12:37:59,062 | INFO | no hypothesis. Finish decoding.
2026-01-28 12:37:59,063 | INFO |  -5.79 * 0.5 =  -2.90 for decoder
2026-01-28 12:37:59,063 | INFO | -10.48 * 0.5 =  -5.24 for ctc
2026-01-28 12:37:59,063 | INFO | total log probability: -8.14
2026-01-28 12:37:59,063 | INFO | normalized log probability: -1.02
2026-01-28 12:37:59,063 | INFO | total number of ended hypotheses: 173
2026-01-28 12:37:59,063 | INFO | best hypo: ▁ces▁cols▁sont

2026-01-28 12:37:59,065 | INFO | speech length: 94720
2026-01-28 12:37:59,094 | INFO | decoder input length: 147
2026-01-28 12:37:59,094 | INFO | max output length: 147
2026-01-28 12:37:59,094 | INFO | min output length: 14
2026-01-28 12:38:01,736 | INFO | end detected at 68
2026-01-28 12:38:01,738 | INFO | -14.01 * 0.5 =  -7.00 for decoder
2026-01-28 12:38:01,738 | INFO | -11.83 * 0.5 =  -5.92 for ctc
2026-01-28 12:38:01,738 | INFO | total log probability: -12.92
2026-01-28 12:38:01,738 | INFO | normalized log probability: -0.21
2026-01-28 12:38:01,738 | INFO | total number of ended hypotheses: 175
2026-01-28 12:38:01,739 | INFO | best hypo: ▁aucun▁problème▁n'a▁j'ai▁eu▁aucun▁problème▁scolaire▁pour▁mes▁enfants▁et▁j'y▁j'ai▁jamais▁très▁bien▁compris▁les▁gens▁qui

2026-01-28 12:38:01,740 | INFO | speech length: 16480
2026-01-28 12:38:01,771 | INFO | decoder input length: 25
2026-01-28 12:38:01,771 | INFO | max output length: 25
2026-01-28 12:38:01,772 | INFO | min output length: 2
2026-01-28 12:38:02,280 | INFO | end detected at 17
2026-01-28 12:38:02,282 | INFO |  -3.01 * 0.5 =  -1.50 for decoder
2026-01-28 12:38:02,282 | INFO |  -2.98 * 0.5 =  -1.49 for ctc
2026-01-28 12:38:02,282 | INFO | total log probability: -2.99
2026-01-28 12:38:02,282 | INFO | normalized log probability: -0.27
2026-01-28 12:38:02,282 | INFO | total number of ended hypotheses: 179
2026-01-28 12:38:02,282 | INFO | best hypo: ▁il▁connaissait▁des▁gens

2026-01-28 12:38:02,284 | INFO | speech length: 283520
2026-01-28 12:38:02,314 | INFO | decoder input length: 442
2026-01-28 12:38:02,314 | INFO | max output length: 442
2026-01-28 12:38:02,314 | INFO | min output length: 44
2026-01-28 12:38:11,711 | INFO | end detected at 140
2026-01-28 12:38:11,712 | INFO | -205.71 * 0.5 = -102.85 for decoder
2026-01-28 12:38:11,712 | INFO | -90.83 * 0.5 = -45.42 for ctc
2026-01-28 12:38:11,712 | INFO | total log probability: -148.27
2026-01-28 12:38:11,712 | INFO | normalized log probability: -1.10
2026-01-28 12:38:11,712 | INFO | total number of ended hypotheses: 160
2026-01-28 12:38:11,714 | INFO | best hypo: ▁septième▁qui▁mettaient▁leurs▁enfants▁d'abord▁dans▁le▁public▁puis▁dans▁le▁privé▁pour▁qu'ils▁puissent▁passer▁après▁éventuellement▁avec▁soient▁plus▁forts▁et▁c'ils▁étaient▁très▁étonnés▁parces▁qu'ils▁cantibles▁entrer▁en▁préparant▁les▁prônaient▁après▁les▁gens▁qu'ils▁du▁public▁apparaîts▁noral▁va▁se▁dire

2026-01-28 12:38:11,715 | INFO | speech length: 31040
2026-01-28 12:38:11,759 | INFO | decoder input length: 48
2026-01-28 12:38:11,759 | INFO | max output length: 48
2026-01-28 12:38:11,760 | INFO | min output length: 4
2026-01-28 12:38:12,231 | INFO | end detected at 14
2026-01-28 12:38:12,232 | INFO |  -1.18 * 0.5 =  -0.59 for decoder
2026-01-28 12:38:12,232 | INFO |  -2.23 * 0.5 =  -1.12 for ctc
2026-01-28 12:38:12,232 | INFO | total log probability: -1.71
2026-01-28 12:38:12,232 | INFO | normalized log probability: -0.19
2026-01-28 12:38:12,232 | INFO | total number of ended hypotheses: 177
2026-01-28 12:38:12,233 | INFO | best hypo: ▁les▁lycées▁du

2026-01-28 12:38:12,234 | INFO | speech length: 79040
2026-01-28 12:38:12,261 | INFO | decoder input length: 123
2026-01-28 12:38:12,261 | INFO | max output length: 123
2026-01-28 12:38:12,261 | INFO | min output length: 12
2026-01-28 12:38:13,352 | INFO | end detected at 27
2026-01-28 12:38:13,353 | INFO |  -1.73 * 0.5 =  -0.87 for decoder
2026-01-28 12:38:13,353 | INFO |  -0.08 * 0.5 =  -0.04 for ctc
2026-01-28 12:38:13,353 | INFO | total log probability: -0.90
2026-01-28 12:38:13,353 | INFO | normalized log probability: -0.04
2026-01-28 12:38:13,353 | INFO | total number of ended hypotheses: 162
2026-01-28 12:38:13,353 | INFO | best hypo: ▁je▁pense▁que▁à▁l'intérieur▁de▁paris▁globalement

2026-01-28 12:38:13,355 | INFO | speech length: 24160
2026-01-28 12:38:13,382 | INFO | decoder input length: 37
2026-01-28 12:38:13,382 | INFO | max output length: 37
2026-01-28 12:38:13,382 | INFO | min output length: 3
2026-01-28 12:38:14,019 | INFO | end detected at 20
2026-01-28 12:38:14,020 | INFO |  -6.77 * 0.5 =  -3.38 for decoder
2026-01-28 12:38:14,020 | INFO | -10.51 * 0.5 =  -5.25 for ctc
2026-01-28 12:38:14,020 | INFO | total log probability: -8.64
2026-01-28 12:38:14,020 | INFO | normalized log probability: -0.79
2026-01-28 12:38:14,020 | INFO | total number of ended hypotheses: 188
2026-01-28 12:38:14,021 | INFO | best hypo: ▁ces▁cols▁sontabandis

2026-01-28 12:38:14,022 | INFO | speech length: 36800
2026-01-28 12:38:14,049 | INFO | decoder input length: 57
2026-01-28 12:38:14,049 | INFO | max output length: 57
2026-01-28 12:38:14,049 | INFO | min output length: 5
2026-01-28 12:38:14,766 | INFO | end detected at 21
2026-01-28 12:38:14,767 | INFO |  -5.36 * 0.5 =  -2.68 for decoder
2026-01-28 12:38:14,767 | INFO |  -3.79 * 0.5 =  -1.89 for ctc
2026-01-28 12:38:14,767 | INFO | total log probability: -4.57
2026-01-28 12:38:14,767 | INFO | normalized log probability: -0.30
2026-01-28 12:38:14,767 | INFO | total number of ended hypotheses: 156
2026-01-28 12:38:14,767 | INFO | best hypo: ▁je▁pense▁moi▁choi▁dans▁le▁vingtième

2026-01-28 12:38:14,768 | INFO | speech length: 8160
2026-01-28 12:38:14,794 | INFO | decoder input length: 12
2026-01-28 12:38:14,794 | INFO | max output length: 12
2026-01-28 12:38:14,794 | INFO | min output length: 1
2026-01-28 12:38:15,118 | INFO | end detected at 10
2026-01-28 12:38:15,120 | INFO |  -1.23 * 0.5 =  -0.62 for decoder
2026-01-28 12:38:15,120 | INFO |  -4.91 * 0.5 =  -2.46 for ctc
2026-01-28 12:38:15,120 | INFO | total log probability: -3.07
2026-01-28 12:38:15,120 | INFO | normalized log probability: -0.61
2026-01-28 12:38:15,120 | INFO | total number of ended hypotheses: 171
2026-01-28 12:38:15,120 | INFO | best hypo: ▁enfant

2026-01-28 12:38:15,121 | INFO | speech length: 10880
2026-01-28 12:38:15,150 | INFO | decoder input length: 16
2026-01-28 12:38:15,150 | INFO | max output length: 16
2026-01-28 12:38:15,150 | INFO | min output length: 1
2026-01-28 12:38:15,559 | INFO | end detected at 13
2026-01-28 12:38:15,560 | INFO |  -1.98 * 0.5 =  -0.99 for decoder
2026-01-28 12:38:15,560 | INFO | -11.76 * 0.5 =  -5.88 for ctc
2026-01-28 12:38:15,560 | INFO | total log probability: -6.87
2026-01-28 12:38:15,560 | INFO | normalized log probability: -0.86
2026-01-28 12:38:15,560 | INFO | total number of ended hypotheses: 181
2026-01-28 12:38:15,560 | INFO | best hypo: ▁le▁monde▁le▁dit

2026-01-28 12:38:15,562 | INFO | speech length: 8160
2026-01-28 12:38:15,581 | INFO | decoder input length: 12
2026-01-28 12:38:15,581 | INFO | max output length: 12
2026-01-28 12:38:15,581 | INFO | min output length: 1
2026-01-28 12:38:15,849 | INFO | end detected at 8
2026-01-28 12:38:15,850 | INFO |  -0.34 * 0.5 =  -0.17 for decoder
2026-01-28 12:38:15,850 | INFO |  -1.45 * 0.5 =  -0.72 for ctc
2026-01-28 12:38:15,850 | INFO | total log probability: -0.89
2026-01-28 12:38:15,850 | INFO | normalized log probability: -0.30
2026-01-28 12:38:15,850 | INFO | total number of ended hypotheses: 160
2026-01-28 12:38:15,850 | INFO | best hypo: ▁six

2026-01-28 12:38:15,852 | INFO | speech length: 268640
2026-01-28 12:38:15,880 | INFO | decoder input length: 419
2026-01-28 12:38:15,880 | INFO | max output length: 419
2026-01-28 12:38:15,880 | INFO | min output length: 41
2026-01-28 12:38:24,931 | INFO | end detected at 132
2026-01-28 12:38:24,932 | INFO | -252.65 * 0.5 = -126.33 for decoder
2026-01-28 12:38:24,932 | INFO | -81.77 * 0.5 = -40.88 for ctc
2026-01-28 12:38:24,932 | INFO | total log probability: -167.21
2026-01-28 12:38:24,932 | INFO | normalized log probability: -1.33
2026-01-28 12:38:24,932 | INFO | total number of ended hypotheses: 183
2026-01-28 12:38:24,934 | INFO | best hypo: ▁l'tablissement▁y▁en▁a▁dit▁moins▁bon▁alors▁le▁problème▁si▁vous▁voulez▁dispose▁à▁paris▁va▁que▁joie▁d'encore▁mathiame▁c'est▁le▁problème▁des▁écoles▁maternelles▁et▁primaires▁lequels▁quand▁vous▁avez▁effect▁vivement▁un▁certain▁nombre▁d'enfants▁ce▁qui▁ne▁sont▁pasère▁qui▁ne▁parlent▁pas▁couramment▁français

2026-01-28 12:38:24,935 | INFO | speech length: 31200
2026-01-28 12:38:24,959 | INFO | decoder input length: 48
2026-01-28 12:38:24,959 | INFO | max output length: 48
2026-01-28 12:38:24,959 | INFO | min output length: 4
2026-01-28 12:38:25,657 | INFO | end detected at 22
2026-01-28 12:38:25,658 | INFO |  -1.90 * 0.5 =  -0.95 for decoder
2026-01-28 12:38:25,658 | INFO |  -0.48 * 0.5 =  -0.24 for ctc
2026-01-28 12:38:25,658 | INFO | total log probability: -1.19
2026-01-28 12:38:25,658 | INFO | normalized log probability: -0.07
2026-01-28 12:38:25,658 | INFO | total number of ended hypotheses: 141
2026-01-28 12:38:25,658 | INFO | best hypo: ▁pour▁les▁enseignances▁c'est▁pas▁facile

2026-01-28 12:38:25,659 | INFO | speech length: 83200
2026-01-28 12:38:25,687 | INFO | decoder input length: 129
2026-01-28 12:38:25,687 | INFO | max output length: 129
2026-01-28 12:38:25,688 | INFO | min output length: 12
2026-01-28 12:38:27,083 | INFO | end detected at 34
2026-01-28 12:38:27,084 | INFO |  -3.43 * 0.5 =  -1.72 for decoder
2026-01-28 12:38:27,084 | INFO |  -4.71 * 0.5 =  -2.36 for ctc
2026-01-28 12:38:27,084 | INFO | total log probability: -4.07
2026-01-28 12:38:27,084 | INFO | normalized log probability: -0.15
2026-01-28 12:38:27,084 | INFO | total number of ended hypotheses: 178
2026-01-28 12:38:27,085 | INFO | best hypo: ▁et▁moi▁je▁vois▁les▁mamans▁je▁fais▁travailler▁dans▁le▁vingtième▁il▁faudrait

2026-01-28 12:38:27,086 | INFO | speech length: 412480
2026-01-28 12:38:27,115 | INFO | decoder input length: 644
2026-01-28 12:38:27,115 | INFO | max output length: 644
2026-01-28 12:38:27,115 | INFO | min output length: 64
2026-01-28 12:38:44,664 | INFO | end detected at 192
2026-01-28 12:38:44,666 | INFO | -349.43 * 0.5 = -174.72 for decoder
2026-01-28 12:38:44,666 | INFO | -86.12 * 0.5 = -43.06 for ctc
2026-01-28 12:38:44,666 | INFO | total log probability: -217.78
2026-01-28 12:38:44,666 | INFO | normalized log probability: -1.16
2026-01-28 12:38:44,666 | INFO | total number of ended hypotheses: 161
2026-01-28 12:38:44,668 | INFO | best hypo: ▁si▁j'étais▁flisse▁l'éducation▁nationale▁je▁mettrai▁beaucoup▁plus▁d'enseignants▁dans▁les▁maternelles▁pour▁qu'un▁enfant▁qui▁sorte▁de▁maternel▁parle▁obligatoirement▁français▁parce▁que▁tous▁ses▁petits▁enfants▁qui▁arrivent▁ils▁vont▁à▁un▁maternel▁ont▁n'est▁pas▁normal▁qui▁arrivent▁en▁c'épé▁ne▁parlant▁pas▁français▁mais▁ils▁rivant▁ainsi▁et▁ne▁parlant▁pas▁français▁parce▁que▁comme▁ils▁sont▁beaucoup▁trop▁en▁nombreux▁en▁maternelle▁on▁ne▁peutent▁pas▁leur▁apprendre

2026-01-28 12:38:44,671 | INFO | speech length: 31040
2026-01-28 12:38:44,703 | INFO | decoder input length: 48
2026-01-28 12:38:44,703 | INFO | max output length: 48
2026-01-28 12:38:44,703 | INFO | min output length: 4
2026-01-28 12:38:45,399 | INFO | end detected at 22
2026-01-28 12:38:45,400 | INFO |  -8.58 * 0.5 =  -4.29 for decoder
2026-01-28 12:38:45,400 | INFO | -28.20 * 0.5 = -14.10 for ctc
2026-01-28 12:38:45,400 | INFO | total log probability: -18.39
2026-01-28 12:38:45,400 | INFO | normalized log probability: -1.15
2026-01-28 12:38:45,400 | INFO | total number of ended hypotheses: 184
2026-01-28 12:38:45,400 | INFO | best hypo: ▁pour▁regarder▁maman▁et▁papa▁lange

2026-01-28 12:38:45,402 | INFO | speech length: 101280
2026-01-28 12:38:45,433 | INFO | decoder input length: 157
2026-01-28 12:38:45,434 | INFO | max output length: 157
2026-01-28 12:38:45,434 | INFO | min output length: 15
2026-01-28 12:38:48,029 | INFO | end detected at 65
2026-01-28 12:38:48,031 | INFO | -19.17 * 0.5 =  -9.58 for decoder
2026-01-28 12:38:48,031 | INFO | -22.80 * 0.5 = -11.40 for ctc
2026-01-28 12:38:48,031 | INFO | total log probability: -20.99
2026-01-28 12:38:48,031 | INFO | normalized log probability: -0.37
2026-01-28 12:38:48,031 | INFO | total number of ended hypotheses: 172
2026-01-28 12:38:48,032 | INFO | best hypo: ▁pour▁imaginer▁du▁bussing▁oui▁oui▁d'abord▁sabé▁je▁j'ai▁vu▁que▁ça▁commencé▁à▁se▁faire▁à▁ajouter▁ça▁c'est▁une

2026-01-28 12:38:48,033 | INFO | speech length: 357440
2026-01-28 12:38:48,072 | INFO | decoder input length: 558
2026-01-28 12:38:48,072 | INFO | max output length: 558
2026-01-28 12:38:48,072 | INFO | min output length: 55
2026-01-28 12:39:01,303 | INFO | end detected at 170
2026-01-28 12:39:01,305 | INFO | -310.35 * 0.5 = -155.18 for decoder
2026-01-28 12:39:01,305 | INFO | -101.80 * 0.5 = -50.90 for ctc
2026-01-28 12:39:01,305 | INFO | total log probability: -206.08
2026-01-28 12:39:01,305 | INFO | normalized log probability: -1.28
2026-01-28 12:39:01,305 | INFO | total number of ended hypotheses: 215
2026-01-28 12:39:01,307 | INFO | best hypo: ▁c'est▁une▁des▁mesures▁du▁plan▁de▁banlieue▁le▁bossy▁mais▁je▁pense▁surtout▁que▁dans▁le▁sixième▁arrondissement▁les▁maternels▁il▁a▁suffisamment▁d'enseignant▁mais▁que▁n'est▁que▁dans▁le▁vintien▁et▁il▁faudrait▁quier▁qu'on▁s'éppare▁et▁qui▁des▁cours▁de▁français▁et▁pour▁les▁petits▁enfants▁et▁partent▁pas▁français▁s'est▁pas▁compliqué▁quand▁même▁c'est▁pas▁très▁difficile▁d'appendre▁le▁français▁d'▁enfants▁s'ass

2026-01-28 12:39:01,310 | INFO | speech length: 104960
2026-01-28 12:39:01,349 | INFO | decoder input length: 163
2026-01-28 12:39:01,349 | INFO | max output length: 163
2026-01-28 12:39:01,349 | INFO | min output length: 16
2026-01-28 12:39:03,660 | INFO | end detected at 56
2026-01-28 12:39:03,662 | INFO | -14.80 * 0.5 =  -7.40 for decoder
2026-01-28 12:39:03,662 | INFO | -16.91 * 0.5 =  -8.46 for ctc
2026-01-28 12:39:03,662 | INFO | total log probability: -15.86
2026-01-28 12:39:03,662 | INFO | normalized log probability: -0.32
2026-01-28 12:39:03,662 | INFO | total number of ended hypotheses: 191
2026-01-28 12:39:03,663 | INFO | best hypo: ▁non▁dans▁les▁mamans▁de▁parpas▁français▁qui▁ne▁voient▁pas▁souvent▁leur▁papate▁par▁définition▁il▁ne▁parle▁pas▁français▁hi▁par▁jamais▁français

2026-01-28 12:39:03,664 | INFO | speech length: 154880
2026-01-28 12:39:03,700 | INFO | decoder input length: 241
2026-01-28 12:39:03,700 | INFO | max output length: 241
2026-01-28 12:39:03,700 | INFO | min output length: 24
2026-01-28 12:39:07,820 | INFO | end detected at 86
2026-01-28 12:39:07,822 | INFO | -14.23 * 0.5 =  -7.11 for decoder
2026-01-28 12:39:07,822 | INFO | -21.18 * 0.5 = -10.59 for ctc
2026-01-28 12:39:07,822 | INFO | total log probability: -17.70
2026-01-28 12:39:07,822 | INFO | normalized log probability: -0.23
2026-01-28 12:39:07,822 | INFO | total number of ended hypotheses: 228
2026-01-28 12:39:07,823 | INFO | best hypo: ▁a▁pas▁pensé▁à▁la▁maison▁alors▁nous▁nous▁essayons▁d'apprendre▁le▁français▁au▁moment▁pour▁que▁elles▁apprennent▁le▁français▁à▁leurs▁enfants▁chose▁que▁ne▁fait▁aucun▁organisme▁d'état▁ca

2026-01-28 12:39:07,825 | INFO | speech length: 62560
2026-01-28 12:39:07,860 | INFO | decoder input length: 97
2026-01-28 12:39:07,860 | INFO | max output length: 97
2026-01-28 12:39:07,860 | INFO | min output length: 9
2026-01-28 12:39:09,127 | INFO | end detected at 36
2026-01-28 12:39:09,128 | INFO |  -3.36 * 0.5 =  -1.68 for decoder
2026-01-28 12:39:09,128 | INFO |  -2.44 * 0.5 =  -1.22 for ctc
2026-01-28 12:39:09,128 | INFO | total log probability: -2.90
2026-01-28 12:39:09,128 | INFO | normalized log probability: -0.09
2026-01-28 12:39:09,128 | INFO | total number of ended hypotheses: 156
2026-01-28 12:39:09,129 | INFO | best hypo: ▁les▁formations▁linguistiques▁y▁sont▁données▁aux▁immigrés▁qui▁arrivent

2026-01-28 12:39:09,130 | INFO | speech length: 125920
2026-01-28 12:39:09,163 | INFO | decoder input length: 196
2026-01-28 12:39:09,163 | INFO | max output length: 196
2026-01-28 12:39:09,163 | INFO | min output length: 19
2026-01-28 12:39:11,538 | INFO | end detected at 53
2026-01-28 12:39:11,540 | INFO |  -3.77 * 0.5 =  -1.89 for decoder
2026-01-28 12:39:11,540 | INFO |  -0.44 * 0.5 =  -0.22 for ctc
2026-01-28 12:39:11,540 | INFO | total log probability: -2.11
2026-01-28 12:39:11,540 | INFO | normalized log probability: -0.04
2026-01-28 12:39:11,540 | INFO | total number of ended hypotheses: 167
2026-01-28 12:39:11,541 | INFO | best hypo: ▁ne▁sont▁pas▁faites▁pour▁les▁gens▁qui▁ne▁parlent▁pas▁du▁tout▁ni▁qui▁n'ont▁jamais▁été▁à▁l'école▁parce▁que▁sa▁personne▁en▁veut

2026-01-28 12:39:11,542 | INFO | speech length: 33440
2026-01-28 12:39:11,576 | INFO | decoder input length: 51
2026-01-28 12:39:11,576 | INFO | max output length: 51
2026-01-28 12:39:11,577 | INFO | min output length: 5
2026-01-28 12:39:12,503 | INFO | end detected at 30
2026-01-28 12:39:12,504 | INFO |  -2.38 * 0.5 =  -1.19 for decoder
2026-01-28 12:39:12,504 | INFO |  -3.74 * 0.5 =  -1.87 for ctc
2026-01-28 12:39:12,504 | INFO | total log probability: -3.06
2026-01-28 12:39:12,504 | INFO | normalized log probability: -0.12
2026-01-28 12:39:12,504 | INFO | total number of ended hypotheses: 157
2026-01-28 12:39:12,504 | INFO | best hypo: ▁parce▁que▁vous▁dire▁qu'apprendre▁à▁lire▁à▁écrire

2026-01-28 12:39:12,506 | INFO | speech length: 67520
2026-01-28 12:39:12,535 | INFO | decoder input length: 105
2026-01-28 12:39:12,535 | INFO | max output length: 105
2026-01-28 12:39:12,535 | INFO | min output length: 10
2026-01-28 12:39:13,976 | INFO | end detected at 40
2026-01-28 12:39:13,977 | INFO |  -3.36 * 0.5 =  -1.68 for decoder
2026-01-28 12:39:13,977 | INFO |  -2.86 * 0.5 =  -1.43 for ctc
2026-01-28 12:39:13,977 | INFO | total log probability: -3.11
2026-01-28 12:39:13,977 | INFO | normalized log probability: -0.09
2026-01-28 12:39:13,977 | INFO | total number of ended hypotheses: 145
2026-01-28 12:39:13,978 | INFO | best hypo: ▁trente▁cinq▁ans▁qu'a▁jamais▁tenu▁un▁crayon▁ça▁se▁fait▁pas▁en▁deux▁cents▁heures

2026-01-28 12:39:13,979 | INFO | speech length: 10560
2026-01-28 12:39:14,007 | INFO | decoder input length: 16
2026-01-28 12:39:14,007 | INFO | max output length: 16
2026-01-28 12:39:14,007 | INFO | min output length: 1
2026-01-28 12:39:14,288 | INFO | end detected at 9
2026-01-28 12:39:14,289 | INFO |  -0.27 * 0.5 =  -0.13 for decoder
2026-01-28 12:39:14,289 | INFO |  -0.50 * 0.5 =  -0.25 for ctc
2026-01-28 12:39:14,289 | INFO | total log probability: -0.38
2026-01-28 12:39:14,289 | INFO | normalized log probability: -0.10
2026-01-28 12:39:14,289 | INFO | total number of ended hypotheses: 179
2026-01-28 12:39:14,290 | INFO | best hypo: ▁non

2026-01-28 12:39:14,291 | INFO | speech length: 67680
2026-01-28 12:39:14,317 | INFO | decoder input length: 105
2026-01-28 12:39:14,317 | INFO | max output length: 105
2026-01-28 12:39:14,317 | INFO | min output length: 10
2026-01-28 12:39:15,200 | INFO | end detected at 23
2026-01-28 12:39:15,201 | INFO |  -5.59 * 0.5 =  -2.80 for decoder
2026-01-28 12:39:15,201 | INFO |  -0.43 * 0.5 =  -0.22 for ctc
2026-01-28 12:39:15,201 | INFO | total log probability: -3.01
2026-01-28 12:39:15,202 | INFO | normalized log probability: -0.18
2026-01-28 12:39:15,202 | INFO | total number of ended hypotheses: 166
2026-01-28 12:39:15,202 | INFO | best hypo: ▁donc▁non▁je▁je▁pense▁que▁le▁problème

2026-01-28 12:39:15,203 | INFO | speech length: 37440
2026-01-28 12:39:15,233 | INFO | decoder input length: 58
2026-01-28 12:39:15,233 | INFO | max output length: 58
2026-01-28 12:39:15,233 | INFO | min output length: 5
2026-01-28 12:39:16,470 | INFO | end detected at 39
2026-01-28 12:39:16,472 | INFO |  -3.13 * 0.5 =  -1.57 for decoder
2026-01-28 12:39:16,472 | INFO |  -3.84 * 0.5 =  -1.92 for ctc
2026-01-28 12:39:16,472 | INFO | total log probability: -3.49
2026-01-28 12:39:16,472 | INFO | normalized log probability: -0.10
2026-01-28 12:39:16,472 | INFO | total number of ended hypotheses: 169
2026-01-28 12:39:16,472 | INFO | best hypo: ▁je▁dis▁pas▁que▁tous▁les▁problèmes▁bien▁là▁mais▁beaucoup▁de▁problèmes

2026-01-28 12:39:16,474 | INFO | speech length: 98560
2026-01-28 12:39:16,505 | INFO | decoder input length: 153
2026-01-28 12:39:16,505 | INFO | max output length: 153
2026-01-28 12:39:16,505 | INFO | min output length: 15
2026-01-28 12:39:19,174 | INFO | end detected at 59
2026-01-28 12:39:19,177 | INFO | -11.49 * 0.5 =  -5.75 for decoder
2026-01-28 12:39:19,177 | INFO |  -5.96 * 0.5 =  -2.98 for ctc
2026-01-28 12:39:19,177 | INFO | total log probability: -8.72
2026-01-28 12:39:19,177 | INFO | normalized log probability: -0.18
2026-01-28 12:39:19,177 | INFO | total number of ended hypotheses: 217
2026-01-28 12:39:19,178 | INFO | best hypo: ▁comment▁voulez▁vous▁apprendre▁à▁lire▁à▁un▁enfant▁qui▁parle▁français▁dans▁par▁définition▁cet▁enfant▁va▁perpire▁encp

2026-01-28 12:39:19,180 | INFO | speech length: 60480
2026-01-28 12:39:19,209 | INFO | decoder input length: 94
2026-01-28 12:39:19,209 | INFO | max output length: 94
2026-01-28 12:39:19,210 | INFO | min output length: 9
2026-01-28 12:39:20,501 | INFO | end detected at 36
2026-01-28 12:39:20,502 | INFO |  -5.83 * 0.5 =  -2.91 for decoder
2026-01-28 12:39:20,503 | INFO |  -6.10 * 0.5 =  -3.05 for ctc
2026-01-28 12:39:20,503 | INFO | total log probability: -5.96
2026-01-28 12:39:20,503 | INFO | normalized log probability: -0.20
2026-01-28 12:39:20,503 | INFO | total number of ended hypotheses: 176
2026-01-28 12:39:20,503 | INFO | best hypo: ▁voilà▁alors▁bien▁sûr▁qu'un▁a▁qui▁s'en▁sorte▁et▁a▁toujours▁des

2026-01-28 12:39:20,505 | INFO | speech length: 17120
2026-01-28 12:39:20,535 | INFO | decoder input length: 26
2026-01-28 12:39:20,535 | INFO | max output length: 26
2026-01-28 12:39:20,535 | INFO | min output length: 2
2026-01-28 12:39:20,966 | INFO | end detected at 14
2026-01-28 12:39:20,967 | INFO |  -1.44 * 0.5 =  -0.72 for decoder
2026-01-28 12:39:20,967 | INFO |  -8.02 * 0.5 =  -4.01 for ctc
2026-01-28 12:39:20,967 | INFO | total log probability: -4.73
2026-01-28 12:39:20,967 | INFO | normalized log probability: -0.53
2026-01-28 12:39:20,967 | INFO | total number of ended hypotheses: 169
2026-01-28 12:39:20,967 | INFO | best hypo: ▁dis▁je▁en▁quelque▁part

2026-01-28 12:39:20,969 | INFO | speech length: 17120
2026-01-28 12:39:20,993 | INFO | decoder input length: 26
2026-01-28 12:39:20,993 | INFO | max output length: 26
2026-01-28 12:39:20,993 | INFO | min output length: 2
2026-01-28 12:39:21,483 | INFO | end detected at 16
2026-01-28 12:39:21,484 | INFO |  -0.90 * 0.5 =  -0.45 for decoder
2026-01-28 12:39:21,484 | INFO |  -1.24 * 0.5 =  -0.62 for ctc
2026-01-28 12:39:21,484 | INFO | total log probability: -1.07
2026-01-28 12:39:21,484 | INFO | normalized log probability: -0.10
2026-01-28 12:39:21,484 | INFO | total number of ended hypotheses: 162
2026-01-28 12:39:21,484 | INFO | best hypo: ▁les▁gens▁y▁sont▁normaux

2026-01-28 12:39:21,486 | INFO | speech length: 26560
2026-01-28 12:39:21,515 | INFO | decoder input length: 41
2026-01-28 12:39:21,515 | INFO | max output length: 41
2026-01-28 12:39:21,515 | INFO | min output length: 4
2026-01-28 12:39:22,210 | INFO | end detected at 22
2026-01-28 12:39:22,211 | INFO |  -1.26 * 0.5 =  -0.63 for decoder
2026-01-28 12:39:22,211 | INFO |  -0.10 * 0.5 =  -0.05 for ctc
2026-01-28 12:39:22,211 | INFO | total log probability: -0.68
2026-01-28 12:39:22,211 | INFO | normalized log probability: -0.04
2026-01-28 12:39:22,211 | INFO | total number of ended hypotheses: 164
2026-01-28 12:39:22,212 | INFO | best hypo: ▁mais▁globalement▁c'est▁très▁difficile

2026-01-28 12:39:22,213 | INFO | speech length: 8160
2026-01-28 12:39:22,231 | INFO | decoder input length: 12
2026-01-28 12:39:22,231 | INFO | max output length: 12
2026-01-28 12:39:22,232 | INFO | min output length: 1
2026-01-28 12:39:22,558 | INFO | adding <eos> in the last position in the loop
2026-01-28 12:39:22,566 | INFO | end detected at 11
2026-01-28 12:39:22,568 | INFO |  -9.10 * 0.5 =  -4.55 for decoder
2026-01-28 12:39:22,568 | INFO |  -0.76 * 0.5 =  -0.38 for ctc
2026-01-28 12:39:22,568 | INFO | total log probability: -4.93
2026-01-28 12:39:22,568 | INFO | normalized log probability: -0.82
2026-01-28 12:39:22,568 | INFO | total number of ended hypotheses: 179
2026-01-28 12:39:22,568 | INFO | best hypo: ▁des▁enfant

2026-01-28 12:39:22,569 | INFO | speech length: 14880
2026-01-28 12:39:22,599 | INFO | decoder input length: 22
2026-01-28 12:39:22,599 | INFO | max output length: 22
2026-01-28 12:39:22,599 | INFO | min output length: 2
2026-01-28 12:39:23,035 | INFO | end detected at 14
2026-01-28 12:39:23,037 | INFO |  -4.75 * 0.5 =  -2.38 for decoder
2026-01-28 12:39:23,037 | INFO |  -8.69 * 0.5 =  -4.35 for ctc
2026-01-28 12:39:23,037 | INFO | total log probability: -6.72
2026-01-28 12:39:23,038 | INFO | normalized log probability: -0.84
2026-01-28 12:39:23,038 | INFO | total number of ended hypotheses: 203
2026-01-28 12:39:23,038 | INFO | best hypo: ▁mille▁longes

2026-01-28 12:39:23,039 | INFO | speech length: 129920
2026-01-28 12:39:23,079 | INFO | decoder input length: 202
2026-01-28 12:39:23,079 | INFO | max output length: 202
2026-01-28 12:39:23,079 | INFO | min output length: 20
2026-01-28 12:39:25,916 | INFO | end detected at 58
2026-01-28 12:39:25,917 | INFO | -15.55 * 0.5 =  -7.77 for decoder
2026-01-28 12:39:25,917 | INFO | -18.08 * 0.5 =  -9.04 for ctc
2026-01-28 12:39:25,917 | INFO | total log probability: -16.82
2026-01-28 12:39:25,917 | INFO | normalized log probability: -0.33
2026-01-28 12:39:25,917 | INFO | total number of ended hypotheses: 168
2026-01-28 12:39:25,918 | INFO | best hypo: ▁bon▁soixante▁réussent▁de▁gamin▁qui▁parlent▁français▁ça▁s'installe▁très▁gazeux▁et▁le▁français▁langue▁étrangère▁dans▁un▁milieu

2026-01-28 12:39:25,920 | INFO | speech length: 32800
2026-01-28 12:39:25,947 | INFO | decoder input length: 50
2026-01-28 12:39:25,947 | INFO | max output length: 50
2026-01-28 12:39:25,947 | INFO | min output length: 5
2026-01-28 12:39:26,512 | INFO | end detected at 17
2026-01-28 12:39:26,513 | INFO |  -1.02 * 0.5 =  -0.51 for decoder
2026-01-28 12:39:26,513 | INFO |  -1.93 * 0.5 =  -0.97 for ctc
2026-01-28 12:39:26,513 | INFO | total log probability: -1.48
2026-01-28 12:39:26,513 | INFO | normalized log probability: -0.11
2026-01-28 12:39:26,513 | INFO | total number of ended hypotheses: 154
2026-01-28 12:39:26,513 | INFO | best hypo: ▁quatre▁vingt▁dix▁pourcent▁de▁la▁classe

2026-01-28 12:39:26,514 | INFO | speech length: 293440
2026-01-28 12:39:26,542 | INFO | decoder input length: 458
2026-01-28 12:39:26,543 | INFO | max output length: 458
2026-01-28 12:39:26,543 | INFO | min output length: 45
2026-01-28 12:39:37,450 | INFO | end detected at 151
2026-01-28 12:39:37,452 | INFO | -332.79 * 0.5 = -166.39 for decoder
2026-01-28 12:39:37,452 | INFO | -158.44 * 0.5 = -79.22 for ctc
2026-01-28 12:39:37,452 | INFO | total log probability: -245.62
2026-01-28 12:39:37,452 | INFO | normalized log probability: -1.68
2026-01-28 12:39:37,452 | INFO | total number of ended hypotheses: 158
2026-01-28 12:39:37,454 | INFO | best hypo: ▁manger▁s'est▁beaucoup▁malin▁facile▁bien▁avec▁tout▁bien▁sûr▁pour▁ça▁je▁parler▁de▁mélange▁oui▁mais▁je▁pense▁que▁vous▁avez▁raison▁je▁pense▁que▁vous▁avez▁raison▁mais▁si▁vous▁voulez▁le▁mélange▁et▁difficile▁quand▁ils▁son▁petits▁pas▁que▁quarant▁t▁ils▁sont▁polisents▁et▁hallantseurs▁oui▁hers▁c'est▁pas▁tellement▁bon▁d'ember▁un▁petit▁enfant▁de▁trois▁quatre▁ans

2026-01-28 12:39:37,456 | INFO | speech length: 37440
2026-01-28 12:39:37,479 | INFO | decoder input length: 58
2026-01-28 12:39:37,479 | INFO | max output length: 58
2026-01-28 12:39:37,479 | INFO | min output length: 5
2026-01-28 12:39:37,927 | INFO | end detected at 13
2026-01-28 12:39:37,928 | INFO |  -0.78 * 0.5 =  -0.39 for decoder
2026-01-28 12:39:37,928 | INFO |  -7.75 * 0.5 =  -3.87 for ctc
2026-01-28 12:39:37,928 | INFO | total log probability: -4.26
2026-01-28 12:39:37,928 | INFO | normalized log probability: -0.53
2026-01-28 12:39:37,928 | INFO | total number of ended hypotheses: 170
2026-01-28 12:39:37,928 | INFO | best hypo: ▁voyons▁bus

2026-01-28 12:39:37,930 | INFO | speech length: 104800
2026-01-28 12:39:37,956 | INFO | decoder input length: 163
2026-01-28 12:39:37,956 | INFO | max output length: 163
2026-01-28 12:39:37,956 | INFO | min output length: 16
2026-01-28 12:39:40,887 | INFO | end detected at 69
2026-01-28 12:39:40,888 | INFO |  -8.77 * 0.5 =  -4.39 for decoder
2026-01-28 12:39:40,888 | INFO |  -3.56 * 0.5 =  -1.78 for ctc
2026-01-28 12:39:40,888 | INFO | total log probability: -6.17
2026-01-28 12:39:40,889 | INFO | normalized log probability: -0.10
2026-01-28 12:39:40,889 | INFO | total number of ended hypotheses: 198
2026-01-28 12:39:40,889 | INFO | best hypo: ▁plus▁loin▁pas▁ce▁que▁déjà▁c'est▁quand▁même▁pour▁eux▁d'aller▁à▁l'école▁c'est▁pas▁évident▁et▁puis▁faudra▁que▁les▁parents▁l'acceptent▁est▁ce▁que

2026-01-28 12:39:40,891 | INFO | speech length: 41600
2026-01-28 12:39:40,920 | INFO | decoder input length: 64
2026-01-28 12:39:40,920 | INFO | max output length: 64
2026-01-28 12:39:40,920 | INFO | min output length: 6
2026-01-28 12:39:42,205 | INFO | end detected at 38
2026-01-28 12:39:42,206 | INFO |  -6.79 * 0.5 =  -3.39 for decoder
2026-01-28 12:39:42,206 | INFO |  -5.88 * 0.5 =  -2.94 for ctc
2026-01-28 12:39:42,206 | INFO | total log probability: -6.33
2026-01-28 12:39:42,206 | INFO | normalized log probability: -0.20
2026-01-28 12:39:42,206 | INFO | total number of ended hypotheses: 183
2026-01-28 12:39:42,207 | INFO | best hypo: ▁les▁mamans▁africaines▁ont▁beaucoup▁de▁mal▁à▁séparer▁de'

2026-01-28 12:39:42,208 | INFO | speech length: 155520
2026-01-28 12:39:42,237 | INFO | decoder input length: 242
2026-01-28 12:39:42,237 | INFO | max output length: 242
2026-01-28 12:39:42,237 | INFO | min output length: 24
2026-01-28 12:39:46,501 | INFO | end detected at 84
2026-01-28 12:39:46,502 | INFO | -27.17 * 0.5 = -13.59 for decoder
2026-01-28 12:39:46,502 | INFO | -20.14 * 0.5 = -10.07 for ctc
2026-01-28 12:39:46,502 | INFO | total log probability: -23.66
2026-01-28 12:39:46,502 | INFO | normalized log probability: -0.31
2026-01-28 12:39:46,502 | INFO | total number of ended hypotheses: 178
2026-01-28 12:39:46,503 | INFO | best hypo: ▁on▁pas▁très▁volontiers▁toxian▁plus▁on▁leur▁explique▁que▁ça▁veut▁dire▁que▁l'enfant▁restera▁déjeuner▁là▁bas▁les▁mamans▁africaines▁ne▁veulent▁pas▁elles▁veulent▁pas

2026-01-28 12:39:46,511 | INFO | Chunk: 0 | WER=26.666667 | S=2 D=2 I=0
2026-01-28 12:39:46,512 | INFO | Chunk: 1 | WER=34.375000 | S=9 D=2 I=0
2026-01-28 12:39:46,512 | INFO | Chunk: 2 | WER=80.000000 | S=2 D=2 I=0
2026-01-28 12:39:46,513 | INFO | Chunk: 3 | WER=12.500000 | S=2 D=0 I=1
2026-01-28 12:39:46,513 | INFO | Chunk: 4 | WER=50.000000 | S=2 D=0 I=0
2026-01-28 12:39:46,515 | INFO | Chunk: 5 | WER=33.898305 | S=13 D=7 I=0
2026-01-28 12:39:46,515 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 12:39:46,515 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 12:39:46,515 | INFO | Chunk: 8 | WER=100.000000 | S=3 D=5 I=0
2026-01-28 12:39:46,516 | INFO | Chunk: 9 | WER=25.000000 | S=1 D=1 I=0
2026-01-28 12:39:46,516 | INFO | Chunk: 10 | WER=100.000000 | S=1 D=2 I=0
2026-01-28 12:39:46,516 | INFO | Chunk: 11 | WER=100.000000 | S=2 D=0 I=1
2026-01-28 12:39:46,516 | INFO | Chunk: 12 | WER=100.000000 | S=1 D=0 I=0
2026-01-28 12:39:46,518 | INFO | Chunk: 13 | WER=35.087719 | S=13 D=5 I=2
2026-01-28 12:39:46,518 | INFO | Chunk: 14 | WER=44.444444 | S=2 D=2 I=0
2026-01-28 12:39:46,518 | INFO | Chunk: 15 | WER=6.666667 | S=0 D=1 I=0
2026-01-28 12:39:46,522 | INFO | Chunk: 16 | WER=24.050633 | S=14 D=2 I=3
2026-01-28 12:39:46,522 | INFO | Chunk: 17 | WER=120.000000 | S=5 D=0 I=1
2026-01-28 12:39:46,522 | INFO | Chunk: 18 | WER=43.750000 | S=7 D=7 I=0
2026-01-28 12:39:46,526 | INFO | Chunk: 19 | WER=40.229885 | S=15 D=15 I=5
2026-01-28 12:39:46,526 | INFO | Chunk: 20 | WER=42.857143 | S=9 D=3 I=0
2026-01-28 12:39:46,527 | INFO | Chunk: 21 | WER=17.142857 | S=5 D=1 I=0
2026-01-28 12:39:46,527 | INFO | Chunk: 22 | WER=10.000000 | S=1 D=0 I=0
2026-01-28 12:39:46,528 | INFO | Chunk: 23 | WER=6.896552 | S=1 D=1 I=0
2026-01-28 12:39:46,528 | INFO | Chunk: 24 | WER=18.181818 | S=1 D=1 I=0
2026-01-28 12:39:46,529 | INFO | Chunk: 25 | WER=5.882353 | S=1 D=0 I=0
2026-01-28 12:39:46,529 | INFO | Chunk: 26 | WER=50.000000 | S=0 D=1 I=0
2026-01-28 12:39:46,529 | INFO | Chunk: 27 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 12:39:46,529 | INFO | Chunk: 28 | WER=20.000000 | S=1 D=2 I=0
2026-01-28 12:39:46,530 | INFO | Chunk: 29 | WER=37.037037 | S=3 D=7 I=0
2026-01-28 12:39:46,530 | INFO | Chunk: 30 | WER=40.000000 | S=3 D=5 I=0
2026-01-28 12:39:46,531 | INFO | Chunk: 31 | WER=100.000000 | S=1 D=0 I=2
2026-01-28 12:39:46,531 | INFO | Chunk: 32 | WER=50.000000 | S=2 D=1 I=0
2026-01-28 12:39:46,531 | INFO | Chunk: 33 | WER=28.571429 | S=1 D=1 I=0
2026-01-28 12:39:46,531 | INFO | Chunk: 34 | WER=50.000000 | S=1 D=0 I=0
2026-01-28 12:39:46,531 | INFO | Chunk: 35 | WER=100.000000 | S=2 D=1 I=0
2026-01-28 12:39:46,532 | INFO | Chunk: 36 | WER=39.130435 | S=5 D=3 I=1
2026-01-28 12:39:46,532 | INFO | Chunk: 37 | WER=33.333333 | S=1 D=2 I=0
2026-01-28 12:39:46,534 | INFO | Chunk: 38 | WER=34.848485 | S=17 D=2 I=4
2026-01-28 12:39:46,535 | INFO | Chunk: 39 | WER=75.000000 | S=1 D=2 I=0
2026-01-28 12:39:46,535 | INFO | Chunk: 40 | WER=16.129032 | S=2 D=1 I=2
2026-01-28 12:39:46,536 | INFO | Chunk: 41 | WER=23.076923 | S=0 D=3 I=0
2026-01-28 12:39:46,536 | INFO | Chunk: 42 | WER=16.129032 | S=3 D=2 I=0
2026-01-28 12:39:46,826 | INFO | File: Rhap-D0002.wav | WER=31.118881 | S=157 D=90 I=20
2026-01-28 12:39:46,826 | INFO | ------------------------------
2026-01-28 12:39:46,826 | INFO | Conf cv Done!
2026-01-28 12:39:47,054 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 12:39:47,071 | INFO | Vocabulary size: 47
2026-01-28 12:39:47,576 | INFO | Gradient checkpoint layers: []
2026-01-28 12:39:48,189 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 12:39:48,192 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 12:39:48,193 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 12:39:48,193 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 12:39:48,196 | INFO | speech length: 53600
2026-01-28 12:39:48,221 | INFO | decoder input length: 83
2026-01-28 12:39:48,221 | INFO | max output length: 83
2026-01-28 12:39:48,221 | INFO | min output length: 8
2026-01-28 12:39:50,630 | INFO | end detected at 79
2026-01-28 12:39:50,632 | INFO |  -6.09 * 0.5 =  -3.04 for decoder
2026-01-28 12:39:50,632 | INFO |  -0.68 * 0.5 =  -0.34 for ctc
2026-01-28 12:39:50,632 | INFO | total log probability: -3.39
2026-01-28 12:39:50,632 | INFO | normalized log probability: -0.05
2026-01-28 12:39:50,632 | INFO | total number of ended hypotheses: 183
2026-01-28 12:39:50,633 | INFO | best hypo: et<space>j'avais<space>la<space>chance<space>d'avoir<space>des<space>enfants<space>qui<space>travaillaient<space>euh<space>je<space>dirais

2026-01-28 12:39:50,634 | INFO | speech length: 142880
2026-01-28 12:39:50,659 | INFO | decoder input length: 222
2026-01-28 12:39:50,659 | INFO | max output length: 222
2026-01-28 12:39:50,659 | INFO | min output length: 22
2026-01-28 12:39:58,051 | INFO | end detected at 186
2026-01-28 12:39:58,052 | INFO | -16.81 * 0.5 =  -8.40 for decoder
2026-01-28 12:39:58,052 | INFO |  -8.10 * 0.5 =  -4.05 for ctc
2026-01-28 12:39:58,052 | INFO | total log probability: -12.46
2026-01-28 12:39:58,052 | INFO | normalized log probability: -0.07
2026-01-28 12:39:58,052 | INFO | total number of ended hypotheses: 172
2026-01-28 12:39:58,054 | INFO | best hypo: i<space>préfèraient<space>rigoler<space>que<space>de<space>travailler<space>i<space>s<space>étaient<space>tout<space>à<space>fait<space>normaux<space>mais<space>je<space>veux<space>dire<space>i<space>s<space>ont<space>travaillé<space>normalement<space>et<space>c'est<space>vrai<space>que<space>bon<space>habitant<space>dans<space>le<space>centre<space>de<space>paris<space>euh

2026-01-28 12:39:58,056 | INFO | speech length: 9440
2026-01-28 12:39:58,075 | INFO | decoder input length: 14
2026-01-28 12:39:58,075 | INFO | max output length: 14
2026-01-28 12:39:58,076 | INFO | min output length: 1
2026-01-28 12:39:58,420 | INFO | adding <eos> in the last position in the loop
2026-01-28 12:39:58,427 | INFO | no hypothesis. Finish decoding.
2026-01-28 12:39:58,428 | INFO |  -3.73 * 0.5 =  -1.87 for decoder
2026-01-28 12:39:58,428 | INFO |  -9.02 * 0.5 =  -4.51 for ctc
2026-01-28 12:39:58,428 | INFO | total log probability: -6.38
2026-01-28 12:39:58,428 | INFO | normalized log probability: -0.80
2026-01-28 12:39:58,428 | INFO | total number of ended hypotheses: 124
2026-01-28 12:39:58,429 | INFO | best hypo: écoles

2026-01-28 12:39:58,430 | INFO | speech length: 94720
2026-01-28 12:39:58,454 | INFO | decoder input length: 147
2026-01-28 12:39:58,454 | INFO | max output length: 147
2026-01-28 12:39:58,454 | INFO | min output length: 14
2026-01-28 12:40:02,821 | INFO | end detected at 125
2026-01-28 12:40:02,822 | INFO |  -9.53 * 0.5 =  -4.77 for decoder
2026-01-28 12:40:02,822 | INFO |  -0.48 * 0.5 =  -0.24 for ctc
2026-01-28 12:40:02,822 | INFO | total log probability: -5.00
2026-01-28 12:40:02,822 | INFO | normalized log probability: -0.04
2026-01-28 12:40:02,822 | INFO | total number of ended hypotheses: 179
2026-01-28 12:40:02,824 | INFO | best hypo: aucun<space>problème<space>moi<space>j'ai<space>eu<space>aucun<space>problème<space>scolaire<space>pour<space>mes<space>enfants<space>et<space>j'ai<space>j'ai<space>jamais<space>très<space>bien<space>compris<space>les<space>gens<space>qui

2026-01-28 12:40:02,825 | INFO | speech length: 16480
2026-01-28 12:40:02,848 | INFO | decoder input length: 25
2026-01-28 12:40:02,848 | INFO | max output length: 25
2026-01-28 12:40:02,848 | INFO | min output length: 2
2026-01-28 12:40:03,490 | INFO | adding <eos> in the last position in the loop
2026-01-28 12:40:03,498 | INFO | no hypothesis. Finish decoding.
2026-01-28 12:40:03,498 | INFO |  -9.16 * 0.5 =  -4.58 for decoder
2026-01-28 12:40:03,498 | INFO |  -7.47 * 0.5 =  -3.74 for ctc
2026-01-28 12:40:03,498 | INFO | total log probability: -8.31
2026-01-28 12:40:03,498 | INFO | normalized log probability: -0.35
2026-01-28 12:40:03,498 | INFO | total number of ended hypotheses: 102
2026-01-28 12:40:03,499 | INFO | best hypo: jeconnaissais<space>des<space>gens

2026-01-28 12:40:03,500 | INFO | speech length: 283520
2026-01-28 12:40:03,522 | INFO | decoder input length: 442
2026-01-28 12:40:03,522 | INFO | max output length: 442
2026-01-28 12:40:03,522 | INFO | min output length: 44
2026-01-28 12:40:23,216 | INFO | end detected at 346
2026-01-28 12:40:23,219 | INFO | -46.88 * 0.5 = -23.44 for decoder
2026-01-28 12:40:23,220 | INFO | -20.68 * 0.5 = -10.34 for ctc
2026-01-28 12:40:23,220 | INFO | total log probability: -33.78
2026-01-28 12:40:23,220 | INFO | normalized log probability: -0.10
2026-01-28 12:40:23,220 | INFO | total number of ended hypotheses: 250
2026-01-28 12:40:23,223 | INFO | best hypo: septièmes<space>qui<space>mettaient<space>leurs<space>enfants<space>d'abord<space>dans<space>le<space>public<space>puis<space>dans<space>le<space>privé<space>pour<space>qu'ils<space>puissent<space>passer<space>après<space>éventuellement<space>euh<space>qu'ils<space>soient<space>plus<space>forts<space>et<space>puis<space>après<space>ils<space>étaient<space>très<space>étonnés<space>parce<space>que<space>quand<space>ils<space>voulaient<space>entrer<space>en<space>prépart<space>on<space>les<space>prenait<space>après<space>les<space>gens<space>qui<space>venaient<space>du<space>public<space>ça<space>paraît<space>normal<space>fin<space>je<space>veux<space>dire

2026-01-28 12:40:23,227 | INFO | speech length: 31040
2026-01-28 12:40:23,259 | INFO | decoder input length: 48
2026-01-28 12:40:23,259 | INFO | max output length: 48
2026-01-28 12:40:23,259 | INFO | min output length: 4
2026-01-28 12:40:23,962 | INFO | end detected at 24
2026-01-28 12:40:23,963 | INFO |  -1.49 * 0.5 =  -0.74 for decoder
2026-01-28 12:40:23,963 | INFO |  -0.03 * 0.5 =  -0.01 for ctc
2026-01-28 12:40:23,963 | INFO | total log probability: -0.76
2026-01-28 12:40:23,963 | INFO | normalized log probability: -0.04
2026-01-28 12:40:23,963 | INFO | total number of ended hypotheses: 163
2026-01-28 12:40:23,964 | INFO | best hypo: euh<space>les<space>lycées<space>du

2026-01-28 12:40:23,965 | INFO | speech length: 79040
2026-01-28 12:40:23,991 | INFO | decoder input length: 123
2026-01-28 12:40:23,991 | INFO | max output length: 123
2026-01-28 12:40:23,991 | INFO | min output length: 12
2026-01-28 12:40:26,574 | INFO | end detected at 63
2026-01-28 12:40:26,575 | INFO |  -4.52 * 0.5 =  -2.26 for decoder
2026-01-28 12:40:26,575 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-28 12:40:26,575 | INFO | total log probability: -2.27
2026-01-28 12:40:26,575 | INFO | normalized log probability: -0.04
2026-01-28 12:40:26,575 | INFO | total number of ended hypotheses: 174
2026-01-28 12:40:26,576 | INFO | best hypo: je<space>pense<space>que<space>à<space>l'intérieur<space>de<space>paris<space>euh<space>globalement<space>euh

2026-01-28 12:40:26,577 | INFO | speech length: 24160
2026-01-28 12:40:26,602 | INFO | decoder input length: 37
2026-01-28 12:40:26,602 | INFO | max output length: 37
2026-01-28 12:40:26,602 | INFO | min output length: 3
2026-01-28 12:40:27,580 | INFO | adding <eos> in the last position in the loop
2026-01-28 12:40:27,588 | INFO | no hypothesis. Finish decoding.
2026-01-28 12:40:27,588 | INFO | -11.18 * 0.5 =  -5.59 for decoder
2026-01-28 12:40:27,588 | INFO | -18.37 * 0.5 =  -9.18 for ctc
2026-01-28 12:40:27,588 | INFO | total log probability: -14.77
2026-01-28 12:40:27,588 | INFO | normalized log probability: -0.41
2026-01-28 12:40:27,589 | INFO | total number of ended hypotheses: 101
2026-01-28 12:40:27,589 | INFO | best hypo: lesécoles<space>sont<space>sont<space>d'un<space>bon<space>livre

2026-01-28 12:40:27,590 | INFO | speech length: 36800
2026-01-28 12:40:27,613 | INFO | decoder input length: 57
2026-01-28 12:40:27,613 | INFO | max output length: 57
2026-01-28 12:40:27,614 | INFO | min output length: 5
2026-01-28 12:40:29,234 | INFO | end detected at 55
2026-01-28 12:40:29,236 | INFO |  -4.80 * 0.5 =  -2.40 for decoder
2026-01-28 12:40:29,236 | INFO |  -3.01 * 0.5 =  -1.50 for ctc
2026-01-28 12:40:29,236 | INFO | total log probability: -3.90
2026-01-28 12:40:29,236 | INFO | normalized log probability: -0.08
2026-01-28 12:40:29,236 | INFO | total number of ended hypotheses: 205
2026-01-28 12:40:29,237 | INFO | best hypo: je<space>pense<space>moi<space>je<space>sois<space>euh<space>dans<space>le<space>vingtième<space>euh

2026-01-28 12:40:29,239 | INFO | speech length: 8160
2026-01-28 12:40:29,259 | INFO | decoder input length: 12
2026-01-28 12:40:29,259 | INFO | max output length: 12
2026-01-28 12:40:29,259 | INFO | min output length: 1
2026-01-28 12:40:29,559 | INFO | adding <eos> in the last position in the loop
2026-01-28 12:40:29,565 | INFO | no hypothesis. Finish decoding.
2026-01-28 12:40:29,566 | INFO |  -3.96 * 0.5 =  -1.98 for decoder
2026-01-28 12:40:29,566 | INFO |  -2.25 * 0.5 =  -1.13 for ctc
2026-01-28 12:40:29,566 | INFO | total log probability: -3.11
2026-01-28 12:40:29,566 | INFO | normalized log probability: -0.26
2026-01-28 12:40:29,566 | INFO | total number of ended hypotheses: 64
2026-01-28 12:40:29,566 | INFO | best hypo: enfant<space>qui

2026-01-28 12:40:29,567 | INFO | speech length: 10880
2026-01-28 12:40:29,588 | INFO | decoder input length: 16
2026-01-28 12:40:29,588 | INFO | max output length: 16
2026-01-28 12:40:29,588 | INFO | min output length: 1
2026-01-28 12:40:29,993 | INFO | adding <eos> in the last position in the loop
2026-01-28 12:40:29,999 | INFO | no hypothesis. Finish decoding.
2026-01-28 12:40:30,000 | INFO |  -8.03 * 0.5 =  -4.01 for decoder
2026-01-28 12:40:30,000 | INFO | -21.35 * 0.5 = -10.67 for ctc
2026-01-28 12:40:30,000 | INFO | total log probability: -14.69
2026-01-28 12:40:30,000 | INFO | normalized log probability: -1.22
2026-01-28 12:40:30,000 | INFO | total number of ended hypotheses: 49
2026-01-28 12:40:30,000 | INFO | best hypo: le<space>ment<space>le

2026-01-28 12:40:30,001 | INFO | speech length: 8160
2026-01-28 12:40:30,020 | INFO | decoder input length: 12
2026-01-28 12:40:30,020 | INFO | max output length: 12
2026-01-28 12:40:30,020 | INFO | min output length: 1
2026-01-28 12:40:30,337 | INFO | adding <eos> in the last position in the loop
2026-01-28 12:40:30,346 | INFO | no hypothesis. Finish decoding.
2026-01-28 12:40:30,348 | INFO |  -0.72 * 0.5 =  -0.36 for decoder
2026-01-28 12:40:30,348 | INFO |  -1.94 * 0.5 =  -0.97 for ctc
2026-01-28 12:40:30,348 | INFO | total log probability: -1.33
2026-01-28 12:40:30,348 | INFO | normalized log probability: -0.27
2026-01-28 12:40:30,348 | INFO | total number of ended hypotheses: 198
2026-01-28 12:40:30,348 | INFO | best hypo: ici

2026-01-28 12:40:30,350 | INFO | speech length: 268640
2026-01-28 12:40:30,374 | INFO | decoder input length: 419
2026-01-28 12:40:30,374 | INFO | max output length: 419
2026-01-28 12:40:30,374 | INFO | min output length: 41
2026-01-28 12:40:47,545 | INFO | end detected at 321
2026-01-28 12:40:47,547 | INFO | -28.77 * 0.5 = -14.38 for decoder
2026-01-28 12:40:47,547 | INFO |  -8.03 * 0.5 =  -4.02 for ctc
2026-01-28 12:40:47,547 | INFO | total log probability: -18.40
2026-01-28 12:40:47,547 | INFO | normalized log probability: -0.06
2026-01-28 12:40:47,547 | INFO | total number of ended hypotheses: 203
2026-01-28 12:40:47,551 | INFO | best hypo: établissement<space>y<space>en<space>a<space>des<space>moins<space>bons<space>alors<space>le<space>problème<space>si<space>vous<space>voulez<space>qui<space>se<space>pose<space>euh<space>à<space>paris<space>enfin<space>que<space>je<space>vois<space>dans<space>vingtième<space>c'est<space>le<space>problème<space>des<space>écoles<space>maternelles<space>et<space>primaires<space>dans<space>lequel<space>quand<space>vous<space>avez<space>effectivement<space>un<space>certain<space>nombre<space>d'enfants<space>qui<space>ne<space>sont<space>pas<space>qui<space>ne<space>parlent<space>pas<space>couramment<space>français<space>euh

2026-01-28 12:40:47,553 | INFO | speech length: 31200
2026-01-28 12:40:47,580 | INFO | decoder input length: 48
2026-01-28 12:40:47,580 | INFO | max output length: 48
2026-01-28 12:40:47,580 | INFO | min output length: 4
2026-01-28 12:40:48,780 | INFO | adding <eos> in the last position in the loop
2026-01-28 12:40:48,788 | INFO | no hypothesis. Finish decoding.
2026-01-28 12:40:48,789 | INFO |  -3.54 * 0.5 =  -1.77 for decoder
2026-01-28 12:40:48,789 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-28 12:40:48,789 | INFO | total log probability: -1.78
2026-01-28 12:40:48,789 | INFO | normalized log probability: -0.04
2026-01-28 12:40:48,789 | INFO | total number of ended hypotheses: 141
2026-01-28 12:40:48,790 | INFO | best hypo: pour<space>les<space>enseignants<space>c'est<space>c'est<space>pas<space>facile

2026-01-28 12:40:48,791 | INFO | speech length: 83200
2026-01-28 12:40:48,826 | INFO | decoder input length: 129
2026-01-28 12:40:48,826 | INFO | max output length: 129
2026-01-28 12:40:48,826 | INFO | min output length: 12
2026-01-28 12:40:51,965 | INFO | end detected at 89
2026-01-28 12:40:51,967 | INFO |  -6.81 * 0.5 =  -3.41 for decoder
2026-01-28 12:40:51,967 | INFO |  -3.01 * 0.5 =  -1.51 for ctc
2026-01-28 12:40:51,967 | INFO | total log probability: -4.91
2026-01-28 12:40:51,967 | INFO | normalized log probability: -0.06
2026-01-28 12:40:51,967 | INFO | total number of ended hypotheses: 229
2026-01-28 12:40:51,968 | INFO | best hypo: et<space>moi<space>je<space>vois<space>les<space>mamans<space>que<space>je<space>fais<space>travailler<space>dans<space>le<space>vingtième<space>il<space>faudrait

2026-01-28 12:40:51,970 | INFO | speech length: 412480
2026-01-28 12:40:51,995 | INFO | decoder input length: 644
2026-01-28 12:40:51,995 | INFO | max output length: 644
2026-01-28 12:40:51,995 | INFO | min output length: 64
2026-01-28 12:41:27,059 | INFO | end detected at 458
2026-01-28 12:41:27,059 | INFO | -259.15 * 0.5 = -129.58 for decoder
2026-01-28 12:41:27,060 | INFO | -61.03 * 0.5 = -30.52 for ctc
2026-01-28 12:41:27,060 | INFO | total log probability: -160.09
2026-01-28 12:41:27,060 | INFO | normalized log probability: -0.35
2026-01-28 12:41:27,060 | INFO | total number of ended hypotheses: 160
2026-01-28 12:41:27,065 | INFO | best hypo: si<space>j'étais<space>ministre<space>de<space>l'éducation<space>nationale<space>je<space>mettrais<space>beaucoup<space>plus<space>d'enseignants<space>dans<space>les<space>maternels<space>pour<space>qu'un<space>enfant<space>qui<space>sorte<space>de<space>maternel<space>parle<space>obligatoirement<space>français<space>parce<space>que<space>tous<space>ces<space>petits<space>enfants<space>qui<space>arrivent<space>ils<space>vont<space>en<space>maternel<space>donc<space>c'est<space>pas<space>normal<space>qu'ils<space>arrivent<space>en<space>cépé<space>ne<space>parlant<space>pas<space>français<space>mais<space>ils<space>arrivent<space>ainsi<space>ils<space>ne<space>parlant<space>pas<space>français<space>parce<space>que<space>comme<space>ils<space>sont<space>beaucoup<space>pr<space>p<space>nonbreux<space>en<space>matrell<space>on<space>e<space>peut<space>pas<space>leur<space>appredre

2026-01-28 12:41:27,067 | INFO | speech length: 31040
2026-01-28 12:41:27,093 | INFO | decoder input length: 48
2026-01-28 12:41:27,093 | INFO | max output length: 48
2026-01-28 12:41:27,093 | INFO | min output length: 4
2026-01-28 12:41:28,334 | INFO | adding <eos> in the last position in the loop
2026-01-28 12:41:28,342 | INFO | no hypothesis. Finish decoding.
2026-01-28 12:41:28,342 | INFO | -14.54 * 0.5 =  -7.27 for decoder
2026-01-28 12:41:28,343 | INFO | -42.18 * 0.5 = -21.09 for ctc
2026-01-28 12:41:28,343 | INFO | total log probability: -28.36
2026-01-28 12:41:28,343 | INFO | normalized log probability: -0.71
2026-01-28 12:41:28,343 | INFO | total number of ended hypotheses: 102
2026-01-28 12:41:28,343 | INFO | best hypo: pour<space>quand<space>ils<space>en<space>vendent<space>pas<space>ballange

2026-01-28 12:41:28,344 | INFO | speech length: 101280
2026-01-28 12:41:28,371 | INFO | decoder input length: 157
2026-01-28 12:41:28,371 | INFO | max output length: 157
2026-01-28 12:41:28,371 | INFO | min output length: 15
2026-01-28 12:41:33,232 | INFO | end detected at 135
2026-01-28 12:41:33,235 | INFO | -16.25 * 0.5 =  -8.12 for decoder
2026-01-28 12:41:33,235 | INFO |  -9.12 * 0.5 =  -4.56 for ctc
2026-01-28 12:41:33,235 | INFO | total log probability: -12.69
2026-01-28 12:41:33,235 | INFO | normalized log probability: -0.10
2026-01-28 12:41:33,235 | INFO | total number of ended hypotheses: 216
2026-01-28 12:41:33,238 | INFO | best hypo: pourrait<space>imaginer<space>du<space>bassing<space>oui<space>oui<space>d'abord<space>s<space>bah<space>j<space>j'ai<space>vu<space>que<space>ça<space>commençait<space>à<space>faire<space>hein<space>je<space>veux<space>dire<space>ça<space>c'est<space>c'est<space>une

2026-01-28 12:41:33,241 | INFO | speech length: 357440
2026-01-28 12:41:33,289 | INFO | decoder input length: 558
2026-01-28 12:41:33,289 | INFO | max output length: 558
2026-01-28 12:41:33,289 | INFO | min output length: 55
2026-01-28 12:42:01,072 | INFO | end detected at 434
2026-01-28 12:42:01,074 | INFO | -202.83 * 0.5 = -101.42 for decoder
2026-01-28 12:42:01,074 | INFO | -124.55 * 0.5 = -62.28 for ctc
2026-01-28 12:42:01,074 | INFO | total log probability: -163.69
2026-01-28 12:42:01,074 | INFO | normalized log probability: -0.39
2026-01-28 12:42:01,074 | INFO | total number of ended hypotheses: 195
2026-01-28 12:42:01,079 | INFO | best hypo: c'est<space>une<space>des<space>mesures<space>du<space>plan<space>banlieue<space>le<space>bassi<space>mais<space>je<space>pense<space>surtout<space>que<space>euh<space>dans<space>le<space>sixième<space>arrondissement<space>les<space>maternels<space>euh<space>y<space>a<space>suffisamment<space>d'enseignants<space>mais<space>que<space>euh<space>mais<space>que<space>euh<space>dans<space>le<space>vingtième<space>il<space>faudrait<space>qu'<space>y<space>ait<space>euh<space>qu'on<space>se<space>sépare<space>qu'<space>y<space>ait<space>des<space>cours<space>de<space>françai<space>pour<space>les<space>petis<space>enfants<space>qi<space>parlent<space>pa<space>français<space>c'est<space>pas<space>compliqué<space>quand<space>même<space>c'est<space>pas<space>très<space>difficile<space>d'apprendrel<space>français<space>ve<space>cet<space>enfat<space>çace<space>ça<space>ça

2026-01-28 12:42:01,083 | INFO | speech length: 104960
2026-01-28 12:42:01,121 | INFO | decoder input length: 163
2026-01-28 12:42:01,122 | INFO | max output length: 163
2026-01-28 12:42:01,122 | INFO | min output length: 16
2026-01-28 12:42:06,852 | INFO | adding <eos> in the last position in the loop
2026-01-28 12:42:06,861 | INFO | no hypothesis. Finish decoding.
2026-01-28 12:42:06,863 | INFO | -40.06 * 0.5 = -20.03 for decoder
2026-01-28 12:42:06,863 | INFO | -77.33 * 0.5 = -38.66 for ctc
2026-01-28 12:42:06,863 | INFO | total log probability: -58.69
2026-01-28 12:42:06,863 | INFO | normalized log probability: -0.38
2026-01-28 12:42:06,863 | INFO | total number of ended hypotheses: 192
2026-01-28 12:42:06,865 | INFO | best hypo: dont<space>les<space>mamans<space>nepartent<space>pas<space>français<space>donc<space>qui<space>ne<space>voient<space>ps<space>souvent<space>leur<space>papa<space>dans<space>par<space>définition<space>il<space>ne<space>parle<space>pas<space>français<space>ils<space>parlent<space>jamais<space>français

2026-01-28 12:42:06,867 | INFO | speech length: 154880
2026-01-28 12:42:06,899 | INFO | decoder input length: 241
2026-01-28 12:42:06,899 | INFO | max output length: 241
2026-01-28 12:42:06,899 | INFO | min output length: 24
2026-01-28 12:42:14,886 | INFO | end detected at 198
2026-01-28 12:42:14,887 | INFO | -19.64 * 0.5 =  -9.82 for decoder
2026-01-28 12:42:14,887 | INFO | -12.11 * 0.5 =  -6.06 for ctc
2026-01-28 12:42:14,887 | INFO | total log probability: -15.88
2026-01-28 12:42:14,887 | INFO | normalized log probability: -0.08
2026-01-28 12:42:14,887 | INFO | total number of ended hypotheses: 173
2026-01-28 12:42:14,890 | INFO | best hypo: ben<space>pas<space>français<space>à<space>la<space>maison<space>alors<space>euh<space>nous<space>nous<space>essayons<space>d'apprendre<space>le<space>français<space>au<space>moment<space>pour<space>que<space>elles<space>s'apprennent<space>le<space>français<space>à<space>leurs<space>enfants<space>chose<space>que<space>ne<space>fait<space>aucun<space>organisme<space>d'état<space>car

2026-01-28 12:42:14,892 | INFO | speech length: 62560
2026-01-28 12:42:14,923 | INFO | decoder input length: 97
2026-01-28 12:42:14,923 | INFO | max output length: 97
2026-01-28 12:42:14,923 | INFO | min output length: 9
2026-01-28 12:42:17,505 | INFO | end detected at 81
2026-01-28 12:42:17,506 | INFO |  -6.38 * 0.5 =  -3.19 for decoder
2026-01-28 12:42:17,506 | INFO |  -0.19 * 0.5 =  -0.09 for ctc
2026-01-28 12:42:17,506 | INFO | total log probability: -3.28
2026-01-28 12:42:17,506 | INFO | normalized log probability: -0.04
2026-01-28 12:42:17,506 | INFO | total number of ended hypotheses: 164
2026-01-28 12:42:17,507 | INFO | best hypo: des<space>formations<space>linguistiques<space>qui<space>sont<space>données<space>aux<space>aux<space>immigrés<space>qui<space>arrivent

2026-01-28 12:42:17,509 | INFO | speech length: 125920
2026-01-28 12:42:17,548 | INFO | decoder input length: 196
2026-01-28 12:42:17,548 | INFO | max output length: 196
2026-01-28 12:42:17,548 | INFO | min output length: 19
2026-01-28 12:42:22,791 | INFO | end detected at 132
2026-01-28 12:42:22,792 | INFO | -10.14 * 0.5 =  -5.07 for decoder
2026-01-28 12:42:22,792 | INFO |  -1.87 * 0.5 =  -0.94 for ctc
2026-01-28 12:42:22,792 | INFO | total log probability: -6.00
2026-01-28 12:42:22,792 | INFO | normalized log probability: -0.05
2026-01-28 12:42:22,792 | INFO | total number of ended hypotheses: 182
2026-01-28 12:42:22,794 | INFO | best hypo: ne<space>sont<space>pas<space>faites<space>pour<space>les<space>gens<space>qui<space>ne<space>parlent<space>pas<space>du<space>tout<space>ni<space>qui<space>n'ont<space>jamais<space>été<space>à<space>l'école<space>parce<space>que<space>ça<space>personne<space>n'en<space>fait

2026-01-28 12:42:22,796 | INFO | speech length: 33440
2026-01-28 12:42:22,821 | INFO | decoder input length: 51
2026-01-28 12:42:22,821 | INFO | max output length: 51
2026-01-28 12:42:22,821 | INFO | min output length: 5
2026-01-28 12:42:24,184 | INFO | adding <eos> in the last position in the loop
2026-01-28 12:42:24,191 | INFO | no hypothesis. Finish decoding.
2026-01-28 12:42:24,191 | INFO |  -4.06 * 0.5 =  -2.03 for decoder
2026-01-28 12:42:24,191 | INFO |  -1.59 * 0.5 =  -0.79 for ctc
2026-01-28 12:42:24,192 | INFO | total log probability: -2.82
2026-01-28 12:42:24,192 | INFO | normalized log probability: -0.06
2026-01-28 12:42:24,192 | INFO | total number of ended hypotheses: 107
2026-01-28 12:42:24,192 | INFO | best hypo: parce<space>que<space>vous<space>dire<space>qu'apprendre<space>à<space>lire<space>à<space>écrire

2026-01-28 12:42:24,193 | INFO | speech length: 67520
2026-01-28 12:42:24,222 | INFO | decoder input length: 105
2026-01-28 12:42:24,222 | INFO | max output length: 105
2026-01-28 12:42:24,222 | INFO | min output length: 10
2026-01-28 12:42:26,884 | INFO | end detected at 86
2026-01-28 12:42:26,885 | INFO |  -6.91 * 0.5 =  -3.46 for decoder
2026-01-28 12:42:26,885 | INFO |  -2.30 * 0.5 =  -1.15 for ctc
2026-01-28 12:42:26,885 | INFO | total log probability: -4.60
2026-01-28 12:42:26,885 | INFO | normalized log probability: -0.06
2026-01-28 12:42:26,885 | INFO | total number of ended hypotheses: 156
2026-01-28 12:42:26,886 | INFO | best hypo: trente<space>cinq<space>ans<space>qui<space>a<space>jamais<space>tenu<space>un<space>crillon<space>ça<space>se<space>fait<space>pas<space>en<space>deux<space>cents<space>heures

2026-01-28 12:42:26,888 | INFO | speech length: 10560
2026-01-28 12:42:26,912 | INFO | decoder input length: 16
2026-01-28 12:42:26,912 | INFO | max output length: 16
2026-01-28 12:42:26,912 | INFO | min output length: 1
2026-01-28 12:42:27,260 | INFO | end detected at 13
2026-01-28 12:42:27,261 | INFO |  -0.64 * 0.5 =  -0.32 for decoder
2026-01-28 12:42:27,261 | INFO |  -0.03 * 0.5 =  -0.01 for ctc
2026-01-28 12:42:27,261 | INFO | total log probability: -0.33
2026-01-28 12:42:27,261 | INFO | normalized log probability: -0.04
2026-01-28 12:42:27,261 | INFO | total number of ended hypotheses: 135
2026-01-28 12:42:27,262 | INFO | best hypo: oui<space>non

2026-01-28 12:42:27,263 | INFO | speech length: 67680
2026-01-28 12:42:27,286 | INFO | decoder input length: 105
2026-01-28 12:42:27,286 | INFO | max output length: 105
2026-01-28 12:42:27,286 | INFO | min output length: 10
2026-01-28 12:42:29,139 | INFO | end detected at 56
2026-01-28 12:42:29,140 | INFO |  -4.16 * 0.5 =  -2.08 for decoder
2026-01-28 12:42:29,140 | INFO |  -0.42 * 0.5 =  -0.21 for ctc
2026-01-28 12:42:29,140 | INFO | total log probability: -2.29
2026-01-28 12:42:29,140 | INFO | normalized log probability: -0.05
2026-01-28 12:42:29,140 | INFO | total number of ended hypotheses: 187
2026-01-28 12:42:29,141 | INFO | best hypo: donc<space>euh<space>non<space>euh<space>je<space>je<space>pense<space>que<space>le<space>problème<space>euh

2026-01-28 12:42:29,142 | INFO | speech length: 37440
2026-01-28 12:42:29,167 | INFO | decoder input length: 58
2026-01-28 12:42:29,167 | INFO | max output length: 58
2026-01-28 12:42:29,168 | INFO | min output length: 5
2026-01-28 12:42:30,731 | INFO | adding <eos> in the last position in the loop
2026-01-28 12:42:30,738 | INFO | no hypothesis. Finish decoding.
2026-01-28 12:42:30,738 | INFO | -23.43 * 0.5 = -11.72 for decoder
2026-01-28 12:42:30,738 | INFO | -46.37 * 0.5 = -23.19 for ctc
2026-01-28 12:42:30,738 | INFO | total log probability: -34.90
2026-01-28 12:42:30,739 | INFO | normalized log probability: -0.62
2026-01-28 12:42:30,739 | INFO | total number of ended hypotheses: 75
2026-01-28 12:42:30,739 | INFO | best hypo: je<space>dis<space>pas<space>tous<space>les<space>problèmes<space>y<space>a<space>beaucoup<space>de<space>probèmes

2026-01-28 12:42:30,740 | INFO | speech length: 98560
2026-01-28 12:42:30,764 | INFO | decoder input length: 153
2026-01-28 12:42:30,764 | INFO | max output length: 153
2026-01-28 12:42:30,764 | INFO | min output length: 15
2026-01-28 12:42:35,541 | INFO | end detected at 139
2026-01-28 12:42:35,542 | INFO | -18.07 * 0.5 =  -9.04 for decoder
2026-01-28 12:42:35,542 | INFO |  -7.76 * 0.5 =  -3.88 for ctc
2026-01-28 12:42:35,542 | INFO | total log probability: -12.92
2026-01-28 12:42:35,542 | INFO | normalized log probability: -0.10
2026-01-28 12:42:35,543 | INFO | total number of ended hypotheses: 169
2026-01-28 12:42:35,544 | INFO | best hypo: comment<space>voulez<space>vous<space>apprendre<space>à<space>lire<space>un<space>enfant<space>qui<space>parle<space>ma<space>français<space>bon<space>donc<space>par<space>définition<space>cet<space>enfant<space>va<space>perpier<space>en<space>en<space>cépé<space>et<space>puis

2026-01-28 12:42:35,546 | INFO | speech length: 60480
2026-01-28 12:42:35,573 | INFO | decoder input length: 94
2026-01-28 12:42:35,573 | INFO | max output length: 94
2026-01-28 12:42:35,573 | INFO | min output length: 9
2026-01-28 12:42:38,394 | INFO | end detected at 83
2026-01-28 12:42:38,396 | INFO |  -9.48 * 0.5 =  -4.74 for decoder
2026-01-28 12:42:38,396 | INFO |  -3.81 * 0.5 =  -1.90 for ctc
2026-01-28 12:42:38,396 | INFO | total log probability: -6.64
2026-01-28 12:42:38,396 | INFO | normalized log probability: -0.09
2026-01-28 12:42:38,396 | INFO | total number of ended hypotheses: 225
2026-01-28 12:42:38,397 | INFO | best hypo: puis<space>voilà<space>alors<space>bien<space>sûr<space>qu'<space>y<space>en<space>a<space>qui<space>s'en<space>sortent<space>il<space>y<space>a<space>toujours<space>des

2026-01-28 12:42:38,399 | INFO | speech length: 17120
2026-01-28 12:42:38,424 | INFO | decoder input length: 26
2026-01-28 12:42:38,424 | INFO | max output length: 26
2026-01-28 12:42:38,424 | INFO | min output length: 2
2026-01-28 12:42:39,082 | INFO | adding <eos> in the last position in the loop
2026-01-28 12:42:39,090 | INFO | no hypothesis. Finish decoding.
2026-01-28 12:42:39,090 | INFO |  -2.91 * 0.5 =  -1.45 for decoder
2026-01-28 12:42:39,090 | INFO |  -2.47 * 0.5 =  -1.23 for ctc
2026-01-28 12:42:39,090 | INFO | total log probability: -2.69
2026-01-28 12:42:39,090 | INFO | normalized log probability: -0.11
2026-01-28 12:42:39,091 | INFO | total number of ended hypotheses: 104
2026-01-28 12:42:39,091 | INFO | best hypo: telligent<space>quelque<space>part

2026-01-28 12:42:39,092 | INFO | speech length: 17120
2026-01-28 12:42:39,118 | INFO | decoder input length: 26
2026-01-28 12:42:39,118 | INFO | max output length: 26
2026-01-28 12:42:39,118 | INFO | min output length: 2
2026-01-28 12:42:39,854 | INFO | adding <eos> in the last position in the loop
2026-01-28 12:42:39,861 | INFO | no hypothesis. Finish decoding.
2026-01-28 12:42:39,862 | INFO |  -4.41 * 0.5 =  -2.21 for decoder
2026-01-28 12:42:39,862 | INFO | -12.01 * 0.5 =  -6.01 for ctc
2026-01-28 12:42:39,862 | INFO | total log probability: -8.21
2026-01-28 12:42:39,862 | INFO | normalized log probability: -0.33
2026-01-28 12:42:39,862 | INFO | total number of ended hypotheses: 92
2026-01-28 12:42:39,862 | INFO | best hypo: des<space>gens<space>i<space>sont<space>normaux

2026-01-28 12:42:39,864 | INFO | speech length: 26560
2026-01-28 12:42:39,890 | INFO | decoder input length: 41
2026-01-28 12:42:39,890 | INFO | max output length: 41
2026-01-28 12:42:39,890 | INFO | min output length: 4
2026-01-28 12:42:41,206 | INFO | adding <eos> in the last position in the loop
2026-01-28 12:42:41,212 | INFO | no hypothesis. Finish decoding.
2026-01-28 12:42:41,213 | INFO |  -3.57 * 0.5 =  -1.79 for decoder
2026-01-28 12:42:41,214 | INFO |  -5.78 * 0.5 =  -2.89 for ctc
2026-01-28 12:42:41,214 | INFO | total log probability: -4.68
2026-01-28 12:42:41,214 | INFO | normalized log probability: -0.12
2026-01-28 12:42:41,214 | INFO | total number of ended hypotheses: 115
2026-01-28 12:42:41,214 | INFO | best hypo: oui<space>globalement<space>c'est<space>très<space>difficile

2026-01-28 12:42:41,215 | INFO | speech length: 8160
2026-01-28 12:42:41,235 | INFO | decoder input length: 12
2026-01-28 12:42:41,235 | INFO | max output length: 12
2026-01-28 12:42:41,235 | INFO | min output length: 1
2026-01-28 12:42:42,176 | INFO | adding <eos> in the last position in the loop
2026-01-28 12:42:42,186 | INFO | no hypothesis. Finish decoding.
2026-01-28 12:42:42,186 | INFO |  -0.98 * 0.5 =  -0.49 for decoder
2026-01-28 12:42:42,186 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 12:42:42,186 | INFO | total log probability: -0.49
2026-01-28 12:42:42,186 | INFO | normalized log probability: -0.04
2026-01-28 12:42:42,186 | INFO | total number of ended hypotheses: 49
2026-01-28 12:42:42,186 | INFO | best hypo: des<space>enfants<sos/eos>

2026-01-28 12:42:42,186 | WARNING | best hypo length: 12 == max output length: 12
2026-01-28 12:42:42,186 | WARNING | decoding may be stopped by the max output length limitation, please consider to increase the maxlenratio.
2026-01-28 12:42:42,187 | INFO | speech length: 14880
2026-01-28 12:42:42,215 | INFO | decoder input length: 22
2026-01-28 12:42:42,215 | INFO | max output length: 22
2026-01-28 12:42:42,215 | INFO | min output length: 2
2026-01-28 12:42:42,670 | INFO | end detected at 14
2026-01-28 12:42:42,671 | INFO |  -0.72 * 0.5 =  -0.36 for decoder
2026-01-28 12:42:42,671 | INFO |  -0.12 * 0.5 =  -0.06 for ctc
2026-01-28 12:42:42,671 | INFO | total log probability: -0.42
2026-01-28 12:42:42,671 | INFO | normalized log probability: -0.05
2026-01-28 12:42:42,671 | INFO | total number of ended hypotheses: 162
2026-01-28 12:42:42,672 | INFO | best hypo: mélange

2026-01-28 12:42:42,673 | INFO | speech length: 129920
2026-01-28 12:42:42,699 | INFO | decoder input length: 202
2026-01-28 12:42:42,699 | INFO | max output length: 202
2026-01-28 12:42:42,699 | INFO | min output length: 20
2026-01-28 12:42:47,823 | INFO | end detected at 133
2026-01-28 12:42:47,824 | INFO | -11.79 * 0.5 =  -5.90 for decoder
2026-01-28 12:42:47,824 | INFO |  -9.08 * 0.5 =  -4.54 for ctc
2026-01-28 12:42:47,824 | INFO | total log probability: -10.44
2026-01-28 12:42:47,824 | INFO | normalized log probability: -0.08
2026-01-28 12:42:47,824 | INFO | total number of ended hypotheses: 166
2026-01-28 12:42:47,826 | INFO | best hypo: dans<space>soixante<space>pour<space>cent<space>de<space>gamins<space>qui<space>parlent<space>français<space>ça<space>s'installe<space>très<space>bref<space>et<space>le<space>français<space>langue<space>étrangère<space>dans<space>un<space>milieu

2026-01-28 12:42:47,828 | INFO | speech length: 32800
2026-01-28 12:42:47,854 | INFO | decoder input length: 50
2026-01-28 12:42:47,854 | INFO | max output length: 50
2026-01-28 12:42:47,854 | INFO | min output length: 5
2026-01-28 12:42:49,142 | INFO | end detected at 47
2026-01-28 12:42:49,143 | INFO |  -3.26 * 0.5 =  -1.63 for decoder
2026-01-28 12:42:49,143 | INFO |  -0.11 * 0.5 =  -0.06 for ctc
2026-01-28 12:42:49,143 | INFO | total log probability: -1.69
2026-01-28 12:42:49,143 | INFO | normalized log probability: -0.04
2026-01-28 12:42:49,143 | INFO | total number of ended hypotheses: 179
2026-01-28 12:42:49,144 | INFO | best hypo: quatre<space>vingt<space>dix<space>pour<space>cent<space>de<space>la<space>classe

2026-01-28 12:42:49,146 | INFO | speech length: 293440
2026-01-28 12:42:49,176 | INFO | decoder input length: 458
2026-01-28 12:42:49,176 | INFO | max output length: 458
2026-01-28 12:42:49,176 | INFO | min output length: 45
2026-01-28 12:43:11,615 | INFO | end detected at 402
2026-01-28 12:43:11,616 | INFO | -58.81 * 0.5 = -29.40 for decoder
2026-01-28 12:43:11,616 | INFO | -51.05 * 0.5 = -25.52 for ctc
2026-01-28 12:43:11,617 | INFO | total log probability: -54.93
2026-01-28 12:43:11,617 | INFO | normalized log probability: -0.14
2026-01-28 12:43:11,617 | INFO | total number of ended hypotheses: 191
2026-01-28 12:43:11,621 | INFO | best hypo: gère<space>c'est<space>beaucoup<space>bas<space>facile<space>bien<space>sûr<space>et<space>tout<space>bien<space>sûr<space>donc<space>je<space>trouve<space>ça<space>que<space>je<space>parlais<space>de<space>mélange<space>oui<space>mais<space>je<space>pense<space>que<space>vous<space>avez<space>raison<space>vous<space>pense<space>que<space>vous<space>avez<space>raison<space>mais<space>si<space>vous<space>voulez<space>le<space>mélange<space>est<space>est<space>difficile<space>quand<space>ils<space>sont<space>petits<space>parce<space>que<space>quand<space>ils<space>sont<space>folis<space>idée<space>que<space>ça<space>y<space>a<space>passe<space>euh<space>oui<space>je<space>crois<space>que<space>c'est<space>pas<space>tellement<space>bon<space>d'emmener<space>un<space>petit<space>enfant<space>de<space>trois<space>quatre<space>ans<space>euh

2026-01-28 12:43:11,623 | INFO | speech length: 37440
2026-01-28 12:43:11,650 | INFO | decoder input length: 58
2026-01-28 12:43:11,650 | INFO | max output length: 58
2026-01-28 12:43:11,650 | INFO | min output length: 5
2026-01-28 12:43:12,343 | INFO | end detected at 23
2026-01-28 12:43:12,344 | INFO |  -1.48 * 0.5 =  -0.74 for decoder
2026-01-28 12:43:12,345 | INFO |  -3.02 * 0.5 =  -1.51 for ctc
2026-01-28 12:43:12,345 | INFO | total log probability: -2.25
2026-01-28 12:43:12,345 | INFO | normalized log probability: -0.14
2026-01-28 12:43:12,345 | INFO | total number of ended hypotheses: 196
2026-01-28 12:43:12,345 | INFO | best hypo: euh<space>oui<space>en<space>bus

2026-01-28 12:43:12,346 | INFO | speech length: 104800
2026-01-28 12:43:12,372 | INFO | decoder input length: 163
2026-01-28 12:43:12,372 | INFO | max output length: 163
2026-01-28 12:43:12,372 | INFO | min output length: 16
2026-01-28 12:43:17,957 | INFO | end detected at 153
2026-01-28 12:43:17,959 | INFO | -12.28 * 0.5 =  -6.14 for decoder
2026-01-28 12:43:17,959 | INFO |  -2.56 * 0.5 =  -1.28 for ctc
2026-01-28 12:43:17,959 | INFO | total log probability: -7.42
2026-01-28 12:43:17,959 | INFO | normalized log probability: -0.05
2026-01-28 12:43:17,959 | INFO | total number of ended hypotheses: 182
2026-01-28 12:43:17,961 | INFO | best hypo: plus<space>loin<space>parce<space>que<space>déjà<space>c'est<space>quand<space>même<space>pour<space>eux<space>d'aller<space>à<space>l'école<space>c'est<space>pas<space>évident<space>hein<space>et<space>puis<space>i<space>faudra<space>que<space>les<space>parents<space>l'acceptent<space>parce<space>que

2026-01-28 12:43:17,963 | INFO | speech length: 41600
2026-01-28 12:43:17,999 | INFO | decoder input length: 64
2026-01-28 12:43:17,999 | INFO | max output length: 64
2026-01-28 12:43:17,999 | INFO | min output length: 6
2026-01-28 12:43:19,825 | INFO | adding <eos> in the last position in the loop
2026-01-28 12:43:19,834 | INFO | no hypothesis. Finish decoding.
2026-01-28 12:43:19,835 | INFO |  -4.69 * 0.5 =  -2.35 for decoder
2026-01-28 12:43:19,835 | INFO |  -1.54 * 0.5 =  -0.77 for ctc
2026-01-28 12:43:19,835 | INFO | total log probability: -3.12
2026-01-28 12:43:19,835 | INFO | normalized log probability: -0.05
2026-01-28 12:43:19,835 | INFO | total number of ended hypotheses: 186
2026-01-28 12:43:19,836 | INFO | best hypo: les<space>mamans<space>africaines<space>elles<space>ont<space>beaucoup<space>de<space>mal<space>à<space>séparer

2026-01-28 12:43:19,838 | INFO | speech length: 155520
2026-01-28 12:43:19,869 | INFO | decoder input length: 242
2026-01-28 12:43:19,869 | INFO | max output length: 242
2026-01-28 12:43:19,869 | INFO | min output length: 24
2026-01-28 12:43:27,724 | INFO | end detected at 187
2026-01-28 12:43:27,725 | INFO | -17.33 * 0.5 =  -8.67 for decoder
2026-01-28 12:43:27,725 | INFO |  -7.24 * 0.5 =  -3.62 for ctc
2026-01-28 12:43:27,725 | INFO | total log probability: -12.29
2026-01-28 12:43:27,725 | INFO | normalized log probability: -0.07
2026-01-28 12:43:27,726 | INFO | total number of ended hypotheses: 205
2026-01-28 12:43:27,728 | INFO | best hypo: pas<space>très<space>volontiers<space>donc<space>si<space>en<space>plus<space>on<space>leur<space>explique<space>que<space>euh<space>euh<space>ça<space>veut<space>dire<space>que<space>l'enfant<space>restera<space>déjeuner<space>là<space>bas<space>les<space>mamans<space>africaines<space>elles<space>veulent<space>pas<space>euh<space>oui<space>elles<space>veulent<space>pas

2026-01-28 12:43:27,739 | INFO | Chunk: 0 | WER=26.666667 | S=1 D=2 I=1
2026-01-28 12:43:27,739 | INFO | Chunk: 1 | WER=18.750000 | S=3 D=0 I=3
2026-01-28 12:43:27,740 | INFO | Chunk: 2 | WER=80.000000 | S=0 D=4 I=0
2026-01-28 12:43:27,740 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 12:43:27,740 | INFO | Chunk: 4 | WER=50.000000 | S=1 D=1 I=0
2026-01-28 12:43:27,742 | INFO | Chunk: 5 | WER=6.779661 | S=4 D=0 I=0
2026-01-28 12:43:27,742 | INFO | Chunk: 6 | WER=33.333333 | S=0 D=0 I=1
2026-01-28 12:43:27,743 | INFO | Chunk: 7 | WER=22.222222 | S=0 D=0 I=2
2026-01-28 12:43:27,743 | INFO | Chunk: 8 | WER=37.500000 | S=2 D=1 I=0
2026-01-28 12:43:27,743 | INFO | Chunk: 9 | WER=37.500000 | S=1 D=0 I=2
2026-01-28 12:43:27,743 | INFO | Chunk: 10 | WER=66.666667 | S=1 D=1 I=0
2026-01-28 12:43:27,743 | INFO | Chunk: 11 | WER=100.000000 | S=1 D=1 I=1
2026-01-28 12:43:27,743 | INFO | Chunk: 12 | WER=100.000000 | S=1 D=0 I=0
2026-01-28 12:43:27,745 | INFO | Chunk: 13 | WER=10.526316 | S=1 D=3 I=2
2026-01-28 12:43:27,745 | INFO | Chunk: 14 | WER=11.111111 | S=1 D=0 I=0
2026-01-28 12:43:27,746 | INFO | Chunk: 15 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 12:43:27,749 | INFO | Chunk: 16 | WER=15.189873 | S=11 D=0 I=1
2026-01-28 12:43:27,749 | INFO | Chunk: 17 | WER=140.000000 | S=5 D=0 I=2
2026-01-28 12:43:27,750 | INFO | Chunk: 18 | WER=25.000000 | S=3 D=4 I=1
2026-01-28 12:43:27,754 | INFO | Chunk: 19 | WER=29.885057 | S=11 D=10 I=5
2026-01-28 12:43:27,754 | INFO | Chunk: 20 | WER=32.142857 | S=5 D=3 I=1
2026-01-28 12:43:27,755 | INFO | Chunk: 21 | WER=17.142857 | S=3 D=1 I=2
2026-01-28 12:43:27,755 | INFO | Chunk: 22 | WER=20.000000 | S=1 D=0 I=1
2026-01-28 12:43:27,756 | INFO | Chunk: 23 | WER=3.448276 | S=1 D=0 I=0
2026-01-28 12:43:27,756 | INFO | Chunk: 24 | WER=18.181818 | S=1 D=1 I=0
2026-01-28 12:43:27,757 | INFO | Chunk: 25 | WER=5.882353 | S=1 D=0 I=0
2026-01-28 12:43:27,757 | INFO | Chunk: 26 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 12:43:27,757 | INFO | Chunk: 27 | WER=37.500000 | S=0 D=0 I=3
2026-01-28 12:43:27,757 | INFO | Chunk: 28 | WER=46.666667 | S=3 D=4 I=0
2026-01-28 12:43:27,758 | INFO | Chunk: 29 | WER=18.518519 | S=3 D=2 I=0
2026-01-28 12:43:27,758 | INFO | Chunk: 30 | WER=10.000000 | S=0 D=2 I=0
2026-01-28 12:43:27,758 | INFO | Chunk: 31 | WER=33.333333 | S=1 D=0 I=0
2026-01-28 12:43:27,759 | INFO | Chunk: 32 | WER=33.333333 | S=1 D=1 I=0
2026-01-28 12:43:27,759 | INFO | Chunk: 33 | WER=14.285714 | S=0 D=1 I=0
2026-01-28 12:43:27,759 | INFO | Chunk: 34 | WER=100.000000 | S=0 D=0 I=2
2026-01-28 12:43:27,759 | INFO | Chunk: 35 | WER=66.666667 | S=0 D=2 I=0
2026-01-28 12:43:27,760 | INFO | Chunk: 36 | WER=26.086957 | S=3 D=2 I=1
2026-01-28 12:43:27,760 | INFO | Chunk: 37 | WER=11.111111 | S=0 D=1 I=0
2026-01-28 12:43:27,762 | INFO | Chunk: 38 | WER=31.818182 | S=7 D=0 I=14
2026-01-28 12:43:27,763 | INFO | Chunk: 39 | WER=50.000000 | S=2 D=0 I=0
2026-01-28 12:43:27,763 | INFO | Chunk: 40 | WER=6.451613 | S=1 D=0 I=1
2026-01-28 12:43:27,764 | INFO | Chunk: 41 | WER=23.076923 | S=0 D=3 I=0
2026-01-28 12:43:27,764 | INFO | Chunk: 42 | WER=16.129032 | S=0 D=1 I=4
2026-01-28 12:43:28,080 | INFO | File: Rhap-D0002.wav | WER=20.396270 | S=86 D=45 I=44
2026-01-28 12:43:28,080 | INFO | ------------------------------
2026-01-28 12:43:28,080 | INFO | Conf ester Done!
2026-01-28 12:47:30,423 | INFO | Chunk: 0 | WER=33.333333 | S=2 D=3 I=0
2026-01-28 12:47:30,425 | INFO | Chunk: 1 | WER=12.500000 | S=3 D=1 I=0
2026-01-28 12:47:30,425 | INFO | Chunk: 2 | WER=100.000000 | S=2 D=3 I=0
2026-01-28 12:47:30,426 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 12:47:30,427 | INFO | Chunk: 4 | WER=50.000000 | S=2 D=0 I=0
2026-01-28 12:47:30,429 | INFO | Chunk: 5 | WER=35.593220 | S=13 D=7 I=1
2026-01-28 12:47:30,429 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 12:47:30,429 | INFO | Chunk: 7 | WER=11.111111 | S=0 D=1 I=0
2026-01-28 12:47:30,429 | INFO | Chunk: 8 | WER=87.500000 | S=4 D=3 I=0
2026-01-28 12:47:30,430 | INFO | Chunk: 9 | WER=12.500000 | S=0 D=1 I=0
2026-01-28 12:47:30,430 | INFO | Chunk: 10 | WER=100.000000 | S=2 D=1 I=0
2026-01-28 12:47:30,430 | INFO | Chunk: 11 | WER=100.000000 | S=2 D=1 I=0
2026-01-28 12:47:30,430 | INFO | Chunk: 12 | WER=100.000000 | S=1 D=0 I=0
2026-01-28 12:47:30,432 | INFO | Chunk: 13 | WER=21.052632 | S=8 D=3 I=1
2026-01-28 12:47:30,432 | INFO | Chunk: 14 | WER=33.333333 | S=2 D=1 I=0
2026-01-28 12:47:30,432 | INFO | Chunk: 15 | WER=13.333333 | S=1 D=1 I=0
2026-01-28 12:47:30,436 | INFO | Chunk: 16 | WER=8.860759 | S=5 D=2 I=0
2026-01-28 12:47:30,436 | INFO | Chunk: 17 | WER=100.000000 | S=5 D=0 I=0
2026-01-28 12:47:30,436 | INFO | Chunk: 18 | WER=31.250000 | S=5 D=4 I=1
2026-01-28 12:47:30,440 | INFO | Chunk: 19 | WER=25.287356 | S=14 D=8 I=0
2026-01-28 12:47:30,441 | INFO | Chunk: 20 | WER=32.142857 | S=8 D=1 I=0
2026-01-28 12:47:30,442 | INFO | Chunk: 21 | WER=25.714286 | S=6 D=3 I=0
2026-01-28 12:47:30,442 | INFO | Chunk: 22 | WER=30.000000 | S=2 D=0 I=1
2026-01-28 12:47:30,443 | INFO | Chunk: 23 | WER=17.241379 | S=2 D=3 I=0
2026-01-28 12:47:30,443 | INFO | Chunk: 24 | WER=27.272727 | S=1 D=1 I=1
2026-01-28 12:47:30,443 | INFO | Chunk: 25 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 12:47:30,443 | INFO | Chunk: 26 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 12:47:30,443 | INFO | Chunk: 27 | WER=12.500000 | S=0 D=1 I=0
2026-01-28 12:47:30,444 | INFO | Chunk: 28 | WER=20.000000 | S=2 D=1 I=0
2026-01-28 12:47:30,444 | INFO | Chunk: 29 | WER=29.629630 | S=2 D=6 I=0
2026-01-28 12:47:30,445 | INFO | Chunk: 30 | WER=20.000000 | S=2 D=2 I=0
2026-01-28 12:47:30,445 | INFO | Chunk: 31 | WER=66.666667 | S=1 D=0 I=1
2026-01-28 12:47:30,445 | INFO | Chunk: 32 | WER=66.666667 | S=3 D=1 I=0
2026-01-28 12:47:30,445 | INFO | Chunk: 33 | WER=85.714286 | S=3 D=3 I=0
2026-01-28 12:47:30,445 | INFO | Chunk: 34 | WER=100.000000 | S=1 D=1 I=0
2026-01-28 12:47:30,445 | INFO | Chunk: 35 | WER=100.000000 | S=2 D=1 I=0
2026-01-28 12:47:30,446 | INFO | Chunk: 36 | WER=66.666667 | S=0 D=2 I=0
2026-01-28 12:47:30,446 | INFO | Chunk: 37 | WER=34.782609 | S=7 D=1 I=0
2026-01-28 12:47:30,446 | INFO | Chunk: 38 | WER=11.111111 | S=0 D=1 I=0
2026-01-28 12:47:30,449 | INFO | Chunk: 39 | WER=36.363636 | S=13 D=5 I=6
2026-01-28 12:47:30,449 | INFO | Chunk: 40 | WER=50.000000 | S=1 D=1 I=0
2026-01-28 12:47:30,450 | INFO | Chunk: 41 | WER=3.225806 | S=0 D=1 I=0
2026-01-28 12:47:30,450 | INFO | Chunk: 42 | WER=38.461538 | S=3 D=2 I=0
2026-01-28 12:47:30,451 | INFO | Chunk: 43 | WER=9.677419 | S=2 D=0 I=1
2026-01-28 12:47:30,758 | INFO | File: Rhap-D0002.wav | WER=25.783972 | S=132 D=77 I=13
2026-01-28 12:47:30,758 | INFO | ------------------------------
2026-01-28 12:47:30,758 | INFO | hmm_tdnn Done!
2026-01-28 12:47:30,964 | INFO | ==================================Rhap-D0003.wav=========================================
2026-01-28 12:47:31,158 | INFO | Using rVAD model
2026-01-28 12:47:44,574 | INFO | Chunk: 0 | WER=16.129032 | S=3 D=0 I=2
2026-01-28 12:47:44,575 | INFO | Chunk: 1 | WER=15.384615 | S=1 D=1 I=0
2026-01-28 12:47:44,577 | INFO | Chunk: 2 | WER=35.714286 | S=12 D=13 I=0
2026-01-28 12:47:44,581 | INFO | Chunk: 3 | WER=35.576923 | S=15 D=22 I=0
2026-01-28 12:47:44,586 | INFO | Chunk: 4 | WER=33.333333 | S=21 D=14 I=1
2026-01-28 12:47:44,587 | INFO | Chunk: 5 | WER=30.434783 | S=5 D=2 I=0
2026-01-28 12:47:44,587 | INFO | Chunk: 6 | WER=16.666667 | S=2 D=0 I=0
2026-01-28 12:47:44,589 | INFO | Chunk: 7 | WER=35.937500 | S=13 D=10 I=0
2026-01-28 12:47:44,590 | INFO | Chunk: 8 | WER=28.571429 | S=3 D=9 I=0
2026-01-28 12:47:44,591 | INFO | Chunk: 9 | WER=27.777778 | S=4 D=6 I=0
2026-01-28 12:47:44,593 | INFO | Chunk: 10 | WER=31.081081 | S=8 D=15 I=0
2026-01-28 12:47:44,595 | INFO | Chunk: 11 | WER=28.125000 | S=10 D=7 I=1
2026-01-28 12:47:44,599 | INFO | Chunk: 12 | WER=36.190476 | S=13 D=24 I=1
2026-01-28 12:47:44,600 | INFO | Chunk: 13 | WER=14.285714 | S=2 D=0 I=0
2026-01-28 12:47:44,600 | INFO | Chunk: 14 | WER=57.692308 | S=5 D=10 I=0
2026-01-28 12:47:44,601 | INFO | Chunk: 15 | WER=27.272727 | S=0 D=2 I=1
2026-01-28 12:47:44,602 | INFO | Chunk: 16 | WER=28.888889 | S=4 D=9 I=0
2026-01-28 12:47:44,602 | INFO | Chunk: 17 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 12:47:44,602 | INFO | Chunk: 18 | WER=59.090909 | S=3 D=10 I=0
2026-01-28 12:47:44,603 | INFO | Chunk: 19 | WER=55.555556 | S=3 D=7 I=0
2026-01-28 12:47:44,603 | INFO | Chunk: 20 | WER=55.555556 | S=2 D=3 I=0
2026-01-28 12:47:44,603 | INFO | Chunk: 21 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 12:47:44,889 | INFO | File: Rhap-D0003.wav | WER=33.482143 | S=129 D=164 I=7
2026-01-28 12:47:44,889 | INFO | ------------------------------
2026-01-28 12:47:44,890 | INFO | w2vec vad chunk Done!
2026-01-28 12:48:06,483 | INFO | Chunk: 0 | WER=51.612903 | S=3 D=11 I=2
2026-01-28 12:48:06,484 | INFO | Chunk: 1 | WER=15.384615 | S=1 D=1 I=0
2026-01-28 12:48:06,485 | INFO | Chunk: 2 | WER=84.285714 | S=5 D=54 I=0
2026-01-28 12:48:06,487 | INFO | Chunk: 3 | WER=76.923077 | S=8 D=72 I=0
2026-01-28 12:48:06,490 | INFO | Chunk: 4 | WER=67.592593 | S=6 D=67 I=0
2026-01-28 12:48:06,490 | INFO | Chunk: 5 | WER=13.043478 | S=2 D=1 I=0
2026-01-28 12:48:06,490 | INFO | Chunk: 6 | WER=16.666667 | S=2 D=0 I=0
2026-01-28 12:48:06,492 | INFO | Chunk: 7 | WER=45.312500 | S=9 D=20 I=0
2026-01-28 12:48:06,493 | INFO | Chunk: 8 | WER=66.666667 | S=0 D=28 I=0
2026-01-28 12:48:06,494 | INFO | Chunk: 9 | WER=16.666667 | S=4 D=2 I=0
2026-01-28 12:48:06,494 | INFO | Chunk: 10 | WER=89.189189 | S=0 D=66 I=0
2026-01-28 12:48:06,496 | INFO | Chunk: 11 | WER=59.375000 | S=4 D=34 I=0
2026-01-28 12:48:06,497 | INFO | Chunk: 12 | WER=88.571429 | S=4 D=89 I=0
2026-01-28 12:48:06,497 | INFO | Chunk: 13 | WER=7.142857 | S=0 D=1 I=0
2026-01-28 12:48:06,498 | INFO | Chunk: 14 | WER=42.307692 | S=2 D=9 I=0
2026-01-28 12:48:06,498 | INFO | Chunk: 15 | WER=18.181818 | S=0 D=2 I=0
2026-01-28 12:48:06,499 | INFO | Chunk: 16 | WER=22.222222 | S=3 D=7 I=0
2026-01-28 12:48:06,499 | INFO | Chunk: 17 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 12:48:06,500 | INFO | Chunk: 18 | WER=50.000000 | S=0 D=11 I=0
2026-01-28 12:48:06,500 | INFO | Chunk: 19 | WER=44.444444 | S=1 D=7 I=0
2026-01-28 12:48:06,500 | INFO | Chunk: 20 | WER=66.666667 | S=3 D=3 I=0
2026-01-28 12:48:06,500 | INFO | Chunk: 21 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 12:48:06,657 | INFO | File: Rhap-D0003.wav | WER=60.714286 | S=57 D=485 I=2
2026-01-28 12:48:06,658 | INFO | ------------------------------
2026-01-28 12:48:06,658 | INFO | whisper med Done!
2026-01-28 12:48:42,710 | INFO | Chunk: 0 | WER=45.161290 | S=1 D=11 I=2
2026-01-28 12:48:42,711 | INFO | Chunk: 1 | WER=15.384615 | S=1 D=1 I=0
2026-01-28 12:48:42,713 | INFO | Chunk: 2 | WER=42.857143 | S=10 D=20 I=0
2026-01-28 12:48:42,716 | INFO | Chunk: 3 | WER=59.615385 | S=14 D=47 I=1
2026-01-28 12:48:42,719 | INFO | Chunk: 4 | WER=64.814815 | S=4 D=66 I=0
2026-01-28 12:48:42,719 | INFO | Chunk: 5 | WER=13.043478 | S=2 D=1 I=0
2026-01-28 12:48:42,719 | INFO | Chunk: 6 | WER=8.333333 | S=1 D=0 I=0
2026-01-28 12:48:42,721 | INFO | Chunk: 7 | WER=40.625000 | S=11 D=15 I=0
2026-01-28 12:48:42,722 | INFO | Chunk: 8 | WER=57.142857 | S=2 D=22 I=0
2026-01-28 12:48:42,723 | INFO | Chunk: 9 | WER=13.888889 | S=3 D=2 I=0
2026-01-28 12:48:42,725 | INFO | Chunk: 10 | WER=58.108108 | S=5 D=38 I=0
2026-01-28 12:48:42,727 | INFO | Chunk: 11 | WER=48.437500 | S=22 D=6 I=3
2026-01-28 12:48:42,728 | INFO | Chunk: 12 | WER=86.666667 | S=2 D=89 I=0
2026-01-28 12:48:42,728 | INFO | Chunk: 13 | WER=7.142857 | S=0 D=1 I=0
2026-01-28 12:48:42,729 | INFO | Chunk: 14 | WER=38.461538 | S=5 D=5 I=0
2026-01-28 12:48:42,729 | INFO | Chunk: 15 | WER=18.181818 | S=0 D=2 I=0
2026-01-28 12:48:42,730 | INFO | Chunk: 16 | WER=15.555556 | S=4 D=3 I=0
2026-01-28 12:48:42,731 | INFO | Chunk: 17 | WER=100.000000 | S=0 D=0 I=1
2026-01-28 12:48:42,731 | INFO | Chunk: 18 | WER=54.545455 | S=1 D=11 I=0
2026-01-28 12:48:42,731 | INFO | Chunk: 19 | WER=50.000000 | S=6 D=3 I=0
2026-01-28 12:48:42,731 | INFO | Chunk: 20 | WER=66.666667 | S=3 D=3 I=0
2026-01-28 12:48:42,732 | INFO | Chunk: 21 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 12:48:42,945 | INFO | File: Rhap-D0003.wav | WER=50.111607 | S=98 D=345 I=6
2026-01-28 12:48:42,945 | INFO | ------------------------------
2026-01-28 12:48:42,945 | INFO | whisper large Done!
2026-01-28 12:48:43,102 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 12:48:43,132 | INFO | Vocabulary size: 350
2026-01-28 12:48:43,678 | INFO | Gradient checkpoint layers: []
2026-01-28 12:48:44,553 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 12:48:44,556 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 12:48:44,556 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 12:48:44,556 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 12:48:44,557 | INFO | speech length: 168960
2026-01-28 12:48:44,598 | INFO | decoder input length: 263
2026-01-28 12:48:44,599 | INFO | max output length: 263
2026-01-28 12:48:44,599 | INFO | min output length: 26
2026-01-28 12:48:51,445 | INFO | end detected at 75
2026-01-28 12:48:51,446 | INFO | -16.77 * 0.5 =  -8.38 for decoder
2026-01-28 12:48:51,446 | INFO | -23.39 * 0.5 = -11.70 for ctc
2026-01-28 12:48:51,446 | INFO | total log probability: -20.08
2026-01-28 12:48:51,446 | INFO | normalized log probability: -0.29
2026-01-28 12:48:51,446 | INFO | total number of ended hypotheses: 162
2026-01-28 12:48:51,447 | INFO | best hypo: ▁vous▁êtes▁venus▁à▁paris▁à▁quel▁âge▁eh▁bien▁voilà▁mes▁parents▁tous▁les▁deux▁ont▁vécu▁à▁paris▁jeunes▁et▁pierre▁se▁mariant▁y▁sont▁partis▁vivre▁en▁province

2026-01-28 12:48:51,449 | INFO | speech length: 56480
2026-01-28 12:48:51,480 | INFO | decoder input length: 87
2026-01-28 12:48:51,480 | INFO | max output length: 87
2026-01-28 12:48:51,480 | INFO | min output length: 8
2026-01-28 12:48:53,884 | INFO | end detected at 35
2026-01-28 12:48:53,885 | INFO |  -3.84 * 0.5 =  -1.92 for decoder
2026-01-28 12:48:53,885 | INFO |  -5.11 * 0.5 =  -2.55 for ctc
2026-01-28 12:48:53,885 | INFO | total log probability: -4.47
2026-01-28 12:48:53,885 | INFO | normalized log probability: -0.15
2026-01-28 12:48:53,885 | INFO | total number of ended hypotheses: 180
2026-01-28 12:48:53,886 | INFO | best hypo: ▁et▁petit▁à▁petit▁ont▁essayé▁d'avoir▁quelque▁chose▁apparu

2026-01-28 12:48:53,887 | INFO | speech length: 333760
2026-01-28 12:48:53,918 | INFO | decoder input length: 521
2026-01-28 12:48:53,918 | INFO | max output length: 521
2026-01-28 12:48:53,918 | INFO | min output length: 52
2026-01-28 12:49:15,166 | INFO | end detected at 162
2026-01-28 12:49:15,167 | INFO | -397.79 * 0.5 = -198.89 for decoder
2026-01-28 12:49:15,167 | INFO | -163.41 * 0.5 = -81.71 for ctc
2026-01-28 12:49:15,167 | INFO | total log probability: -280.60
2026-01-28 12:49:15,167 | INFO | normalized log probability: -1.81
2026-01-28 12:49:15,167 | INFO | total number of ended hypotheses: 179
2026-01-28 12:49:15,169 | INFO | best hypo: ▁duo▁exploita▁pour▁revenir▁voir▁leurs▁amis▁pippot▁ils▁étaient▁très▁mélomanes▁à▁leur▁avoir▁envie▁de▁renfermer▁la▁musique▁chaleur▁quand▁ils▁pouvaient▁faiser▁çaà▁ce▁qu'ils▁fait▁fais▁au▁moment▁de▁la▁guerre▁nous▁étions▁toujours▁en▁bretagne▁et▁puis▁nous▁avons▁e▁avaient▁ces▁rendroit▁de▁paris▁où▁pouvait▁river▁y▁avaient▁deux▁pièces▁soyées

2026-01-28 12:49:15,171 | INFO | speech length: 466720
2026-01-28 12:49:15,209 | INFO | decoder input length: 728
2026-01-28 12:49:15,210 | INFO | max output length: 728
2026-01-28 12:49:15,210 | INFO | min output length: 72
2026-01-28 12:49:50,499 | INFO | end detected at 226
2026-01-28 12:49:50,501 | INFO | -570.99 * 0.5 = -285.49 for decoder
2026-01-28 12:49:50,501 | INFO | -167.32 * 0.5 = -83.66 for ctc
2026-01-28 12:49:50,501 | INFO | total log probability: -369.15
2026-01-28 12:49:50,501 | INFO | normalized log probability: -1.68
2026-01-28 12:49:50,501 | INFO | total number of ended hypotheses: 149
2026-01-28 12:49:50,503 | INFO | best hypo: ▁finalement▁quand▁j'avais▁été▁sous▁très▁gros▁bombardements▁américains▁puis▁anglais▁et▁nous▁sommes▁après▁le▁bombardement▁en▁gaîment▁mon▁pèredibant▁il▁faut▁partir▁tout▁le▁monde▁part▁et▁lui▁il▁avait▁passé▁les▁quatre▁nuits▁ou▁trois▁nuits▁à▁ramper▁dans▁les▁dégots▁et▁pour▁aider▁les▁gens▁à▁mourir▁puis▁qu'ons▁pouvait▁pas▁sortir▁les▁gens▁de▁là▁'étaient▁impos▁sible▁'on▁s'y▁retrouvés▁sans▁e▁les▁gaz▁et▁les▁téléphone▁électricité▁et▁les▁pompiers▁'étttttte▁la▁villeait▁assez▁haute▁la▁loire▁est▁en▁bas

2026-01-28 12:49:50,506 | INFO | speech length: 446080
2026-01-28 12:49:50,540 | INFO | decoder input length: 696
2026-01-28 12:49:50,540 | INFO | max output length: 696
2026-01-28 12:49:50,540 | INFO | min output length: 69
2026-01-28 12:50:27,265 | INFO | end detected at 244
2026-01-28 12:50:27,266 | INFO | -702.78 * 0.5 = -351.39 for decoder
2026-01-28 12:50:27,266 | INFO | -215.39 * 0.5 = -107.69 for ctc
2026-01-28 12:50:27,266 | INFO | total log probability: -459.09
2026-01-28 12:50:27,266 | INFO | normalized log probability: -1.91
2026-01-28 12:50:27,266 | INFO | total number of ended hypotheses: 137
2026-01-28 12:50:27,269 | INFO | best hypo: ▁et▁les▁malheureux▁allaient▁puiser▁de▁l'eau▁dans▁la▁loire▁et▁remonter▁pour▁arroser▁voyez▁c'était▁c'étaient▁des▁pharaons▁et▁c'était▁assez▁assez▁terribles▁et▁les▁maisons▁brûlées▁et▁il▁avaient▁des▁gens▁ladams▁qui▁étaient▁prisonniers▁et▁'ons▁au▁pouvait▁pas▁alors▁mon▁père▁et▁passer▁ce▁nuits▁à▁faire▁somme▁et▁puis▁quand▁il▁avait▁entendu▁le▁bombardement▁anglais▁le▁d'il▁dergnier▁soi▁et▁il▁la▁guitcha▁sur▁les▁anglais▁parce▁qu'il▁avait▁été▁pilote▁d'escai▁la▁ballante▁les▁jésuites▁la▁l'a▁bien▁reconnu▁et▁la▁nuita▁sur▁les▁anglais

2026-01-28 12:50:27,271 | INFO | speech length: 103200
2026-01-28 12:50:27,305 | INFO | decoder input length: 160
2026-01-28 12:50:27,305 | INFO | max output length: 160
2026-01-28 12:50:27,305 | INFO | min output length: 16
2026-01-28 12:50:32,729 | INFO | end detected at 59
2026-01-28 12:50:32,730 | INFO | -16.97 * 0.5 =  -8.49 for decoder
2026-01-28 12:50:32,730 | INFO | -20.52 * 0.5 = -10.26 for ctc
2026-01-28 12:50:32,731 | INFO | total log probability: -18.75
2026-01-28 12:50:32,731 | INFO | normalized log probability: -0.35
2026-01-28 12:50:32,731 | INFO | total number of ended hypotheses: 164
2026-01-28 12:50:32,731 | INFO | best hypo: ▁il▁faut▁démolir▁ce▁qu'il▁faut▁dévoiler▁avecc▁et▁puis▁il▁a▁décidé▁de▁partir▁lors▁nous▁sommes▁partis▁le▁lendemain

2026-01-28 12:50:32,733 | INFO | speech length: 48960
2026-01-28 12:50:32,764 | INFO | decoder input length: 76
2026-01-28 12:50:32,764 | INFO | max output length: 76
2026-01-28 12:50:32,764 | INFO | min output length: 7
2026-01-28 12:50:35,657 | INFO | end detected at 32
2026-01-28 12:50:35,659 | INFO |  -2.11 * 0.5 =  -1.05 for decoder
2026-01-28 12:50:35,659 | INFO |  -4.73 * 0.5 =  -2.37 for ctc
2026-01-28 12:50:35,659 | INFO | total log probability: -3.42
2026-01-28 12:50:35,659 | INFO | normalized log probability: -0.12
2026-01-28 12:50:35,659 | INFO | total number of ended hypotheses: 166
2026-01-28 12:50:35,660 | INFO | best hypo: ▁nous▁avons▁attendu▁jusqu'à▁sept▁ou▁huit▁heures▁dans▁la▁gare

2026-01-28 12:50:35,661 | INFO | speech length: 252960
2026-01-28 12:50:35,693 | INFO | decoder input length: 394
2026-01-28 12:50:35,693 | INFO | max output length: 394
2026-01-28 12:50:35,693 | INFO | min output length: 39
2026-01-28 12:50:54,044 | INFO | end detected at 147
2026-01-28 12:50:54,045 | INFO | -180.84 * 0.5 = -90.42 for decoder
2026-01-28 12:50:54,045 | INFO | -74.57 * 0.5 = -37.28 for ctc
2026-01-28 12:50:54,045 | INFO | total log probability: -127.70
2026-01-28 12:50:54,045 | INFO | normalized log probability: -0.90
2026-01-28 12:50:54,045 | INFO | total number of ended hypotheses: 167
2026-01-28 12:50:54,047 | INFO | best hypo: ▁parce▁que▁tous▁les▁officiers▁allemands▁tout▁l'état▁major▁allemand▁rentrait▁à▁paris▁alors▁les▁trains▁qui▁conclusent▁cette▁île▁l'habitude▁de▁dire▁quelque▁chose▁à▁se▁passer▁comme▁ça▁n'était▁pas▁normal▁et▁finalement▁nous▁sommes▁itrés▁à▁paris▁nous▁nous▁sommes▁installés▁dans▁un▁petit▁appartement▁et▁nous▁somme▁plus▁jamais▁partis

2026-01-28 12:50:54,049 | INFO | speech length: 241280
2026-01-28 12:50:54,081 | INFO | decoder input length: 376
2026-01-28 12:50:54,081 | INFO | max output length: 376
2026-01-28 12:50:54,081 | INFO | min output length: 37
2026-01-28 12:51:05,704 | INFO | end detected at 97
2026-01-28 12:51:05,705 | INFO | -81.09 * 0.5 = -40.54 for decoder
2026-01-28 12:51:05,705 | INFO | -27.64 * 0.5 = -13.82 for ctc
2026-01-28 12:51:05,705 | INFO | total log probability: -54.36
2026-01-28 12:51:05,705 | INFO | normalized log probability: -0.60
2026-01-28 12:51:05,705 | INFO | total number of ended hypotheses: 163
2026-01-28 12:51:05,706 | INFO | best hypo: ▁un▁toujours▁resté▁à▁paris▁en▁allant▁fidèlement▁en▁bretagne▁souvent▁et▁l'été▁mais▁nous▁vivions▁à▁paris▁puis▁que▁vous▁êtes▁vraiment▁parisiennes▁ouissez▁pas▁oui▁oui▁mais▁comme▁les▁parents▁tous▁les▁deux▁ont▁vécu

2026-01-28 12:51:05,708 | INFO | speech length: 175840
2026-01-28 12:51:05,740 | INFO | decoder input length: 274
2026-01-28 12:51:05,740 | INFO | max output length: 274
2026-01-28 12:51:05,740 | INFO | min output length: 27
2026-01-28 12:51:13,252 | INFO | end detected at 73
2026-01-28 12:51:13,253 | INFO | -31.47 * 0.5 = -15.74 for decoder
2026-01-28 12:51:13,254 | INFO | -10.33 * 0.5 =  -5.17 for ctc
2026-01-28 12:51:13,254 | INFO | total log probability: -20.90
2026-01-28 12:51:13,254 | INFO | normalized log probability: -0.32
2026-01-28 12:51:13,254 | INFO | total number of ended hypotheses: 205
2026-01-28 12:51:13,254 | INFO | best hypo: ▁toute▁leur▁jeunesse▁à▁paris▁puis▁nous▁dans▁ma▁famille▁aussi▁avant▁oui▁enfin▁n'est▁attachée▁à▁paris▁je▁me▁rends▁compte▁que▁je▁suis▁attaché▁à▁paris▁malgré▁les

2026-01-28 12:51:13,256 | INFO | speech length: 440000
2026-01-28 12:51:13,288 | INFO | decoder input length: 687
2026-01-28 12:51:13,288 | INFO | max output length: 687
2026-01-28 12:51:13,288 | INFO | min output length: 68
2026-01-28 12:51:39,354 | INFO | end detected at 172
2026-01-28 12:51:39,356 | INFO | -388.52 * 0.5 = -194.26 for decoder
2026-01-28 12:51:39,356 | INFO | -140.35 * 0.5 = -70.18 for ctc
2026-01-28 12:51:39,356 | INFO | total log probability: -264.43
2026-01-28 12:51:39,356 | INFO | normalized log probability: -1.63
2026-01-28 12:51:39,356 | INFO | total number of ended hypotheses: 200
2026-01-28 12:51:39,358 | INFO | best hypo: ▁malgré▁la▁vie▁très▁différente▁maintenant▁et▁donc▁on▁était▁arrivé▁à▁vous▁votre▁arrivée▁à▁paris▁et▁donc▁depuisque▁vous▁avez▁toujours▁habité▁à▁paris▁oui▁toujours▁paris▁et▁dans▁quel▁dans▁quel▁quartier▁d'quels▁nous▁avongeons▁il▁est▁continué▁à▁habiter▁le▁vingt▁huitième▁là▁où▁nous▁étions▁était▁un▁quartiertrêmement▁en▁sympaothique▁mais▁apartements▁était▁un▁peu▁petit▁allé▁à▁l'école

2026-01-28 12:51:39,360 | INFO | speech length: 356640
2026-01-28 12:51:39,394 | INFO | decoder input length: 556
2026-01-28 12:51:39,394 | INFO | max output length: 556
2026-01-28 12:51:39,394 | INFO | min output length: 55
2026-01-28 12:52:01,207 | INFO | end detected at 147
2026-01-28 12:52:01,209 | INFO | -319.91 * 0.5 = -159.95 for decoder
2026-01-28 12:52:01,209 | INFO | -93.47 * 0.5 = -46.74 for ctc
2026-01-28 12:52:01,209 | INFO | total log probability: -206.69
2026-01-28 12:52:01,209 | INFO | normalized log probability: -1.50
2026-01-28 12:52:01,209 | INFO | total number of ended hypotheses: 206
2026-01-28 12:52:01,211 | INFO | best hypo: ▁les▁parents▁connaissaient▁avec▁beaucoup▁d'amis▁lorsque▁l'art▁ça▁fut▁un▁quartier▁très▁agréable▁et▁après▁nous▁avons▁été▁habités▁dans▁le▁seizième▁pas▁très▁loin▁d'ici▁et▁à▁la▁fin▁de▁leurs▁ies▁mes▁parents▁on▁été▁vivres▁à▁cas▁d'unes▁puis▁à▁nouveau▁dans▁un▁petit▁appartement▁ils▁ataient▁deux▁où▁étaient▁très▁comptant▁là

2026-01-28 12:52:01,213 | INFO | speech length: 476800
2026-01-28 12:52:01,248 | INFO | decoder input length: 744
2026-01-28 12:52:01,248 | INFO | max output length: 744
2026-01-28 12:52:01,248 | INFO | min output length: 74
2026-01-28 12:52:35,762 | INFO | end detected at 224
2026-01-28 12:52:35,764 | INFO | -538.73 * 0.5 = -269.36 for decoder
2026-01-28 12:52:35,764 | INFO | -193.00 * 0.5 = -96.50 for ctc
2026-01-28 12:52:35,764 | INFO | total log probability: -365.86
2026-01-28 12:52:35,764 | INFO | normalized log probability: -1.68
2026-01-28 12:52:35,764 | INFO | total number of ended hypotheses: 173
2026-01-28 12:52:35,767 | INFO | best hypo: ▁facile▁à▁vivre▁ensoleillés▁interagréables▁et▁j'étais▁très▁contente▁se▁plaisez▁beaucoup▁mais▁vous▁et▁votre▁mari▁votre▁famille▁avec▁leurs▁enfants▁vous▁êtes▁toujours▁res▁à▁paris▁d'adis▁oui▁nous▁sommes▁en▁resttés▁à▁pariss▁ou▁plusieurs▁fois▁de▁propositions▁nous▁s'en▁aller▁finalement▁mais▁ça▁s'est▁pas▁fait▁exavier▁d'instants▁là▁si▁on▁changerait▁de▁situations▁on▁avait▁plusieurs▁au▁propositions▁de▁rapaidement▁allons▁on▁changissez▁ce▁qu'on▁voulait▁ce▁comme▁x▁à▁se▁passer▁ils▁étaient▁ces▁cés

2026-01-28 12:52:35,769 | INFO | speech length: 56000
2026-01-28 12:52:35,800 | INFO | decoder input length: 87
2026-01-28 12:52:35,800 | INFO | max output length: 87
2026-01-28 12:52:35,800 | INFO | min output length: 8
2026-01-28 12:52:39,207 | INFO | end detected at 37
2026-01-28 12:52:39,208 | INFO |  -8.46 * 0.5 =  -4.23 for decoder
2026-01-28 12:52:39,208 | INFO |  -6.63 * 0.5 =  -3.31 for ctc
2026-01-28 12:52:39,208 | INFO | total log probability: -7.54
2026-01-28 12:52:39,208 | INFO | normalized log probability: -0.23
2026-01-28 12:52:39,208 | INFO | total number of ended hypotheses: 168
2026-01-28 12:52:39,208 | INFO | best hypo: ▁ettonnant▁à▁dire▁maintenant▁oui▁et▁c'est▁comme▁ça▁que▁ça▁se▁passait

2026-01-28 12:52:39,210 | INFO | speech length: 64960
2026-01-28 12:52:39,240 | INFO | decoder input length: 101
2026-01-28 12:52:39,241 | INFO | max output length: 101
2026-01-28 12:52:39,241 | INFO | min output length: 10
2026-01-28 12:52:44,017 | INFO | end detected at 52
2026-01-28 12:52:44,019 | INFO |  -5.75 * 0.5 =  -2.87 for decoder
2026-01-28 12:52:44,019 | INFO | -12.36 * 0.5 =  -6.18 for ctc
2026-01-28 12:52:44,019 | INFO | total log probability: -9.05
2026-01-28 12:52:44,019 | INFO | normalized log probability: -0.21
2026-01-28 12:52:44,019 | INFO | total number of ended hypotheses: 189
2026-01-28 12:52:44,019 | INFO | best hypo: ▁marie▁a▁changé▁plusieurs▁fois▁de▁situation▁pas▁beaucoup▁d'ailleurs▁une▁gens▁qui▁change▁beaucoup▁de▁lieu

2026-01-28 12:52:44,021 | INFO | speech length: 68960
2026-01-28 12:52:44,053 | INFO | decoder input length: 107
2026-01-28 12:52:44,053 | INFO | max output length: 107
2026-01-28 12:52:44,053 | INFO | min output length: 10
2026-01-28 12:52:46,919 | INFO | end detected at 33
2026-01-28 12:52:46,921 | INFO |  -6.42 * 0.5 =  -3.21 for decoder
2026-01-28 12:52:46,921 | INFO | -11.38 * 0.5 =  -5.69 for ctc
2026-01-28 12:52:46,921 | INFO | total log probability: -8.90
2026-01-28 12:52:46,921 | INFO | normalized log probability: -0.36
2026-01-28 12:52:46,921 | INFO | total number of ended hypotheses: 184
2026-01-28 12:52:46,921 | INFO | best hypo: ▁mais▁les▁huit▁jours▁suivants▁il▁arrête▁plusieurs▁propositions▁à▁une

2026-01-28 12:52:46,923 | INFO | speech length: 176320
2026-01-28 12:52:46,954 | INFO | decoder input length: 275
2026-01-28 12:52:46,955 | INFO | max output length: 275
2026-01-28 12:52:46,955 | INFO | min output length: 27
2026-01-28 12:52:57,303 | INFO | end detected at 94
2026-01-28 12:52:57,305 | INFO | -91.85 * 0.5 = -45.93 for decoder
2026-01-28 12:52:57,305 | INFO | -42.30 * 0.5 = -21.15 for ctc
2026-01-28 12:52:57,305 | INFO | total log probability: -67.08
2026-01-28 12:52:57,305 | INFO | normalized log probability: -0.77
2026-01-28 12:52:57,305 | INFO | total number of ended hypotheses: 165
2026-01-28 12:52:57,306 | INFO | best hypo: ▁presque▁sentiment▁beaucoup▁bouché▁lui▁même▁enfin▁la▁fouse▁et▁comme▁zax▁à▁se▁passer▁dont▁nous▁sommes▁restés▁à▁paris▁mon▁mari▁est▁né▁tout▁près▁d'ici▁il▁a▁vécu▁toute▁sa▁vie▁pas▁loin▁d'ici

2026-01-28 12:52:57,308 | INFO | speech length: 12000
2026-01-28 12:52:57,343 | INFO | decoder input length: 18
2026-01-28 12:52:57,343 | INFO | max output length: 18
2026-01-28 12:52:57,343 | INFO | min output length: 1
2026-01-28 12:52:58,271 | INFO | end detected at 9
2026-01-28 12:52:58,273 | INFO |  -0.48 * 0.5 =  -0.24 for decoder
2026-01-28 12:52:58,273 | INFO |  -3.45 * 0.5 =  -1.72 for ctc
2026-01-28 12:52:58,273 | INFO | total log probability: -1.97
2026-01-28 12:52:58,273 | INFO | normalized log probability: -0.49
2026-01-28 12:52:58,273 | INFO | total number of ended hypotheses: 180
2026-01-28 12:52:58,273 | INFO | best hypo: ▁hey

2026-01-28 12:52:58,274 | INFO | speech length: 102080
2026-01-28 12:52:58,305 | INFO | decoder input length: 159
2026-01-28 12:52:58,305 | INFO | max output length: 159
2026-01-28 12:52:58,306 | INFO | min output length: 15
2026-01-28 12:53:03,940 | INFO | end detected at 52
2026-01-28 12:53:03,943 | INFO | -18.65 * 0.5 =  -9.32 for decoder
2026-01-28 12:53:03,943 | INFO | -30.66 * 0.5 = -15.33 for ctc
2026-01-28 12:53:03,943 | INFO | total log probability: -24.66
2026-01-28 12:53:03,943 | INFO | normalized log probability: -0.57
2026-01-28 12:53:03,943 | INFO | total number of ended hypotheses: 216
2026-01-28 12:53:03,944 | INFO | best hypo: ▁il▁étaitsse▁il▁était▁attaché▁à▁paris▁morphage▁nous▁serions▁partis▁volontiers▁oui▁conway▁aveugle

2026-01-28 12:53:03,946 | INFO | speech length: 101280
2026-01-28 12:53:03,978 | INFO | decoder input length: 157
2026-01-28 12:53:03,978 | INFO | max output length: 157
2026-01-28 12:53:03,978 | INFO | min output length: 15
2026-01-28 12:53:09,507 | INFO | end detected at 50
2026-01-28 12:53:09,509 | INFO | -11.80 * 0.5 =  -5.90 for decoder
2026-01-28 12:53:09,509 | INFO | -31.71 * 0.5 = -15.85 for ctc
2026-01-28 12:53:09,509 | INFO | total log probability: -21.75
2026-01-28 12:53:09,509 | INFO | normalized log probability: -0.53
2026-01-28 12:53:09,509 | INFO | total number of ended hypotheses: 200
2026-01-28 12:53:09,509 | INFO | best hypo: ▁les▁projets▁de▁partir▁vers▁l'étranger▁se▁vouent▁donc▁remuer▁non▁à▁l'étranger

2026-01-28 12:53:09,511 | INFO | speech length: 44640
2026-01-28 12:53:09,542 | INFO | decoder input length: 69
2026-01-28 12:53:09,542 | INFO | max output length: 69
2026-01-28 12:53:09,542 | INFO | min output length: 6
2026-01-28 12:53:12,208 | INFO | end detected at 26
2026-01-28 12:53:12,209 | INFO |  -3.85 * 0.5 =  -1.93 for decoder
2026-01-28 12:53:12,210 | INFO | -10.23 * 0.5 =  -5.12 for ctc
2026-01-28 12:53:12,210 | INFO | total log probability: -7.04
2026-01-28 12:53:12,210 | INFO | normalized log probability: -0.37
2026-01-28 12:53:12,210 | INFO | total number of ended hypotheses: 165
2026-01-28 12:53:12,210 | INFO | best hypo: ▁il▁est▁finalement▁bon▁choisi▁de▁rester▁dans▁lui

2026-01-28 12:53:12,212 | INFO | speech length: 45440
2026-01-28 12:53:12,251 | INFO | decoder input length: 70
2026-01-28 12:53:12,251 | INFO | max output length: 70
2026-01-28 12:53:12,251 | INFO | min output length: 7
2026-01-28 12:53:13,985 | INFO | end detected at 17
2026-01-28 12:53:13,986 | INFO |  -4.38 * 0.5 =  -2.19 for decoder
2026-01-28 12:53:13,986 | INFO |  -1.89 * 0.5 =  -0.95 for ctc
2026-01-28 12:53:13,986 | INFO | total log probability: -3.13
2026-01-28 12:53:13,986 | INFO | normalized log probability: -0.28
2026-01-28 12:53:13,986 | INFO | total number of ended hypotheses: 163
2026-01-28 12:53:13,986 | INFO | best hypo: ▁oui▁donc▁pour▁votre▁âme

2026-01-28 12:53:13,993 | INFO | Chunk: 0 | WER=16.129032 | S=3 D=1 I=1
2026-01-28 12:53:13,993 | INFO | Chunk: 1 | WER=23.076923 | S=1 D=2 I=0
2026-01-28 12:53:13,996 | INFO | Chunk: 2 | WER=51.428571 | S=21 D=13 I=2
2026-01-28 12:53:14,000 | INFO | Chunk: 3 | WER=36.538462 | S=18 D=17 I=3
2026-01-28 12:53:14,005 | INFO | Chunk: 4 | WER=42.592593 | S=32 D=11 I=3
2026-01-28 12:53:14,006 | INFO | Chunk: 5 | WER=26.086957 | S=5 D=1 I=0
2026-01-28 12:53:14,006 | INFO | Chunk: 6 | WER=16.666667 | S=2 D=0 I=0
2026-01-28 12:53:14,008 | INFO | Chunk: 7 | WER=34.375000 | S=15 D=7 I=0
2026-01-28 12:53:14,009 | INFO | Chunk: 8 | WER=23.809524 | S=6 D=4 I=0
2026-01-28 12:53:14,010 | INFO | Chunk: 9 | WER=27.777778 | S=5 D=5 I=0
2026-01-28 12:53:14,013 | INFO | Chunk: 10 | WER=37.837838 | S=12 D=12 I=4
2026-01-28 12:53:14,015 | INFO | Chunk: 11 | WER=37.500000 | S=15 D=6 I=3
2026-01-28 12:53:14,019 | INFO | Chunk: 12 | WER=55.238095 | S=32 D=22 I=4
2026-01-28 12:53:14,020 | INFO | Chunk: 13 | WER=14.285714 | S=2 D=0 I=0
2026-01-28 12:53:14,020 | INFO | Chunk: 14 | WER=53.846154 | S=6 D=8 I=0
2026-01-28 12:53:14,021 | INFO | Chunk: 15 | WER=45.454545 | S=1 D=2 I=2
2026-01-28 12:53:14,022 | INFO | Chunk: 16 | WER=33.333333 | S=9 D=6 I=0
2026-01-28 12:53:14,022 | INFO | Chunk: 17 | WER=100.000000 | S=1 D=0 I=0
2026-01-28 12:53:14,022 | INFO | Chunk: 18 | WER=50.000000 | S=4 D=7 I=0
2026-01-28 12:53:14,023 | INFO | Chunk: 19 | WER=44.444444 | S=5 D=3 I=0
2026-01-28 12:53:14,023 | INFO | Chunk: 20 | WER=66.666667 | S=4 D=1 I=1
2026-01-28 12:53:14,023 | INFO | Chunk: 21 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 12:53:14,326 | INFO | File: Rhap-D0003.wav | WER=39.174107 | S=201 D=127 I=23
2026-01-28 12:53:14,327 | INFO | ------------------------------
2026-01-28 12:53:14,327 | INFO | Conf cv Done!
2026-01-28 12:53:14,486 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 12:53:14,504 | INFO | Vocabulary size: 47
2026-01-28 12:53:15,125 | INFO | Gradient checkpoint layers: []
2026-01-28 12:53:16,046 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 12:53:16,049 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 12:53:16,049 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 12:53:16,050 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 12:53:16,052 | INFO | speech length: 168960
2026-01-28 12:53:16,082 | INFO | decoder input length: 263
2026-01-28 12:53:16,082 | INFO | max output length: 263
2026-01-28 12:53:16,082 | INFO | min output length: 26
2026-01-28 12:53:35,012 | INFO | end detected at 170
2026-01-28 12:53:35,014 | INFO | -14.36 * 0.5 =  -7.18 for decoder
2026-01-28 12:53:35,014 | INFO |  -5.89 * 0.5 =  -2.95 for ctc
2026-01-28 12:53:35,014 | INFO | total log probability: -10.13
2026-01-28 12:53:35,014 | INFO | normalized log probability: -0.06
2026-01-28 12:53:35,014 | INFO | total number of ended hypotheses: 191
2026-01-28 12:53:35,016 | INFO | best hypo: quand<space>vous<space>êtes<space>venu<space>à<space>paris<space>à<space>quel<space>âge<space>eh<space>bien<space>euh<space>voilà<space>mes<space>parents<space>tous<space>les<space>deux<space>ont<space>vécu<space>à<space>paris<space>jeunes<space>et<space>puis<space>en<space>se<space>mariant<space>i<space>sont<space>partis<space>vivre<space>en<space>province

2026-01-28 12:53:35,018 | INFO | speech length: 56480
2026-01-28 12:53:35,050 | INFO | decoder input length: 87
2026-01-28 12:53:35,050 | INFO | max output length: 87
2026-01-28 12:53:35,050 | INFO | min output length: 8
2026-01-28 12:53:41,683 | INFO | end detected at 71
2026-01-28 12:53:41,685 | INFO |  -5.78 * 0.5 =  -2.89 for decoder
2026-01-28 12:53:41,685 | INFO |  -3.28 * 0.5 =  -1.64 for ctc
2026-01-28 12:53:41,685 | INFO | total log probability: -4.53
2026-01-28 12:53:41,685 | INFO | normalized log probability: -0.07
2026-01-28 12:53:41,685 | INFO | total number of ended hypotheses: 222
2026-01-28 12:53:41,686 | INFO | best hypo: et<space>petit<space>à<space>petit<space>i<space>s<space>ont<space>essayé<space>d'avoir<space>quelque<space>chose<space>à<space>part

2026-01-28 12:53:41,687 | INFO | speech length: 333760
2026-01-28 12:53:41,714 | INFO | decoder input length: 521
2026-01-28 12:53:41,714 | INFO | max output length: 521
2026-01-28 12:53:41,714 | INFO | min output length: 52
2026-01-28 12:54:24,913 | INFO | end detected at 383
2026-01-28 12:54:24,915 | INFO | -72.95 * 0.5 = -36.47 for decoder
2026-01-28 12:54:24,915 | INFO | -24.21 * 0.5 = -12.11 for ctc
2026-01-28 12:54:24,915 | INFO | total log probability: -48.58
2026-01-28 12:54:24,915 | INFO | normalized log probability: -0.13
2026-01-28 12:54:24,915 | INFO | total number of ended hypotheses: 201
2026-01-28 12:54:24,919 | INFO | best hypo: judio<space>ou<space>quelque<space>cho<space>comme<space>ça<space>pour<space>revenir<space>euh<space>voir<space>leurs<space>amis<space>puis<space>pour<space>i<space>s<space>étaient<space>très<space>mélomanes<space>alors<space>avoir<space>envie<space>de<space>on<space>parle<space>de<space>la<space>musique<space>tout<space>alors<space>quand<space>i<space>pouvaient<space>faisait<space>ça<space>ce<space>qui<space>fait<space>que<space>au<space>moment<space>de<space>la<space>guerre<space>nous<space>étions<space>toujours<space>en<space>bretagne<space>et<space>puis<space>nous<space>avons<space>été<space>euh<space>y<space>avait<space>cet<space>endroit<space>à<space>paris<space>où<space>on<space>pouvait<space>arriver<space>y<space>avait<space>deux<space>pièces<space>foyer<space>mais<space>euh<space>euh

2026-01-28 12:54:24,922 | INFO | speech length: 466720
2026-01-28 12:54:24,950 | INFO | decoder input length: 728
2026-01-28 12:54:24,950 | INFO | max output length: 728
2026-01-28 12:54:24,950 | INFO | min output length: 72
2026-01-28 12:55:35,312 | INFO | end detected at 525
2026-01-28 12:55:35,313 | INFO | -447.23 * 0.5 = -223.62 for decoder
2026-01-28 12:55:35,313 | INFO | -76.37 * 0.5 = -38.19 for ctc
2026-01-28 12:55:35,313 | INFO | total log probability: -261.80
2026-01-28 12:55:35,313 | INFO | normalized log probability: -0.51
2026-01-28 12:55:35,313 | INFO | total number of ended hypotheses: 170
2026-01-28 12:55:35,319 | INFO | best hypo: finalement<space>euh<space>quand<space>euh<space>j'avons<space>été<space>ce<space>très<space>gros<space>bombardement<space>américain<space>puis<space>anglais<space>et<space>nous<space>sommes<space>au<space>au<space>après<space>le<space>bombardement<space>anglais<space>mon<space>mon<space>père<space>a<space>lit<space>bon<space>il<space>f<space>i<space>faut<space>partir<space>tout<space>le<space>monde<space>partait<space>lui<space>il<space>avait<space>passé<space>les<space>quatre<space>nuits<space>ou<space>trois<space>nuits<space>à<space>à<space>rentrer<space>dans<space>les<space>décomptes<space>pour<space>aider<space>les<space>gens<space>à<space>mourir<space>parce<space>qu'on<space>pouvait<space>pas<space>sortir<space>les<space>gens<space>de<space>là<space>c'était<space>impossible<space>on<space>s'est<space>retrouvés<space>sans<space>eau<space>les<space>gavr<space>les<space>téléphone<space>l'électricit<space>et<space>les<space>pompiers<space>t<space>a<space>a<space>ten<space>la<space>ville<space>est<space>assez<space>haute<space>et<space>la<space>la<space>loi<space>est<space>en<space>bas

2026-01-28 12:55:35,321 | INFO | speech length: 446080
2026-01-28 12:55:35,349 | INFO | decoder input length: 696
2026-01-28 12:55:35,350 | INFO | max output length: 696
2026-01-28 12:55:35,350 | INFO | min output length: 69
2026-01-28 12:56:46,287 | INFO | end detected at 537
2026-01-28 12:56:46,289 | INFO | -563.84 * 0.5 = -281.92 for decoder
2026-01-28 12:56:46,289 | INFO | -132.03 * 0.5 = -66.01 for ctc
2026-01-28 12:56:46,289 | INFO | total log probability: -347.93
2026-01-28 12:56:46,289 | INFO | normalized log probability: -0.66
2026-01-28 12:56:46,289 | INFO | total number of ended hypotheses: 207
2026-01-28 12:56:46,295 | INFO | best hypo: elle<space>est<space>malheureux<space>à<space>aller<space>puiser<space>de<space>l'eau<space>dans<space>la<space>loire<space>et<space>remonter<space>pour<space>arroser<space>voyez<space>c'était<space>c'était<space>effarant<space>oui<space>c'était<space>assez<space>assez<space>terrible<space>et<space>les<space>les<space>maisons<space>brûlées<space>et<space>y<space>avait<space>des<space>gens<space>là<space>dedans<space>qui<space>étaient<space>prisonniers<space>on<space>pouvait<space>pas<space>alors<space>mon<space>père<space>avait<space>passé<space>ses<space>nuits<space>à<space>faire<space>ça<space>et<space>puis<space>quand<space>il<space>a<space>entendu<space>le<space>bombardement<space>anglais<space>le<space>d<space>le<space>dernier<space>soir<space>il<space>l'a<space>dit<space>chasson<space>les<space>anglais<space>c'est<space>poarce<space>qu'il<space>avait<space>été<space>pilone<space>déchet<space>bon<space>bon<space>bon<space>bon<space>des<space>dies<space>set<space>eshuit<space>alors<space>e<space>il<space>l'a<space>bien<space>reconnu<space>l'andit<space>fason<space>les<space>anglais

2026-01-28 12:56:46,297 | INFO | speech length: 103200
2026-01-28 12:56:46,325 | INFO | decoder input length: 160
2026-01-28 12:56:46,325 | INFO | max output length: 160
2026-01-28 12:56:46,325 | INFO | min output length: 16
2026-01-28 12:56:57,033 | INFO | end detected at 120
2026-01-28 12:56:57,035 | INFO | -14.17 * 0.5 =  -7.08 for decoder
2026-01-28 12:56:57,035 | INFO |  -7.81 * 0.5 =  -3.91 for ctc
2026-01-28 12:56:57,035 | INFO | total log probability: -10.99
2026-01-28 12:56:57,035 | INFO | normalized log probability: -0.10
2026-01-28 12:56:57,035 | INFO | total number of ended hypotheses: 191
2026-01-28 12:56:57,037 | INFO | best hypo: il<space>voulait<space>me<space>dire<space>ce<space>qu'i<space>faut<space>dévolir<space>etc<space>et<space>puis<space>il<space>a<space>décidé<space>de<space>partir<space>alors<space>nous<space>sommes<space>partis<space>le<space>lendemain

2026-01-28 12:56:57,039 | INFO | speech length: 48960
2026-01-28 12:56:57,067 | INFO | decoder input length: 76
2026-01-28 12:56:57,067 | INFO | max output length: 76
2026-01-28 12:56:57,068 | INFO | min output length: 7
2026-01-28 12:57:02,313 | INFO | end detected at 67
2026-01-28 12:57:02,314 | INFO |  -6.20 * 0.5 =  -3.10 for decoder
2026-01-28 12:57:02,315 | INFO |  -2.86 * 0.5 =  -1.43 for ctc
2026-01-28 12:57:02,315 | INFO | total log probability: -4.53
2026-01-28 12:57:02,315 | INFO | normalized log probability: -0.07
2026-01-28 12:57:02,315 | INFO | total number of ended hypotheses: 179
2026-01-28 12:57:02,315 | INFO | best hypo: nous<space>avons<space>attendu<space>je<space>crois<space>sept<space>ou<space>huit<space>heures<space>dans<space>le<space>gard

2026-01-28 12:57:02,317 | INFO | speech length: 252960
2026-01-28 12:57:02,344 | INFO | decoder input length: 394
2026-01-28 12:57:02,344 | INFO | max output length: 394
2026-01-28 12:57:02,344 | INFO | min output length: 39
2026-01-28 12:57:31,534 | INFO | end detected at 344
2026-01-28 12:57:31,536 | INFO | -51.08 * 0.5 = -25.54 for decoder
2026-01-28 12:57:31,536 | INFO | -28.54 * 0.5 = -14.27 for ctc
2026-01-28 12:57:31,536 | INFO | total log probability: -39.81
2026-01-28 12:57:31,536 | INFO | normalized log probability: -0.12
2026-01-28 12:57:31,536 | INFO | total number of ended hypotheses: 219
2026-01-28 12:57:31,539 | INFO | best hypo: parce<space>que<space>tous<space>les<space>officiellement<space>tout<space>l'état<space>major<space>allemand<space>rentrait<space>à<space>paris<space>alors<space>les<space>trains<space>étaient<space>conclus<space>ça<space>c'était<space>l'habitude<space>d'<space>y<space>avait<space>quelque<space>chose<space>ça<space>se<space>passait<space>comme<space>ça<space>c'était<space>pas<space>normal<space>et<space>finalement<space>nous<space>sommes<space>montrés<space>à<space>paris<space>et<space>nous<space>nous<space>sommes<space>installés<space>dans<space>ce<space>petit<space>appartement<space>et<space>ne<space>sommes<space>plus<space>jamais<space>repartis

2026-01-28 12:57:31,542 | INFO | speech length: 241280
2026-01-28 12:57:31,570 | INFO | decoder input length: 376
2026-01-28 12:57:31,570 | INFO | max output length: 376
2026-01-28 12:57:31,570 | INFO | min output length: 37
2026-01-28 12:57:44,018 | INFO | end detected at 234
2026-01-28 12:57:44,020 | INFO | -31.63 * 0.5 = -15.82 for decoder
2026-01-28 12:57:44,020 | INFO | -30.13 * 0.5 = -15.06 for ctc
2026-01-28 12:57:44,020 | INFO | total log probability: -30.88
2026-01-28 12:57:44,020 | INFO | normalized log probability: -0.14
2026-01-28 12:57:44,020 | INFO | total number of ended hypotheses: 212
2026-01-28 12:57:44,023 | INFO | best hypo: sont<space>toujours<space>restés<space>à<space>paris<space>en<space>allant<space>fidèlement<space>en<space>bretagne<space>euh<space>souvent<space>elle<space>était<space>mais<space>nous<space>vivions<space>à<space>paris<space>oui<space>vous<space>êtes<space>euh<space>vraiment<space>parisienne<space>bon<space>je<space>sais<space>pas<space>oui<space>euh<space>oui<space>mais<space>l<space>comme<space>les<space>parents<space>tous<space>les<space>deux<space>ont<space>vécu

2026-01-28 12:57:44,025 | INFO | speech length: 175840
2026-01-28 12:57:44,051 | INFO | decoder input length: 274
2026-01-28 12:57:44,051 | INFO | max output length: 274
2026-01-28 12:57:44,051 | INFO | min output length: 27
2026-01-28 12:57:52,311 | INFO | end detected at 184
2026-01-28 12:57:52,314 | INFO | -19.82 * 0.5 =  -9.91 for decoder
2026-01-28 12:57:52,314 | INFO | -11.74 * 0.5 =  -5.87 for ctc
2026-01-28 12:57:52,314 | INFO | total log probability: -15.78
2026-01-28 12:57:52,314 | INFO | normalized log probability: -0.09
2026-01-28 12:57:52,314 | INFO | total number of ended hypotheses: 229
2026-01-28 12:57:52,316 | INFO | best hypo: toute<space>leur<space>jeunesse<space>à<space>paris<space>puis<space>me<space>doute<space>dans<space>ma<space>famille<space>aussi<space>avant<space>oui<space>euh<space>enfin<space>on<space>n'est<space>attaché<space>à<space>paris<space>moi<space>je<space>je<space>me<space>rends<space>compte<space>que<space>je<space>suis<space>attaché<space>à<space>paris<space>malgré<space>les

2026-01-28 12:57:52,318 | INFO | speech length: 440000
2026-01-28 12:57:52,343 | INFO | decoder input length: 687
2026-01-28 12:57:52,343 | INFO | max output length: 687
2026-01-28 12:57:52,343 | INFO | min output length: 68
2026-01-28 12:58:26,715 | INFO | end detected at 434
2026-01-28 12:58:26,717 | INFO | -370.10 * 0.5 = -185.05 for decoder
2026-01-28 12:58:26,717 | INFO | -54.19 * 0.5 = -27.10 for ctc
2026-01-28 12:58:26,717 | INFO | total log probability: -212.15
2026-01-28 12:58:26,717 | INFO | normalized log probability: -0.50
2026-01-28 12:58:26,717 | INFO | total number of ended hypotheses: 175
2026-01-28 12:58:26,721 | INFO | best hypo: malgré<space>la<space>vie<space>est<space>très<space>différente<space>maintenant<space>et<space>mais<space>euh<space>donc<space>euh<space>on<space>était<space>arrivés<space>à<space>votre<space>arrivée<space>à<space>paris<space>donc<space>euh<space>depuis<space>est<space>ce<space>que<space>vous<space>avez<space>toujours<space>habité<space>à<space>paris<space>et<space>nous<space>oui<space>toujours<space>à<space>au<space>paris<space>et<space>dans<space>quel<space>euh<space>dans<space>quel<space>quartier<space>donc<space>euh<space>il<space>a<space>toujours<space>ils<space>continuent<space>à<space>habiter<space>le<space>le<space>huitième<space>à<space>où<space>nous<space>étions<space>qui<space>était<space>un<space>quartier<space>extrêmement<space>sympathique<space>mais<space>la<space>martemort<space>était<space>un<space>peu<space>petit<space>j'a<space>j'alle<space>à<space>l'école<space>à<space>bon

2026-01-28 12:58:26,723 | INFO | speech length: 356640
2026-01-28 12:58:26,750 | INFO | decoder input length: 556
2026-01-28 12:58:26,750 | INFO | max output length: 556
2026-01-28 12:58:26,750 | INFO | min output length: 55
2026-01-28 12:58:50,934 | INFO | end detected at 361
2026-01-28 12:58:50,936 | INFO | -83.57 * 0.5 = -41.78 for decoder
2026-01-28 12:58:50,936 | INFO | -30.10 * 0.5 = -15.05 for ctc
2026-01-28 12:58:50,936 | INFO | total log probability: -56.83
2026-01-28 12:58:50,936 | INFO | normalized log probability: -0.16
2026-01-28 12:58:50,936 | INFO | total number of ended hypotheses: 196
2026-01-28 12:58:50,940 | INFO | best hypo: les<space>parents<space>connaissaient<space>avait<space>beaucoup<space>d'amis<space>dans<space>ce<space>col<space>là<space>tout<space>ça<space>c'était<space>un<space>quartier<space>très<space>agréable<space>et<space>après<space>nous<space>avons<space>été<space>habités<space>pour<space>cent<space>hesitation<space>dans<space>le<space>seizième<space>pa<space>pas<space>très<space>loin<space>d'ici<space>et<space>à<space>la<space>fin<space>de<space>leur<space>vie<space>mes<space>parents<space>ont<space>été<space>livres<space>à<space>chiyune<space>mais<space>à<space>à<space>nouveau<space>dans<space>un<space>petit<space>appartement<space>ils<space>étaient<space>deux<space>j'étaient<space>très<space>contents<space>là

2026-01-28 12:58:50,942 | INFO | speech length: 476800
2026-01-28 12:58:50,969 | INFO | decoder input length: 744
2026-01-28 12:58:50,969 | INFO | max output length: 744
2026-01-28 12:58:50,969 | INFO | min output length: 74
2026-01-28 12:59:34,785 | INFO | end detected at 541
2026-01-28 12:59:34,787 | INFO | -538.92 * 0.5 = -269.46 for decoder
2026-01-28 12:59:34,787 | INFO | -91.15 * 0.5 = -45.58 for ctc
2026-01-28 12:59:34,787 | INFO | total log probability: -315.04
2026-01-28 12:59:34,787 | INFO | normalized log probability: -0.59
2026-01-28 12:59:34,787 | INFO | total number of ended hypotheses: 158
2026-01-28 12:59:34,793 | INFO | best hypo: facile<space>à<space>vivre<space>ensoleillé<space>c'est<space>était<space>agréable<space>et<space>et<space>i<space>s<space>étaient<space>très<space>contents<space>et<space>j'ai<space>beaucoup<space>mais<space>vous<space>et<space>votre<space>mari<space>euh<space>votre<space>famille<space>va<space>avec<space>les<space>enfants<space>on<space>vous<space>êtes<space>toujours<space>restés<space>à<space>paris<space>il<space>a<space>dit<space>oui<space>nous<space>sommes<space>restés<space>à<space>paris<space>euh<space>plusieurs<space>fois<space>euh<space>de<space>propositions<space>de<space>s'en<space>aller<space>et<space>puis<space>finalement<space>euh<space>bah<space>ça<space>ne<space>s'est<space>pas<space>fait<space>vous<space>savez<space>dans<space>ce<space>temps<space>là<space>si<space>on<space>changeait<space>de<space>situation<space>on<space>savait<space>plusieurs<space>prop<space>sitions<space>h<space>rapidement<space>hein<space>alors<space>on<space>choisisait<space>ce<space>qu'on<space>voulait<space>c'est<space>comme<space>ça<space>que<space>ça<space>se<space>passait<space>i<space>c'ait<space>c'es<space>c'est

2026-01-28 12:59:34,795 | INFO | speech length: 56000
2026-01-28 12:59:34,821 | INFO | decoder input length: 87
2026-01-28 12:59:34,821 | INFO | max output length: 87
2026-01-28 12:59:34,821 | INFO | min output length: 8
2026-01-28 12:59:36,897 | INFO | end detected at 71
2026-01-28 12:59:36,899 | INFO |  -5.29 * 0.5 =  -2.65 for decoder
2026-01-28 12:59:36,899 | INFO |  -2.18 * 0.5 =  -1.09 for ctc
2026-01-28 12:59:36,899 | INFO | total log probability: -3.74
2026-01-28 12:59:36,899 | INFO | normalized log probability: -0.06
2026-01-28 12:59:36,899 | INFO | total number of ended hypotheses: 186
2026-01-28 12:59:36,900 | INFO | best hypo: étonnant<space>à<space>dire<space>maintenant<space>et<space>c'est<space>comme<space>ça<space>que<space>ça<space>se<space>passait

2026-01-28 12:59:36,901 | INFO | speech length: 64960
2026-01-28 12:59:36,927 | INFO | decoder input length: 101
2026-01-28 12:59:36,927 | INFO | max output length: 101
2026-01-28 12:59:36,927 | INFO | min output length: 10
2026-01-28 12:59:39,867 | INFO | adding <eos> in the last position in the loop
2026-01-28 12:59:39,875 | INFO | no hypothesis. Finish decoding.
2026-01-28 12:59:39,876 | INFO | -26.27 * 0.5 = -13.14 for decoder
2026-01-28 12:59:39,876 | INFO | -79.99 * 0.5 = -39.99 for ctc
2026-01-28 12:59:39,876 | INFO | total log probability: -53.13
2026-01-28 12:59:39,876 | INFO | normalized log probability: -0.54
2026-01-28 12:59:39,876 | INFO | total number of ended hypotheses: 82
2026-01-28 12:59:39,877 | INFO | best hypo: marie<space>a<space>changé<space>plusieurs<space>fois<space>de<space>situation<space>pas<space>beaucoup<space>d'ailleurs<space>a<space>des<space>gens<space>qui<space>change<space>beaucoup

2026-01-28 12:59:39,878 | INFO | speech length: 68960
2026-01-28 12:59:39,903 | INFO | decoder input length: 107
2026-01-28 12:59:39,903 | INFO | max output length: 107
2026-01-28 12:59:39,903 | INFO | min output length: 10
2026-01-28 12:59:42,681 | INFO | end detected at 86
2026-01-28 12:59:42,684 | INFO |  -8.42 * 0.5 =  -4.21 for decoder
2026-01-28 12:59:42,684 | INFO | -15.06 * 0.5 =  -7.53 for ctc
2026-01-28 12:59:42,684 | INFO | total log probability: -11.74
2026-01-28 12:59:42,684 | INFO | normalized log probability: -0.16
2026-01-28 12:59:42,684 | INFO | total number of ended hypotheses: 237
2026-01-28 12:59:42,685 | INFO | best hypo: une<space>fois<space>mais<space>les<space>huit<space>jours<space>suivants<space>il<space>avait<space>plusieurs<space>euh<space>propositions

2026-01-28 12:59:42,687 | INFO | speech length: 176320
2026-01-28 12:59:42,713 | INFO | decoder input length: 275
2026-01-28 12:59:42,713 | INFO | max output length: 275
2026-01-28 12:59:42,713 | INFO | min output length: 27
2026-01-28 12:59:52,249 | INFO | end detected at 223
2026-01-28 12:59:52,251 | INFO | -29.02 * 0.5 = -14.51 for decoder
2026-01-28 12:59:52,252 | INFO | -36.74 * 0.5 = -18.37 for ctc
2026-01-28 12:59:52,252 | INFO | total log probability: -32.88
2026-01-28 12:59:52,252 | INFO | normalized log probability: -0.16
2026-01-28 12:59:52,252 | INFO | total number of ended hypotheses: 235
2026-01-28 12:59:52,254 | INFO | best hypo: presque<space>sans<space>tellement<space>beaucoup<space>boucher<space>lui<space>même<space>enfin<space>c'est<space>c'est<space>comme<space>ça<space>que<space>ça<space>se<space>passait<space>non<space>nous<space>sommes<space>restés<space>à<space>paris<space>mon<space>mon<space>mari<space>est<space>né<space>tout<space>près<space>du<space>fil<space>il<space>m'a<space>vécu<space>toute<space>sa<space>vie<space>pas<space>loin<space>d'ici<space>aussi

2026-01-28 12:59:52,257 | INFO | speech length: 12000
2026-01-28 12:59:52,283 | INFO | decoder input length: 18
2026-01-28 12:59:52,283 | INFO | max output length: 18
2026-01-28 12:59:52,283 | INFO | min output length: 1
2026-01-28 12:59:52,662 | INFO | end detected at 13
2026-01-28 12:59:52,663 | INFO |  -0.56 * 0.5 =  -0.28 for decoder
2026-01-28 12:59:52,663 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 12:59:52,663 | INFO | total log probability: -0.28
2026-01-28 12:59:52,663 | INFO | normalized log probability: -0.04
2026-01-28 12:59:52,663 | INFO | total number of ended hypotheses: 163
2026-01-28 12:59:52,664 | INFO | best hypo: et<space>euh

2026-01-28 12:59:52,666 | INFO | speech length: 102080
2026-01-28 12:59:52,692 | INFO | decoder input length: 159
2026-01-28 12:59:52,692 | INFO | max output length: 159
2026-01-28 12:59:52,692 | INFO | min output length: 15
2026-01-28 12:59:57,399 | INFO | end detected at 123
2026-01-28 12:59:57,401 | INFO | -13.86 * 0.5 =  -6.93 for decoder
2026-01-28 12:59:57,402 | INFO | -14.02 * 0.5 =  -7.01 for ctc
2026-01-28 12:59:57,402 | INFO | total log probability: -13.94
2026-01-28 12:59:57,402 | INFO | normalized log probability: -0.13
2026-01-28 12:59:57,402 | INFO | total number of ended hypotheses: 185
2026-01-28 12:59:57,403 | INFO | best hypo: non<space>il<space>était<space>un<space>s<space>il<space>était<space>attaché<space>à<space>paris<space>mais<space>enfin<space>i<space>s<space>nous<space>serions<space>parti<space>euh<space>volontiers<space>hein<space>ouais

2026-01-28 12:59:57,405 | INFO | speech length: 101280
2026-01-28 12:59:57,432 | INFO | decoder input length: 157
2026-01-28 12:59:57,432 | INFO | max output length: 157
2026-01-28 12:59:57,432 | INFO | min output length: 15
2026-01-28 13:00:01,345 | INFO | end detected at 104
2026-01-28 13:00:01,348 | INFO | -12.82 * 0.5 =  -6.41 for decoder
2026-01-28 13:00:01,348 | INFO | -13.70 * 0.5 =  -6.85 for ctc
2026-01-28 13:00:01,348 | INFO | total log probability: -13.26
2026-01-28 13:00:01,348 | INFO | normalized log probability: -0.14
2026-01-28 13:00:01,348 | INFO | total number of ended hypotheses: 236
2026-01-28 13:00:01,349 | INFO | best hypo: des<space>projets<space>de<space>partir<space>euh<space>euh<space>vers<space>l'étranger<space>je<space>vois<space>d'autres<space>euh<space>on<space>aura<space>à<space>l'étranger<space>euh

2026-01-28 13:00:01,351 | INFO | speech length: 44640
2026-01-28 13:00:01,377 | INFO | decoder input length: 69
2026-01-28 13:00:01,377 | INFO | max output length: 69
2026-01-28 13:00:01,377 | INFO | min output length: 6
2026-01-28 13:00:02,911 | INFO | end detected at 49
2026-01-28 13:00:02,913 | INFO |  -5.20 * 0.5 =  -2.60 for decoder
2026-01-28 13:00:02,913 | INFO |  -7.54 * 0.5 =  -3.77 for ctc
2026-01-28 13:00:02,913 | INFO | total log probability: -6.37
2026-01-28 13:00:02,913 | INFO | normalized log probability: -0.17
2026-01-28 13:00:02,913 | INFO | total number of ended hypotheses: 178
2026-01-28 13:00:02,913 | INFO | best hypo: et<space>finalement<space>bon<space>choisit<space>de<space>rester

2026-01-28 13:00:02,916 | INFO | speech length: 45440
2026-01-28 13:00:02,941 | INFO | decoder input length: 70
2026-01-28 13:00:02,941 | INFO | max output length: 70
2026-01-28 13:00:02,941 | INFO | min output length: 7
2026-01-28 13:00:04,282 | INFO | end detected at 41
2026-01-28 13:00:04,284 | INFO |  -2.89 * 0.5 =  -1.44 for decoder
2026-01-28 13:00:04,284 | INFO |  -3.97 * 0.5 =  -1.99 for ctc
2026-01-28 13:00:04,285 | INFO | total log probability: -3.43
2026-01-28 13:00:04,285 | INFO | normalized log probability: -0.12
2026-01-28 13:00:04,285 | INFO | total number of ended hypotheses: 233
2026-01-28 13:00:04,285 | INFO | best hypo: oui<space>euh<space>donc<space>euh<space>pour<space>votre

2026-01-28 13:00:04,292 | INFO | Chunk: 0 | WER=16.129032 | S=2 D=0 I=3
2026-01-28 13:00:04,293 | INFO | Chunk: 1 | WER=23.076923 | S=2 D=0 I=1
2026-01-28 13:00:04,295 | INFO | Chunk: 2 | WER=28.571429 | S=11 D=3 I=6
2026-01-28 13:00:04,300 | INFO | Chunk: 3 | WER=22.115385 | S=19 D=2 I=2
2026-01-28 13:00:04,306 | INFO | Chunk: 4 | WER=30.555556 | S=19 D=8 I=6
2026-01-28 13:00:04,306 | INFO | Chunk: 5 | WER=34.782609 | S=6 D=1 I=1
2026-01-28 13:00:04,306 | INFO | Chunk: 6 | WER=16.666667 | S=2 D=0 I=0
2026-01-28 13:00:04,308 | INFO | Chunk: 7 | WER=18.750000 | S=7 D=5 I=0
2026-01-28 13:00:04,310 | INFO | Chunk: 8 | WER=28.571429 | S=4 D=4 I=4
2026-01-28 13:00:04,311 | INFO | Chunk: 9 | WER=22.222222 | S=6 D=1 I=1
2026-01-28 13:00:04,314 | INFO | Chunk: 10 | WER=29.729730 | S=12 D=0 I=10
2026-01-28 13:00:04,316 | INFO | Chunk: 11 | WER=23.437500 | S=8 D=2 I=5
2026-01-28 13:00:04,321 | INFO | Chunk: 12 | WER=31.428571 | S=15 D=7 I=11
2026-01-28 13:00:04,322 | INFO | Chunk: 13 | WER=14.285714 | S=1 D=1 I=0
2026-01-28 13:00:04,322 | INFO | Chunk: 14 | WER=46.153846 | S=3 D=9 I=0
2026-01-28 13:00:04,322 | INFO | Chunk: 15 | WER=27.272727 | S=0 D=1 I=2
2026-01-28 13:00:04,324 | INFO | Chunk: 16 | WER=11.111111 | S=3 D=1 I=1
2026-01-28 13:00:04,324 | INFO | Chunk: 17 | WER=100.000000 | S=0 D=0 I=1
2026-01-28 13:00:04,324 | INFO | Chunk: 18 | WER=50.000000 | S=6 D=3 I=2
2026-01-28 13:00:04,325 | INFO | Chunk: 19 | WER=50.000000 | S=5 D=1 I=3
2026-01-28 13:00:04,325 | INFO | Chunk: 20 | WER=55.555556 | S=2 D=3 I=0
2026-01-28 13:00:04,325 | INFO | Chunk: 21 | WER=50.000000 | S=0 D=0 I=2
2026-01-28 13:00:04,674 | INFO | File: Rhap-D0003.wav | WER=27.232143 | S=135 D=50 I=59
2026-01-28 13:00:04,674 | INFO | ------------------------------
2026-01-28 13:00:04,674 | INFO | Conf ester Done!
2026-01-28 13:03:25,746 | INFO | Chunk: 0 | WER=19.354839 | S=2 D=2 I=2
2026-01-28 13:03:25,747 | INFO | Chunk: 1 | WER=7.692308 | S=1 D=0 I=0
2026-01-28 13:03:25,751 | INFO | Chunk: 2 | WER=34.285714 | S=15 D=8 I=1
2026-01-28 13:03:25,756 | INFO | Chunk: 3 | WER=25.961538 | S=16 D=10 I=1
2026-01-28 13:03:25,761 | INFO | Chunk: 4 | WER=33.333333 | S=22 D=14 I=0
2026-01-28 13:03:25,761 | INFO | Chunk: 5 | WER=26.086957 | S=5 D=1 I=0
2026-01-28 13:03:25,762 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 13:03:25,764 | INFO | Chunk: 7 | WER=31.250000 | S=12 D=8 I=0
2026-01-28 13:03:25,765 | INFO | Chunk: 8 | WER=30.952381 | S=8 D=5 I=0
2026-01-28 13:03:25,766 | INFO | Chunk: 9 | WER=27.777778 | S=4 D=5 I=1
2026-01-28 13:03:25,768 | INFO | Chunk: 10 | WER=35.135135 | S=18 D=7 I=1
2026-01-28 13:03:25,771 | INFO | Chunk: 11 | WER=32.812500 | S=16 D=1 I=4
2026-01-28 13:03:25,775 | INFO | Chunk: 12 | WER=37.142857 | S=18 D=19 I=2
2026-01-28 13:03:25,776 | INFO | Chunk: 13 | WER=7.142857 | S=1 D=0 I=0
2026-01-28 13:03:25,776 | INFO | Chunk: 14 | WER=53.846154 | S=2 D=11 I=1
2026-01-28 13:03:25,776 | INFO | Chunk: 15 | WER=54.545455 | S=2 D=2 I=2
2026-01-28 13:03:25,778 | INFO | Chunk: 16 | WER=17.777778 | S=2 D=6 I=0
2026-01-28 13:03:25,778 | INFO | Chunk: 17 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 13:03:25,778 | INFO | Chunk: 18 | WER=40.909091 | S=4 D=4 I=1
2026-01-28 13:03:25,779 | INFO | Chunk: 19 | WER=38.888889 | S=4 D=3 I=0
2026-01-28 13:03:25,779 | INFO | Chunk: 20 | WER=44.444444 | S=0 D=4 I=0
2026-01-28 13:03:25,779 | INFO | Chunk: 21 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 13:03:26,089 | INFO | File: Rhap-D0003.wav | WER=31.026786 | S=152 D=110 I=16
2026-01-28 13:03:26,089 | INFO | ------------------------------
2026-01-28 13:03:26,089 | INFO | hmm_tdnn Done!
2026-01-28 13:03:26,239 | INFO | ==================================Rhap-D0004.wav=========================================
2026-01-28 13:03:26,395 | INFO | Using rVAD model
2026-01-28 13:03:40,385 | INFO | Chunk: 0 | WER=24.000000 | S=6 D=3 I=3
2026-01-28 13:03:40,386 | INFO | Chunk: 1 | WER=26.666667 | S=2 D=2 I=4
2026-01-28 13:03:40,391 | INFO | Chunk: 2 | WER=30.188679 | S=11 D=18 I=3
2026-01-28 13:03:40,393 | INFO | Chunk: 3 | WER=33.846154 | S=8 D=12 I=2
2026-01-28 13:03:40,393 | INFO | Chunk: 4 | WER=13.333333 | S=2 D=1 I=1
2026-01-28 13:03:40,395 | INFO | Chunk: 5 | WER=13.793103 | S=4 D=4 I=0
2026-01-28 13:03:40,396 | INFO | Chunk: 6 | WER=40.000000 | S=3 D=3 I=0
2026-01-28 13:03:40,396 | INFO | Chunk: 7 | WER=40.000000 | S=1 D=0 I=1
2026-01-28 13:03:40,396 | INFO | Chunk: 8 | WER=43.750000 | S=2 D=3 I=2
2026-01-28 13:03:40,398 | INFO | Chunk: 9 | WER=32.307692 | S=6 D=15 I=0
2026-01-28 13:03:40,398 | INFO | Chunk: 10 | WER=50.000000 | S=0 D=0 I=4
2026-01-28 13:03:40,399 | INFO | Chunk: 11 | WER=38.000000 | S=3 D=16 I=0
2026-01-28 13:03:40,399 | INFO | Chunk: 12 | WER=100.000000 | S=5 D=0 I=0
2026-01-28 13:03:40,403 | INFO | Chunk: 13 | WER=30.693069 | S=12 D=18 I=1
2026-01-28 13:03:40,404 | INFO | Chunk: 14 | WER=83.333333 | S=4 D=0 I=1
2026-01-28 13:03:40,405 | INFO | Chunk: 15 | WER=37.735849 | S=6 D=10 I=4
2026-01-28 13:03:40,406 | INFO | Chunk: 16 | WER=9.090909 | S=2 D=1 I=1
2026-01-28 13:03:40,407 | INFO | Chunk: 17 | WER=19.047619 | S=0 D=2 I=2
2026-01-28 13:03:40,408 | INFO | Chunk: 18 | WER=37.837838 | S=3 D=8 I=3
2026-01-28 13:03:40,408 | INFO | Chunk: 19 | WER=15.384615 | S=1 D=1 I=0
2026-01-28 13:03:40,411 | INFO | Chunk: 20 | WER=17.808219 | S=6 D=5 I=2
2026-01-28 13:03:40,416 | INFO | Chunk: 21 | WER=16.666667 | S=9 D=8 I=2
2026-01-28 13:03:40,417 | INFO | Chunk: 22 | WER=34.210526 | S=3 D=7 I=3
2026-01-28 13:03:40,418 | INFO | Chunk: 23 | WER=30.769231 | S=2 D=2 I=4
2026-01-28 13:03:40,832 | INFO | File: Rhap-D0004.wav | WER=25.947522 | S=105 D=129 I=33
2026-01-28 13:03:40,832 | INFO | ------------------------------
2026-01-28 13:03:40,832 | INFO | w2vec vad chunk Done!
2026-01-28 13:04:09,133 | INFO | Chunk: 0 | WER=24.000000 | S=6 D=5 I=1
2026-01-28 13:04:09,134 | INFO | Chunk: 1 | WER=20.000000 | S=1 D=1 I=4
2026-01-28 13:04:09,136 | INFO | Chunk: 2 | WER=68.867925 | S=4 D=69 I=0
2026-01-28 13:04:09,138 | INFO | Chunk: 3 | WER=38.461538 | S=5 D=20 I=0
2026-01-28 13:04:09,139 | INFO | Chunk: 4 | WER=16.666667 | S=2 D=2 I=1
2026-01-28 13:04:09,140 | INFO | Chunk: 5 | WER=31.034483 | S=2 D=16 I=0
2026-01-28 13:04:09,141 | INFO | Chunk: 6 | WER=26.666667 | S=0 D=4 I=0
2026-01-28 13:04:09,141 | INFO | Chunk: 7 | WER=20.000000 | S=0 D=0 I=1
2026-01-28 13:04:09,141 | INFO | Chunk: 8 | WER=31.250000 | S=0 D=2 I=3
2026-01-28 13:04:09,143 | INFO | Chunk: 9 | WER=35.384615 | S=3 D=20 I=0
2026-01-28 13:04:09,143 | INFO | Chunk: 10 | WER=50.000000 | S=0 D=0 I=4
2026-01-28 13:04:09,144 | INFO | Chunk: 11 | WER=56.000000 | S=1 D=27 I=0
2026-01-28 13:04:09,144 | INFO | Chunk: 12 | WER=120.000000 | S=2 D=1 I=3
2026-01-28 13:04:09,147 | INFO | Chunk: 13 | WER=60.396040 | S=9 D=51 I=1
2026-01-28 13:04:09,147 | INFO | Chunk: 14 | WER=83.333333 | S=2 D=3 I=0
2026-01-28 13:04:09,148 | INFO | Chunk: 15 | WER=83.018868 | S=3 D=41 I=0
2026-01-28 13:04:09,148 | INFO | Chunk: 16 | WER=65.909091 | S=2 D=27 I=0
2026-01-28 13:04:09,149 | INFO | Chunk: 17 | WER=33.333333 | S=0 D=5 I=2
2026-01-28 13:04:09,150 | INFO | Chunk: 18 | WER=32.432432 | S=2 D=7 I=3
2026-01-28 13:04:09,150 | INFO | Chunk: 19 | WER=38.461538 | S=2 D=2 I=1
2026-01-28 13:04:09,152 | INFO | Chunk: 20 | WER=61.643836 | S=2 D=42 I=1
2026-01-28 13:04:09,153 | INFO | Chunk: 21 | WER=80.701754 | S=1 D=91 I=0
2026-01-28 13:04:09,154 | INFO | Chunk: 22 | WER=31.578947 | S=5 D=4 I=3
2026-01-28 13:04:09,155 | INFO | Chunk: 23 | WER=34.615385 | S=0 D=5 I=4
2026-01-28 13:04:09,428 | INFO | File: Rhap-D0004.wav | WER=50.340136 | S=61 D=435 I=22
2026-01-28 13:04:09,428 | INFO | ------------------------------
2026-01-28 13:04:09,428 | INFO | whisper med Done!
2026-01-28 13:04:55,489 | INFO | Chunk: 0 | WER=24.000000 | S=6 D=5 I=1
2026-01-28 13:04:55,490 | INFO | Chunk: 1 | WER=20.000000 | S=1 D=1 I=4
2026-01-28 13:04:55,494 | INFO | Chunk: 2 | WER=56.603774 | S=11 D=49 I=0
2026-01-28 13:04:55,496 | INFO | Chunk: 3 | WER=44.615385 | S=9 D=14 I=6
2026-01-28 13:04:55,496 | INFO | Chunk: 4 | WER=13.333333 | S=0 D=2 I=2
2026-01-28 13:04:55,498 | INFO | Chunk: 5 | WER=32.758621 | S=1 D=16 I=2
2026-01-28 13:04:55,498 | INFO | Chunk: 6 | WER=20.000000 | S=0 D=3 I=0
2026-01-28 13:04:55,498 | INFO | Chunk: 7 | WER=40.000000 | S=0 D=0 I=2
2026-01-28 13:04:55,499 | INFO | Chunk: 8 | WER=31.250000 | S=0 D=2 I=3
2026-01-28 13:04:55,501 | INFO | Chunk: 9 | WER=30.769231 | S=8 D=12 I=0
2026-01-28 13:04:55,501 | INFO | Chunk: 10 | WER=50.000000 | S=0 D=0 I=4
2026-01-28 13:04:55,502 | INFO | Chunk: 11 | WER=34.000000 | S=3 D=13 I=1
2026-01-28 13:04:55,502 | INFO | Chunk: 12 | WER=100.000000 | S=5 D=0 I=0
2026-01-28 13:04:55,505 | INFO | Chunk: 13 | WER=53.465347 | S=12 D=41 I=1
2026-01-28 13:04:55,506 | INFO | Chunk: 14 | WER=66.666667 | S=2 D=2 I=0
2026-01-28 13:04:55,507 | INFO | Chunk: 15 | WER=30.188679 | S=5 D=7 I=4
2026-01-28 13:04:55,508 | INFO | Chunk: 16 | WER=15.909091 | S=4 D=2 I=1
2026-01-28 13:04:55,509 | INFO | Chunk: 17 | WER=76.190476 | S=0 D=13 I=3
2026-01-28 13:04:55,510 | INFO | Chunk: 18 | WER=27.027027 | S=1 D=6 I=3
2026-01-28 13:04:55,510 | INFO | Chunk: 19 | WER=38.461538 | S=2 D=2 I=1
2026-01-28 13:04:55,512 | INFO | Chunk: 20 | WER=34.246575 | S=6 D=17 I=2
2026-01-28 13:04:55,514 | INFO | Chunk: 21 | WER=80.701754 | S=1 D=91 I=0
2026-01-28 13:04:55,515 | INFO | Chunk: 22 | WER=42.105263 | S=2 D=10 I=4
2026-01-28 13:04:55,515 | INFO | Chunk: 23 | WER=38.461538 | S=0 D=6 I=4
2026-01-28 13:04:55,854 | INFO | File: Rhap-D0004.wav | WER=41.302235 | S=87 D=302 I=36
2026-01-28 13:04:55,854 | INFO | ------------------------------
2026-01-28 13:04:55,854 | INFO | whisper large Done!
2026-01-28 13:04:56,021 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 13:04:56,051 | INFO | Vocabulary size: 350
2026-01-28 13:04:56,588 | INFO | Gradient checkpoint layers: []
2026-01-28 13:04:57,485 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 13:04:57,488 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 13:04:57,488 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 13:04:57,489 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 13:04:57,489 | INFO | speech length: 228800
2026-01-28 13:04:57,525 | INFO | decoder input length: 357
2026-01-28 13:04:57,525 | INFO | max output length: 357
2026-01-28 13:04:57,525 | INFO | min output length: 35
2026-01-28 13:05:11,964 | INFO | end detected at 111
2026-01-28 13:05:11,965 | INFO | -118.42 * 0.5 = -59.21 for decoder
2026-01-28 13:05:11,965 | INFO | -29.37 * 0.5 = -14.68 for ctc
2026-01-28 13:05:11,965 | INFO | total log probability: -73.89
2026-01-28 13:05:11,965 | INFO | normalized log probability: -0.70
2026-01-28 13:05:11,965 | INFO | total number of ended hypotheses: 162
2026-01-28 13:05:11,966 | INFO | best hypo: ▁et▁bien▁on▁a▁toujours▁un▁peu▁cela▁qui▁vieillissent▁ionadotes▁qui▁sont▁partis▁du▁quartier▁qui▁reviennent▁de▁temps▁en▁temps▁et▁puis▁bons▁quand▁même▁évidemment▁des▁nouveaux▁mais▁il▁faut▁quand▁même▁bien▁connaître▁que▁ils▁sont▁plus▁jeunes▁au▁moins▁proches▁du▁livre

2026-01-28 13:05:11,969 | INFO | speech length: 156480
2026-01-28 13:05:12,002 | INFO | decoder input length: 244
2026-01-28 13:05:12,002 | INFO | max output length: 244
2026-01-28 13:05:12,002 | INFO | min output length: 24
2026-01-28 13:05:21,436 | INFO | end detected at 79
2026-01-28 13:05:21,437 | INFO | -33.88 * 0.5 = -16.94 for decoder
2026-01-28 13:05:21,437 | INFO | -24.76 * 0.5 = -12.38 for ctc
2026-01-28 13:05:21,437 | INFO | total log probability: -29.32
2026-01-28 13:05:21,437 | INFO | normalized log probability: -0.40
2026-01-28 13:05:21,437 | INFO | total number of ended hypotheses: 180
2026-01-28 13:05:21,438 | INFO | best hypo: ▁ils▁ont▁en▁général▁bon▁pour▁vivre▁dans▁ce▁quartier▁il▁faut▁quand▁même▁beaucoup▁d'argent▁ce▁que▁des▁loyers▁sont▁très▁chers▁ici▁l'immobilier▁est▁très▁cher▁dans▁les▁cartes

2026-01-28 13:05:21,440 | INFO | speech length: 451040
2026-01-28 13:05:21,472 | INFO | decoder input length: 704
2026-01-28 13:05:21,472 | INFO | max output length: 704
2026-01-28 13:05:21,472 | INFO | min output length: 70
2026-01-28 13:05:59,098 | INFO | end detected at 218
2026-01-28 13:05:59,100 | INFO | -519.62 * 0.5 = -259.81 for decoder
2026-01-28 13:05:59,100 | INFO | -139.95 * 0.5 = -69.98 for ctc
2026-01-28 13:05:59,100 | INFO | total log probability: -329.79
2026-01-28 13:05:59,100 | INFO | normalized log probability: -1.55
2026-01-28 13:05:59,100 | INFO | total number of ended hypotheses: 148
2026-01-28 13:05:59,102 | INFO | best hypo: ▁y▁un▁moment▁il▁disait▁que▁on▁avait▁eu▁des▁chiffres▁chroquiailles▁et▁quelques▁années▁il▁avait▁dit▁que▁l'immobilier▁et▁va▁augmenté▁d'environ▁neuf▁pourcents▁à▁paris▁et▁sur▁le▁quartier▁vert▁ziippe▁carartier▁vert▁et▁iz▁x▁avait▁augmenté▁d'à▁peu▁près▁d'ouze▁à▁treize▁pourcents▁la▁par▁contre▁dans▁ouvrier▁ce▁qui▁fait▁finalement▁l'immobilier▁'ici▁dans▁ce▁coin▁et▁très▁et▁très▁cher▁et▁'en'arrivait▁de▁la▁manière▁des▁clientèles▁c▁qui▁sont▁en▁loction▁la▁moyenne▁s'environ▁trois▁ans

2026-01-28 13:05:59,104 | INFO | speech length: 263360
2026-01-28 13:05:59,139 | INFO | decoder input length: 411
2026-01-28 13:05:59,139 | INFO | max output length: 411
2026-01-28 13:05:59,139 | INFO | min output length: 41
2026-01-28 13:06:15,204 | INFO | end detected at 136
2026-01-28 13:06:15,205 | INFO | -166.85 * 0.5 = -83.43 for decoder
2026-01-28 13:06:15,206 | INFO | -90.31 * 0.5 = -45.16 for ctc
2026-01-28 13:06:15,206 | INFO | total log probability: -128.58
2026-01-28 13:06:15,206 | INFO | normalized log probability: -0.99
2026-01-28 13:06:15,206 | INFO | total number of ended hypotheses: 188
2026-01-28 13:06:15,207 | INFO | best hypo: ▁il▁y▁a▁des▁déménagements▁et▁des▁aménagements▁permanents▁y▁restent▁pas▁longtemps▁sur▁le▁coin▁par▁amoyapa▁de▁très▁grands▁appartements▁donc▁que▁des▁jeune▁couples▁papas▁waldeckia▁un▁enfant▁voilà▁mais▁qui▁a▁un▁enfant▁ou▁de▁en▁moins▁de▁coque▁le▁deuxième▁mariapparte▁et▁très▁souvent▁et▁par▁tan▁banlieue

2026-01-28 13:06:15,209 | INFO | speech length: 140640
2026-01-28 13:06:15,243 | INFO | decoder input length: 219
2026-01-28 13:06:15,243 | INFO | max output length: 219
2026-01-28 13:06:15,243 | INFO | min output length: 21
2026-01-28 13:06:21,443 | INFO | end detected at 58
2026-01-28 13:06:21,445 | INFO |  -8.40 * 0.5 =  -4.20 for decoder
2026-01-28 13:06:21,445 | INFO |  -8.81 * 0.5 =  -4.41 for ctc
2026-01-28 13:06:21,445 | INFO | total log probability: -8.61
2026-01-28 13:06:21,445 | INFO | normalized log probability: -0.18
2026-01-28 13:06:21,445 | INFO | total number of ended hypotheses: 190
2026-01-28 13:06:21,445 | INFO | best hypo: ▁reste▁pas▁à▁paris▁donc▁ça▁cet▁un▁petit▁peu▁pour▁ce▁qui▁a▁été▁des▁changements▁de▁du▁quartier▁moi▁je▁suis▁là▁depuis▁vingt▁trois▁ans

2026-01-28 13:06:21,447 | INFO | speech length: 224960
2026-01-28 13:06:21,478 | INFO | decoder input length: 351
2026-01-28 13:06:21,478 | INFO | max output length: 351
2026-01-28 13:06:21,478 | INFO | min output length: 35
2026-01-28 13:06:37,011 | INFO | end detected at 139
2026-01-28 13:06:37,013 | INFO | -181.37 * 0.5 = -90.69 for decoder
2026-01-28 13:06:37,013 | INFO | -129.91 * 0.5 = -64.95 for ctc
2026-01-28 13:06:37,013 | INFO | total log probability: -155.64
2026-01-28 13:06:37,013 | INFO | normalized log probability: -1.19
2026-01-28 13:06:37,013 | INFO | total number of ended hypotheses: 201
2026-01-28 13:06:37,014 | INFO | best hypo: ▁mais▁on▁que▁c'est▁vrai▁que▁ça▁fait▁quand▁même▁beaucoup▁de▁changements▁ce▁qui▁fait▁que▁pour▁nous▁maintenant▁dans▁des▁ptits▁dans▁des▁titres▁comme▁ça▁où▁on▁n'a▁pas▁ce▁pas▁cesage▁de▁l'avenue▁l'égal▁ne▁rentre▁pas▁dans▁les▁quartiers▁et▁maintenant▁ou▁on▁n'a▁'a▁out▁mal▁à▁maintenir▁mode▁clientèle▁de▁car

2026-01-28 13:06:37,016 | INFO | speech length: 58560
2026-01-28 13:06:37,046 | INFO | decoder input length: 91
2026-01-28 13:06:37,046 | INFO | max output length: 91
2026-01-28 13:06:37,046 | INFO | min output length: 9
2026-01-28 13:06:38,444 | INFO | end detected at 38
2026-01-28 13:06:38,445 | INFO |  -9.68 * 0.5 =  -4.84 for decoder
2026-01-28 13:06:38,445 | INFO | -26.41 * 0.5 = -13.21 for ctc
2026-01-28 13:06:38,445 | INFO | total log probability: -18.05
2026-01-28 13:06:38,445 | INFO | normalized log probability: -0.60
2026-01-28 13:06:38,445 | INFO | total number of ended hypotheses: 184
2026-01-28 13:06:38,446 | INFO | best hypo: ▁et▁puis▁quand▁même▁appelé▁la▁france▁et▁en▁allemagne▁il▁a▁fumé

2026-01-28 13:06:38,448 | INFO | speech length: 37600
2026-01-28 13:06:38,475 | INFO | decoder input length: 58
2026-01-28 13:06:38,475 | INFO | max output length: 58
2026-01-28 13:06:38,475 | INFO | min output length: 5
2026-01-28 13:06:40,107 | INFO | end detected at 19
2026-01-28 13:06:40,108 | INFO |  -1.09 * 0.5 =  -0.54 for decoder
2026-01-28 13:06:40,108 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-28 13:06:40,108 | INFO | total log probability: -0.55
2026-01-28 13:06:40,108 | INFO | normalized log probability: -0.04
2026-01-28 13:06:40,108 | INFO | total number of ended hypotheses: 143
2026-01-28 13:06:40,108 | INFO | best hypo: ▁du▁coup▁vous▁pouvez▁pas▁travailler

2026-01-28 13:06:40,110 | INFO | speech length: 76320
2026-01-28 13:06:40,143 | INFO | decoder input length: 118
2026-01-28 13:06:40,143 | INFO | max output length: 118
2026-01-28 13:06:40,143 | INFO | min output length: 11
2026-01-28 13:06:44,462 | INFO | end detected at 43
2026-01-28 13:06:44,463 | INFO |  -3.50 * 0.5 =  -1.75 for decoder
2026-01-28 13:06:44,464 | INFO |  -2.84 * 0.5 =  -1.42 for ctc
2026-01-28 13:06:44,464 | INFO | total log probability: -3.17
2026-01-28 13:06:44,464 | INFO | normalized log probability: -0.08
2026-01-28 13:06:44,464 | INFO | total number of ended hypotheses: 158
2026-01-28 13:06:44,464 | INFO | best hypo: ▁ganapa▁et▁puis▁les▁lycées▁si▁vous▁voulez▁maintenant▁les▁livres▁sont▁gratuits

2026-01-28 13:06:44,466 | INFO | speech length: 237280
2026-01-28 13:06:44,499 | INFO | decoder input length: 370
2026-01-28 13:06:44,499 | INFO | max output length: 370
2026-01-28 13:06:44,499 | INFO | min output length: 37
2026-01-28 13:06:57,868 | INFO | end detected at 116
2026-01-28 13:06:57,870 | INFO | -195.34 * 0.5 = -97.67 for decoder
2026-01-28 13:06:57,870 | INFO | -50.44 * 0.5 = -25.22 for ctc
2026-01-28 13:06:57,870 | INFO | total log probability: -122.89
2026-01-28 13:06:57,870 | INFO | normalized log probability: -1.12
2026-01-28 13:06:57,870 | INFO | total number of ended hypotheses: 167
2026-01-28 13:06:57,871 | INFO | best hypo: ▁nous▁dans▁le▁quartier▁on▁n'a▁on▁n'a▁pas▁de▁lycée▁déjà▁y▁a▁deux▁collèges▁la▁rue▁d'alesia▁mais▁ce▁se▁y▁a▁pas▁de▁lycée▁et▁est▁la▁seule▁école▁privée▁il▁y▁avait▁dans▁le▁quartier▁mais▁moi▁avec▁qui▁je▁travaille▁un▁peu▁quand▁je▁se▁arrivais▁ici▁à▁fermé▁ya▁maintenant

2026-01-28 13:06:57,873 | INFO | speech length: 43200
2026-01-28 13:06:57,905 | INFO | decoder input length: 67
2026-01-28 13:06:57,905 | INFO | max output length: 67
2026-01-28 13:06:57,905 | INFO | min output length: 6
2026-01-28 13:07:01,022 | INFO | end detected at 35
2026-01-28 13:07:01,023 | INFO |  -2.66 * 0.5 =  -1.33 for decoder
2026-01-28 13:07:01,023 | INFO |  -0.46 * 0.5 =  -0.23 for ctc
2026-01-28 13:07:01,024 | INFO | total log probability: -1.56
2026-01-28 13:07:01,024 | INFO | normalized log probability: -0.05
2026-01-28 13:07:01,024 | INFO | total number of ended hypotheses: 156
2026-01-28 13:07:01,024 | INFO | best hypo: ▁peut▁être▁au▁moins▁douze▁ans▁je▁pense▁un▁douze▁treize▁ans

2026-01-28 13:07:01,025 | INFO | speech length: 193600
2026-01-28 13:07:01,057 | INFO | decoder input length: 302
2026-01-28 13:07:01,057 | INFO | max output length: 302
2026-01-28 13:07:01,057 | INFO | min output length: 30
2026-01-28 13:07:12,694 | INFO | end detected at 95
2026-01-28 13:07:12,696 | INFO | -81.74 * 0.5 = -40.87 for decoder
2026-01-28 13:07:12,696 | INFO | -32.74 * 0.5 = -16.37 for ctc
2026-01-28 13:07:12,696 | INFO | total log probability: -57.24
2026-01-28 13:07:12,696 | INFO | normalized log probability: -0.65
2026-01-28 13:07:12,697 | INFO | total number of ended hypotheses: 196
2026-01-28 13:07:12,698 | INFO | best hypo: ▁donc▁de▁ce▁point▁de▁vue▁là▁il▁y▁a▁deux▁maternelles▁mais▁de▁spots▁de▁villa▁sont▁des▁écoles▁publiques▁y▁a▁deux▁maternelles▁et▁une▁école▁primaire▁deprise▁la▁veine▁ça▁ne▁se▁j'imagine▁que▁des▁materns

2026-01-28 13:07:12,699 | INFO | speech length: 16000
2026-01-28 13:07:12,734 | INFO | decoder input length: 24
2026-01-28 13:07:12,734 | INFO | max output length: 24
2026-01-28 13:07:12,734 | INFO | min output length: 2
2026-01-28 13:07:14,538 | INFO | end detected at 18
2026-01-28 13:07:14,538 | INFO |  -6.04 * 0.5 =  -3.02 for decoder
2026-01-28 13:07:14,539 | INFO |  -9.94 * 0.5 =  -4.97 for ctc
2026-01-28 13:07:14,539 | INFO | total log probability: -7.99
2026-01-28 13:07:14,539 | INFO | normalized log probability: -0.61
2026-01-28 13:07:14,539 | INFO | total number of ended hypotheses: 171
2026-01-28 13:07:14,539 | INFO | best hypo: ▁il▁voit▁de▁livres▁d'eau

2026-01-28 13:07:14,540 | INFO | speech length: 429120
2026-01-28 13:07:14,575 | INFO | decoder input length: 670
2026-01-28 13:07:14,576 | INFO | max output length: 670
2026-01-28 13:07:14,576 | INFO | min output length: 67
2026-01-28 13:07:48,316 | INFO | end detected at 224
2026-01-28 13:07:48,318 | INFO | -561.90 * 0.5 = -280.95 for decoder
2026-01-28 13:07:48,318 | INFO | -206.51 * 0.5 = -103.26 for ctc
2026-01-28 13:07:48,318 | INFO | total log probability: -384.20
2026-01-28 13:07:48,318 | INFO | normalized log probability: -1.78
2026-01-28 13:07:48,318 | INFO | total number of ended hypotheses: 185
2026-01-28 13:07:48,320 | INFO | best hypo: ▁oui▁dont▁alors▁bourgheed▁pour▁ce▁qui▁est▁du▁livre▁jeunesse▁sans▁va▁dire▁du▁livre▁de▁détente▁mais▁pas▁du▁livre▁scolaire▁dans▁sa▁vrai▁au▁niveau▁scolaire▁n'en▁aa▁jamais▁vraiment▁travaillé▁avec▁l'hi▁zi▁zashghooors▁spétialement▁li▁spétialement▁wak▁boomattt▁ce▁sont▁les▁jeunes▁parents▁qui▁sont▁moins▁orientés▁sur▁le▁livre▁et▁donc▁automatiquement▁orientes▁moins▁les▁enfants▁sur▁le▁livre▁et▁tourents▁sur▁les▁grands▁parents▁qui▁achètent▁les▁livres▁pués▁donts▁part▁parler▁si▁la▁crie▁ge

2026-01-28 13:07:48,323 | INFO | speech length: 22080
2026-01-28 13:07:48,355 | INFO | decoder input length: 34
2026-01-28 13:07:48,355 | INFO | max output length: 34
2026-01-28 13:07:48,356 | INFO | min output length: 3
2026-01-28 13:07:50,029 | INFO | end detected at 17
2026-01-28 13:07:50,030 | INFO |  -1.72 * 0.5 =  -0.86 for decoder
2026-01-28 13:07:50,030 | INFO |  -7.15 * 0.5 =  -3.58 for ctc
2026-01-28 13:07:50,030 | INFO | total log probability: -4.44
2026-01-28 13:07:50,030 | INFO | normalized log probability: -0.40
2026-01-28 13:07:50,030 | INFO | total number of ended hypotheses: 170
2026-01-28 13:07:50,030 | INFO | best hypo: ▁c'est▁oui▁radio▁français

2026-01-28 13:07:50,032 | INFO | speech length: 208640
2026-01-28 13:07:50,072 | INFO | decoder input length: 325
2026-01-28 13:07:50,072 | INFO | max output length: 325
2026-01-28 13:07:50,072 | INFO | min output length: 32
2026-01-28 13:08:06,776 | INFO | end detected at 136
2026-01-28 13:08:06,777 | INFO | -142.18 * 0.5 = -71.09 for decoder
2026-01-28 13:08:06,777 | INFO | -37.27 * 0.5 = -18.64 for ctc
2026-01-28 13:08:06,777 | INFO | total log probability: -89.73
2026-01-28 13:08:06,777 | INFO | normalized log probability: -0.69
2026-01-28 13:08:06,777 | INFO | total number of ended hypotheses: 175
2026-01-28 13:08:06,779 | INFO | best hypo: ▁de▁l'humanité▁de▁la▁lecture▁oui▁oui▁oui▁ou▁absolument▁très▁inquiets▁les▁gens▁sont▁très▁absorbés▁par▁l'image▁maintenant▁à▁le▁loisir▁surtout▁sur▁ces▁générations▁là▁quand▁même▁maintenant▁ces▁dégénérations▁d'ontchoi▁ici▁ce▁qu'ont▁des▁enfants▁sont▁un▁peu▁pro▁autour▁des▁trente▁ans▁je▁pens

2026-01-28 13:08:06,781 | INFO | speech length: 174720
2026-01-28 13:08:06,821 | INFO | decoder input length: 272
2026-01-28 13:08:06,821 | INFO | max output length: 272
2026-01-28 13:08:06,821 | INFO | min output length: 27
2026-01-28 13:08:18,848 | INFO | end detected at 100
2026-01-28 13:08:18,850 | INFO | -60.72 * 0.5 = -30.36 for decoder
2026-01-28 13:08:18,850 | INFO | -24.85 * 0.5 = -12.42 for ctc
2026-01-28 13:08:18,850 | INFO | total log probability: -42.78
2026-01-28 13:08:18,850 | INFO | normalized log probability: -0.45
2026-01-28 13:08:18,850 | INFO | total number of ended hypotheses: 168
2026-01-28 13:08:18,851 | INFO | best hypo: ▁donc▁hissons▁déjà▁de▁la▁génération▁d'images▁et▁et▁d'internet▁il▁faut▁quand▁même▁le▁dire▁n'on▁ne▁sait▁pas▁tant▁le▁livre▁électronique▁pour▁le▁moment▁qu'▁est▁une▁concurrence▁et▁est▁un▁peu▁bon▁évidemment▁les▁achats▁sur▁internet

2026-01-28 13:08:18,853 | INFO | speech length: 104640
2026-01-28 13:08:18,889 | INFO | decoder input length: 163
2026-01-28 13:08:18,889 | INFO | max output length: 163
2026-01-28 13:08:18,889 | INFO | min output length: 16
2026-01-28 13:08:24,935 | INFO | end detected at 53
2026-01-28 13:08:24,937 | INFO | -11.86 * 0.5 =  -5.93 for decoder
2026-01-28 13:08:24,937 | INFO |  -6.91 * 0.5 =  -3.46 for ctc
2026-01-28 13:08:24,937 | INFO | total log probability: -9.38
2026-01-28 13:08:24,937 | INFO | normalized log probability: -0.21
2026-01-28 13:08:24,937 | INFO | total number of ended hypotheses: 213
2026-01-28 13:08:24,938 | INFO | best hypo: ▁donc▁sur▁les▁sites▁de▁vente▁de▁livres▁aim▁oui▁oui▁et▁puis▁surtout▁le▁temps▁que▁les▁gens▁passent▁devant

2026-01-28 13:08:24,940 | INFO | speech length: 194080
2026-01-28 13:08:24,974 | INFO | decoder input length: 302
2026-01-28 13:08:24,974 | INFO | max output length: 302
2026-01-28 13:08:24,974 | INFO | min output length: 30
2026-01-28 13:08:36,768 | INFO | end detected at 90
2026-01-28 13:08:36,769 | INFO | -42.04 * 0.5 = -21.02 for decoder
2026-01-28 13:08:36,769 | INFO |  -5.31 * 0.5 =  -2.66 for ctc
2026-01-28 13:08:36,770 | INFO | total log probability: -23.68
2026-01-28 13:08:36,770 | INFO | normalized log probability: -0.29
2026-01-28 13:08:36,770 | INFO | total number of ended hypotheses: 194
2026-01-28 13:08:36,771 | INFO | best hypo: ▁assassin▁manger▁énormément▁le▁le▁loisir▁lecture▁est▁devenu▁le▁loisir▁image▁quelque▁part▁on▁est▁quand▁on▁passe▁quatre▁heures▁sur▁internet▁on▁on▁a▁plus▁beaucoup▁de▁temps▁pour▁ouvrir▁un▁livre

2026-01-28 13:08:36,773 | INFO | speech length: 72640
2026-01-28 13:08:36,812 | INFO | decoder input length: 113
2026-01-28 13:08:36,812 | INFO | max output length: 113
2026-01-28 13:08:36,812 | INFO | min output length: 11
2026-01-28 13:08:40,197 | INFO | end detected at 32
2026-01-28 13:08:40,198 | INFO |  -2.49 * 0.5 =  -1.24 for decoder
2026-01-28 13:08:40,198 | INFO |  -1.84 * 0.5 =  -0.92 for ctc
2026-01-28 13:08:40,198 | INFO | total log probability: -2.17
2026-01-28 13:08:40,198 | INFO | normalized log probability: -0.09
2026-01-28 13:08:40,198 | INFO | total number of ended hypotheses: 204
2026-01-28 13:08:40,199 | INFO | best hypo: ▁donc▁et▁ça▁ce▁sont▁des▁jeunes▁qui▁sont▁vraiment▁en▁génération

2026-01-28 13:08:40,201 | INFO | speech length: 289120
2026-01-28 13:08:40,233 | INFO | decoder input length: 451
2026-01-28 13:08:40,233 | INFO | max output length: 451
2026-01-28 13:08:40,233 | INFO | min output length: 45
2026-01-28 13:09:01,822 | INFO | end detected at 162
2026-01-28 13:09:01,823 | INFO | -211.68 * 0.5 = -105.84 for decoder
2026-01-28 13:09:01,823 | INFO | -55.44 * 0.5 = -27.72 for ctc
2026-01-28 13:09:01,823 | INFO | total log probability: -133.56
2026-01-28 13:09:01,823 | INFO | normalized log probability: -0.85
2026-01-28 13:09:01,823 | INFO | total number of ended hypotheses: 149
2026-01-28 13:09:01,825 | INFO | best hypo: ▁ou▁pis▁d'autre▁part▁il▁y▁a▁un▁problème▁d'horaire▁un▁jeu▁moi▁je▁dis▁souvent▁j'ai▁l'impression▁que▁je▁ne▁sais▁pas▁dans▁d'autres▁quartiers▁un▁peu▁périphériques▁de▁paris▁mais▁moi▁j'ai▁l'impression▁par▁ici▁qu'on▁devent▁peu▁un▁quartier▁dortoir▁les▁gens▁partôt▁le▁matin▁rendre▁tous▁soi▁çà▁seul▁problème▁de▁paris▁je▁pense▁à▁la▁région▁parisienne▁aussi

2026-01-28 13:09:01,827 | INFO | speech length: 456640
2026-01-28 13:09:01,867 | INFO | decoder input length: 713
2026-01-28 13:09:01,867 | INFO | max output length: 713
2026-01-28 13:09:01,867 | INFO | min output length: 71
2026-01-28 13:09:39,476 | INFO | end detected at 232
2026-01-28 13:09:39,477 | INFO | -678.69 * 0.5 = -339.34 for decoder
2026-01-28 13:09:39,477 | INFO | -264.78 * 0.5 = -132.39 for ctc
2026-01-28 13:09:39,477 | INFO | total log probability: -471.73
2026-01-28 13:09:39,477 | INFO | normalized log probability: -2.07
2026-01-28 13:09:39,477 | INFO | total number of ended hypotheses: 142
2026-01-28 13:09:39,479 | INFO | best hypo: ▁les▁légendes▁de▁plus▁en▁plus▁de▁transports▁en▁commun▁de▁plus▁en▁plus▁de▁transports▁en▁tout▁caporal▁et▁travailler▁donc▁dans▁les▁chants▁du▁quatorzième▁étaient▁privilégiés▁restés▁dans▁la▁ville▁mais▁non▁néanmoins▁je▁vois▁j'ai▁je▁ne▁connais▁plus▁eux▁et▁y▁at▁quand▁même▁des▁bonne▁tie▁l'immeubles▁que▁je▁connais▁plus▁tropt▁mais▁chez▁les▁jeunes▁y▁voisrains▁qui▁sont▁là▁même▁quand▁même▁depuis▁le▁maintenant▁'étemps▁et▁peu▁près▁eux▁'ont▁pas▁'enfant▁mais▁ils▁cads▁tous▁les▁deux▁et▁'est▁vraigés▁horaires▁je▁ne▁vois▁pas▁retirent▁le▁matin▁mais▁le▁soir

2026-01-28 13:09:39,482 | INFO | speech length: 130400
2026-01-28 13:09:39,515 | INFO | decoder input length: 203
2026-01-28 13:09:39,515 | INFO | max output length: 203
2026-01-28 13:09:39,515 | INFO | min output length: 20
2026-01-28 13:09:49,168 | INFO | end detected at 85
2026-01-28 13:09:49,169 | INFO | -26.07 * 0.5 = -13.04 for decoder
2026-01-28 13:09:49,169 | INFO | -25.02 * 0.5 = -12.51 for ctc
2026-01-28 13:09:49,169 | INFO | total log probability: -25.55
2026-01-28 13:09:49,169 | INFO | normalized log probability: -0.33
2026-01-28 13:09:49,169 | INFO | total number of ended hypotheses: 191
2026-01-28 13:09:49,170 | INFO | best hypo: ▁je▁me▁dis▁souvent▁quand▁je▁vais▁voir▁entrer▁je▁me▁dis▁ah▁j'aurais▁décès▁de▁fait▁fait▁en▁je▁devrais▁être▁fermer▁là▁normalement▁sa▁veut▁dire▁que▁en▁général▁je▁n'y▁vois▁pas▁avant▁vingt▁heures

2026-01-28 13:09:49,172 | INFO | speech length: 102240
2026-01-28 13:09:49,209 | INFO | decoder input length: 159
2026-01-28 13:09:49,209 | INFO | max output length: 159
2026-01-28 13:09:49,209 | INFO | min output length: 15
2026-01-28 13:09:55,849 | INFO | end detected at 58
2026-01-28 13:09:55,850 | INFO |  -7.58 * 0.5 =  -3.79 for decoder
2026-01-28 13:09:55,850 | INFO | -12.24 * 0.5 =  -6.12 for ctc
2026-01-28 13:09:55,850 | INFO | total log probability: -9.91
2026-01-28 13:09:55,850 | INFO | normalized log probability: -0.18
2026-01-28 13:09:55,850 | INFO | total number of ended hypotheses: 171
2026-01-28 13:09:55,851 | INFO | best hypo: ▁ça▁peut▁près▁le▁leurs▁heures▁de▁si▁je▁les▁vois▁si▁je▁les▁vois▁vers▁vingt▁heures▁ou▁dix▁neuf▁heures▁trente▁ce▁qui▁sont▁partis

2026-01-28 13:09:55,860 | INFO | Chunk: 0 | WER=28.000000 | S=5 D=6 I=3
2026-01-28 13:09:55,861 | INFO | Chunk: 1 | WER=20.000000 | S=1 D=1 I=4
2026-01-28 13:09:55,865 | INFO | Chunk: 2 | WER=48.113208 | S=26 D=20 I=5
2026-01-28 13:09:55,867 | INFO | Chunk: 3 | WER=55.384615 | S=16 D=16 I=4
2026-01-28 13:09:55,868 | INFO | Chunk: 4 | WER=20.000000 | S=2 D=3 I=1
2026-01-28 13:09:55,870 | INFO | Chunk: 5 | WER=34.482759 | S=11 D=2 I=7
2026-01-28 13:09:55,870 | INFO | Chunk: 6 | WER=93.333333 | S=10 D=3 I=1
2026-01-28 13:09:55,871 | INFO | Chunk: 7 | WER=20.000000 | S=0 D=0 I=1
2026-01-28 13:09:55,871 | INFO | Chunk: 8 | WER=50.000000 | S=1 D=5 I=2
2026-01-28 13:09:55,873 | INFO | Chunk: 9 | WER=23.076923 | S=5 D=8 I=2
2026-01-28 13:09:55,873 | INFO | Chunk: 10 | WER=50.000000 | S=0 D=0 I=4
2026-01-28 13:09:55,874 | INFO | Chunk: 11 | WER=46.000000 | S=7 D=13 I=3
2026-01-28 13:09:55,875 | INFO | Chunk: 12 | WER=120.000000 | S=5 D=0 I=1
2026-01-28 13:09:55,879 | INFO | Chunk: 13 | WER=46.534653 | S=27 D=18 I=2
2026-01-28 13:09:55,879 | INFO | Chunk: 14 | WER=66.666667 | S=3 D=1 I=0
2026-01-28 13:09:55,881 | INFO | Chunk: 15 | WER=33.962264 | S=8 D=5 I=5
2026-01-28 13:09:55,882 | INFO | Chunk: 16 | WER=27.272727 | S=8 D=2 I=2
2026-01-28 13:09:55,882 | INFO | Chunk: 17 | WER=28.571429 | S=2 D=2 I=2
2026-01-28 13:09:55,883 | INFO | Chunk: 18 | WER=29.729730 | S=2 D=6 I=3
2026-01-28 13:09:55,884 | INFO | Chunk: 19 | WER=23.076923 | S=0 D=2 I=1
2026-01-28 13:09:55,886 | INFO | Chunk: 20 | WER=30.136986 | S=10 D=7 I=5
2026-01-28 13:09:55,892 | INFO | Chunk: 21 | WER=39.473684 | S=26 D=15 I=4
2026-01-28 13:09:55,893 | INFO | Chunk: 22 | WER=42.105263 | S=10 D=2 I=4
2026-01-28 13:09:55,893 | INFO | Chunk: 23 | WER=34.615385 | S=2 D=3 I=4
2026-01-28 13:09:56,320 | INFO | File: Rhap-D0004.wav | WER=36.540330 | S=206 D=120 I=50
2026-01-28 13:09:56,320 | INFO | ------------------------------
2026-01-28 13:09:56,320 | INFO | Conf cv Done!
2026-01-28 13:09:56,483 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 13:09:56,500 | INFO | Vocabulary size: 47
2026-01-28 13:09:57,020 | INFO | Gradient checkpoint layers: []
2026-01-28 13:09:57,962 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 13:09:57,966 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 13:09:57,966 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 13:09:57,966 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 13:09:57,968 | INFO | speech length: 228800
2026-01-28 13:09:57,999 | INFO | decoder input length: 357
2026-01-28 13:09:57,999 | INFO | max output length: 357
2026-01-28 13:09:57,999 | INFO | min output length: 35
2026-01-28 13:10:29,263 | INFO | end detected at 285
2026-01-28 13:10:29,264 | INFO | -24.96 * 0.5 = -12.48 for decoder
2026-01-28 13:10:29,264 | INFO |  -4.35 * 0.5 =  -2.18 for ctc
2026-01-28 13:10:29,264 | INFO | total log probability: -14.66
2026-01-28 13:10:29,265 | INFO | normalized log probability: -0.05
2026-01-28 13:10:29,265 | INFO | total number of ended hypotheses: 188
2026-01-28 13:10:29,268 | INFO | best hypo: euh<space>eh<space>ben<space>on<space>a<space>toujours<space>un<space>peu<space>cela<space>qui<space>vieillissent<space>euh<space>y<space>en<space>a<space>d'autres<space>qui<space>sont<space>partis<space>du<space>quartier<space>qui<space>reviennent<space>de<space>temps<space>en<space>temps<space>et<space>puis<space>bon<space>quand<space>même<space>évidemment<space>des<space>nouveaux<space>mais<space>euh<space>i<space>faut<space>quand<space>même<space>bien<space>reconnaître<space>que<space>i<space>sont<space>plus<space>jeunes<space>donc<space>moins<space>proches<space>du<space>livre

2026-01-28 13:10:29,270 | INFO | speech length: 156480
2026-01-28 13:10:29,298 | INFO | decoder input length: 244
2026-01-28 13:10:29,298 | INFO | max output length: 244
2026-01-28 13:10:29,298 | INFO | min output length: 24
2026-01-28 13:10:47,698 | INFO | end detected at 188
2026-01-28 13:10:47,700 | INFO | -16.73 * 0.5 =  -8.37 for decoder
2026-01-28 13:10:47,700 | INFO |  -2.72 * 0.5 =  -1.36 for ctc
2026-01-28 13:10:47,700 | INFO | total log probability: -9.73
2026-01-28 13:10:47,700 | INFO | normalized log probability: -0.05
2026-01-28 13:10:47,700 | INFO | total number of ended hypotheses: 195
2026-01-28 13:10:47,702 | INFO | best hypo: euh<space>i<space>s<space>ont<space>en<space>général<space>bon<space>pour<space>vivre<space>dans<space>ce<space>quartier<space>i<space>faut<space>quand<space>même<space>beaucoup<space>d'argent<space>parce<space>que<space>des<space>loyers<space>sont<space>très<space>très<space>chers<space>ici<space>euh<space>l'immobilier<space>est<space>très<space>cher<space>dans<space>le<space>car

2026-01-28 13:10:47,704 | INFO | speech length: 451040
2026-01-28 13:10:47,734 | INFO | decoder input length: 704
2026-01-28 13:10:47,734 | INFO | max output length: 704
2026-01-28 13:10:47,734 | INFO | min output length: 70
2026-01-28 13:11:58,489 | INFO | end detected at 527
2026-01-28 13:11:58,490 | INFO | -499.61 * 0.5 = -249.81 for decoder
2026-01-28 13:11:58,490 | INFO | -67.99 * 0.5 = -33.99 for ctc
2026-01-28 13:11:58,490 | INFO | total log probability: -283.80
2026-01-28 13:11:58,490 | INFO | normalized log probability: -0.55
2026-01-28 13:11:58,490 | INFO | total number of ended hypotheses: 177
2026-01-28 13:11:58,496 | INFO | best hypo: y<space>a<space>un<space>moment<space>euh<space>i<space>disaient<space>que<space>on<space>avait<space>eu<space>des<space>chiffres<space>euh<space>je<space>crois<space>qu'<space>y<space>a<space>y<space>a<space>quelques<space>années<space>i<space>s<space>avaient<space>dit<space>que<space>l'immobilier<space>elle<space>va<space>augmenter<space>d'environ<space>neuf<space>pour<space>cent<space>à<space>paris<space>et<space>euh<space>sur<space>le<space>quartier<space>vert<space>euh<space>le<space>quartier<space>vert<space>euh<space>i<space>s<space>ont<space>dit<space>que<space>ça<space>avait<space>augmenté<space>d'à<space>peu<space>près<space>douze<space>à<space>treize<space>pour<space>cent<space>là<space>par<space>contre<space>donc<space>vous<space>voyez<space>ce<space>qui<space>fait<space>que<space>finalement<space>le<space>l'immobilier<space>de<space>ci<space>dans<space>ce<space>coin<space>es<space>très<space>très<space>cher<space>et<space>on<space>n'arrive<space>plus<space>à<space>manir<space>des<space>clientèles<space>ce<space>qui<space>sons<space>e<space>locations<space>la<space>moyenne<space>ses<space>environ<space>trois<space>ans

2026-01-28 13:11:58,498 | INFO | speech length: 263360
2026-01-28 13:11:58,526 | INFO | decoder input length: 411
2026-01-28 13:11:58,526 | INFO | max output length: 411
2026-01-28 13:11:58,526 | INFO | min output length: 41
2026-01-28 13:12:38,112 | INFO | end detected at 341
2026-01-28 13:12:38,114 | INFO | -45.51 * 0.5 = -22.76 for decoder
2026-01-28 13:12:38,114 | INFO | -30.38 * 0.5 = -15.19 for ctc
2026-01-28 13:12:38,114 | INFO | total log probability: -37.95
2026-01-28 13:12:38,114 | INFO | normalized log probability: -0.11
2026-01-28 13:12:38,114 | INFO | total number of ended hypotheses: 180
2026-01-28 13:12:38,118 | INFO | best hypo: y<space>a<space>des<space>déménagements<space>et<space>des<space>aménagements<space>permanences<space>i<space>reste<space>pas<space>longtemps<space>sur<space>le<space>coin<space>apparemment<space>y<space>a<space>pas<space>de<space>très<space>grands<space>appartements<space>donc<space>des<space>jeunes<space>couples<space>pas<space>s'installent<space>dès<space>qu'<space>y<space>a<space>un<space>enfant<space>voilà<space>voilà<space>dès<space>qu'<space>y<space>a<space>un<space>enfant<space>où<space>de<space>au<space>moins<space>deux<space>le<space>d<space>quoi<space>que<space>le<space>deuxième<space>arrive<space>y<space>parte<space>et<space>très<space>souvent<space>i<space>partout<space>en<space>banlieue

2026-01-28 13:12:38,120 | INFO | speech length: 140640
2026-01-28 13:12:38,153 | INFO | decoder input length: 219
2026-01-28 13:12:38,153 | INFO | max output length: 219
2026-01-28 13:12:38,153 | INFO | min output length: 21
2026-01-28 13:12:53,553 | INFO | end detected at 149
2026-01-28 13:12:53,554 | INFO | -11.64 * 0.5 =  -5.82 for decoder
2026-01-28 13:12:53,554 | INFO |  -0.98 * 0.5 =  -0.49 for ctc
2026-01-28 13:12:53,554 | INFO | total log probability: -6.31
2026-01-28 13:12:53,554 | INFO | normalized log probability: -0.04
2026-01-28 13:12:53,554 | INFO | total number of ended hypotheses: 211
2026-01-28 13:12:53,556 | INFO | best hypo: reste<space>pas<space>à<space>paris<space>donc<space>euh<space>ça<space>c'est<space>un<space>petit<space>peu<space>pour<space>ce<space>qui<space>a<space>été<space>des<space>changements<space>de<space>du<space>quartier<space>moi<space>je<space>suis<space>là<space>depuis<space>euh<space>vingt<space>trois<space>ans

2026-01-28 13:12:53,558 | INFO | speech length: 224960
2026-01-28 13:12:53,593 | INFO | decoder input length: 351
2026-01-28 13:12:53,593 | INFO | max output length: 351
2026-01-28 13:12:53,593 | INFO | min output length: 35
2026-01-28 13:13:29,918 | INFO | end detected at 323
2026-01-28 13:13:29,919 | INFO | -42.01 * 0.5 = -21.00 for decoder
2026-01-28 13:13:29,919 | INFO | -25.81 * 0.5 = -12.90 for ctc
2026-01-28 13:13:29,919 | INFO | total log probability: -33.91
2026-01-28 13:13:29,919 | INFO | normalized log probability: -0.11
2026-01-28 13:13:29,920 | INFO | total number of ended hypotheses: 195
2026-01-28 13:13:29,923 | INFO | best hypo: donc<space>c'est<space>vrai<space>que<space>ça<space>fait<space>quand<space>même<space>euh<space>beaucoup<space>de<space>changements<space>ce<space>qui<space>fait<space>que<space>pour<space>nous<space>maintenant<space>dans<space>des<space>petites<space>dans<space>des<space>petits<space>crus<space>comme<space>ça<space>où<space>on<space>n'a<space>pas<space>ce<space>passage<space>de<space>l'avenue<space>les<space>gens<space>ne<space>rentrent<space>pas<space>dans<space>les<space>quartiers<space>maintenant<space>donc<space>euh<space>on<space>a<space>bien<space>coup<space>de<space>mal<space>à<space>s'intenir<space>bon<space>à<space>une<space>clientèle<space>de<space>carte

2026-01-28 13:13:29,925 | INFO | speech length: 58560
2026-01-28 13:13:29,953 | INFO | decoder input length: 91
2026-01-28 13:13:29,953 | INFO | max output length: 91
2026-01-28 13:13:29,953 | INFO | min output length: 9
2026-01-28 13:13:38,206 | INFO | end detected at 89
2026-01-28 13:13:38,207 | INFO | -18.01 * 0.5 =  -9.01 for decoder
2026-01-28 13:13:38,207 | INFO | -25.30 * 0.5 = -12.65 for ctc
2026-01-28 13:13:38,207 | INFO | total log probability: -21.66
2026-01-28 13:13:38,207 | INFO | normalized log probability: -0.27
2026-01-28 13:13:38,207 | INFO | total number of ended hypotheses: 194
2026-01-28 13:13:38,208 | INFO | best hypo: et<space>puis<space>y<space>a<space>quand<space>même<space>un<space>problème<space>fréancier<space>en<space>a<space>quand<space>même<space>mis<space>là<space>dessus<space>mais

2026-01-28 13:13:38,210 | INFO | speech length: 37600
2026-01-28 13:13:38,243 | INFO | decoder input length: 58
2026-01-28 13:13:38,243 | INFO | max output length: 58
2026-01-28 13:13:38,243 | INFO | min output length: 5
2026-01-28 13:13:42,450 | INFO | end detected at 47
2026-01-28 13:13:42,452 | INFO |  -3.31 * 0.5 =  -1.66 for decoder
2026-01-28 13:13:42,452 | INFO |  -2.27 * 0.5 =  -1.14 for ctc
2026-01-28 13:13:42,452 | INFO | total log probability: -2.79
2026-01-28 13:13:42,452 | INFO | normalized log probability: -0.07
2026-01-28 13:13:42,452 | INFO | total number of ended hypotheses: 216
2026-01-28 13:13:42,453 | INFO | best hypo: du<space>coup<space>vous<space>ne<space>pouvez<space>pas<space>travailler

2026-01-28 13:13:42,455 | INFO | speech length: 76320
2026-01-28 13:13:42,487 | INFO | decoder input length: 118
2026-01-28 13:13:42,487 | INFO | max output length: 118
2026-01-28 13:13:42,487 | INFO | min output length: 11
2026-01-28 13:13:51,653 | INFO | end detected at 92
2026-01-28 13:13:51,654 | INFO |  -6.80 * 0.5 =  -3.40 for decoder
2026-01-28 13:13:51,655 | INFO |  -0.23 * 0.5 =  -0.12 for ctc
2026-01-28 13:13:51,655 | INFO | total log probability: -3.51
2026-01-28 13:13:51,655 | INFO | normalized log probability: -0.04
2026-01-28 13:13:51,655 | INFO | total number of ended hypotheses: 179
2026-01-28 13:13:51,656 | INFO | best hypo: y<space>en<space>a<space>pas<space>et<space>puis<space>les<space>lycées<space>si<space>vous<space>voulez<space>euh<space>maintenant<space>les<space>livres<space>sont<space>gratuits

2026-01-28 13:13:51,657 | INFO | speech length: 237280
2026-01-28 13:13:51,684 | INFO | decoder input length: 370
2026-01-28 13:13:51,684 | INFO | max output length: 370
2026-01-28 13:13:51,684 | INFO | min output length: 37
2026-01-28 13:14:25,056 | INFO | end detected at 278
2026-01-28 13:14:25,058 | INFO | -26.85 * 0.5 = -13.42 for decoder
2026-01-28 13:14:25,058 | INFO | -11.17 * 0.5 =  -5.59 for ctc
2026-01-28 13:14:25,058 | INFO | total log probability: -19.01
2026-01-28 13:14:25,058 | INFO | normalized log probability: -0.07
2026-01-28 13:14:25,058 | INFO | total number of ended hypotheses: 222
2026-01-28 13:14:25,061 | INFO | best hypo: euh<space>nous<space>dans<space>le<space>quartier<space>on<space>n'a<space>on<space>n'a<space>pas<space>de<space>lycée<space>déjà<space>y<space>a<space>deux<space>collèges<space>la<space>rue<space>euh<space>rue<space>d'alésia<space>mais<space>s<space>s<space>y<space>a<space>pas<space>de<space>lycée<space>et<space>la<space>seule<space>école<space>privée<space>euh<space>qui<space>y<space>avait<space>dans<space>le<space>quartier<space>moi<space>avec<space>qui<space>je<space>travaillais<space>un<space>peu<space>quand<space>je<space>suis<space>arrivé<space>ici<space>à<space>fermer<space>y<space>a<space>maintenant

2026-01-28 13:14:25,064 | INFO | speech length: 43200
2026-01-28 13:14:25,095 | INFO | decoder input length: 67
2026-01-28 13:14:25,095 | INFO | max output length: 67
2026-01-28 13:14:25,095 | INFO | min output length: 6
2026-01-28 13:14:31,008 | INFO | end detected at 63
2026-01-28 13:14:31,009 | INFO |  -4.56 * 0.5 =  -2.28 for decoder
2026-01-28 13:14:31,009 | INFO |  -0.13 * 0.5 =  -0.06 for ctc
2026-01-28 13:14:31,009 | INFO | total log probability: -2.34
2026-01-28 13:14:31,009 | INFO | normalized log probability: -0.04
2026-01-28 13:14:31,009 | INFO | total number of ended hypotheses: 176
2026-01-28 13:14:31,010 | INFO | best hypo: peut<space>être<space>au<space>moins<space>douze<space>ans<space>je<space>pense<space>à<space>douze<space>treize<space>ans

2026-01-28 13:14:31,012 | INFO | speech length: 193600
2026-01-28 13:14:31,043 | INFO | decoder input length: 302
2026-01-28 13:14:31,043 | INFO | max output length: 302
2026-01-28 13:14:31,044 | INFO | min output length: 30
2026-01-28 13:14:57,986 | INFO | end detected at 239
2026-01-28 13:14:57,988 | INFO | -23.50 * 0.5 = -11.75 for decoder
2026-01-28 13:14:57,988 | INFO | -21.84 * 0.5 = -10.92 for ctc
2026-01-28 13:14:57,988 | INFO | total log probability: -22.67
2026-01-28 13:14:57,988 | INFO | normalized log probability: -0.10
2026-01-28 13:14:57,988 | INFO | total number of ended hypotheses: 256
2026-01-28 13:14:57,991 | INFO | best hypo: donc<space>euh<space>de<space>ce<space>point<space>de<space>vue<space>là<space>puis<space>y<space>a<space>deux<space>maternels<space>mais<space>euh<space>de<space>ce<space>point<space>de<space>vue<space>là<space>ce<space>sont<space>des<space>écoles<space>publiques<space>hein<space>y<space>a<space>y<space>a<space>deux<space>maternelles<space>et<space>une<space>école<space>primaire<space>reprise<space>d'avance<space>ça<space>monsieur<space>j'imagine<space>que<space>des<space>maternels

2026-01-28 13:14:57,994 | INFO | speech length: 16000
2026-01-28 13:14:58,026 | INFO | decoder input length: 24
2026-01-28 13:14:58,026 | INFO | max output length: 24
2026-01-28 13:14:58,026 | INFO | min output length: 2
2026-01-28 13:15:00,115 | INFO | adding <eos> in the last position in the loop
2026-01-28 13:15:00,124 | INFO | no hypothesis. Finish decoding.
2026-01-28 13:15:00,125 | INFO |  -2.40 * 0.5 =  -1.20 for decoder
2026-01-28 13:15:00,125 | INFO |  -5.30 * 0.5 =  -2.65 for ctc
2026-01-28 13:15:00,125 | INFO | total log probability: -3.85
2026-01-28 13:15:00,125 | INFO | normalized log probability: -0.18
2026-01-28 13:15:00,125 | INFO | total number of ended hypotheses: 133
2026-01-28 13:15:00,125 | INFO | best hypo: soit<space>de<space>livres<space>donc

2026-01-28 13:15:00,126 | INFO | speech length: 429120
2026-01-28 13:15:00,154 | INFO | decoder input length: 670
2026-01-28 13:15:00,154 | INFO | max output length: 670
2026-01-28 13:15:00,154 | INFO | min output length: 67
2026-01-28 13:16:08,636 | INFO | end detected at 560
2026-01-28 13:16:08,638 | INFO | -553.81 * 0.5 = -276.90 for decoder
2026-01-28 13:16:08,638 | INFO | -97.29 * 0.5 = -48.65 for ctc
2026-01-28 13:16:08,638 | INFO | total log probability: -325.55
2026-01-28 13:16:08,638 | INFO | normalized log probability: -0.59
2026-01-28 13:16:08,638 | INFO | total number of ended hypotheses: 204
2026-01-28 13:16:08,644 | INFO | best hypo: oui<space>bon<space>alors<space>euh<space>pour<space>ce<space>qui<space>est<space>de<space>pour<space>ce<space>qui<space>est<space>du<space>livre<space>jeunesse<space>on<space>va<space>dire<space>du<space>livre<space>de<space>détente<space>mais<space>pas<space>pas<space>du<space>livre<space>scolaire<space>hein<space>c'est<space>vrai<space>qu'au<space>niveau<space>scolaire<space>mais<space>on<space>n'a<space>jamais<space>vraiment<space>travaillé<space>euh<space>avec<space>euh<space>je<space>lui<space>disais<space>aux<space>algomes<space>pas<space>spécialement<space>pas<space>spécialement<space>c'est<space>vrai<space>que<space>bon<space>maintenant<space>ce<space>sont<space>de<space>jeunes<space>parents<space>qui<space>sont<space>moins<space>orientés<space>sur<space>le<space>livre<space>donc<space>qui<space>automatiquement<space>oriententment<space>les<space>enfants<space>sur<space>le<space>livre<space>don<space>jes<space>mais<space>c<space>esus<space>les<space>rans<space>parents<space>qui<space>achètent<space>les<space>livres<space>quis<space>jes<space>don<space>c<space>pas<space>parles<space>c'est<space>la<space>cris<space>générales

2026-01-28 13:16:08,647 | INFO | speech length: 22080
2026-01-28 13:16:08,675 | INFO | decoder input length: 34
2026-01-28 13:16:08,675 | INFO | max output length: 34
2026-01-28 13:16:08,675 | INFO | min output length: 3
2026-01-28 13:16:11,255 | INFO | end detected at 31
2026-01-28 13:16:11,256 | INFO |  -3.53 * 0.5 =  -1.77 for decoder
2026-01-28 13:16:11,256 | INFO |  -3.39 * 0.5 =  -1.70 for ctc
2026-01-28 13:16:11,256 | INFO | total log probability: -3.46
2026-01-28 13:16:11,256 | INFO | normalized log probability: -0.14
2026-01-28 13:16:11,256 | INFO | total number of ended hypotheses: 183
2026-01-28 13:16:11,256 | INFO | best hypo: c'est<space>fin<space>des<space>français

2026-01-28 13:16:11,258 | INFO | speech length: 208640
2026-01-28 13:16:11,285 | INFO | decoder input length: 325
2026-01-28 13:16:11,285 | INFO | max output length: 325
2026-01-28 13:16:11,285 | INFO | min output length: 32
2026-01-28 13:16:34,435 | INFO | end detected at 295
2026-01-28 13:16:34,436 | INFO | -29.29 * 0.5 = -14.65 for decoder
2026-01-28 13:16:34,436 | INFO | -13.82 * 0.5 =  -6.91 for ctc
2026-01-28 13:16:34,436 | INFO | total log probability: -21.56
2026-01-28 13:16:34,437 | INFO | normalized log probability: -0.07
2026-01-28 13:16:34,437 | INFO | total number of ended hypotheses: 182
2026-01-28 13:16:34,440 | INFO | best hypo: de<space>l'humanité<space>et<space>de<space>la<space>lecture<space>oui<space>oui<space>oui<space>absolument<space>très<space>inqui<space>les<space>gens<space>sont<space>très<space>absorbés<space>par<space>l'image<space>maintenant<space>à<space>le<space>loisir<space>surtout<space>sur<space>ces<space>générations<space>là<space>quand<space>même<space>maintenant<space>c'est<space>des<space>générations<space>bon<space>je<space>vois<space>ici<space>ce<space>qu'ont<space>des<space>enfants<space>sont<space>à<space>peu<space>près<space>autour<space>des<space>trente<space>ans<space>je<space>pense

2026-01-28 13:16:34,441 | INFO | speech length: 174720
2026-01-28 13:16:34,468 | INFO | decoder input length: 272
2026-01-28 13:16:34,468 | INFO | max output length: 272
2026-01-28 13:16:34,468 | INFO | min output length: 27
2026-01-28 13:16:44,267 | INFO | end detected at 232
2026-01-28 13:16:44,269 | INFO | -18.24 * 0.5 =  -9.12 for decoder
2026-01-28 13:16:44,269 | INFO |  -3.34 * 0.5 =  -1.67 for ctc
2026-01-28 13:16:44,269 | INFO | total log probability: -10.79
2026-01-28 13:16:44,269 | INFO | normalized log probability: -0.05
2026-01-28 13:16:44,269 | INFO | total number of ended hypotheses: 188
2026-01-28 13:16:44,272 | INFO | best hypo: donc<space>euh<space>i<space>sont<space>déjà<space>de<space>la<space>génération<space>de<space>l'image<space>et<space>et<space>d'internet<space>i<space>faut<space>quand<space>même<space>le<space>dire<space>donc<space>c'est<space>pas<space>tant<space>le<space>livre<space>électronique<space>pour<space>le<space>moment<space>qu'est<space>une<space>concurrence<space>c'est<space>un<space>peu<space>bon<space>évidemment<space>les<space>achats<space>sur<space>internet

2026-01-28 13:16:44,274 | INFO | speech length: 104640
2026-01-28 13:16:44,301 | INFO | decoder input length: 163
2026-01-28 13:16:44,301 | INFO | max output length: 163
2026-01-28 13:16:44,301 | INFO | min output length: 16
2026-01-28 13:16:48,534 | INFO | end detected at 118
2026-01-28 13:16:48,535 | INFO |  -9.20 * 0.5 =  -4.60 for decoder
2026-01-28 13:16:48,535 | INFO |  -0.93 * 0.5 =  -0.47 for ctc
2026-01-28 13:16:48,535 | INFO | total log probability: -5.07
2026-01-28 13:16:48,535 | INFO | normalized log probability: -0.04
2026-01-28 13:16:48,535 | INFO | total number of ended hypotheses: 155
2026-01-28 13:16:48,537 | INFO | best hypo: euh<space>donc<space>sur<space>les<space>sites<space>de<space>vente<space>de<space>livres<space>ça<space>et<space>puis<space>oui<space>et<space>puis<space>surtout<space>le<space>le<space>temps<space>que<space>les<space>gens<space>passent<space>devant

2026-01-28 13:16:48,538 | INFO | speech length: 194080
2026-01-28 13:16:48,564 | INFO | decoder input length: 302
2026-01-28 13:16:48,564 | INFO | max output length: 302
2026-01-28 13:16:48,564 | INFO | min output length: 30
2026-01-28 13:16:58,179 | INFO | end detected at 208
2026-01-28 13:16:58,181 | INFO | -17.42 * 0.5 =  -8.71 for decoder
2026-01-28 13:16:58,181 | INFO |  -4.30 * 0.5 =  -2.15 for ctc
2026-01-28 13:16:58,181 | INFO | total log probability: -10.86
2026-01-28 13:16:58,181 | INFO | normalized log probability: -0.05
2026-01-28 13:16:58,181 | INFO | total number of ended hypotheses: 184
2026-01-28 13:16:58,183 | INFO | best hypo: à<space>ça<space>ça<space>a<space>mangé<space>énormément<space>le<space>le<space>loisir<space>lecture<space>euh<space>est<space>devenu<space>le<space>loisir<space>image<space>hein<space>quelque<space>part<space>on<space>est<space>quand<space>on<space>passe<space>quatre<space>heures<space>sur<space>internet<space>on<space>on<space>n'a<space>plus<space>beaucoup<space>de<space>temps<space>pour<space>ouvrir<space>un<space>livre

2026-01-28 13:16:58,185 | INFO | speech length: 72640
2026-01-28 13:16:58,210 | INFO | decoder input length: 113
2026-01-28 13:16:58,210 | INFO | max output length: 113
2026-01-28 13:16:58,210 | INFO | min output length: 11
2026-01-28 13:17:00,710 | INFO | end detected at 77
2026-01-28 13:17:00,712 | INFO |  -6.10 * 0.5 =  -3.05 for decoder
2026-01-28 13:17:00,712 | INFO |  -2.65 * 0.5 =  -1.32 for ctc
2026-01-28 13:17:00,712 | INFO | total log probability: -4.37
2026-01-28 13:17:00,712 | INFO | normalized log probability: -0.06
2026-01-28 13:17:00,712 | INFO | total number of ended hypotheses: 220
2026-01-28 13:17:00,713 | INFO | best hypo: donc<space>euh<space>et<space>et<space>ça<space>ce<space>sont<space>des<space>jeunes<space>qui<space>sont<space>vraiment<space>en<space>génération

2026-01-28 13:17:00,715 | INFO | speech length: 289120
2026-01-28 13:17:00,741 | INFO | decoder input length: 451
2026-01-28 13:17:00,742 | INFO | max output length: 451
2026-01-28 13:17:00,742 | INFO | min output length: 45
2026-01-28 13:17:20,914 | INFO | end detected at 368
2026-01-28 13:17:20,916 | INFO | -47.79 * 0.5 = -23.89 for decoder
2026-01-28 13:17:20,916 | INFO | -14.50 * 0.5 =  -7.25 for ctc
2026-01-28 13:17:20,916 | INFO | total log probability: -31.14
2026-01-28 13:17:20,916 | INFO | normalized log probability: -0.09
2026-01-28 13:17:20,916 | INFO | total number of ended hypotheses: 200
2026-01-28 13:17:20,920 | INFO | best hypo: et<space>puis<space>d'autre<space>part<space>y<space>a<space>un<space>problème<space>d'horraire<space>hein<space>je<space>moi<space>je<space>dis<space>souvent<space>j'ai<space>l'impression<space>que<space>je<space>sais<space>pas<space>dans<space>d'autres<space>quartiers<space>un<space>peu<space>périphériques<space>de<space>paris<space>euh<space>mais<space>moi<space>j'ai<space>l'impression<space>ici<space>qu'on<space>devient<space>un<space>peu<space>un<space>quartier<space>d'hortoire<space>les<space>gens<space>parent<space>tôt<space>le<space>matin<space>rentre<space>tout<space>le<space>soir<space>ça<space>c'est<space>le<space>problème<space>de<space>paris<space>je<space>pense<space>et<space>de<space>la<space>région<space>parisienne

2026-01-28 13:17:20,922 | INFO | speech length: 456640
2026-01-28 13:17:20,949 | INFO | decoder input length: 713
2026-01-28 13:17:20,949 | INFO | max output length: 713
2026-01-28 13:17:20,949 | INFO | min output length: 71
2026-01-28 13:18:04,301 | INFO | end detected at 576
2026-01-28 13:18:04,303 | INFO | -667.65 * 0.5 = -333.82 for decoder
2026-01-28 13:18:04,303 | INFO | -33.01 * 0.5 = -16.50 for ctc
2026-01-28 13:18:04,303 | INFO | total log probability: -350.33
2026-01-28 13:18:04,303 | INFO | normalized log probability: -0.61
2026-01-28 13:18:04,303 | INFO | total number of ended hypotheses: 175
2026-01-28 13:18:04,309 | INFO | best hypo: les<space>les<space>gens<space>de<space>plus<space>en<space>plus<space>de<space>transports<space>en<space>commun<space>euh<space>de<space>plus<space>en<space>plus<space>de<space>transports<space>en<space>tout<space>cas<space>pour<space>aller<space>travailler<space>donc<space>euh<space>penser<space>que<space>les<space>gens<space>du<space>quatorzième<space>étaient<space>privilégiés<space>restés<space>dans<space>la<space>ville<space>mais<space>nous<space>non<space>moi<space>je<space>vois<space>j'ai<space>je<space>connais<space>plus<space>euh<space>il<space>y<space>a<space>quand<space>même<space>une<space>bonne<space>partie<space>de<space>l'immeuble<space>que<space>je<space>connais<space>plus<space>trop<space>mais<space>j'ai<space>des<space>jeunes<space>voisins<space>qui<space>sont<space>là<space>quand<space>même<space>depuis<space>maintenant<space>euh<space>sept<space>ans<space>à<space>peu<space>près<space>euh<space>bon<space>euh<space>il<space>non<space>pas<space>d'enfants<space>mais<space>i<space>sont<space>cades<space>tous<space>les<space>deus<space>et<space>c'est<space>vrai<space>qu<space>les<space>horaires<space>je<space>les<space>vois<space>pas<space>partir<space>le<space>matin<space>mais<space>le<space>soir<space>euh

2026-01-28 13:18:04,311 | INFO | speech length: 130400
2026-01-28 13:18:04,338 | INFO | decoder input length: 203
2026-01-28 13:18:04,338 | INFO | max output length: 203
2026-01-28 13:18:04,338 | INFO | min output length: 20
2026-01-28 13:18:11,194 | INFO | end detected at 190
2026-01-28 13:18:11,195 | INFO | -18.78 * 0.5 =  -9.39 for decoder
2026-01-28 13:18:11,196 | INFO | -13.64 * 0.5 =  -6.82 for ctc
2026-01-28 13:18:11,196 | INFO | total log probability: -16.21
2026-01-28 13:18:11,196 | INFO | normalized log probability: -0.09
2026-01-28 13:18:11,196 | INFO | total number of ended hypotheses: 180
2026-01-28 13:18:11,198 | INFO | best hypo: je<space>me<space>dis<space>souvent<space>quand<space>je<space>les<space>vois<space>rentrer<space>je<space>me<space>dis<space>ah<space>je<space>re<space>ça<space>de<space>fait<space>faire<space>je<space>devrais<space>être<space>fermé<space>là<space>normalement<space>ça<space>veut<space>dire<space>que<space>en<space>général<space>et<space>je<space>les<space>vois<space>pas<space>avant<space>vingt<space>heures

2026-01-28 13:18:11,200 | INFO | speech length: 102240
2026-01-28 13:18:11,228 | INFO | decoder input length: 159
2026-01-28 13:18:11,228 | INFO | max output length: 159
2026-01-28 13:18:11,228 | INFO | min output length: 15
2026-01-28 13:18:16,332 | INFO | end detected at 144
2026-01-28 13:18:16,334 | INFO | -13.66 * 0.5 =  -6.83 for decoder
2026-01-28 13:18:16,334 | INFO |  -6.03 * 0.5 =  -3.02 for ctc
2026-01-28 13:18:16,334 | INFO | total log probability: -9.84
2026-01-28 13:18:16,334 | INFO | normalized log probability: -0.07
2026-01-28 13:18:16,334 | INFO | total number of ended hypotheses: 191
2026-01-28 13:18:16,336 | INFO | best hypo: c'est<space>à<space>peu<space>près<space>le<space>le<space>leurs<space>heures<space>de<space>si<space>je<space>les<space>vois<space>euh<space>si<space>je<space>les<space>vois<space>vers<space>vingt<space>heures<space>ou<space>dix<space>neuf<space>heures<space>trente<space>et<space>qu'i<space>sont<space>partis

2026-01-28 13:18:16,343 | INFO | Chunk: 0 | WER=22.000000 | S=5 D=1 I=5
2026-01-28 13:18:16,344 | INFO | Chunk: 1 | WER=30.000000 | S=2 D=0 I=7
2026-01-28 13:18:16,350 | INFO | Chunk: 2 | WER=29.245283 | S=16 D=5 I=10
2026-01-28 13:18:16,352 | INFO | Chunk: 3 | WER=30.769231 | S=10 D=5 I=5
2026-01-28 13:18:16,353 | INFO | Chunk: 4 | WER=20.000000 | S=1 D=2 I=3
2026-01-28 13:18:16,355 | INFO | Chunk: 5 | WER=20.689655 | S=5 D=0 I=7
2026-01-28 13:18:16,355 | INFO | Chunk: 6 | WER=73.333333 | S=1 D=4 I=6
2026-01-28 13:18:16,355 | INFO | Chunk: 7 | WER=40.000000 | S=0 D=0 I=2
2026-01-28 13:18:16,356 | INFO | Chunk: 8 | WER=31.250000 | S=0 D=2 I=3
2026-01-28 13:18:16,358 | INFO | Chunk: 9 | WER=16.923077 | S=8 D=3 I=0
2026-01-28 13:18:16,358 | INFO | Chunk: 10 | WER=50.000000 | S=0 D=0 I=4
2026-01-28 13:18:16,359 | INFO | Chunk: 11 | WER=36.000000 | S=11 D=5 I=2
2026-01-28 13:18:16,360 | INFO | Chunk: 12 | WER=100.000000 | S=4 D=1 I=0
2026-01-28 13:18:16,365 | INFO | Chunk: 13 | WER=25.742574 | S=19 D=1 I=6
2026-01-28 13:18:16,365 | INFO | Chunk: 14 | WER=66.666667 | S=3 D=1 I=0
2026-01-28 13:18:16,366 | INFO | Chunk: 15 | WER=15.094340 | S=3 D=1 I=4
2026-01-28 13:18:16,368 | INFO | Chunk: 16 | WER=11.363636 | S=3 D=0 I=2
2026-01-28 13:18:16,368 | INFO | Chunk: 17 | WER=19.047619 | S=1 D=0 I=3
2026-01-28 13:18:16,369 | INFO | Chunk: 18 | WER=21.621622 | S=1 D=2 I=5
2026-01-28 13:18:16,370 | INFO | Chunk: 19 | WER=23.076923 | S=0 D=1 I=2
2026-01-28 13:18:16,372 | INFO | Chunk: 20 | WER=16.438356 | S=6 D=2 I=4
2026-01-28 13:18:16,378 | INFO | Chunk: 21 | WER=20.175439 | S=11 D=4 I=8
2026-01-28 13:18:16,379 | INFO | Chunk: 22 | WER=28.947368 | S=6 D=2 I=3
2026-01-28 13:18:16,380 | INFO | Chunk: 23 | WER=23.076923 | S=0 D=0 I=6
2026-01-28 13:18:16,860 | INFO | File: Rhap-D0004.wav | WER=23.226433 | S=120 D=32 I=87
2026-01-28 13:18:16,861 | INFO | ------------------------------
2026-01-28 13:18:16,861 | INFO | Conf ester Done!
2026-01-28 13:21:12,028 | INFO | Chunk: 0 | WER=20.000000 | S=5 D=2 I=3
2026-01-28 13:21:12,029 | INFO | Chunk: 1 | WER=16.666667 | S=1 D=1 I=3
2026-01-28 13:21:12,036 | INFO | Chunk: 2 | WER=19.811321 | S=13 D=4 I=4
2026-01-28 13:21:12,039 | INFO | Chunk: 3 | WER=29.230769 | S=7 D=9 I=3
2026-01-28 13:21:12,039 | INFO | Chunk: 4 | WER=16.666667 | S=1 D=3 I=1
2026-01-28 13:21:12,041 | INFO | Chunk: 5 | WER=15.517241 | S=7 D=0 I=2
2026-01-28 13:21:12,042 | INFO | Chunk: 6 | WER=100.000000 | S=3 D=3 I=9
2026-01-28 13:21:12,042 | INFO | Chunk: 7 | WER=60.000000 | S=2 D=0 I=1
2026-01-28 13:21:12,042 | INFO | Chunk: 8 | WER=18.750000 | S=0 D=1 I=2
2026-01-28 13:21:12,044 | INFO | Chunk: 9 | WER=20.000000 | S=7 D=6 I=0
2026-01-28 13:21:12,045 | INFO | Chunk: 10 | WER=50.000000 | S=0 D=0 I=4
2026-01-28 13:21:12,046 | INFO | Chunk: 11 | WER=42.000000 | S=16 D=5 I=0
2026-01-28 13:21:12,046 | INFO | Chunk: 12 | WER=100.000000 | S=3 D=2 I=0
2026-01-28 13:21:12,051 | INFO | Chunk: 13 | WER=33.663366 | S=25 D=7 I=2
2026-01-28 13:21:12,052 | INFO | Chunk: 14 | WER=66.666667 | S=3 D=1 I=0
2026-01-28 13:21:12,053 | INFO | Chunk: 15 | WER=24.528302 | S=5 D=4 I=4
2026-01-28 13:21:12,055 | INFO | Chunk: 16 | WER=9.090909 | S=2 D=1 I=1
2026-01-28 13:21:12,055 | INFO | Chunk: 17 | WER=14.285714 | S=0 D=1 I=2
2026-01-28 13:21:12,056 | INFO | Chunk: 18 | WER=27.027027 | S=3 D=3 I=4
2026-01-28 13:21:12,056 | INFO | Chunk: 19 | WER=7.692308 | S=0 D=1 I=0
2026-01-28 13:21:12,059 | INFO | Chunk: 20 | WER=19.178082 | S=10 D=3 I=1
2026-01-28 13:21:12,065 | INFO | Chunk: 21 | WER=15.789474 | S=10 D=5 I=3
2026-01-28 13:21:12,066 | INFO | Chunk: 22 | WER=47.368421 | S=8 D=6 I=4
2026-01-28 13:21:12,067 | INFO | Chunk: 23 | WER=19.230769 | S=0 D=0 I=5
2026-01-28 13:21:12,520 | INFO | File: Rhap-D0004.wav | WER=23.615160 | S=135 D=59 I=49
2026-01-28 13:21:12,520 | INFO | ------------------------------
2026-01-28 13:21:12,521 | INFO | hmm_tdnn Done!
2026-01-28 13:21:12,657 | INFO | ==================================Rhap-D0005.wav=========================================
2026-01-28 13:21:12,802 | INFO | Using rVAD model
2026-01-28 13:21:24,459 | INFO | Chunk: 0 | WER=0.000000 | S=1 D=0 I=2
2026-01-28 13:21:24,459 | INFO | Chunk: 1 | WER=100.000000 | S=1 D=0 I=0
2026-01-28 13:21:24,459 | INFO | Chunk: 2 | WER=100.000000 | S=1 D=1 I=0
2026-01-28 13:21:24,460 | INFO | Chunk: 3 | WER=78.125000 | S=8 D=14 I=3
2026-01-28 13:21:24,461 | INFO | Chunk: 4 | WER=47.058824 | S=6 D=7 I=3
2026-01-28 13:21:24,461 | INFO | Chunk: 5 | WER=100.000000 | S=5 D=0 I=0
2026-01-28 13:21:24,461 | INFO | Chunk: 6 | WER=100.000000 | S=1 D=2 I=0
2026-01-28 13:21:24,461 | INFO | Chunk: 7 | WER=300.000000 | S=1 D=0 I=2
2026-01-28 13:21:24,461 | INFO | Chunk: 8 | WER=500.000000 | S=1 D=0 I=4
2026-01-28 13:21:24,461 | INFO | Chunk: 9 | WER=233.333333 | S=1 D=0 I=6
2026-01-28 13:21:24,463 | INFO | Chunk: 10 | WER=29.508197 | S=2 D=13 I=3
2026-01-28 13:21:24,464 | INFO | Chunk: 11 | WER=33.333333 | S=2 D=3 I=5
2026-01-28 13:21:24,464 | INFO | Chunk: 12 | WER=90.000000 | S=9 D=0 I=0
2026-01-28 13:21:24,466 | INFO | Chunk: 13 | WER=46.666667 | S=4 D=6 I=11
2026-01-28 13:21:24,466 | INFO | Chunk: 14 | WER=100.000000 | S=1 D=3 I=0
2026-01-28 13:21:24,466 | INFO | Chunk: 15 | WER=70.000000 | S=1 D=3 I=3
2026-01-28 13:21:24,469 | INFO | Chunk: 16 | WER=37.974684 | S=16 D=13 I=1
2026-01-28 13:21:24,469 | INFO | Chunk: 17 | WER=150.000000 | S=4 D=0 I=2
2026-01-28 13:21:24,469 | INFO | Chunk: 18 | WER=58.823529 | S=2 D=3 I=5
2026-01-28 13:21:24,470 | INFO | Chunk: 19 | WER=88.888889 | S=5 D=3 I=0
2026-01-28 13:21:24,470 | INFO | Chunk: 20 | WER=100.000000 | S=9 D=0 I=0
2026-01-28 13:21:24,470 | INFO | Chunk: 21 | WER=90.476190 | S=2 D=9 I=8
2026-01-28 13:21:24,470 | INFO | Chunk: 22 | WER=125.000000 | S=0 D=2 I=3
2026-01-28 13:21:24,470 | INFO | Chunk: 23 | WER=250.000000 | S=2 D=0 I=3
2026-01-28 13:21:24,471 | INFO | Chunk: 24 | WER=70.454545 | S=12 D=15 I=4
2026-01-28 13:21:24,475 | INFO | Chunk: 25 | WER=47.674419 | S=19 D=19 I=3
2026-01-28 13:21:24,475 | INFO | Chunk: 26 | WER=59.090909 | S=4 D=8 I=1
2026-01-28 13:21:24,475 | INFO | Chunk: 27 | WER=100.000000 | S=1 D=5 I=0
2026-01-28 13:21:24,476 | INFO | Chunk: 28 | WER=56.097561 | S=6 D=14 I=3
2026-01-28 13:21:24,477 | INFO | Chunk: 29 | WER=73.076923 | S=1 D=10 I=8
2026-01-28 13:21:24,477 | INFO | Chunk: 30 | WER=133.333333 | S=1 D=1 I=2
2026-01-28 13:21:24,477 | INFO | Chunk: 31 | WER=100.000000 | S=6 D=2 I=0
2026-01-28 13:21:24,483 | INFO | Chunk: 32 | WER=32.258065 | S=11 D=27 I=2
2026-01-28 13:21:24,694 | INFO | File: Rhap-D0005.wav | WER=46.452477 | S=183 D=130 I=34
2026-01-28 13:21:24,694 | INFO | ------------------------------
2026-01-28 13:21:24,694 | INFO | w2vec vad chunk Done!
2026-01-28 13:21:46,916 | INFO | Chunk: 0 | WER=0.000000 | S=1 D=0 I=1
2026-01-28 13:21:46,916 | INFO | Chunk: 1 | WER=300.000000 | S=1 D=0 I=2
2026-01-28 13:21:46,917 | INFO | Chunk: 2 | WER=150.000000 | S=2 D=0 I=1
2026-01-28 13:21:46,917 | INFO | Chunk: 3 | WER=68.750000 | S=6 D=13 I=3
2026-01-28 13:21:46,918 | INFO | Chunk: 4 | WER=44.117647 | S=5 D=6 I=4
2026-01-28 13:21:46,918 | INFO | Chunk: 5 | WER=100.000000 | S=5 D=0 I=0
2026-01-28 13:21:46,918 | INFO | Chunk: 6 | WER=100.000000 | S=1 D=2 I=0
2026-01-28 13:21:46,918 | INFO | Chunk: 7 | WER=100.000000 | S=1 D=0 I=0
2026-01-28 13:21:46,919 | INFO | Chunk: 8 | WER=700.000000 | S=1 D=0 I=6
2026-01-28 13:21:46,919 | INFO | Chunk: 9 | WER=200.000000 | S=0 D=0 I=6
2026-01-28 13:21:46,920 | INFO | Chunk: 10 | WER=42.622951 | S=3 D=23 I=0
2026-01-28 13:21:46,921 | INFO | Chunk: 11 | WER=20.000000 | S=1 D=5 I=0
2026-01-28 13:21:46,921 | INFO | Chunk: 12 | WER=100.000000 | S=9 D=0 I=1
2026-01-28 13:21:46,922 | INFO | Chunk: 13 | WER=75.555556 | S=2 D=32 I=0
2026-01-28 13:21:46,922 | INFO | Chunk: 14 | WER=100.000000 | S=1 D=3 I=0
2026-01-28 13:21:46,922 | INFO | Chunk: 15 | WER=60.000000 | S=1 D=1 I=4
2026-01-28 13:21:46,923 | INFO | Chunk: 16 | WER=86.075949 | S=4 D=64 I=0
2026-01-28 13:21:46,923 | INFO | Chunk: 17 | WER=150.000000 | S=4 D=0 I=2
2026-01-28 13:21:46,924 | INFO | Chunk: 18 | WER=100.000000 | S=1 D=8 I=8
2026-01-28 13:21:46,924 | INFO | Chunk: 19 | WER=88.888889 | S=5 D=3 I=0
2026-01-28 13:21:46,924 | INFO | Chunk: 20 | WER=122.222222 | S=9 D=0 I=2
2026-01-28 13:21:46,925 | INFO | Chunk: 21 | WER=80.952381 | S=1 D=9 I=7
2026-01-28 13:21:46,925 | INFO | Chunk: 22 | WER=100.000000 | S=0 D=2 I=2
2026-01-28 13:21:46,925 | INFO | Chunk: 23 | WER=100.000000 | S=2 D=0 I=0
2026-01-28 13:21:46,926 | INFO | Chunk: 24 | WER=52.272727 | S=6 D=17 I=0
2026-01-28 13:21:46,927 | INFO | Chunk: 25 | WER=76.744186 | S=4 D=62 I=0
2026-01-28 13:21:46,928 | INFO | Chunk: 26 | WER=59.090909 | S=6 D=7 I=0
2026-01-28 13:21:46,928 | INFO | Chunk: 27 | WER=100.000000 | S=1 D=5 I=0
2026-01-28 13:21:46,929 | INFO | Chunk: 28 | WER=41.463415 | S=2 D=14 I=1
2026-01-28 13:21:46,929 | INFO | Chunk: 29 | WER=76.923077 | S=1 D=11 I=8
2026-01-28 13:21:46,929 | INFO | Chunk: 30 | WER=100.000000 | S=2 D=1 I=0
2026-01-28 13:21:46,929 | INFO | Chunk: 31 | WER=100.000000 | S=5 D=3 I=0
2026-01-28 13:21:46,932 | INFO | Chunk: 32 | WER=72.580645 | S=3 D=87 I=0
2026-01-28 13:21:47,068 | INFO | File: Rhap-D0005.wav | WER=63.186078 | S=119 D=336 I=17
2026-01-28 13:21:47,068 | INFO | ------------------------------
2026-01-28 13:21:47,069 | INFO | whisper med Done!
2026-01-28 13:22:21,188 | INFO | Chunk: 0 | WER=0.000000 | S=1 D=0 I=10
2026-01-28 13:22:21,189 | INFO | Chunk: 1 | WER=800.000000 | S=1 D=0 I=7
2026-01-28 13:22:21,189 | INFO | Chunk: 2 | WER=150.000000 | S=2 D=0 I=1
2026-01-28 13:22:21,189 | INFO | Chunk: 3 | WER=62.500000 | S=1 D=16 I=3
2026-01-28 13:22:21,190 | INFO | Chunk: 4 | WER=38.235294 | S=3 D=6 I=4
2026-01-28 13:22:21,190 | INFO | Chunk: 5 | WER=100.000000 | S=5 D=0 I=0
2026-01-28 13:22:21,191 | INFO | Chunk: 6 | WER=100.000000 | S=1 D=2 I=0
2026-01-28 13:22:21,191 | INFO | Chunk: 7 | WER=300.000000 | S=1 D=0 I=2
2026-01-28 13:22:21,191 | INFO | Chunk: 8 | WER=700.000000 | S=1 D=0 I=6
2026-01-28 13:22:21,191 | INFO | Chunk: 9 | WER=233.333333 | S=1 D=0 I=6
2026-01-28 13:22:21,192 | INFO | Chunk: 10 | WER=39.344262 | S=3 D=21 I=0
2026-01-28 13:22:21,193 | INFO | Chunk: 11 | WER=23.333333 | S=1 D=1 I=5
2026-01-28 13:22:21,194 | INFO | Chunk: 12 | WER=110.000000 | S=8 D=1 I=2
2026-01-28 13:22:21,195 | INFO | Chunk: 13 | WER=33.333333 | S=1 D=7 I=7
2026-01-28 13:22:21,195 | INFO | Chunk: 14 | WER=100.000000 | S=1 D=3 I=0
2026-01-28 13:22:21,195 | INFO | Chunk: 15 | WER=90.000000 | S=2 D=3 I=4
2026-01-28 13:22:21,196 | INFO | Chunk: 16 | WER=83.544304 | S=3 D=63 I=0
2026-01-28 13:22:21,197 | INFO | Chunk: 17 | WER=150.000000 | S=4 D=0 I=2
2026-01-28 13:22:21,197 | INFO | Chunk: 18 | WER=88.235294 | S=5 D=3 I=7
2026-01-28 13:22:21,197 | INFO | Chunk: 19 | WER=88.888889 | S=5 D=3 I=0
2026-01-28 13:22:21,197 | INFO | Chunk: 20 | WER=122.222222 | S=1 D=4 I=6
2026-01-28 13:22:21,198 | INFO | Chunk: 21 | WER=85.714286 | S=2 D=9 I=7
2026-01-28 13:22:21,198 | INFO | Chunk: 22 | WER=150.000000 | S=0 D=2 I=4
2026-01-28 13:22:21,198 | INFO | Chunk: 23 | WER=150.000000 | S=2 D=0 I=1
2026-01-28 13:22:21,199 | INFO | Chunk: 24 | WER=38.636364 | S=7 D=10 I=0
2026-01-28 13:22:21,201 | INFO | Chunk: 25 | WER=79.069767 | S=5 D=63 I=0
2026-01-28 13:22:21,201 | INFO | Chunk: 26 | WER=63.636364 | S=1 D=8 I=5
2026-01-28 13:22:21,201 | INFO | Chunk: 27 | WER=100.000000 | S=1 D=5 I=0
2026-01-28 13:22:21,202 | INFO | Chunk: 28 | WER=41.463415 | S=3 D=13 I=1
2026-01-28 13:22:21,203 | INFO | Chunk: 29 | WER=76.923077 | S=1 D=11 I=8
2026-01-28 13:22:21,203 | INFO | Chunk: 30 | WER=100.000000 | S=3 D=0 I=0
2026-01-28 13:22:21,203 | INFO | Chunk: 31 | WER=100.000000 | S=7 D=1 I=0
2026-01-28 13:22:21,206 | INFO | Chunk: 32 | WER=75.000000 | S=13 D=80 I=0
2026-01-28 13:22:21,372 | INFO | File: Rhap-D0005.wav | WER=58.768407 | S=129 D=273 I=37
2026-01-28 13:22:21,372 | INFO | ------------------------------
2026-01-28 13:22:21,372 | INFO | whisper large Done!
2026-01-28 13:22:21,530 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 13:22:21,560 | INFO | Vocabulary size: 350
2026-01-28 13:22:22,127 | INFO | Gradient checkpoint layers: []
2026-01-28 13:22:23,041 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 13:22:23,045 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 13:22:23,045 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 13:22:23,046 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 13:22:23,046 | INFO | speech length: 10400
2026-01-28 13:22:23,081 | INFO | decoder input length: 15
2026-01-28 13:22:23,082 | INFO | max output length: 15
2026-01-28 13:22:23,082 | INFO | min output length: 1
2026-01-28 13:22:23,962 | INFO | end detected at 9
2026-01-28 13:22:23,964 | INFO |  -2.60 * 0.5 =  -1.30 for decoder
2026-01-28 13:22:23,964 | INFO |  -3.09 * 0.5 =  -1.54 for ctc
2026-01-28 13:22:23,964 | INFO | total log probability: -2.85
2026-01-28 13:22:23,964 | INFO | normalized log probability: -0.95
2026-01-28 13:22:23,964 | INFO | total number of ended hypotheses: 177
2026-01-28 13:22:23,964 | INFO | best hypo: ▁un

2026-01-28 13:22:23,967 | INFO | speech length: 12320
2026-01-28 13:22:24,015 | INFO | decoder input length: 18
2026-01-28 13:22:24,015 | INFO | max output length: 18
2026-01-28 13:22:24,015 | INFO | min output length: 1
2026-01-28 13:22:25,120 | INFO | end detected at 11
2026-01-28 13:22:25,121 | INFO |  -1.06 * 0.5 =  -0.53 for decoder
2026-01-28 13:22:25,121 | INFO |  -2.21 * 0.5 =  -1.10 for ctc
2026-01-28 13:22:25,121 | INFO | total log probability: -1.64
2026-01-28 13:22:25,121 | INFO | normalized log probability: -0.27
2026-01-28 13:22:25,121 | INFO | total number of ended hypotheses: 160
2026-01-28 13:22:25,121 | INFO | best hypo: ▁vous▁savez

2026-01-28 13:22:25,123 | INFO | speech length: 131840
2026-01-28 13:22:25,159 | INFO | decoder input length: 205
2026-01-28 13:22:25,160 | INFO | max output length: 205
2026-01-28 13:22:25,160 | INFO | min output length: 20
2026-01-28 13:22:32,120 | INFO | end detected at 62
2026-01-28 13:22:32,121 | INFO | -32.57 * 0.5 = -16.28 for decoder
2026-01-28 13:22:32,122 | INFO | -29.82 * 0.5 = -14.91 for ctc
2026-01-28 13:22:32,122 | INFO | total log probability: -31.19
2026-01-28 13:22:32,122 | INFO | normalized log probability: -0.59
2026-01-28 13:22:32,122 | INFO | total number of ended hypotheses: 192
2026-01-28 13:22:32,122 | INFO | best hypo: ▁ah▁oui▁oui▁oui▁bad▁au▁bayeu▁de▁saint▁jeff▁plusieurs▁mais▁fins▁de▁métier▁en▁fa▁disant▁j'a▁j'ai▁d'abord▁travaillé▁dans▁des

2026-01-28 13:22:32,124 | INFO | speech length: 231200
2026-01-28 13:22:32,156 | INFO | decoder input length: 360
2026-01-28 13:22:32,156 | INFO | max output length: 360
2026-01-28 13:22:32,156 | INFO | min output length: 36
2026-01-28 13:22:44,527 | INFO | end detected at 99
2026-01-28 13:22:44,528 | INFO | -76.26 * 0.5 = -38.13 for decoder
2026-01-28 13:22:44,528 | INFO | -12.70 * 0.5 =  -6.35 for ctc
2026-01-28 13:22:44,528 | INFO | total log probability: -44.48
2026-01-28 13:22:44,528 | INFO | normalized log probability: -0.47
2026-01-28 13:22:44,528 | INFO | total number of ended hypotheses: 159
2026-01-28 13:22:44,530 | INFO | best hypo: ▁mais▁comment▁les▁entreprises▁de▁d'automatisme▁va▁on▁métier▁donc▁c'est▁l'électronique▁au▁début▁les▁automatismes▁de▁corps▁et▁peu▁après▁va▁j'ai▁travaillé▁sur▁les▁microprocesseurs▁l'informatique▁et▁maintenant▁je▁suis

2026-01-28 13:22:44,532 | INFO | speech length: 33280
2026-01-28 13:22:44,560 | INFO | decoder input length: 51
2026-01-28 13:22:44,560 | INFO | max output length: 51
2026-01-28 13:22:44,560 | INFO | min output length: 5
2026-01-28 13:22:46,894 | INFO | end detected at 25
2026-01-28 13:22:46,894 | INFO |  -4.45 * 0.5 =  -2.23 for decoder
2026-01-28 13:22:46,894 | INFO |  -1.39 * 0.5 =  -0.70 for ctc
2026-01-28 13:22:46,895 | INFO | total log probability: -2.92
2026-01-28 13:22:46,895 | INFO | normalized log probability: -0.14
2026-01-28 13:22:46,895 | INFO | total number of ended hypotheses: 163
2026-01-28 13:22:46,895 | INFO | best hypo: ▁spécialisée▁dans▁la▁programmation▁scientifique

2026-01-28 13:22:46,896 | INFO | speech length: 8640
2026-01-28 13:22:46,923 | INFO | decoder input length: 13
2026-01-28 13:22:46,924 | INFO | max output length: 13
2026-01-28 13:22:46,924 | INFO | min output length: 1
2026-01-28 13:22:47,837 | INFO | end detected at 10
2026-01-28 13:22:47,839 | INFO |  -0.86 * 0.5 =  -0.43 for decoder
2026-01-28 13:22:47,839 | INFO |  -3.04 * 0.5 =  -1.52 for ctc
2026-01-28 13:22:47,839 | INFO | total log probability: -1.95
2026-01-28 13:22:47,839 | INFO | normalized log probability: -0.39
2026-01-28 13:22:47,839 | INFO | total number of ended hypotheses: 180
2026-01-28 13:22:47,839 | INFO | best hypo: ▁rien

2026-01-28 13:22:47,840 | INFO | speech length: 14880
2026-01-28 13:22:47,867 | INFO | decoder input length: 22
2026-01-28 13:22:47,867 | INFO | max output length: 22
2026-01-28 13:22:47,867 | INFO | min output length: 2
2026-01-28 13:22:48,637 | INFO | end detected at 8
2026-01-28 13:22:48,638 | INFO |  -0.54 * 0.5 =  -0.27 for decoder
2026-01-28 13:22:48,638 | INFO |  -2.68 * 0.5 =  -1.34 for ctc
2026-01-28 13:22:48,638 | INFO | total log probability: -1.61
2026-01-28 13:22:48,638 | INFO | normalized log probability: -0.40
2026-01-28 13:22:48,638 | INFO | total number of ended hypotheses: 145
2026-01-28 13:22:48,638 | INFO | best hypo: ▁hey

2026-01-28 13:22:48,639 | INFO | speech length: 22720
2026-01-28 13:22:48,671 | INFO | decoder input length: 35
2026-01-28 13:22:48,671 | INFO | max output length: 35
2026-01-28 13:22:48,671 | INFO | min output length: 3
2026-01-28 13:22:50,620 | INFO | end detected at 21
2026-01-28 13:22:50,621 | INFO |  -3.55 * 0.5 =  -1.77 for decoder
2026-01-28 13:22:50,621 | INFO |  -2.97 * 0.5 =  -1.48 for ctc
2026-01-28 13:22:50,621 | INFO | total log probability: -3.26
2026-01-28 13:22:50,621 | INFO | normalized log probability: -0.22
2026-01-28 13:22:50,621 | INFO | total number of ended hypotheses: 186
2026-01-28 13:22:50,622 | INFO | best hypo: ▁il▁dans▁un▁laboratoire▁du▁seigneurie

2026-01-28 13:22:50,623 | INFO | speech length: 43360
2026-01-28 13:22:50,652 | INFO | decoder input length: 67
2026-01-28 13:22:50,652 | INFO | max output length: 67
2026-01-28 13:22:50,652 | INFO | min output length: 6
2026-01-28 13:22:52,968 | INFO | end detected at 24
2026-01-28 13:22:52,970 | INFO |  -4.02 * 0.5 =  -2.01 for decoder
2026-01-28 13:22:52,970 | INFO |  -3.74 * 0.5 =  -1.87 for ctc
2026-01-28 13:22:52,970 | INFO | total log probability: -3.88
2026-01-28 13:22:52,970 | INFO | normalized log probability: -0.26
2026-01-28 13:22:52,970 | INFO | total number of ended hypotheses: 195
2026-01-28 13:22:52,970 | INFO | best hypo: ▁et▁donc▁ça▁se▁passe▁bien▁sinon

2026-01-28 13:22:52,972 | INFO | speech length: 422880
2026-01-28 13:22:53,005 | INFO | decoder input length: 660
2026-01-28 13:22:53,005 | INFO | max output length: 660
2026-01-28 13:22:53,005 | INFO | min output length: 66
2026-01-28 13:23:13,823 | INFO | end detected at 130
2026-01-28 13:23:13,825 | INFO | -310.65 * 0.5 = -155.33 for decoder
2026-01-28 13:23:13,825 | INFO | -82.91 * 0.5 = -41.45 for ctc
2026-01-28 13:23:13,825 | INFO | total log probability: -196.78
2026-01-28 13:23:13,825 | INFO | normalized log probability: -1.56
2026-01-28 13:23:13,825 | INFO | total number of ended hypotheses: 139
2026-01-28 13:23:13,826 | INFO | best hypo: ▁bah▁eu▁je▁suis▁si▁si▁quand▁même▁parce▁que▁moi▁j'ai▁suivi▁les▁évolutions▁donc▁que▁ça▁fait▁à▁peu▁preuve▁vingt▁cinq▁ans▁que▁je▁suis▁dans▁ce▁laboratoire▁et▁au▁début▁n'y▁avait▁pratiquement▁pas▁d'informaque▁moi▁je▁suis▁rentré▁pour▁me▁faire▁automatismes▁d'▁appareillage▁avec▁des▁microprocesseurs

2026-01-28 13:23:13,828 | INFO | speech length: 152640
2026-01-28 13:23:13,861 | INFO | decoder input length: 238
2026-01-28 13:23:13,862 | INFO | max output length: 238
2026-01-28 13:23:13,862 | INFO | min output length: 23
2026-01-28 13:23:22,843 | INFO | end detected at 78
2026-01-28 13:23:22,844 | INFO | -17.03 * 0.5 =  -8.52 for decoder
2026-01-28 13:23:22,844 | INFO | -19.12 * 0.5 =  -9.56 for ctc
2026-01-28 13:23:22,845 | INFO | total log probability: -18.07
2026-01-28 13:23:22,845 | INFO | normalized log probability: -0.27
2026-01-28 13:23:22,845 | INFO | total number of ended hypotheses: 200
2026-01-28 13:23:22,845 | INFO | best hypo: ▁et▁petit▁à▁petit▁l'informatique▁s'est▁développé▁et▁y▁a▁de▁plus▁en▁plus▁d'appareils▁comme▁j'étais▁le▁seul▁informaticien▁cette▁venue▁assez▁dure▁puisque▁mato

2026-01-28 13:23:22,848 | INFO | speech length: 49760
2026-01-28 13:23:22,881 | INFO | decoder input length: 77
2026-01-28 13:23:22,881 | INFO | max output length: 77
2026-01-28 13:23:22,881 | INFO | min output length: 7
2026-01-28 13:23:26,384 | INFO | end detected at 35
2026-01-28 13:23:26,385 | INFO |  -7.73 * 0.5 =  -3.86 for decoder
2026-01-28 13:23:26,385 | INFO |  -4.64 * 0.5 =  -2.32 for ctc
2026-01-28 13:23:26,385 | INFO | total log probability: -6.18
2026-01-28 13:23:26,385 | INFO | normalized log probability: -0.22
2026-01-28 13:23:26,385 | INFO | total number of ended hypotheses: 176
2026-01-28 13:23:26,386 | INFO | best hypo: ▁bâtiment▁puisque▁maintenant▁je▁responsable▁il▁y▁a▁un▁parc

2026-01-28 13:23:26,388 | INFO | speech length: 245760
2026-01-28 13:23:26,424 | INFO | decoder input length: 383
2026-01-28 13:23:26,424 | INFO | max output length: 383
2026-01-28 13:23:26,424 | INFO | min output length: 38
2026-01-28 13:23:40,876 | INFO | end detected at 125
2026-01-28 13:23:40,877 | INFO | -163.63 * 0.5 = -81.82 for decoder
2026-01-28 13:23:40,877 | INFO | -46.01 * 0.5 = -23.01 for ctc
2026-01-28 13:23:40,877 | INFO | total log probability: -104.82
2026-01-28 13:23:40,877 | INFO | normalized log probability: -0.90
2026-01-28 13:23:40,877 | INFO | total number of ended hypotheses: 169
2026-01-28 13:23:40,879 | INFO | best hypo: ▁une▁cinquantaine▁d'ordinateurs▁plus▁un▁certain▁nombre▁d'imprimantes▁étou▁ça▁et▁bon▁à▁sa▁pose▁des▁problèmes▁de▁maintenant▁en▁fin▁de▁maintenance▁se▁de▁mise▁à▁jour▁et▁tout▁çailà▁donc▁que▁c'ait▁un▁moment▁donnait▁cette▁bonnée▁surtout▁en▁même▁temps▁j'avais▁de▁mon▁travail▁aussi

2026-01-28 13:23:40,881 | INFO | speech length: 55360
2026-01-28 13:23:40,913 | INFO | decoder input length: 86
2026-01-28 13:23:40,913 | INFO | max output length: 86
2026-01-28 13:23:40,913 | INFO | min output length: 8
2026-01-28 13:23:44,971 | INFO | end detected at 41
2026-01-28 13:23:44,972 | INFO |  -9.76 * 0.5 =  -4.88 for decoder
2026-01-28 13:23:44,972 | INFO |  -9.96 * 0.5 =  -4.98 for ctc
2026-01-28 13:23:44,972 | INFO | total log probability: -9.86
2026-01-28 13:23:44,972 | INFO | normalized log probability: -0.29
2026-01-28 13:23:44,972 | INFO | total number of ended hypotheses: 192
2026-01-28 13:23:44,973 | INFO | best hypo: ▁écrire▁des▁programmes▁étout▁des▁logiciels▁donc▁bombe▁à▁la▁manoche

2026-01-28 13:23:44,975 | INFO | speech length: 279200
2026-01-28 13:23:45,008 | INFO | decoder input length: 435
2026-01-28 13:23:45,008 | INFO | max output length: 435
2026-01-28 13:23:45,008 | INFO | min output length: 43
2026-01-28 13:24:04,374 | INFO | end detected at 156
2026-01-28 13:24:04,375 | INFO | -267.88 * 0.5 = -133.94 for decoder
2026-01-28 13:24:04,375 | INFO | -131.62 * 0.5 = -65.81 for ctc
2026-01-28 13:24:04,375 | INFO | total log probability: -199.75
2026-01-28 13:24:04,375 | INFO | normalized log probability: -1.31
2026-01-28 13:24:04,375 | INFO | total number of ended hypotheses: 128
2026-01-28 13:24:04,377 | INFO | best hypo: ▁le▁plus▁ou▁moins▁aidé▁par▁quelqu'un▁un▁mi▁temps▁heureusement▁que▁ce▁silencieux▁s'est▁maintenable▁c'était▁trop▁de▁ya▁trop▁de▁boulots▁surtout▁que▁il▁a▁fallu▁imer▁les▁gensciers▁informatiques▁en▁vingt▁cinq▁en▁vingt▁cinq▁ans▁les▁janlier▁à▁vingt▁cinq▁ans▁les▁gens▁ne▁connaissait▁ay▁riens▁savait▁oitant▁travailler▁sip▁eux▁il▁si▁savaient▁pas▁utiliser▁un▁itinateur

2026-01-28 13:24:04,379 | INFO | speech length: 27200
2026-01-28 13:24:04,412 | INFO | decoder input length: 42
2026-01-28 13:24:04,412 | INFO | max output length: 42
2026-01-28 13:24:04,412 | INFO | min output length: 4
2026-01-28 13:24:06,229 | INFO | end detected at 20
2026-01-28 13:24:06,231 | INFO |  -1.15 * 0.5 =  -0.58 for decoder
2026-01-28 13:24:06,231 | INFO |  -0.39 * 0.5 =  -0.20 for ctc
2026-01-28 13:24:06,231 | INFO | total log probability: -0.77
2026-01-28 13:24:06,231 | INFO | normalized log probability: -0.05
2026-01-28 13:24:06,231 | INFO | total number of ended hypotheses: 145
2026-01-28 13:24:06,231 | INFO | best hypo: ▁maintenant▁les▁jeunes▁qui▁arrivent

2026-01-28 13:24:06,233 | INFO | speech length: 75200
2026-01-28 13:24:06,265 | INFO | decoder input length: 117
2026-01-28 13:24:06,265 | INFO | max output length: 117
2026-01-28 13:24:06,265 | INFO | min output length: 11
2026-01-28 13:24:10,249 | INFO | end detected at 43
2026-01-28 13:24:10,250 | INFO |  -5.20 * 0.5 =  -2.60 for decoder
2026-01-28 13:24:10,251 | INFO |  -6.54 * 0.5 =  -3.27 for ctc
2026-01-28 13:24:10,251 | INFO | total log probability: -5.87
2026-01-28 13:24:10,251 | INFO | normalized log probability: -0.15
2026-01-28 13:24:10,251 | INFO | total number of ended hypotheses: 158
2026-01-28 13:24:10,251 | INFO | best hypo: ▁son▁évêque▁mais▁vont▁sacrer▁un▁problème▁pour▁se▁mettre▁à▁l'informatique▁assez▁évident

2026-01-28 13:24:10,253 | INFO | speech length: 24480
2026-01-28 13:24:10,294 | INFO | decoder input length: 37
2026-01-28 13:24:10,294 | INFO | max output length: 37
2026-01-28 13:24:10,294 | INFO | min output length: 3
2026-01-28 13:24:12,628 | INFO | end detected at 22
2026-01-28 13:24:12,630 | INFO |  -1.86 * 0.5 =  -0.93 for decoder
2026-01-28 13:24:12,630 | INFO |  -3.37 * 0.5 =  -1.68 for ctc
2026-01-28 13:24:12,630 | INFO | total log probability: -2.61
2026-01-28 13:24:12,630 | INFO | normalized log probability: -0.16
2026-01-28 13:24:12,630 | INFO | total number of ended hypotheses: 163
2026-01-28 13:24:12,630 | INFO | best hypo: ▁maintenant▁ça▁se▁passe▁mieux

2026-01-28 13:24:12,632 | INFO | speech length: 37120
2026-01-28 13:24:12,661 | INFO | decoder input length: 57
2026-01-28 13:24:12,661 | INFO | max output length: 57
2026-01-28 13:24:12,661 | INFO | min output length: 5
2026-01-28 13:24:15,751 | INFO | end detected at 30
2026-01-28 13:24:15,752 | INFO |  -9.87 * 0.5 =  -4.94 for decoder
2026-01-28 13:24:15,752 | INFO | -27.60 * 0.5 = -13.80 for ctc
2026-01-28 13:24:15,752 | INFO | total log probability: -18.73
2026-01-28 13:24:15,752 | INFO | normalized log probability: -0.89
2026-01-28 13:24:15,752 | INFO | total number of ended hypotheses: 161
2026-01-28 13:24:15,753 | INFO | best hypo: ▁moi▁te▁jeunes▁comme▁moi▁je▁qui▁connais▁ce▁pas▁roche

2026-01-28 13:24:15,754 | INFO | speech length: 105280
2026-01-28 13:24:15,792 | INFO | decoder input length: 164
2026-01-28 13:24:15,792 | INFO | max output length: 164
2026-01-28 13:24:15,792 | INFO | min output length: 16
2026-01-28 13:24:21,002 | INFO | end detected at 45
2026-01-28 13:24:21,004 | INFO | -11.54 * 0.5 =  -5.77 for decoder
2026-01-28 13:24:21,004 | INFO | -20.87 * 0.5 = -10.43 for ctc
2026-01-28 13:24:21,004 | INFO | total log probability: -16.21
2026-01-28 13:24:21,004 | INFO | normalized log probability: -0.44
2026-01-28 13:24:21,004 | INFO | total number of ended hypotheses: 190
2026-01-28 13:24:21,005 | INFO | best hypo: ▁oh▁vous▁faites▁vous▁êtes▁une▁génération▁qui▁qui▁a▁vous▁vécu▁avec▁dans▁les▁cahiers▁des▁gens

2026-01-28 13:24:21,006 | INFO | speech length: 24960
2026-01-28 13:24:21,039 | INFO | decoder input length: 38
2026-01-28 13:24:21,040 | INFO | max output length: 38
2026-01-28 13:24:21,040 | INFO | min output length: 3
2026-01-28 13:24:23,130 | INFO | end detected at 20
2026-01-28 13:24:23,131 | INFO |  -6.19 * 0.5 =  -3.10 for decoder
2026-01-28 13:24:23,131 | INFO |  -7.03 * 0.5 =  -3.52 for ctc
2026-01-28 13:24:23,131 | INFO | total log probability: -6.61
2026-01-28 13:24:23,132 | INFO | normalized log probability: -0.66
2026-01-28 13:24:23,132 | INFO | total number of ended hypotheses: 192
2026-01-28 13:24:23,132 | INFO | best hypo: ▁l'histoire▁du▁no

2026-01-28 13:24:23,134 | INFO | speech length: 12480
2026-01-28 13:24:23,167 | INFO | decoder input length: 19
2026-01-28 13:24:23,167 | INFO | max output length: 19
2026-01-28 13:24:23,167 | INFO | min output length: 1
2026-01-28 13:24:24,951 | INFO | end detected at 17
2026-01-28 13:24:24,953 | INFO |  -4.89 * 0.5 =  -2.45 for decoder
2026-01-28 13:24:24,953 | INFO |  -6.34 * 0.5 =  -3.17 for ctc
2026-01-28 13:24:24,953 | INFO | total log probability: -5.62
2026-01-28 13:24:24,953 | INFO | normalized log probability: -0.51
2026-01-28 13:24:24,953 | INFO | total number of ended hypotheses: 188
2026-01-28 13:24:24,953 | INFO | best hypo: ▁ce▁dire▁fin▁cesser

2026-01-28 13:24:24,955 | INFO | speech length: 130880
2026-01-28 13:24:24,988 | INFO | decoder input length: 204
2026-01-28 13:24:24,988 | INFO | max output length: 204
2026-01-28 13:24:24,988 | INFO | min output length: 20
2026-01-28 13:24:34,661 | INFO | end detected at 85
2026-01-28 13:24:34,662 | INFO | -32.51 * 0.5 = -16.26 for decoder
2026-01-28 13:24:34,662 | INFO | -30.11 * 0.5 = -15.06 for ctc
2026-01-28 13:24:34,662 | INFO | total log probability: -31.31
2026-01-28 13:24:34,662 | INFO | normalized log probability: -0.41
2026-01-28 13:24:34,662 | INFO | total number of ended hypotheses: 176
2026-01-28 13:24:34,663 | INFO | best hypo: ▁mais▁c'est▁un▁peu▁vrai▁sa▁les▁gens▁du▁but▁quand▁il▁voyait▁un▁ordinateur▁une▁souris▁zaïse▁et▁savait▁parce▁que▁c'était▁vous▁avait▁pord▁il▁avait▁peur▁de▁toucher▁un▁clavier▁enfin

2026-01-28 13:24:34,665 | INFO | speech length: 295680
2026-01-28 13:24:34,705 | INFO | decoder input length: 461
2026-01-28 13:24:34,705 | INFO | max output length: 461
2026-01-28 13:24:34,705 | INFO | min output length: 46
2026-01-28 13:24:56,124 | INFO | end detected at 165
2026-01-28 13:24:56,125 | INFO | -288.57 * 0.5 = -144.28 for decoder
2026-01-28 13:24:56,126 | INFO | -149.18 * 0.5 = -74.59 for ctc
2026-01-28 13:24:56,126 | INFO | total log probability: -218.87
2026-01-28 13:24:56,126 | INFO | normalized log probability: -1.40
2026-01-28 13:24:56,126 | INFO | total number of ended hypotheses: 204
2026-01-28 13:24:56,127 | INFO | best hypo: ▁puis▁disait▁le▁même▁pur▁si▁tu▁faisais▁d'appuyer▁sur▁une▁mauvaise▁souche▁ils▁avaient▁l'impression▁qu'il▁avait▁corsée▁la▁machine▁hite▁que▁brûle▁avait▁pu▁messsé▁la▁peur▁du▁débutant▁mais▁bomba▁et▁faux▁bomba▁et▁puis▁elle▁alors▁réussie▁avec▁les▁chose▁qui▁n'alait▁pas▁bon▁c'était▁tout▁de▁suite▁repppos▁que▁ça▁mang▁pas▁pourquible▁et▁malheureusement

2026-01-28 13:24:56,130 | INFO | speech length: 71680
2026-01-28 13:24:56,163 | INFO | decoder input length: 111
2026-01-28 13:24:56,163 | INFO | max output length: 111
2026-01-28 13:24:56,163 | INFO | min output length: 11
2026-01-28 13:25:01,200 | INFO | end detected at 51
2026-01-28 13:25:01,202 | INFO | -21.32 * 0.5 = -10.66 for decoder
2026-01-28 13:25:01,202 | INFO | -27.56 * 0.5 = -13.78 for ctc
2026-01-28 13:25:01,202 | INFO | total log probability: -24.44
2026-01-28 13:25:01,202 | INFO | normalized log probability: -0.58
2026-01-28 13:25:01,202 | INFO | total number of ended hypotheses: 190
2026-01-28 13:25:01,203 | INFO | best hypo: ▁peux▁tu▁utilises▁que▁c'est▁très▁bien▁que▁saveur▁serva▁pas▁toujours▁tout▁seul▁à▁cieillette

2026-01-28 13:25:01,205 | INFO | speech length: 122400
2026-01-28 13:25:01,242 | INFO | decoder input length: 190
2026-01-28 13:25:01,242 | INFO | max output length: 190
2026-01-28 13:25:01,242 | INFO | min output length: 19
2026-01-28 13:25:09,761 | INFO | end detected at 80
2026-01-28 13:25:09,763 | INFO | -21.14 * 0.5 = -10.57 for decoder
2026-01-28 13:25:09,763 | INFO | -22.66 * 0.5 = -11.33 for ctc
2026-01-28 13:25:09,763 | INFO | total log probability: -21.90
2026-01-28 13:25:09,763 | INFO | normalized log probability: -0.30
2026-01-28 13:25:09,763 | INFO | total number of ended hypotheses: 196
2026-01-28 13:25:09,764 | INFO | best hypo: ▁liés▁à▁l'utilisateur▁ce▁plupart▁du▁temps▁c'est▁l'utilisateur▁que▁fait▁une▁fosse▁manum▁mais▁sa▁poussée▁pas▁obligatoire▁les▁programmes▁aussi▁les▁erreurs▁et▁plus

2026-01-28 13:25:09,766 | INFO | speech length: 92000
2026-01-28 13:25:09,799 | INFO | decoder input length: 143
2026-01-28 13:25:09,799 | INFO | max output length: 143
2026-01-28 13:25:09,799 | INFO | min output length: 14
2026-01-28 13:25:14,676 | INFO | end detected at 52
2026-01-28 13:25:14,677 | INFO | -16.68 * 0.5 =  -8.34 for decoder
2026-01-28 13:25:14,677 | INFO |  -4.96 * 0.5 =  -2.48 for ctc
2026-01-28 13:25:14,678 | INFO | total log probability: -10.82
2026-01-28 13:25:14,678 | INFO | normalized log probability: -0.24
2026-01-28 13:25:14,678 | INFO | total number of ended hypotheses: 189
2026-01-28 13:25:14,678 | INFO | best hypo: ▁donc▁ce▁qui▁fait▁que▁c'est▁c'est▁pas▁facile▁facile▁parce▁que▁bon▁à▁falmettre▁tous▁les▁gens▁courants▁tout▁ça

2026-01-28 13:25:14,680 | INFO | speech length: 11680
2026-01-28 13:25:14,711 | INFO | decoder input length: 17
2026-01-28 13:25:14,711 | INFO | max output length: 17
2026-01-28 13:25:14,711 | INFO | min output length: 1
2026-01-28 13:25:15,815 | INFO | end detected at 13
2026-01-28 13:25:15,816 | INFO |  -4.34 * 0.5 =  -2.17 for decoder
2026-01-28 13:25:15,817 | INFO | -11.93 * 0.5 =  -5.97 for ctc
2026-01-28 13:25:15,817 | INFO | total log probability: -8.14
2026-01-28 13:25:15,817 | INFO | normalized log probability: -1.16
2026-01-28 13:25:15,817 | INFO | total number of ended hypotheses: 165
2026-01-28 13:25:15,817 | INFO | best hypo: ▁en▁savoir▁mieux

2026-01-28 13:25:15,819 | INFO | speech length: 32320
2026-01-28 13:25:15,846 | INFO | decoder input length: 50
2026-01-28 13:25:15,846 | INFO | max output length: 50
2026-01-28 13:25:15,846 | INFO | min output length: 5
2026-01-28 13:25:18,156 | INFO | end detected at 28
2026-01-28 13:25:18,158 | INFO |  -6.26 * 0.5 =  -3.13 for decoder
2026-01-28 13:25:18,158 | INFO | -15.89 * 0.5 =  -7.95 for ctc
2026-01-28 13:25:18,158 | INFO | total log probability: -11.08
2026-01-28 13:25:18,158 | INFO | normalized log probability: -0.55
2026-01-28 13:25:18,158 | INFO | total number of ended hypotheses: 191
2026-01-28 13:25:18,159 | INFO | best hypo: ▁oui▁serait▁une▁chasse▁règle▁ce▁serait▁une

2026-01-28 13:25:18,160 | INFO | speech length: 394240
2026-01-28 13:25:18,193 | INFO | decoder input length: 615
2026-01-28 13:25:18,193 | INFO | max output length: 615
2026-01-28 13:25:18,193 | INFO | min output length: 61
2026-01-28 13:25:54,782 | INFO | end detected at 253
2026-01-28 13:25:54,784 | INFO | -610.52 * 0.5 = -305.26 for decoder
2026-01-28 13:25:54,784 | INFO | -252.15 * 0.5 = -126.07 for ctc
2026-01-28 13:25:54,784 | INFO | total log probability: -431.33
2026-01-28 13:25:54,784 | INFO | normalized log probability: -1.75
2026-01-28 13:25:54,784 | INFO | total number of ended hypotheses: 167
2026-01-28 13:25:54,786 | INFO | best hypo: ▁a▁sauf▁qu'il▁faut▁changer▁quand▁même▁de▁système▁faut▁changer▁on▁y▁a▁des▁mises▁à▁jour▁le▁problème▁de▁l'informatique▁s'excava▁ça▁va▁trop▁vite▁parce▁que▁bon▁c'est▁ça▁bien▁ridicule▁face▁à▁l'a▁troprouville▁c'est▁c'est▁très▁bien▁c'est▁bien▁mais▁le▁problème▁s'est▁quand▁tu▁commencs▁à▁être▁au▁courant▁d'un▁sytème▁en▁arive▁pas▁un▁autre▁un▁enfant▁bref▁quoù▁tu▁connais▁les▁poêlets▁'ils▁d'inateur▁tâché▁quant▁achète▁d'un▁traordinateur▁et▁il▁est▁déjà▁dépaché▁que▁c'est▁c'est▁pratiquement▁oui▁ah▁oui▁oui▁pas▁super▁la▁nouvelle▁génération

2026-01-28 13:25:54,793 | INFO | Chunk: 0 | WER=0.000000 | S=1 D=0 I=0
2026-01-28 13:25:54,794 | INFO | Chunk: 1 | WER=100.000000 | S=2 D=0 I=0
2026-01-28 13:25:54,794 | INFO | Chunk: 2 | WER=81.250000 | S=11 D=10 I=5
2026-01-28 13:25:54,795 | INFO | Chunk: 3 | WER=44.117647 | S=8 D=2 I=5
2026-01-28 13:25:54,795 | INFO | Chunk: 4 | WER=100.000000 | S=5 D=0 I=0
2026-01-28 13:25:54,796 | INFO | Chunk: 5 | WER=100.000000 | S=1 D=2 I=0
2026-01-28 13:25:54,796 | INFO | Chunk: 6 | WER=100.000000 | S=1 D=0 I=0
2026-01-28 13:25:54,796 | INFO | Chunk: 7 | WER=600.000000 | S=1 D=0 I=5
2026-01-28 13:25:54,796 | INFO | Chunk: 8 | WER=166.666667 | S=1 D=0 I=4
2026-01-28 13:25:54,798 | INFO | Chunk: 9 | WER=40.983607 | S=9 D=11 I=5
2026-01-28 13:25:54,799 | INFO | Chunk: 10 | WER=33.333333 | S=4 D=3 I=3
2026-01-28 13:25:54,799 | INFO | Chunk: 11 | WER=90.000000 | S=9 D=0 I=0
2026-01-28 13:25:54,800 | INFO | Chunk: 12 | WER=57.777778 | S=11 D=4 I=11
2026-01-28 13:25:54,801 | INFO | Chunk: 13 | WER=80.000000 | S=1 D=3 I=4
2026-01-28 13:25:54,803 | INFO | Chunk: 14 | WER=58.227848 | S=23 D=18 I=5
2026-01-28 13:25:54,803 | INFO | Chunk: 15 | WER=125.000000 | S=4 D=0 I=1
2026-01-28 13:25:54,804 | INFO | Chunk: 16 | WER=94.117647 | S=4 D=7 I=5
2026-01-28 13:25:54,804 | INFO | Chunk: 17 | WER=88.888889 | S=4 D=4 I=0
2026-01-28 13:25:54,804 | INFO | Chunk: 18 | WER=122.222222 | S=9 D=0 I=2
2026-01-28 13:25:54,805 | INFO | Chunk: 19 | WER=76.190476 | S=3 D=8 I=5
2026-01-28 13:25:54,805 | INFO | Chunk: 20 | WER=100.000000 | S=0 D=2 I=2
2026-01-28 13:25:54,805 | INFO | Chunk: 21 | WER=200.000000 | S=2 D=0 I=2
2026-01-28 13:25:54,806 | INFO | Chunk: 22 | WER=70.454545 | S=15 D=12 I=4
2026-01-28 13:25:54,809 | INFO | Chunk: 23 | WER=59.302326 | S=28 D=22 I=1
2026-01-28 13:25:54,809 | INFO | Chunk: 24 | WER=68.181818 | S=6 D=7 I=2
2026-01-28 13:25:54,810 | INFO | Chunk: 25 | WER=60.975610 | S=7 D=15 I=3
2026-01-28 13:25:54,811 | INFO | Chunk: 26 | WER=65.384615 | S=2 D=9 I=6
2026-01-28 13:25:54,811 | INFO | Chunk: 27 | WER=100.000000 | S=3 D=0 I=0
2026-01-28 13:25:54,811 | INFO | Chunk: 28 | WER=100.000000 | S=8 D=0 I=0
2026-01-28 13:25:54,817 | INFO | Chunk: 29 | WER=44.354839 | S=32 D=19 I=4
2026-01-28 13:25:55,096 | INFO | File: Rhap-D0005.wav | WER=53.396739 | S=240 D=113 I=40
2026-01-28 13:25:55,096 | INFO | ------------------------------
2026-01-28 13:25:55,097 | INFO | Conf cv Done!
2026-01-28 13:25:55,260 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 13:25:55,277 | INFO | Vocabulary size: 47
2026-01-28 13:25:55,798 | INFO | Gradient checkpoint layers: []
2026-01-28 13:25:56,661 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 13:25:56,664 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 13:25:56,664 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 13:25:56,665 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 13:25:56,667 | INFO | speech length: 10400
2026-01-28 13:25:56,702 | INFO | decoder input length: 15
2026-01-28 13:25:56,702 | INFO | max output length: 15
2026-01-28 13:25:56,702 | INFO | min output length: 1
2026-01-28 13:25:58,081 | INFO | adding <eos> in the last position in the loop
2026-01-28 13:25:58,093 | INFO | end detected at 14
2026-01-28 13:25:58,095 | INFO |  -0.97 * 0.5 =  -0.48 for decoder
2026-01-28 13:25:58,095 | INFO |  -5.34 * 0.5 =  -2.67 for ctc
2026-01-28 13:25:58,095 | INFO | total log probability: -3.15
2026-01-28 13:25:58,095 | INFO | normalized log probability: -0.39
2026-01-28 13:25:58,095 | INFO | total number of ended hypotheses: 215
2026-01-28 13:25:58,096 | INFO | best hypo: il<space>y<space>a

2026-01-28 13:25:58,097 | INFO | speech length: 12320
2026-01-28 13:25:58,125 | INFO | decoder input length: 18
2026-01-28 13:25:58,125 | INFO | max output length: 18
2026-01-28 13:25:58,125 | INFO | min output length: 1
2026-01-28 13:25:59,798 | INFO | adding <eos> in the last position in the loop
2026-01-28 13:25:59,807 | INFO | no hypothesis. Finish decoding.
2026-01-28 13:25:59,808 | INFO |  -1.05 * 0.5 =  -0.52 for decoder
2026-01-28 13:25:59,809 | INFO |  -1.77 * 0.5 =  -0.89 for ctc
2026-01-28 13:25:59,809 | INFO | total log probability: -1.41
2026-01-28 13:25:59,809 | INFO | normalized log probability: -0.13
2026-01-28 13:25:59,809 | INFO | total number of ended hypotheses: 170
2026-01-28 13:25:59,809 | INFO | best hypo: vous<space>avez

2026-01-28 13:25:59,811 | INFO | speech length: 131840
2026-01-28 13:25:59,837 | INFO | decoder input length: 205
2026-01-28 13:25:59,837 | INFO | max output length: 205
2026-01-28 13:25:59,837 | INFO | min output length: 20
2026-01-28 13:26:16,918 | INFO | end detected at 160
2026-01-28 13:26:16,920 | INFO | -22.26 * 0.5 = -11.13 for decoder
2026-01-28 13:26:16,920 | INFO | -17.01 * 0.5 =  -8.50 for ctc
2026-01-28 13:26:16,920 | INFO | total log probability: -19.63
2026-01-28 13:26:16,920 | INFO | normalized log probability: -0.13
2026-01-28 13:26:16,920 | INFO | total number of ended hypotheses: 210
2026-01-28 13:26:16,922 | INFO | best hypo: bah<space>oui<space>oui<space>oui<space>oui<space>ben<space>euh<space>d<space>oh<space>ben<space>euh<space>deux<space>fin<space>j'ai<space>fait<space>plusieurs<space>mé<space>fin<space>de<space>métier<space>mais<space>enfin<space>disons<space>euh<space>j'ai<space>j'ai<space>d'abord<space>euh<space>travaillé<space>dans<space>des

2026-01-28 13:26:16,925 | INFO | speech length: 231200
2026-01-28 13:26:16,960 | INFO | decoder input length: 360
2026-01-28 13:26:16,960 | INFO | max output length: 360
2026-01-28 13:26:16,960 | INFO | min output length: 36
2026-01-28 13:26:43,662 | INFO | end detected at 237
2026-01-28 13:26:43,664 | INFO | -21.95 * 0.5 = -10.98 for decoder
2026-01-28 13:26:43,664 | INFO |  -6.13 * 0.5 =  -3.07 for ctc
2026-01-28 13:26:43,664 | INFO | total log probability: -14.04
2026-01-28 13:26:43,664 | INFO | normalized log probability: -0.06
2026-01-28 13:26:43,664 | INFO | total number of ended hypotheses: 231
2026-01-28 13:26:43,667 | INFO | best hypo: euh<space>comment<space>les<space>entreprises<space>de<space>d'automatismes<space>enfin<space>ont<space>métier<space>donc<space>c'est<space>l'électronique<space>au<space>début<space>les<space>automatismes<space>hein<space>de<space>corps<space>et<space>puis<space>après<space>ben<space>j'ai<space>travaillé<space>sur<space>les<space>micro<space>processeurs<space>l'informatique<space>et<space>euh<space>maintenant<space>je<space>suis

2026-01-28 13:26:43,669 | INFO | speech length: 33280
2026-01-28 13:26:43,696 | INFO | decoder input length: 51
2026-01-28 13:26:43,696 | INFO | max output length: 51
2026-01-28 13:26:43,696 | INFO | min output length: 5
2026-01-28 13:26:46,920 | INFO | end detected at 48
2026-01-28 13:26:46,922 | INFO |  -4.50 * 0.5 =  -2.25 for decoder
2026-01-28 13:26:46,922 | INFO |  -3.47 * 0.5 =  -1.74 for ctc
2026-01-28 13:26:46,922 | INFO | total log probability: -3.99
2026-01-28 13:26:46,922 | INFO | normalized log probability: -0.10
2026-01-28 13:26:46,922 | INFO | total number of ended hypotheses: 203
2026-01-28 13:26:46,922 | INFO | best hypo: spécialisée<space>dans<space>la<space>programmation<space>city

2026-01-28 13:26:46,924 | INFO | speech length: 8640
2026-01-28 13:26:46,943 | INFO | decoder input length: 13
2026-01-28 13:26:46,943 | INFO | max output length: 13
2026-01-28 13:26:46,943 | INFO | min output length: 1
2026-01-28 13:26:47,244 | INFO | adding <eos> in the last position in the loop
2026-01-28 13:26:47,251 | INFO | no hypothesis. Finish decoding.
2026-01-28 13:26:47,252 | INFO |  -4.00 * 0.5 =  -2.00 for decoder
2026-01-28 13:26:47,253 | INFO |  -4.68 * 0.5 =  -2.34 for ctc
2026-01-28 13:26:47,253 | INFO | total log probability: -4.34
2026-01-28 13:26:47,253 | INFO | normalized log probability: -0.72
2026-01-28 13:26:47,253 | INFO | total number of ended hypotheses: 160
2026-01-28 13:26:47,253 | INFO | best hypo: rien

2026-01-28 13:26:47,254 | INFO | speech length: 14880
2026-01-28 13:26:47,279 | INFO | decoder input length: 22
2026-01-28 13:26:47,279 | INFO | max output length: 22
2026-01-28 13:26:47,279 | INFO | min output length: 2
2026-01-28 13:26:47,636 | INFO | end detected at 13
2026-01-28 13:26:47,637 | INFO |  -0.59 * 0.5 =  -0.30 for decoder
2026-01-28 13:26:47,637 | INFO |  -0.15 * 0.5 =  -0.07 for ctc
2026-01-28 13:26:47,637 | INFO | total log probability: -0.37
2026-01-28 13:26:47,637 | INFO | normalized log probability: -0.05
2026-01-28 13:26:47,637 | INFO | total number of ended hypotheses: 162
2026-01-28 13:26:47,637 | INFO | best hypo: et<space>euh

2026-01-28 13:26:47,639 | INFO | speech length: 22720
2026-01-28 13:26:47,663 | INFO | decoder input length: 35
2026-01-28 13:26:47,663 | INFO | max output length: 35
2026-01-28 13:26:47,664 | INFO | min output length: 3
2026-01-28 13:26:48,930 | INFO | adding <eos> in the last position in the loop
2026-01-28 13:26:48,938 | INFO | no hypothesis. Finish decoding.
2026-01-28 13:26:48,939 | INFO |  -3.21 * 0.5 =  -1.61 for decoder
2026-01-28 13:26:48,939 | INFO | -11.91 * 0.5 =  -5.96 for ctc
2026-01-28 13:26:48,939 | INFO | total log probability: -7.56
2026-01-28 13:26:48,939 | INFO | normalized log probability: -0.24
2026-01-28 13:26:48,939 | INFO | total number of ended hypotheses: 110
2026-01-28 13:26:48,939 | INFO | best hypo: et<space>dans<space>un<space>laboratoire<space>du<space>cnrs

2026-01-28 13:26:48,940 | INFO | speech length: 43360
2026-01-28 13:26:48,968 | INFO | decoder input length: 67
2026-01-28 13:26:48,968 | INFO | max output length: 67
2026-01-28 13:26:48,968 | INFO | min output length: 6
2026-01-28 13:26:53,466 | INFO | end detected at 48
2026-01-28 13:26:53,468 | INFO |  -3.87 * 0.5 =  -1.93 for decoder
2026-01-28 13:26:53,468 | INFO |  -2.92 * 0.5 =  -1.46 for ctc
2026-01-28 13:26:53,468 | INFO | total log probability: -3.39
2026-01-28 13:26:53,468 | INFO | normalized log probability: -0.08
2026-01-28 13:26:53,468 | INFO | total number of ended hypotheses: 176
2026-01-28 13:26:53,468 | INFO | best hypo: et<space>donc<space>euh<space>et<space>ça<space>se<space>passe<space>bien<space>sinon<space>euh

2026-01-28 13:26:53,470 | INFO | speech length: 422880
2026-01-28 13:26:53,500 | INFO | decoder input length: 660
2026-01-28 13:26:53,500 | INFO | max output length: 660
2026-01-28 13:26:53,500 | INFO | min output length: 66
2026-01-28 13:27:37,472 | INFO | end detected at 316
2026-01-28 13:27:37,474 | INFO | -52.78 * 0.5 = -26.39 for decoder
2026-01-28 13:27:37,474 | INFO | -13.72 * 0.5 =  -6.86 for ctc
2026-01-28 13:27:37,474 | INFO | total log probability: -33.25
2026-01-28 13:27:37,474 | INFO | normalized log probability: -0.11
2026-01-28 13:27:37,474 | INFO | total number of ended hypotheses: 185
2026-01-28 13:27:37,477 | INFO | best hypo: bah<space>euh<space>si<space>si<space>quand<space>même<space>euh<space>parce<space>que<space>euh<space>euh<space>moi<space>j'ai<space>suivi<space>les<space>l'évolution<space>donc<space>euh<space>ça<space>fait<space>à<space>peu<space>près<space>vingt<space>cinq<space>ans<space>que<space>je<space>suis<space>dans<space>ce<space>laboratoire<space>au<space>début<space>il<space>n'y<space>avait<space>pratiquement<space>pas<space>d'informatique<space>moi<space>je<space>suis<space>rentré<space>pour<space>euh<space>faire<space>des<space>automatismes<space>euh<space>d'appareillage<space>avec<space>des<space>micro<space>professeurs

2026-01-28 13:27:37,480 | INFO | speech length: 152640
2026-01-28 13:27:37,509 | INFO | decoder input length: 238
2026-01-28 13:27:37,509 | INFO | max output length: 238
2026-01-28 13:27:37,509 | INFO | min output length: 23
2026-01-28 13:27:58,291 | INFO | end detected at 191
2026-01-28 13:27:58,293 | INFO | -15.71 * 0.5 =  -7.86 for decoder
2026-01-28 13:27:58,293 | INFO |  -2.23 * 0.5 =  -1.11 for ctc
2026-01-28 13:27:58,293 | INFO | total log probability: -8.97
2026-01-28 13:27:58,293 | INFO | normalized log probability: -0.05
2026-01-28 13:27:58,293 | INFO | total number of ended hypotheses: 222
2026-01-28 13:27:58,295 | INFO | best hypo: et<space>petit<space>à<space>petit<space>l'informatique<space>s'est<space>développé<space>et<space>euh<space>il<space>y<space>a<space>de<space>plus<space>en<space>plus<space>d'appareils<space>et<space>comme<space>j'étais<space>le<space>seul<space>informaticien<space>euh<space>ben<space>ça<space>c'est<space>devenu<space>assez<space>dur<space>puisque<space>maintenant

2026-01-28 13:27:58,297 | INFO | speech length: 49760
2026-01-28 13:27:58,327 | INFO | decoder input length: 77
2026-01-28 13:27:58,327 | INFO | max output length: 77
2026-01-28 13:27:58,327 | INFO | min output length: 7
2026-01-28 13:28:05,685 | INFO | adding <eos> in the last position in the loop
2026-01-28 13:28:05,695 | INFO | no hypothesis. Finish decoding.
2026-01-28 13:28:05,696 | INFO |  -6.09 * 0.5 =  -3.04 for decoder
2026-01-28 13:28:05,696 | INFO |  -5.52 * 0.5 =  -2.76 for ctc
2026-01-28 13:28:05,696 | INFO | total log probability: -5.80
2026-01-28 13:28:05,696 | INFO | normalized log probability: -0.08
2026-01-28 13:28:05,696 | INFO | total number of ended hypotheses: 183
2026-01-28 13:28:05,697 | INFO | best hypo: relativement<space>puisque<space>maintenant<space>je<space>suis<space>responsable<space>bon<space>il<space>y<space>a<space>un<space>parc

2026-01-28 13:28:05,699 | INFO | speech length: 245760
2026-01-28 13:28:05,732 | INFO | decoder input length: 383
2026-01-28 13:28:05,732 | INFO | max output length: 383
2026-01-28 13:28:05,732 | INFO | min output length: 38
2026-01-28 13:28:41,943 | INFO | end detected at 299
2026-01-28 13:28:41,945 | INFO | -36.41 * 0.5 = -18.21 for decoder
2026-01-28 13:28:41,945 | INFO | -13.83 * 0.5 =  -6.92 for ctc
2026-01-28 13:28:41,945 | INFO | total log probability: -25.12
2026-01-28 13:28:41,945 | INFO | normalized log probability: -0.09
2026-01-28 13:28:41,945 | INFO | total number of ended hypotheses: 194
2026-01-28 13:28:41,948 | INFO | best hypo: une<space>cinquantaine<space>d'ordinateurs<space>euh<space>plus<space>un<space>certain<space>nombre<space>d'imprimantes<space>et<space>tout<space>ça<space>et<space>euh<space>bon<space>ben<space>ça<space>pose<space>des<space>problèmes<space>de<space>maintenant<space>enfin<space>de<space>maintenant<space>ce<space>de<space>de<space>mise<space>à<space>jour<space>et<space>tout<space>ça<space>voilà<space>donc<space>c'est<space>c'est<space>un<space>moment<space>donné<space>ça<space>de<space>donner<space>surtout<space>qu'en<space>même<space>temps<space>j'avais<space>mon<space>travail<space>aussi

2026-01-28 13:28:41,951 | INFO | speech length: 55360
2026-01-28 13:28:41,982 | INFO | decoder input length: 86
2026-01-28 13:28:41,982 | INFO | max output length: 86
2026-01-28 13:28:41,982 | INFO | min output length: 8
2026-01-28 13:28:49,307 | INFO | end detected at 83
2026-01-28 13:28:49,308 | INFO |  -6.80 * 0.5 =  -3.40 for decoder
2026-01-28 13:28:49,309 | INFO | -10.00 * 0.5 =  -5.00 for ctc
2026-01-28 13:28:49,309 | INFO | total log probability: -8.40
2026-01-28 13:28:49,309 | INFO | normalized log probability: -0.11
2026-01-28 13:28:49,309 | INFO | total number of ended hypotheses: 214
2026-01-28 13:28:49,310 | INFO | best hypo: écrire<space>des<space>programmes<space>et<space>tous<space>des<space>logiciels<space>donc<space>euh<space>bon<space>bah<space>là<space>maintenant

2026-01-28 13:28:49,312 | INFO | speech length: 279200
2026-01-28 13:28:49,345 | INFO | decoder input length: 435
2026-01-28 13:28:49,345 | INFO | max output length: 435
2026-01-28 13:28:49,345 | INFO | min output length: 43
2026-01-28 13:29:35,015 | INFO | end detected at 408
2026-01-28 13:29:35,016 | INFO | -92.10 * 0.5 = -46.05 for decoder
2026-01-28 13:29:35,016 | INFO | -108.32 * 0.5 = -54.16 for ctc
2026-01-28 13:29:35,016 | INFO | total log probability: -100.21
2026-01-28 13:29:35,016 | INFO | normalized log probability: -0.25
2026-01-28 13:29:35,016 | INFO | total number of ended hypotheses: 161
2026-01-28 13:29:35,021 | INFO | best hypo: plus<space>ou<space>moins<space>aidé<space>euh<space>par<space>quelqu'un<space>à<space>mi<space>temps<space>heureusement<space>parce<space>que<space>sino<space>c'est<space>c'est<space>c'st<space>c'est<space>intenable<space>s<space>et<space>qu'il<space>y<space>a<space>trop<space>de<space>il<space>y<space>a<space>trop<space>de<space>boulots<space>euh<space>surtout<space>que<space>a<space>fallu<space>former<space>les<space>gens<space>aussi<space>à<space>l'informatique<space>hein<space>en<space>vingt<space>cin<space>en<space>vingt<space>cinq<space>ans<space>les<space>janvier<space>à<space>vingt<space>cinq<space>ans<space>les<space>gens<space>n<space>le<space>connaissaient<space>aérien<space>hein<space>ça<space>avait<space>pas<space>travailler<space>ainsi<space>eux<space>ils<space>avaient<space>pas<space>utilisé<space>un<space>ordinateur

2026-01-28 13:29:35,023 | INFO | speech length: 27200
2026-01-28 13:29:35,050 | INFO | decoder input length: 42
2026-01-28 13:29:35,050 | INFO | max output length: 42
2026-01-28 13:29:35,051 | INFO | min output length: 4
2026-01-28 13:29:38,457 | INFO | adding <eos> in the last position in the loop
2026-01-28 13:29:38,466 | INFO | no hypothesis. Finish decoding.
2026-01-28 13:29:38,467 | INFO |  -4.49 * 0.5 =  -2.24 for decoder
2026-01-28 13:29:38,467 | INFO |  -4.04 * 0.5 =  -2.02 for ctc
2026-01-28 13:29:38,467 | INFO | total log probability: -4.26
2026-01-28 13:29:38,467 | INFO | normalized log probability: -0.11
2026-01-28 13:29:38,467 | INFO | total number of ended hypotheses: 129
2026-01-28 13:29:38,467 | INFO | best hypo: hum<space>maintenant<space>les<space>jeunes<space>qui<space>arrivent

2026-01-28 13:29:38,469 | INFO | speech length: 75200
2026-01-28 13:29:38,497 | INFO | decoder input length: 117
2026-01-28 13:29:38,497 | INFO | max output length: 117
2026-01-28 13:29:38,497 | INFO | min output length: 11
2026-01-28 13:29:47,965 | INFO | end detected at 106
2026-01-28 13:29:47,966 | INFO |  -9.58 * 0.5 =  -4.79 for decoder
2026-01-28 13:29:47,967 | INFO |  -1.65 * 0.5 =  -0.83 for ctc
2026-01-28 13:29:47,967 | INFO | total log probability: -5.61
2026-01-28 13:29:47,967 | INFO | normalized log probability: -0.06
2026-01-28 13:29:47,967 | INFO | total number of ended hypotheses: 178
2026-01-28 13:29:47,968 | INFO | best hypo: si<space>s'en<space>est<space>avec<space>mais<space>euh<space>bon<space>ça<space>a<space>été<space>un<space>problème<space>pour<space>se<space>mettre<space>à<space>l'informatique<space>ça<space>c'est<space>évident

2026-01-28 13:29:47,970 | INFO | speech length: 24480
2026-01-28 13:29:47,996 | INFO | decoder input length: 37
2026-01-28 13:29:47,996 | INFO | max output length: 37
2026-01-28 13:29:47,997 | INFO | min output length: 3
2026-01-28 13:29:51,165 | INFO | adding <eos> in the last position in the loop
2026-01-28 13:29:51,174 | INFO | no hypothesis. Finish decoding.
2026-01-28 13:29:51,175 | INFO |  -2.65 * 0.5 =  -1.32 for decoder
2026-01-28 13:29:51,175 | INFO |  -0.03 * 0.5 =  -0.02 for ctc
2026-01-28 13:29:51,175 | INFO | total log probability: -1.34
2026-01-28 13:29:51,175 | INFO | normalized log probability: -0.04
2026-01-28 13:29:51,175 | INFO | total number of ended hypotheses: 139
2026-01-28 13:29:51,175 | INFO | best hypo: bon<space>maintenant<space>ça<space>se<space>passe<space>mieux

2026-01-28 13:29:51,177 | INFO | speech length: 37120
2026-01-28 13:29:51,203 | INFO | decoder input length: 57
2026-01-28 13:29:51,204 | INFO | max output length: 57
2026-01-28 13:29:51,204 | INFO | min output length: 5
2026-01-28 13:29:56,050 | INFO | adding <eos> in the last position in the loop
2026-01-28 13:29:56,058 | INFO | no hypothesis. Finish decoding.
2026-01-28 13:29:56,059 | INFO | -22.70 * 0.5 = -11.35 for decoder
2026-01-28 13:29:56,059 | INFO | -65.09 * 0.5 = -32.54 for ctc
2026-01-28 13:29:56,059 | INFO | total log probability: -43.89
2026-01-28 13:29:56,059 | INFO | normalized log probability: -0.90
2026-01-28 13:29:56,059 | INFO | total number of ended hypotheses: 157
2026-01-28 13:29:56,060 | INFO | best hypo: moi<space>que<space>je<space>ne<space>co<space>moi<space>je<space>qui<space>connaisse<space>pas<space>mange

2026-01-28 13:29:56,061 | INFO | speech length: 105280
2026-01-28 13:29:56,090 | INFO | decoder input length: 164
2026-01-28 13:29:56,090 | INFO | max output length: 164
2026-01-28 13:29:56,090 | INFO | min output length: 16
2026-01-28 13:30:07,200 | INFO | end detected at 114
2026-01-28 13:30:07,203 | INFO | -10.74 * 0.5 =  -5.37 for decoder
2026-01-28 13:30:07,203 | INFO |  -6.85 * 0.5 =  -3.42 for ctc
2026-01-28 13:30:07,203 | INFO | total log probability: -8.79
2026-01-28 13:30:07,203 | INFO | normalized log probability: -0.09
2026-01-28 13:30:07,203 | INFO | total number of ended hypotheses: 261
2026-01-28 13:30:07,204 | INFO | best hypo: oui<space>mais<space>vous<space>faites<space>vous<space>êtes<space>une<space>génération<space>qui<space>qui<space>qui<space>a<space>vous<space>vécu<space>avec<space>tandis<space>que<space>y<space>a<space>des<space>gens

2026-01-28 13:30:07,208 | INFO | speech length: 24960
2026-01-28 13:30:07,237 | INFO | decoder input length: 38
2026-01-28 13:30:07,237 | INFO | max output length: 38
2026-01-28 13:30:07,237 | INFO | min output length: 3
2026-01-28 13:30:10,176 | INFO | end detected at 33
2026-01-28 13:30:10,177 | INFO |  -3.30 * 0.5 =  -1.65 for decoder
2026-01-28 13:30:10,178 | INFO |  -0.28 * 0.5 =  -0.14 for ctc
2026-01-28 13:30:10,178 | INFO | total log probability: -1.79
2026-01-28 13:30:10,178 | INFO | normalized log probability: -0.06
2026-01-28 13:30:10,178 | INFO | total number of ended hypotheses: 172
2026-01-28 13:30:10,178 | INFO | best hypo: l'histoire<space>du<space>mot<space>hein<space>euh

2026-01-28 13:30:10,180 | INFO | speech length: 12480
2026-01-28 13:30:10,212 | INFO | decoder input length: 19
2026-01-28 13:30:10,212 | INFO | max output length: 19
2026-01-28 13:30:10,212 | INFO | min output length: 1
2026-01-28 13:30:11,818 | INFO | adding <eos> in the last position in the loop
2026-01-28 13:30:11,825 | INFO | no hypothesis. Finish decoding.
2026-01-28 13:30:11,826 | INFO | -12.59 * 0.5 =  -6.29 for decoder
2026-01-28 13:30:11,826 | INFO | -19.28 * 0.5 =  -9.64 for ctc
2026-01-28 13:30:11,826 | INFO | total log probability: -15.94
2026-01-28 13:30:11,826 | INFO | normalized log probability: -1.00
2026-01-28 13:30:11,826 | INFO | total number of ended hypotheses: 81
2026-01-28 13:30:11,827 | INFO | best hypo: c'en<space>est<space>c'est

2026-01-28 13:30:11,828 | INFO | speech length: 130880
2026-01-28 13:30:11,867 | INFO | decoder input length: 204
2026-01-28 13:30:11,867 | INFO | max output length: 204
2026-01-28 13:30:11,867 | INFO | min output length: 20
2026-01-28 13:30:29,244 | INFO | end detected at 196
2026-01-28 13:30:29,245 | INFO | -29.16 * 0.5 = -14.58 for decoder
2026-01-28 13:30:29,245 | INFO | -93.00 * 0.5 = -46.50 for ctc
2026-01-28 13:30:29,245 | INFO | total log probability: -61.08
2026-01-28 13:30:29,245 | INFO | normalized log probability: -0.32
2026-01-28 13:30:29,245 | INFO | total number of ended hypotheses: 171
2026-01-28 13:30:29,247 | INFO | best hypo: mais<space>c'est<space>un<space>peu<space>vrai<space>ça<space>les<space>gens<space>au<space>début<space>quand<space>i<space>voyait<space>un<space>ordinateur<space>une<space>souris<space>euh<space>tout<space>ça<space>i<space>savait<space>pas<space>ce<space>que<space>c'était<space>vous<space>avez<space>pas<space>i<space>s<space>avaient<space>peur<space>de<space>toucher<space>un<space>plavier<space>enfin<space>bon

2026-01-28 13:30:29,250 | INFO | speech length: 295680
2026-01-28 13:30:29,279 | INFO | decoder input length: 461
2026-01-28 13:30:29,279 | INFO | max output length: 461
2026-01-28 13:30:29,279 | INFO | min output length: 46
2026-01-28 13:31:14,568 | INFO | end detected at 399
2026-01-28 13:31:14,570 | INFO | -84.83 * 0.5 = -42.42 for decoder
2026-01-28 13:31:14,570 | INFO | -67.54 * 0.5 = -33.77 for ctc
2026-01-28 13:31:14,570 | INFO | total log probability: -76.19
2026-01-28 13:31:14,570 | INFO | normalized log probability: -0.20
2026-01-28 13:31:14,570 | INFO | total number of ended hypotheses: 170
2026-01-28 13:31:14,575 | INFO | best hypo: parce<space>que<space>puis<space>il<space>il<space>a<space>le<space>même<space>pur<space>si<space>tu<space>se<space>fais<space>d'appuyer<space>sur<space>le<space>mauvais<space>souche<space>il<space>avait<space>l'impression<space>que<space>fallait<space>casser<space>la<space>machine<space>alors<space>que<space>oui<space>oui<space>non<space>mais<space>oui<space>mais<space>ça<space>c'est<space>la<space>peur<space>du<space>débutant<space>mais<space>bon<space>ben<space>faut<space>bon<space>ben<space>et<space>puis<space>alors<space>si<space>y<space>avait<space>quelque<space>chose<space>qui<space>n'allait<space>pas<space>euh<space>bon<space>c'était<space>tout<space>de<space>suite<space>à<space>peu<space>pourquoi<space>ça<space>marche<space>pas<space>pourquoi<space>c'est<space>pour<space>ça<space>et<space>malheureusement<space>hein

2026-01-28 13:31:14,577 | INFO | speech length: 71680
2026-01-28 13:31:14,606 | INFO | decoder input length: 111
2026-01-28 13:31:14,606 | INFO | max output length: 111
2026-01-28 13:31:14,607 | INFO | min output length: 11
2026-01-28 13:31:23,705 | INFO | end detected at 98
2026-01-28 13:31:23,707 | INFO | -10.64 * 0.5 =  -5.32 for decoder
2026-01-28 13:31:23,708 | INFO | -15.37 * 0.5 =  -7.68 for ctc
2026-01-28 13:31:23,708 | INFO | total log probability: -13.00
2026-01-28 13:31:23,708 | INFO | normalized log probability: -0.14
2026-01-28 13:31:23,708 | INFO | total number of ended hypotheses: 214
2026-01-28 13:31:23,709 | INFO | best hypo: peu<space>qui<space>utilise<space>et<space>c'est<space>très<space>bien<space>que<space>ça<space>va<space>ça<space>va<space>pas<space>toujours<space>tout<space>seul<space>hein<space>c'est<space>y<space>a

2026-01-28 13:31:23,711 | INFO | speech length: 122400
2026-01-28 13:31:23,742 | INFO | decoder input length: 190
2026-01-28 13:31:23,742 | INFO | max output length: 190
2026-01-28 13:31:23,742 | INFO | min output length: 19
2026-01-28 13:31:39,952 | INFO | end detected at 183
2026-01-28 13:31:39,953 | INFO | -29.67 * 0.5 = -14.84 for decoder
2026-01-28 13:31:39,953 | INFO | -60.94 * 0.5 = -30.47 for ctc
2026-01-28 13:31:39,954 | INFO | total log probability: -45.31
2026-01-28 13:31:39,954 | INFO | normalized log probability: -0.26
2026-01-28 13:31:39,954 | INFO | total number of ended hypotheses: 197
2026-01-28 13:31:39,956 | INFO | best hypo: liés<space>à<space>l<space>à<space>l'utilisateur<space>ce<space>ce<space>plupart<space>du<space>temps<space>c'est<space>l'utilateur<space>qui<space>a<space>fait<space>le<space>fausse<space>malheur<space>mais<space>ça<space>c'est<space>pas<space>obligatoire<space>les<space>programmes<space>aussi<space>des<space>euh<space>les<space>erreurs<space>et<space>puis

2026-01-28 13:31:39,958 | INFO | speech length: 92000
2026-01-28 13:31:39,987 | INFO | decoder input length: 143
2026-01-28 13:31:39,987 | INFO | max output length: 143
2026-01-28 13:31:39,987 | INFO | min output length: 14
2026-01-28 13:31:52,910 | INFO | end detected at 127
2026-01-28 13:31:52,911 | INFO | -10.18 * 0.5 =  -5.09 for decoder
2026-01-28 13:31:52,911 | INFO |  -3.98 * 0.5 =  -1.99 for ctc
2026-01-28 13:31:52,911 | INFO | total log probability: -7.08
2026-01-28 13:31:52,911 | INFO | normalized log probability: -0.06
2026-01-28 13:31:52,911 | INFO | total number of ended hypotheses: 187
2026-01-28 13:31:52,913 | INFO | best hypo: donc<space>ce<space>qui<space>fait<space>que<space>c'est<space>c'est<space>pas<space>facile<space>facile<space>parce<space>que<space>euh<space>bon<space>ben<space>fallait<space>mettre<space>tous<space>les<space>gens<space>au<space>courant<space>tout<space>ça

2026-01-28 13:31:52,915 | INFO | speech length: 11680
2026-01-28 13:31:52,943 | INFO | decoder input length: 17
2026-01-28 13:31:52,944 | INFO | max output length: 17
2026-01-28 13:31:52,944 | INFO | min output length: 1
2026-01-28 13:31:54,493 | INFO | adding <eos> in the last position in the loop
2026-01-28 13:31:54,500 | INFO | no hypothesis. Finish decoding.
2026-01-28 13:31:54,501 | INFO |  -3.87 * 0.5 =  -1.93 for decoder
2026-01-28 13:31:54,502 | INFO |  -6.30 * 0.5 =  -3.15 for ctc
2026-01-28 13:31:54,502 | INFO | total log probability: -5.08
2026-01-28 13:31:54,502 | INFO | normalized log probability: -0.32
2026-01-28 13:31:54,502 | INFO | total number of ended hypotheses: 100
2026-01-28 13:31:54,502 | INFO | best hypo: non<space>ça<space>va<space>dire

2026-01-28 13:31:54,503 | INFO | speech length: 32320
2026-01-28 13:31:54,529 | INFO | decoder input length: 50
2026-01-28 13:31:54,529 | INFO | max output length: 50
2026-01-28 13:31:54,529 | INFO | min output length: 5
2026-01-28 13:31:59,157 | INFO | adding <eos> in the last position in the loop
2026-01-28 13:31:59,171 | INFO | no hypothesis. Finish decoding.
2026-01-28 13:31:59,173 | INFO |  -6.31 * 0.5 =  -3.16 for decoder
2026-01-28 13:31:59,173 | INFO | -12.13 * 0.5 =  -6.07 for ctc
2026-01-28 13:31:59,173 | INFO | total log probability: -9.22
2026-01-28 13:31:59,173 | INFO | normalized log probability: -0.26
2026-01-28 13:31:59,173 | INFO | total number of ended hypotheses: 181
2026-01-28 13:31:59,174 | INFO | best hypo: oui<space>c'est<space>rien<space>ça<space>serait<space>ça<space>serait

2026-01-28 13:31:59,176 | INFO | speech length: 394240
2026-01-28 13:31:59,205 | INFO | decoder input length: 615
2026-01-28 13:31:59,205 | INFO | max output length: 615
2026-01-28 13:31:59,205 | INFO | min output length: 61
2026-01-28 13:33:12,822 | INFO | end detected at 568
2026-01-28 13:33:12,823 | INFO | -541.81 * 0.5 = -270.91 for decoder
2026-01-28 13:33:12,823 | INFO | -182.05 * 0.5 = -91.03 for ctc
2026-01-28 13:33:12,823 | INFO | total log probability: -361.93
2026-01-28 13:33:12,823 | INFO | normalized log probability: -0.65
2026-01-28 13:33:12,823 | INFO | total number of ended hypotheses: 175
2026-01-28 13:33:12,830 | INFO | best hypo: ah<space>sauf<space>qu'il<space>faut<space>changer<space>quand<space>même<space>de<space>système<space>faut<space>changer<space>bon<space>euh<space>y<space>a<space>des<space>mises<space>à<space>jour<space>le<space>problème<space>de<space>l'informatique<space>c'est<space>que<space>ça<space>va<space>ça<space>va<space>trop<space>vite<space>parce<space>que<space>bon<space>c'est<space>ça<space>vient<space>ridicule<space>fin<space>ça<space>va<space>trop<space>vite<space>c'est<space>c'est<space>très<space>bien<space>c'est<space>mais<space>mais<space>le<space>problème<space>c'est<space>que<space>quand<space>tu<space>commences<space>à<space>être<space>au<space>courant<space>d'un<space>systèm<space>euh<space>il<space>en<space>arrive<space>un<space>autre<space>enfin<space>bref<space>quoi<space>tu<space>connaîts<space>les<space>p<space>les<space>les<space>les<space>ordinateurs<space>que<space>che<space>qunde<space>e<space>c'est<space>un<space>rdinateur<space>ile<space>est<space>est<space>il<space>est<space>déjà<space>dépasé<space>quoi<space>chest<space>cest<space>pratiquement<space>oui<space>bah<space>oui<space>oui<space>parce<space>quil<space>l<space>a<space>la<space>nouvelle<space>gération<space>t<space>aui<space>a

2026-01-28 13:33:12,837 | INFO | Chunk: 0 | WER=0.000000 | S=1 D=0 I=2
2026-01-28 13:33:12,837 | INFO | Chunk: 1 | WER=100.000000 | S=2 D=0 I=0
2026-01-28 13:33:12,838 | INFO | Chunk: 2 | WER=75.000000 | S=5 D=8 I=11
2026-01-28 13:33:12,839 | INFO | Chunk: 3 | WER=41.176471 | S=6 D=1 I=7
2026-01-28 13:33:12,839 | INFO | Chunk: 4 | WER=100.000000 | S=5 D=0 I=0
2026-01-28 13:33:12,839 | INFO | Chunk: 5 | WER=100.000000 | S=1 D=2 I=0
2026-01-28 13:33:12,839 | INFO | Chunk: 6 | WER=200.000000 | S=1 D=0 I=1
2026-01-28 13:33:12,840 | INFO | Chunk: 7 | WER=500.000000 | S=0 D=0 I=5
2026-01-28 13:33:12,840 | INFO | Chunk: 8 | WER=233.333333 | S=0 D=0 I=7
2026-01-28 13:33:12,842 | INFO | Chunk: 9 | WER=37.704918 | S=4 D=10 I=9
2026-01-28 13:33:12,843 | INFO | Chunk: 10 | WER=33.333333 | S=1 D=1 I=8
2026-01-28 13:33:12,843 | INFO | Chunk: 11 | WER=110.000000 | S=9 D=0 I=2
2026-01-28 13:33:12,845 | INFO | Chunk: 12 | WER=40.000000 | S=3 D=1 I=14
2026-01-28 13:33:12,845 | INFO | Chunk: 13 | WER=80.000000 | S=1 D=2 I=5
2026-01-28 13:33:12,848 | INFO | Chunk: 14 | WER=45.569620 | S=12 D=10 I=14
2026-01-28 13:33:12,848 | INFO | Chunk: 15 | WER=150.000000 | S=4 D=0 I=2
2026-01-28 13:33:12,849 | INFO | Chunk: 16 | WER=82.352941 | S=4 D=2 I=8
2026-01-28 13:33:12,849 | INFO | Chunk: 17 | WER=88.888889 | S=5 D=3 I=0
2026-01-28 13:33:12,849 | INFO | Chunk: 18 | WER=122.222222 | S=9 D=0 I=2
2026-01-28 13:33:12,850 | INFO | Chunk: 19 | WER=71.428571 | S=1 D=7 I=7
2026-01-28 13:33:12,850 | INFO | Chunk: 20 | WER=150.000000 | S=0 D=2 I=4
2026-01-28 13:33:12,850 | INFO | Chunk: 21 | WER=250.000000 | S=2 D=0 I=3
2026-01-28 13:33:12,851 | INFO | Chunk: 22 | WER=59.090909 | S=9 D=10 I=7
2026-01-28 13:33:12,855 | INFO | Chunk: 23 | WER=45.348837 | S=19 D=12 I=8
2026-01-28 13:33:12,855 | INFO | Chunk: 24 | WER=68.181818 | S=5 D=5 I=5
2026-01-28 13:33:12,856 | INFO | Chunk: 25 | WER=48.780488 | S=5 D=10 I=5
2026-01-28 13:33:12,857 | INFO | Chunk: 26 | WER=69.230769 | S=0 D=9 I=9
2026-01-28 13:33:12,857 | INFO | Chunk: 27 | WER=133.333333 | S=1 D=1 I=2
2026-01-28 13:33:12,857 | INFO | Chunk: 28 | WER=100.000000 | S=4 D=2 I=2
2026-01-28 13:33:12,864 | INFO | Chunk: 29 | WER=31.451613 | S=25 D=8 I=6
2026-01-28 13:33:13,122 | INFO | File: Rhap-D0005.wav | WER=42.798913 | S=167 D=49 I=99
2026-01-28 13:33:13,122 | INFO | ------------------------------
2026-01-28 13:33:13,122 | INFO | Conf ester Done!
2026-01-28 13:36:45,150 | INFO | Chunk: 0 | WER=0.000000 | S=1 D=0 I=1
2026-01-28 13:36:45,151 | INFO | Chunk: 1 | WER=100.000000 | S=1 D=0 I=0
2026-01-28 13:36:45,151 | INFO | Chunk: 2 | WER=100.000000 | S=2 D=0 I=0
2026-01-28 13:36:45,152 | INFO | Chunk: 3 | WER=71.875000 | S=6 D=10 I=7
2026-01-28 13:36:45,154 | INFO | Chunk: 4 | WER=38.235294 | S=6 D=3 I=4
2026-01-28 13:36:45,154 | INFO | Chunk: 5 | WER=100.000000 | S=5 D=0 I=0
2026-01-28 13:36:45,154 | INFO | Chunk: 6 | WER=100.000000 | S=1 D=2 I=0
2026-01-28 13:36:45,154 | INFO | Chunk: 7 | WER=100.000000 | S=1 D=0 I=0
2026-01-28 13:36:45,154 | INFO | Chunk: 8 | WER=600.000000 | S=1 D=0 I=5
2026-01-28 13:36:45,154 | INFO | Chunk: 9 | WER=233.333333 | S=0 D=1 I=6
2026-01-28 13:36:45,156 | INFO | Chunk: 10 | WER=39.344262 | S=7 D=11 I=6
2026-01-28 13:36:45,157 | INFO | Chunk: 11 | WER=30.000000 | S=3 D=1 I=5
2026-01-28 13:36:45,158 | INFO | Chunk: 12 | WER=100.000000 | S=9 D=0 I=1
2026-01-28 13:36:45,159 | INFO | Chunk: 13 | WER=40.000000 | S=4 D=5 I=9
2026-01-28 13:36:45,159 | INFO | Chunk: 14 | WER=100.000000 | S=1 D=3 I=0
2026-01-28 13:36:45,160 | INFO | Chunk: 15 | WER=90.000000 | S=2 D=2 I=5
2026-01-28 13:36:45,163 | INFO | Chunk: 16 | WER=41.772152 | S=15 D=14 I=4
2026-01-28 13:36:45,163 | INFO | Chunk: 17 | WER=125.000000 | S=4 D=0 I=1
2026-01-28 13:36:45,163 | INFO | Chunk: 18 | WER=76.470588 | S=3 D=4 I=6
2026-01-28 13:36:45,164 | INFO | Chunk: 19 | WER=88.888889 | S=5 D=3 I=0
2026-01-28 13:36:45,164 | INFO | Chunk: 20 | WER=100.000000 | S=6 D=3 I=0
2026-01-28 13:36:45,164 | INFO | Chunk: 21 | WER=66.666667 | S=2 D=7 I=5
2026-01-28 13:36:45,164 | INFO | Chunk: 22 | WER=125.000000 | S=0 D=2 I=3
2026-01-28 13:36:45,165 | INFO | Chunk: 23 | WER=150.000000 | S=2 D=0 I=1
2026-01-28 13:36:45,166 | INFO | Chunk: 24 | WER=63.636364 | S=17 D=10 I=1
2026-01-28 13:36:45,170 | INFO | Chunk: 25 | WER=38.372093 | S=14 D=15 I=4
2026-01-28 13:36:45,170 | INFO | Chunk: 26 | WER=63.636364 | S=3 D=7 I=4
2026-01-28 13:36:45,170 | INFO | Chunk: 27 | WER=100.000000 | S=1 D=5 I=0
2026-01-28 13:36:45,171 | INFO | Chunk: 28 | WER=48.780488 | S=7 D=11 I=2
2026-01-28 13:36:45,172 | INFO | Chunk: 29 | WER=73.076923 | S=2 D=9 I=8
2026-01-28 13:36:45,172 | INFO | Chunk: 30 | WER=133.333333 | S=1 D=1 I=2
2026-01-28 13:36:45,172 | INFO | Chunk: 31 | WER=100.000000 | S=4 D=2 I=2
2026-01-28 13:36:45,179 | INFO | Chunk: 32 | WER=34.677419 | S=19 D=21 I=3
2026-01-28 13:36:45,430 | INFO | File: Rhap-D0005.wav | WER=41.900937 | S=167 D=103 I=43
2026-01-28 13:36:45,430 | INFO | ------------------------------
2026-01-28 13:36:45,430 | INFO | hmm_tdnn Done!
2026-01-28 13:36:45,599 | INFO | ==================================Rhap-D0006.wav=========================================
2026-01-28 13:36:45,781 | INFO | Using rVAD model
2026-01-28 13:37:05,001 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 13:37:05,001 | INFO | Chunk: 1 | WER=50.000000 | S=0 D=0 I=1
2026-01-28 13:37:05,001 | INFO | Chunk: 2 | WER=100.000000 | S=1 D=2 I=0
2026-01-28 13:37:05,001 | INFO | Chunk: 3 | WER=100.000000 | S=1 D=1 I=0
2026-01-28 13:37:05,002 | INFO | Chunk: 4 | WER=100.000000 | S=1 D=0 I=0
2026-01-28 13:37:05,002 | INFO | Chunk: 5 | WER=100.000000 | S=1 D=1 I=0
2026-01-28 13:37:05,002 | INFO | Chunk: 6 | WER=133.333333 | S=3 D=0 I=1
2026-01-28 13:37:05,008 | INFO | Chunk: 7 | WER=11.926606 | S=9 D=4 I=0
2026-01-28 13:37:05,009 | INFO | Chunk: 8 | WER=6.000000 | S=3 D=0 I=0
2026-01-28 13:37:05,010 | INFO | Chunk: 9 | WER=44.444444 | S=1 D=3 I=0
2026-01-28 13:37:05,010 | INFO | Chunk: 10 | WER=26.666667 | S=3 D=1 I=0
2026-01-28 13:37:05,011 | INFO | Chunk: 11 | WER=36.363636 | S=5 D=7 I=0
2026-01-28 13:37:05,011 | INFO | Chunk: 12 | WER=45.454545 | S=1 D=4 I=0
2026-01-28 13:37:05,012 | INFO | Chunk: 13 | WER=40.000000 | S=4 D=8 I=0
2026-01-28 13:37:05,012 | INFO | Chunk: 14 | WER=100.000000 | S=2 D=1 I=0
2026-01-28 13:37:05,012 | INFO | Chunk: 15 | WER=50.000000 | S=0 D=1 I=0
2026-01-28 13:37:05,012 | INFO | Chunk: 16 | WER=0.000000 | S=1 D=0 I=0
2026-01-28 13:37:05,013 | INFO | Chunk: 17 | WER=26.086957 | S=3 D=3 I=0
2026-01-28 13:37:05,013 | INFO | Chunk: 18 | WER=33.333333 | S=0 D=0 I=1
2026-01-28 13:37:05,013 | INFO | Chunk: 19 | WER=50.000000 | S=0 D=3 I=0
2026-01-28 13:37:05,014 | INFO | Chunk: 20 | WER=13.333333 | S=1 D=1 I=0
2026-01-28 13:37:05,019 | INFO | Chunk: 21 | WER=24.038462 | S=14 D=11 I=0
2026-01-28 13:37:05,019 | INFO | Chunk: 22 | WER=72.727273 | S=2 D=5 I=1
2026-01-28 13:37:05,021 | INFO | Chunk: 23 | WER=31.666667 | S=6 D=12 I=1
2026-01-28 13:37:05,021 | INFO | Chunk: 24 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 13:37:05,022 | INFO | Chunk: 25 | WER=11.111111 | S=0 D=0 I=1
2026-01-28 13:37:05,022 | INFO | Chunk: 26 | WER=73.333333 | S=3 D=8 I=0
2026-01-28 13:37:05,022 | INFO | Chunk: 27 | WER=83.333333 | S=2 D=3 I=0
2026-01-28 13:37:05,022 | INFO | Chunk: 28 | WER=42.857143 | S=1 D=2 I=0
2026-01-28 13:37:05,022 | INFO | Chunk: 29 | WER=38.461538 | S=2 D=3 I=0
2026-01-28 13:37:05,025 | INFO | Chunk: 30 | WER=24.193548 | S=8 D=6 I=1
2026-01-28 13:37:05,025 | INFO | Chunk: 31 | WER=22.222222 | S=0 D=2 I=0
2026-01-28 13:37:05,025 | INFO | Chunk: 32 | WER=66.666667 | S=1 D=1 I=0
2026-01-28 13:37:05,025 | INFO | Chunk: 33 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 13:37:05,025 | INFO | Chunk: 34 | WER=66.666667 | S=2 D=0 I=0
2026-01-28 13:37:05,026 | INFO | Chunk: 35 | WER=55.555556 | S=5 D=5 I=0
2026-01-28 13:37:05,027 | INFO | Chunk: 36 | WER=38.888889 | S=6 D=7 I=1
2026-01-28 13:37:05,028 | INFO | Chunk: 37 | WER=19.047619 | S=6 D=2 I=0
2026-01-28 13:37:05,028 | INFO | Chunk: 38 | WER=24.000000 | S=2 D=4 I=0
2026-01-28 13:37:05,029 | INFO | Chunk: 39 | WER=28.571429 | S=1 D=1 I=0
2026-01-28 13:37:05,029 | INFO | Chunk: 40 | WER=42.857143 | S=0 D=3 I=0
2026-01-28 13:37:05,029 | INFO | Chunk: 41 | WER=62.500000 | S=4 D=6 I=0
2026-01-28 13:37:05,030 | INFO | Chunk: 42 | WER=41.666667 | S=9 D=6 I=0
2026-01-28 13:37:05,030 | INFO | Chunk: 43 | WER=100.000000 | S=1 D=0 I=1
2026-01-28 13:37:05,031 | INFO | Chunk: 44 | WER=10.526316 | S=0 D=2 I=0
2026-01-28 13:37:05,031 | INFO | Chunk: 45 | WER=50.000000 | S=4 D=6 I=0
2026-01-28 13:37:05,031 | INFO | Chunk: 46 | WER=20.000000 | S=2 D=2 I=0
2026-01-28 13:37:05,032 | INFO | Chunk: 47 | WER=14.285714 | S=0 D=0 I=1
2026-01-28 13:37:05,032 | INFO | Chunk: 48 | WER=19.047619 | S=1 D=2 I=1
2026-01-28 13:37:05,032 | INFO | Chunk: 49 | WER=50.000000 | S=5 D=0 I=0
2026-01-28 13:37:05,033 | INFO | Chunk: 50 | WER=17.857143 | S=2 D=3 I=0
2026-01-28 13:37:05,033 | INFO | Chunk: 51 | WER=33.333333 | S=1 D=0 I=0
2026-01-28 13:37:05,033 | INFO | Chunk: 52 | WER=0.000000 | S=1 D=0 I=1
2026-01-28 13:37:05,035 | INFO | Chunk: 53 | WER=22.000000 | S=9 D=2 I=0
2026-01-28 13:37:05,036 | INFO | Chunk: 54 | WER=42.500000 | S=9 D=7 I=1
2026-01-28 13:37:05,479 | INFO | File: Rhap-D0006.wav | WER=29.597701 | S=150 D=148 I=11
2026-01-28 13:37:05,479 | INFO | ------------------------------
2026-01-28 13:37:05,479 | INFO | w2vec vad chunk Done!
2026-01-28 13:37:45,070 | INFO | Chunk: 0 | WER=100.000000 | S=1 D=0 I=2
2026-01-28 13:37:45,070 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 13:37:45,071 | INFO | Chunk: 2 | WER=100.000000 | S=2 D=1 I=0
2026-01-28 13:37:45,071 | INFO | Chunk: 3 | WER=100.000000 | S=1 D=1 I=0
2026-01-28 13:37:45,071 | INFO | Chunk: 4 | WER=100.000000 | S=1 D=0 I=0
2026-01-28 13:37:45,071 | INFO | Chunk: 5 | WER=100.000000 | S=2 D=0 I=0
2026-01-28 13:37:45,071 | INFO | Chunk: 6 | WER=166.666667 | S=3 D=0 I=2
2026-01-28 13:37:45,074 | INFO | Chunk: 7 | WER=54.128440 | S=2 D=57 I=0
2026-01-28 13:37:45,075 | INFO | Chunk: 8 | WER=36.000000 | S=4 D=14 I=0
2026-01-28 13:37:45,076 | INFO | Chunk: 9 | WER=66.666667 | S=1 D=5 I=0
2026-01-28 13:37:45,076 | INFO | Chunk: 10 | WER=33.333333 | S=2 D=2 I=1
2026-01-28 13:37:45,077 | INFO | Chunk: 11 | WER=24.242424 | S=6 D=2 I=0
2026-01-28 13:37:45,077 | INFO | Chunk: 12 | WER=36.363636 | S=1 D=3 I=0
2026-01-28 13:37:45,077 | INFO | Chunk: 13 | WER=40.000000 | S=4 D=8 I=0
2026-01-28 13:37:45,078 | INFO | Chunk: 14 | WER=100.000000 | S=1 D=2 I=0
2026-01-28 13:37:45,078 | INFO | Chunk: 15 | WER=100.000000 | S=0 D=1 I=1
2026-01-28 13:37:45,078 | INFO | Chunk: 16 | WER=0.000000 | S=1 D=0 I=0
2026-01-28 13:37:45,078 | INFO | Chunk: 17 | WER=30.434783 | S=2 D=5 I=0
2026-01-28 13:37:45,078 | INFO | Chunk: 18 | WER=33.333333 | S=1 D=0 I=0
2026-01-28 13:37:45,079 | INFO | Chunk: 19 | WER=50.000000 | S=0 D=3 I=0
2026-01-28 13:37:45,079 | INFO | Chunk: 20 | WER=40.000000 | S=1 D=5 I=0
2026-01-28 13:37:45,082 | INFO | Chunk: 21 | WER=64.423077 | S=6 D=60 I=1
2026-01-28 13:37:45,082 | INFO | Chunk: 22 | WER=27.272727 | S=0 D=3 I=0
2026-01-28 13:37:45,083 | INFO | Chunk: 23 | WER=48.333333 | S=8 D=21 I=0
2026-01-28 13:37:45,083 | INFO | Chunk: 24 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 13:37:45,084 | INFO | Chunk: 25 | WER=11.111111 | S=0 D=0 I=1
2026-01-28 13:37:45,084 | INFO | Chunk: 26 | WER=46.666667 | S=1 D=6 I=0
2026-01-28 13:37:45,084 | INFO | Chunk: 27 | WER=66.666667 | S=2 D=0 I=2
2026-01-28 13:37:45,084 | INFO | Chunk: 28 | WER=42.857143 | S=0 D=3 I=0
2026-01-28 13:37:45,084 | INFO | Chunk: 29 | WER=30.769231 | S=0 D=4 I=0
2026-01-28 13:37:45,086 | INFO | Chunk: 30 | WER=30.645161 | S=4 D=14 I=1
2026-01-28 13:37:45,086 | INFO | Chunk: 31 | WER=22.222222 | S=0 D=2 I=0
2026-01-28 13:37:45,087 | INFO | Chunk: 32 | WER=33.333333 | S=0 D=0 I=1
2026-01-28 13:37:45,087 | INFO | Chunk: 33 | WER=14.285714 | S=1 D=0 I=0
2026-01-28 13:37:45,087 | INFO | Chunk: 34 | WER=100.000000 | S=3 D=0 I=0
2026-01-28 13:37:45,087 | INFO | Chunk: 35 | WER=44.444444 | S=2 D=6 I=0
2026-01-28 13:37:45,088 | INFO | Chunk: 36 | WER=38.888889 | S=2 D=12 I=0
2026-01-28 13:37:45,089 | INFO | Chunk: 37 | WER=35.714286 | S=1 D=14 I=0
2026-01-28 13:37:45,089 | INFO | Chunk: 38 | WER=36.000000 | S=3 D=6 I=0
2026-01-28 13:37:45,090 | INFO | Chunk: 39 | WER=28.571429 | S=1 D=0 I=1
2026-01-28 13:37:45,090 | INFO | Chunk: 40 | WER=71.428571 | S=0 D=5 I=0
2026-01-28 13:37:45,090 | INFO | Chunk: 41 | WER=18.750000 | S=1 D=2 I=0
2026-01-28 13:37:45,091 | INFO | Chunk: 42 | WER=30.555556 | S=3 D=8 I=0
2026-01-28 13:37:45,091 | INFO | Chunk: 43 | WER=100.000000 | S=0 D=1 I=1
2026-01-28 13:37:45,091 | INFO | Chunk: 44 | WER=21.052632 | S=1 D=2 I=1
2026-01-28 13:37:45,092 | INFO | Chunk: 45 | WER=40.000000 | S=3 D=5 I=0
2026-01-28 13:37:45,092 | INFO | Chunk: 46 | WER=20.000000 | S=1 D=3 I=0
2026-01-28 13:37:45,092 | INFO | Chunk: 47 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 13:37:45,093 | INFO | Chunk: 48 | WER=23.809524 | S=0 D=4 I=1
2026-01-28 13:37:45,093 | INFO | Chunk: 49 | WER=60.000000 | S=3 D=2 I=1
2026-01-28 13:37:45,094 | INFO | Chunk: 50 | WER=53.571429 | S=4 D=11 I=0
2026-01-28 13:37:45,094 | INFO | Chunk: 51 | WER=33.333333 | S=0 D=0 I=1
2026-01-28 13:37:45,094 | INFO | Chunk: 52 | WER=0.000000 | S=1 D=0 I=0
2026-01-28 13:37:45,095 | INFO | Chunk: 53 | WER=54.000000 | S=3 D=24 I=0
2026-01-28 13:37:45,096 | INFO | Chunk: 54 | WER=30.000000 | S=7 D=5 I=0
2026-01-28 13:37:45,428 | INFO | File: Rhap-D0006.wav | WER=42.241379 | S=103 D=325 I=13
2026-01-28 13:37:45,429 | INFO | ------------------------------
2026-01-28 13:37:45,429 | INFO | whisper med Done!
2026-01-28 13:38:40,082 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 13:38:40,082 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 13:38:40,082 | INFO | Chunk: 2 | WER=133.333333 | S=3 D=0 I=1
2026-01-28 13:38:40,082 | INFO | Chunk: 3 | WER=250.000000 | S=2 D=0 I=3
2026-01-28 13:38:40,082 | INFO | Chunk: 4 | WER=100.000000 | S=1 D=0 I=0
2026-01-28 13:38:40,083 | INFO | Chunk: 5 | WER=100.000000 | S=1 D=1 I=0
2026-01-28 13:38:40,083 | INFO | Chunk: 6 | WER=133.333333 | S=2 D=0 I=2
2026-01-28 13:38:40,086 | INFO | Chunk: 7 | WER=56.880734 | S=18 D=44 I=0
2026-01-28 13:38:40,088 | INFO | Chunk: 8 | WER=30.000000 | S=3 D=12 I=0
2026-01-28 13:38:40,088 | INFO | Chunk: 9 | WER=66.666667 | S=1 D=5 I=0
2026-01-28 13:38:40,088 | INFO | Chunk: 10 | WER=26.666667 | S=1 D=2 I=1
2026-01-28 13:38:40,089 | INFO | Chunk: 11 | WER=12.121212 | S=0 D=4 I=0
2026-01-28 13:38:40,090 | INFO | Chunk: 12 | WER=36.363636 | S=0 D=3 I=1
2026-01-28 13:38:40,090 | INFO | Chunk: 13 | WER=33.333333 | S=2 D=8 I=0
2026-01-28 13:38:40,091 | INFO | Chunk: 14 | WER=33.333333 | S=0 D=1 I=0
2026-01-28 13:38:40,091 | INFO | Chunk: 15 | WER=100.000000 | S=0 D=1 I=1
2026-01-28 13:38:40,091 | INFO | Chunk: 16 | WER=0.000000 | S=1 D=0 I=0
2026-01-28 13:38:40,091 | INFO | Chunk: 17 | WER=30.434783 | S=2 D=5 I=0
2026-01-28 13:38:40,092 | INFO | Chunk: 18 | WER=33.333333 | S=0 D=0 I=1
2026-01-28 13:38:40,092 | INFO | Chunk: 19 | WER=50.000000 | S=1 D=2 I=0
2026-01-28 13:38:40,092 | INFO | Chunk: 20 | WER=6.666667 | S=0 D=1 I=0
2026-01-28 13:38:40,095 | INFO | Chunk: 21 | WER=65.384615 | S=4 D=63 I=1
2026-01-28 13:38:40,096 | INFO | Chunk: 22 | WER=36.363636 | S=2 D=2 I=0
2026-01-28 13:38:40,097 | INFO | Chunk: 23 | WER=51.666667 | S=4 D=27 I=0
2026-01-28 13:38:40,097 | INFO | Chunk: 24 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 13:38:40,098 | INFO | Chunk: 25 | WER=22.222222 | S=0 D=0 I=2
2026-01-28 13:38:40,098 | INFO | Chunk: 26 | WER=40.000000 | S=1 D=5 I=0
2026-01-28 13:38:40,098 | INFO | Chunk: 27 | WER=33.333333 | S=0 D=1 I=1
2026-01-28 13:38:40,098 | INFO | Chunk: 28 | WER=42.857143 | S=0 D=3 I=0
2026-01-28 13:38:40,099 | INFO | Chunk: 29 | WER=38.461538 | S=3 D=2 I=0
2026-01-28 13:38:40,101 | INFO | Chunk: 30 | WER=17.741935 | S=7 D=2 I=2
2026-01-28 13:38:40,102 | INFO | Chunk: 31 | WER=22.222222 | S=0 D=2 I=0
2026-01-28 13:38:40,102 | INFO | Chunk: 32 | WER=33.333333 | S=0 D=0 I=1
2026-01-28 13:38:40,102 | INFO | Chunk: 33 | WER=14.285714 | S=1 D=0 I=0
2026-01-28 13:38:40,102 | INFO | Chunk: 34 | WER=100.000000 | S=2 D=1 I=0
2026-01-28 13:38:40,103 | INFO | Chunk: 35 | WER=33.333333 | S=0 D=6 I=0
2026-01-28 13:38:40,104 | INFO | Chunk: 36 | WER=30.555556 | S=2 D=9 I=0
2026-01-28 13:38:40,105 | INFO | Chunk: 37 | WER=19.047619 | S=3 D=5 I=0
2026-01-28 13:38:40,105 | INFO | Chunk: 38 | WER=32.000000 | S=2 D=5 I=1
2026-01-28 13:38:40,106 | INFO | Chunk: 39 | WER=28.571429 | S=1 D=0 I=1
2026-01-28 13:38:40,106 | INFO | Chunk: 40 | WER=28.571429 | S=0 D=2 I=0
2026-01-28 13:38:40,106 | INFO | Chunk: 41 | WER=18.750000 | S=0 D=3 I=0
2026-01-28 13:38:40,107 | INFO | Chunk: 42 | WER=38.888889 | S=2 D=12 I=0
2026-01-28 13:38:40,107 | INFO | Chunk: 43 | WER=100.000000 | S=0 D=1 I=1
2026-01-28 13:38:40,108 | INFO | Chunk: 44 | WER=26.315789 | S=0 D=4 I=1
2026-01-28 13:38:40,108 | INFO | Chunk: 45 | WER=45.000000 | S=3 D=5 I=1
2026-01-28 13:38:40,109 | INFO | Chunk: 46 | WER=25.000000 | S=1 D=3 I=1
2026-01-28 13:38:40,109 | INFO | Chunk: 47 | WER=14.285714 | S=0 D=0 I=1
2026-01-28 13:38:40,110 | INFO | Chunk: 48 | WER=19.047619 | S=1 D=2 I=1
2026-01-28 13:38:40,110 | INFO | Chunk: 49 | WER=50.000000 | S=2 D=2 I=1
2026-01-28 13:38:40,111 | INFO | Chunk: 50 | WER=39.285714 | S=5 D=6 I=0
2026-01-28 13:38:40,111 | INFO | Chunk: 51 | WER=33.333333 | S=0 D=0 I=1
2026-01-28 13:38:40,111 | INFO | Chunk: 52 | WER=0.000000 | S=1 D=0 I=0
2026-01-28 13:38:40,112 | INFO | Chunk: 53 | WER=18.000000 | S=5 D=4 I=0
2026-01-28 13:38:40,113 | INFO | Chunk: 54 | WER=20.000000 | S=3 D=5 I=0
2026-01-28 13:38:40,508 | INFO | File: Rhap-D0006.wav | WER=36.877395 | S=97 D=265 I=23
2026-01-28 13:38:40,509 | INFO | ------------------------------
2026-01-28 13:38:40,509 | INFO | whisper large Done!
2026-01-28 13:38:40,672 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 13:38:40,704 | INFO | Vocabulary size: 350
2026-01-28 13:38:41,262 | INFO | Gradient checkpoint layers: []
2026-01-28 13:38:42,234 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 13:38:43,308 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 13:38:43,308 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 13:38:43,309 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 13:38:43,310 | INFO | speech length: 18720
2026-01-28 13:38:43,352 | INFO | decoder input length: 28
2026-01-28 13:38:43,352 | INFO | max output length: 28
2026-01-28 13:38:43,352 | INFO | min output length: 2
2026-01-28 13:38:45,486 | INFO | end detected at 19
2026-01-28 13:38:45,488 | INFO |  -1.40 * 0.5 =  -0.70 for decoder
2026-01-28 13:38:45,488 | INFO |  -6.65 * 0.5 =  -3.33 for ctc
2026-01-28 13:38:45,488 | INFO | total log probability: -4.03
2026-01-28 13:38:45,488 | INFO | normalized log probability: -0.29
2026-01-28 13:38:45,488 | INFO | total number of ended hypotheses: 169
2026-01-28 13:38:45,488 | INFO | best hypo: ▁honnêtement▁pas▁vraiment

2026-01-28 13:38:45,491 | INFO | speech length: 8160
2026-01-28 13:38:45,516 | INFO | decoder input length: 12
2026-01-28 13:38:45,516 | INFO | max output length: 12
2026-01-28 13:38:45,516 | INFO | min output length: 1
2026-01-28 13:38:46,679 | INFO | adding <eos> in the last position in the loop
2026-01-28 13:38:46,690 | INFO | end detected at 11
2026-01-28 13:38:46,692 | INFO |  -1.89 * 0.5 =  -0.95 for decoder
2026-01-28 13:38:46,692 | INFO |  -3.10 * 0.5 =  -1.55 for ctc
2026-01-28 13:38:46,692 | INFO | total log probability: -2.50
2026-01-28 13:38:46,692 | INFO | normalized log probability: -0.83
2026-01-28 13:38:46,692 | INFO | total number of ended hypotheses: 227
2026-01-28 13:38:46,692 | INFO | best hypo: ▁ch

2026-01-28 13:38:46,694 | INFO | speech length: 10560
2026-01-28 13:38:46,722 | INFO | decoder input length: 16
2026-01-28 13:38:46,722 | INFO | max output length: 16
2026-01-28 13:38:46,722 | INFO | min output length: 1
2026-01-28 13:38:47,577 | INFO | end detected at 8
2026-01-28 13:38:47,579 | INFO |  -0.81 * 0.5 =  -0.41 for decoder
2026-01-28 13:38:47,579 | INFO |  -1.18 * 0.5 =  -0.59 for ctc
2026-01-28 13:38:47,579 | INFO | total log probability: -1.00
2026-01-28 13:38:47,579 | INFO | normalized log probability: -0.33
2026-01-28 13:38:47,579 | INFO | total number of ended hypotheses: 160
2026-01-28 13:38:47,579 | INFO | best hypo: ▁deux

2026-01-28 13:38:47,580 | INFO | speech length: 14080
2026-01-28 13:38:47,615 | INFO | decoder input length: 21
2026-01-28 13:38:47,615 | INFO | max output length: 21
2026-01-28 13:38:47,615 | INFO | min output length: 2
2026-01-28 13:38:49,604 | INFO | end detected at 18
2026-01-28 13:38:49,606 | INFO |  -2.47 * 0.5 =  -1.23 for decoder
2026-01-28 13:38:49,606 | INFO |  -5.45 * 0.5 =  -2.72 for ctc
2026-01-28 13:38:49,606 | INFO | total log probability: -3.96
2026-01-28 13:38:49,606 | INFO | normalized log probability: -0.40
2026-01-28 13:38:49,606 | INFO | total number of ended hypotheses: 200
2026-01-28 13:38:49,607 | INFO | best hypo: ▁mais▁je▁n'avais▁pas

2026-01-28 13:38:49,608 | INFO | speech length: 440800
2026-01-28 13:38:49,643 | INFO | decoder input length: 688
2026-01-28 13:38:49,643 | INFO | max output length: 688
2026-01-28 13:38:49,643 | INFO | min output length: 68
2026-01-28 13:39:29,239 | INFO | end detected at 210
2026-01-28 13:39:29,240 | INFO | -629.39 * 0.5 = -314.70 for decoder
2026-01-28 13:39:29,240 | INFO | -209.34 * 0.5 = -104.67 for ctc
2026-01-28 13:39:29,240 | INFO | total log probability: -419.37
2026-01-28 13:39:29,240 | INFO | normalized log probability: -2.06
2026-01-28 13:39:29,240 | INFO | total number of ended hypotheses: 176
2026-01-28 13:39:29,242 | INFO | best hypo: ▁non▁c'est▁plus▁au▁niveau▁de▁la▁faque▁je▁vois▁moilafax▁ç▁m'a▁fait▁beaucoup▁de▁bien▁ça▁m'a▁sortit▁un▁petit▁peu▁de▁mon▁ticocon▁du▁septième▁et▁ça▁m'a▁fait▁voir▁même▁si▁c'est▁ca▁même▁une▁fac▁dans▁le▁septième▁face▁a▁des▁gens▁qui▁viennement▁de▁partout'▁et▁ça▁m'en▁frapper▁de▁v▁voir'▁que▁j'avais▁à▁une▁amieu▁qui▁vivait▁un▁ans▁pas▁complètement▁étonome▁ce▁que▁les▁ses▁parenton▁sont▁divorcés▁nont▁pas▁les▁moyens▁de▁le▁faire▁vivre▁de▁dos▁gens▁venaient▁de▁g▁venaient▁de▁l'étranger

2026-01-28 13:39:29,245 | INFO | speech length: 281600
2026-01-28 13:39:29,281 | INFO | decoder input length: 439
2026-01-28 13:39:29,281 | INFO | max output length: 439
2026-01-28 13:39:29,281 | INFO | min output length: 43
2026-01-28 13:39:43,742 | INFO | end detected at 117
2026-01-28 13:39:43,744 | INFO | -182.46 * 0.5 = -91.23 for decoder
2026-01-28 13:39:43,744 | INFO | -33.16 * 0.5 = -16.58 for ctc
2026-01-28 13:39:43,744 | INFO | total log probability: -107.81
2026-01-28 13:39:43,744 | INFO | normalized log probability: -0.98
2026-01-28 13:39:43,744 | INFO | total number of ended hypotheses: 187
2026-01-28 13:39:43,745 | INFO | best hypo: ▁et▁qui▁pour▁pouvoir▁se▁payer▁la▁petite▁prépare▁en▁plus▁pour▁réussir▁le▁concours▁devait▁travailler▁le▁week▁end▁chez▁monoprix▁facez▁les▁petites▁choses▁qui▁ont▁beaucoup▁de▁joué▁ce▁qui▁m'ont▁fait▁réaliser▁vraiment▁ce▁que▁c'était▁l'argent▁et▁je▁pense

2026-01-28 13:39:43,748 | INFO | speech length: 51040
2026-01-28 13:39:43,781 | INFO | decoder input length: 79
2026-01-28 13:39:43,782 | INFO | max output length: 79
2026-01-28 13:39:43,782 | INFO | min output length: 7
2026-01-28 13:39:45,929 | INFO | end detected at 23
2026-01-28 13:39:45,931 | INFO |  -5.42 * 0.5 =  -2.71 for decoder
2026-01-28 13:39:45,931 | INFO | -15.04 * 0.5 =  -7.52 for ctc
2026-01-28 13:39:45,931 | INFO | total log probability: -10.23
2026-01-28 13:39:45,931 | INFO | normalized log probability: -0.60
2026-01-28 13:39:45,931 | INFO | total number of ended hypotheses: 213
2026-01-28 13:39:45,932 | INFO | best hypo: ▁c'a▁été▁très▁bénéficiaire▁eux▁aussi

2026-01-28 13:39:45,934 | INFO | speech length: 77760
2026-01-28 13:39:45,970 | INFO | decoder input length: 121
2026-01-28 13:39:45,970 | INFO | max output length: 121
2026-01-28 13:39:45,970 | INFO | min output length: 12
2026-01-28 13:39:50,397 | INFO | end detected at 43
2026-01-28 13:39:50,398 | INFO |  -9.75 * 0.5 =  -4.87 for decoder
2026-01-28 13:39:50,398 | INFO |  -4.95 * 0.5 =  -2.47 for ctc
2026-01-28 13:39:50,398 | INFO | total log probability: -7.35
2026-01-28 13:39:50,398 | INFO | normalized log probability: -0.20
2026-01-28 13:39:50,398 | INFO | total number of ended hypotheses: 167
2026-01-28 13:39:50,398 | INFO | best hypo: ▁ben▁non▁vraiment▁cet▁arge▁avec▁cet▁argent▁là▁je▁me▁paie▁maintenant▁quasiment▁tout

2026-01-28 13:39:50,400 | INFO | speech length: 148480
2026-01-28 13:39:50,435 | INFO | decoder input length: 231
2026-01-28 13:39:50,435 | INFO | max output length: 231
2026-01-28 13:39:50,435 | INFO | min output length: 23
2026-01-28 13:39:58,271 | INFO | end detected at 70
2026-01-28 13:39:58,272 | INFO | -14.17 * 0.5 =  -7.09 for decoder
2026-01-28 13:39:58,272 | INFO | -18.07 * 0.5 =  -9.04 for ctc
2026-01-28 13:39:58,272 | INFO | total log probability: -16.12
2026-01-28 13:39:58,272 | INFO | normalized log probability: -0.26
2026-01-28 13:39:58,272 | INFO | total number of ended hypotheses: 198
2026-01-28 13:39:58,273 | INFO | best hypo: ▁a▁part▁la▁nourriture▁et▁logement▁là▁mais▁ce▁soit▁déshabit▁les▁vacances▁la▁gemme▁fermée▁des▁vaccins▁par▁exemple▁à▁ces▁mois▁qui▁s'est▁payée

2026-01-28 13:39:58,275 | INFO | speech length: 36800
2026-01-28 13:39:58,304 | INFO | decoder input length: 57
2026-01-28 13:39:58,304 | INFO | max output length: 57
2026-01-28 13:39:58,304 | INFO | min output length: 5
2026-01-28 13:40:00,124 | INFO | end detected at 21
2026-01-28 13:40:00,126 | INFO |  -5.22 * 0.5 =  -2.61 for decoder
2026-01-28 13:40:00,126 | INFO |  -9.95 * 0.5 =  -4.97 for ctc
2026-01-28 13:40:00,126 | INFO | total log probability: -7.58
2026-01-28 13:40:00,126 | INFO | normalized log probability: -0.51
2026-01-28 13:40:00,126 | INFO | total number of ended hypotheses: 202
2026-01-28 13:40:00,127 | INFO | best hypo: ▁non▁si▁les▁parents▁commencent▁et

2026-01-28 13:40:00,128 | INFO | speech length: 95680
2026-01-28 13:40:00,162 | INFO | decoder input length: 149
2026-01-28 13:40:00,162 | INFO | max output length: 149
2026-01-28 13:40:00,162 | INFO | min output length: 14
2026-01-28 13:40:05,052 | INFO | end detected at 53
2026-01-28 13:40:05,054 | INFO | -14.18 * 0.5 =  -7.09 for decoder
2026-01-28 13:40:05,054 | INFO | -20.75 * 0.5 = -10.37 for ctc
2026-01-28 13:40:05,054 | INFO | total log probability: -17.46
2026-01-28 13:40:05,054 | INFO | normalized log probability: -0.38
2026-01-28 13:40:05,054 | INFO | total number of ended hypotheses: 188
2026-01-28 13:40:05,054 | INFO | best hypo: ▁le▁fait▁d'avoir▁été▁la▁fac▁de▁mettre▁un▁peu▁sorti▁de▁m'anti▁cocon▁ça▁m'a▁renforcé▁l'adent

2026-01-28 13:40:05,056 | INFO | speech length: 9440
2026-01-28 13:40:05,080 | INFO | decoder input length: 14
2026-01-28 13:40:05,080 | INFO | max output length: 14
2026-01-28 13:40:05,080 | INFO | min output length: 1
2026-01-28 13:40:05,816 | INFO | end detected at 9
2026-01-28 13:40:05,817 | INFO |  -0.67 * 0.5 =  -0.33 for decoder
2026-01-28 13:40:05,817 | INFO |  -1.00 * 0.5 =  -0.50 for ctc
2026-01-28 13:40:05,817 | INFO | total log probability: -0.84
2026-01-28 13:40:05,817 | INFO | normalized log probability: -0.17
2026-01-28 13:40:05,817 | INFO | total number of ended hypotheses: 139
2026-01-28 13:40:05,817 | INFO | best hypo: ▁avez

2026-01-28 13:40:05,819 | INFO | speech length: 8640
2026-01-28 13:40:05,849 | INFO | decoder input length: 13
2026-01-28 13:40:05,849 | INFO | max output length: 13
2026-01-28 13:40:05,849 | INFO | min output length: 1
2026-01-28 13:40:06,811 | INFO | end detected at 9
2026-01-28 13:40:06,812 | INFO |  -0.95 * 0.5 =  -0.47 for decoder
2026-01-28 13:40:06,812 | INFO |  -1.08 * 0.5 =  -0.54 for ctc
2026-01-28 13:40:06,812 | INFO | total log probability: -1.01
2026-01-28 13:40:06,812 | INFO | normalized log probability: -0.25
2026-01-28 13:40:06,812 | INFO | total number of ended hypotheses: 179
2026-01-28 13:40:06,812 | INFO | best hypo: ▁non

2026-01-28 13:40:06,814 | INFO | speech length: 85280
2026-01-28 13:40:06,849 | INFO | decoder input length: 132
2026-01-28 13:40:06,850 | INFO | max output length: 132
2026-01-28 13:40:06,850 | INFO | min output length: 13
2026-01-28 13:40:12,807 | INFO | end detected at 54
2026-01-28 13:40:12,808 | INFO |  -9.12 * 0.5 =  -4.56 for decoder
2026-01-28 13:40:12,808 | INFO | -11.89 * 0.5 =  -5.95 for ctc
2026-01-28 13:40:12,809 | INFO | total log probability: -10.50
2026-01-28 13:40:12,809 | INFO | normalized log probability: -0.23
2026-01-28 13:40:12,809 | INFO | total number of ended hypotheses: 181
2026-01-28 13:40:12,809 | INFO | best hypo: ▁hontement▁on▁vit▁vraiment▁dans▁un▁dix▁micro▁vin▁vraiment▁un▁petit▁microcosme▁ça▁fait▁du▁bien▁sortirable

2026-01-28 13:40:12,811 | INFO | speech length: 28480
2026-01-28 13:40:12,846 | INFO | decoder input length: 44
2026-01-28 13:40:12,846 | INFO | max output length: 44
2026-01-28 13:40:12,846 | INFO | min output length: 4
2026-01-28 13:40:14,236 | INFO | end detected at 13
2026-01-28 13:40:14,237 | INFO |  -1.28 * 0.5 =  -0.64 for decoder
2026-01-28 13:40:14,237 | INFO |  -1.12 * 0.5 =  -0.56 for ctc
2026-01-28 13:40:14,237 | INFO | total log probability: -1.20
2026-01-28 13:40:14,237 | INFO | normalized log probability: -0.15
2026-01-28 13:40:14,237 | INFO | total number of ended hypotheses: 160
2026-01-28 13:40:14,237 | INFO | best hypo: ▁type▁de▁travail

2026-01-28 13:40:14,239 | INFO | speech length: 31040
2026-01-28 13:40:14,267 | INFO | decoder input length: 48
2026-01-28 13:40:14,267 | INFO | max output length: 48
2026-01-28 13:40:14,267 | INFO | min output length: 4
2026-01-28 13:40:15,743 | INFO | end detected at 14
2026-01-28 13:40:15,744 | INFO |  -2.96 * 0.5 =  -1.48 for decoder
2026-01-28 13:40:15,744 | INFO |  -9.08 * 0.5 =  -4.54 for ctc
2026-01-28 13:40:15,744 | INFO | total log probability: -6.02
2026-01-28 13:40:15,744 | INFO | normalized log probability: -0.75
2026-01-28 13:40:15,744 | INFO | total number of ended hypotheses: 166
2026-01-28 13:40:15,744 | INFO | best hypo: ▁il▁existe▁en▁france

2026-01-28 13:40:15,746 | INFO | speech length: 48320
2026-01-28 13:40:15,779 | INFO | decoder input length: 75
2026-01-28 13:40:15,779 | INFO | max output length: 75
2026-01-28 13:40:15,779 | INFO | min output length: 7
2026-01-28 13:40:19,494 | INFO | end detected at 34
2026-01-28 13:40:19,496 | INFO | -13.47 * 0.5 =  -6.74 for decoder
2026-01-28 13:40:19,496 | INFO | -11.39 * 0.5 =  -5.69 for ctc
2026-01-28 13:40:19,496 | INFO | total log probability: -12.43
2026-01-28 13:40:19,496 | INFO | normalized log probability: -0.46
2026-01-28 13:40:19,496 | INFO | total number of ended hypotheses: 195
2026-01-28 13:40:19,497 | INFO | best hypo: ▁ar▁moi▁déjà▁je▁vous▁travaille▁dans▁le▁public▁et▁panon▁privé

2026-01-28 13:40:19,498 | INFO | speech length: 473120
2026-01-28 13:40:19,536 | INFO | decoder input length: 738
2026-01-28 13:40:19,536 | INFO | max output length: 738
2026-01-28 13:40:19,536 | INFO | min output length: 73
2026-01-28 13:40:54,743 | INFO | end detected at 222
2026-01-28 13:40:54,744 | INFO | -490.83 * 0.5 = -245.42 for decoder
2026-01-28 13:40:54,744 | INFO | -170.19 * 0.5 = -85.09 for ctc
2026-01-28 13:40:54,744 | INFO | total log probability: -330.51
2026-01-28 13:40:54,744 | INFO | normalized log probability: -1.53
2026-01-28 13:40:54,744 | INFO | total number of ended hypotheses: 150
2026-01-28 13:40:54,747 | INFO | best hypo: ▁eh▁je▁possible▁de▁savoir▁pourquoi▁parce▁que▁je▁travaille▁dans▁puisriver▁la▁chez▁les▁petits▁boulots▁en▁plus▁et▁ça▁n'en▁regène▁pas▁parce▁que▁c'est▁vraiment▁uniquement▁tournée▁vers▁l'argent▁et▁à▁cette▁dimension▁outacte▁avec▁le▁maladie▁qui▁a▁complètement▁disparu▁et▁pour▁le▁coup▁cette▁dimension▁spécialisée▁est▁la▁médecine▁est▁a▁encore▁plus▁rencée▁parce▁que▁pour▁gagner▁encore▁plus▁et▁il▁bette▁le▁spécialiste▁et▁spécialisme▁de▁spécialiste▁donc▁ça▁série▁un▁on▁dropoime▁patrop▁et▁en▁plus▁la▁vraie▁médecine

2026-01-28 13:40:54,749 | INFO | speech length: 35840
2026-01-28 13:40:54,784 | INFO | decoder input length: 55
2026-01-28 13:40:54,784 | INFO | max output length: 55
2026-01-28 13:40:54,785 | INFO | min output length: 5
2026-01-28 13:40:56,978 | INFO | end detected at 24
2026-01-28 13:40:56,979 | INFO |  -9.12 * 0.5 =  -4.56 for decoder
2026-01-28 13:40:56,979 | INFO | -10.92 * 0.5 =  -5.46 for ctc
2026-01-28 13:40:56,979 | INFO | total log probability: -10.02
2026-01-28 13:40:56,979 | INFO | normalized log probability: -0.67
2026-01-28 13:40:56,979 | INFO | total number of ended hypotheses: 129
2026-01-28 13:40:56,979 | INFO | best hypo: ▁c'est▁l'hôpital▁congre

2026-01-28 13:40:56,981 | INFO | speech length: 232000
2026-01-28 13:40:57,013 | INFO | decoder input length: 362
2026-01-28 13:40:57,014 | INFO | max output length: 362
2026-01-28 13:40:57,014 | INFO | min output length: 36
2026-01-28 13:41:11,933 | INFO | end detected at 124
2026-01-28 13:41:11,934 | INFO | -176.80 * 0.5 = -88.40 for decoder
2026-01-28 13:41:11,934 | INFO | -65.66 * 0.5 = -32.83 for ctc
2026-01-28 13:41:11,934 | INFO | total log probability: -121.23
2026-01-28 13:41:11,934 | INFO | normalized log probability: -1.04
2026-01-28 13:41:11,934 | INFO | total number of ended hypotheses: 189
2026-01-28 13:41:11,935 | INFO | best hypo: ▁tout▁ce▁que▁jamais▁ce▁que▁j'avais▁la▁vraie▁médecine▁c'est▁c'est▁les▁cas▁concrets▁quel▁est▁le▁niveau▁de▁santé▁de▁la▁population▁quels▁sont▁les▁tels▁tels▁et▁tels▁qu's▁on▁les▁verrait▁pas▁dans▁votre▁hôpital▁privé▁les▁classes▁ou▁moyennes▁et▁ne▁vont▁pas▁en▁king▁priver

2026-01-28 13:41:11,938 | INFO | speech length: 26880
2026-01-28 13:41:11,972 | INFO | decoder input length: 41
2026-01-28 13:41:11,972 | INFO | max output length: 41
2026-01-28 13:41:11,972 | INFO | min output length: 4
2026-01-28 13:41:13,524 | INFO | end detected at 16
2026-01-28 13:41:13,525 | INFO |  -0.83 * 0.5 =  -0.41 for decoder
2026-01-28 13:41:13,525 | INFO |  -0.73 * 0.5 =  -0.36 for ctc
2026-01-28 13:41:13,525 | INFO | total log probability: -0.78
2026-01-28 13:41:13,525 | INFO | normalized log probability: -0.06
2026-01-28 13:41:13,525 | INFO | total number of ended hypotheses: 144
2026-01-28 13:41:13,525 | INFO | best hypo: ▁sinon▁j'aimerais

2026-01-28 13:41:13,527 | INFO | speech length: 25120
2026-01-28 13:41:13,558 | INFO | decoder input length: 38
2026-01-28 13:41:13,558 | INFO | max output length: 38
2026-01-28 13:41:13,558 | INFO | min output length: 3
2026-01-28 13:41:16,218 | INFO | end detected at 28
2026-01-28 13:41:16,219 | INFO |  -9.71 * 0.5 =  -4.85 for decoder
2026-01-28 13:41:16,220 | INFO | -11.13 * 0.5 =  -5.57 for ctc
2026-01-28 13:41:16,220 | INFO | total log probability: -10.42
2026-01-28 13:41:16,220 | INFO | normalized log probability: -0.50
2026-01-28 13:41:16,220 | INFO | total number of ended hypotheses: 163
2026-01-28 13:41:16,220 | INFO | best hypo: ▁comme▁spécialité▁je▁cépa▁je▁peux▁pas▁venir

2026-01-28 13:41:16,222 | INFO | speech length: 35200
2026-01-28 13:41:16,255 | INFO | decoder input length: 54
2026-01-28 13:41:16,255 | INFO | max output length: 54
2026-01-28 13:41:16,255 | INFO | min output length: 5
2026-01-28 13:41:19,693 | INFO | end detected at 36
2026-01-28 13:41:19,694 | INFO |  -2.39 * 0.5 =  -1.19 for decoder
2026-01-28 13:41:19,695 | INFO |  -8.44 * 0.5 =  -4.22 for ctc
2026-01-28 13:41:19,695 | INFO | total log probability: -5.41
2026-01-28 13:41:19,695 | INFO | normalized log probability: -0.20
2026-01-28 13:41:19,695 | INFO | total number of ended hypotheses: 216
2026-01-28 13:41:19,695 | INFO | best hypo: ▁mais▁c'est▁surtout▁l'hôpital▁public▁immédiat

2026-01-28 13:41:19,697 | INFO | speech length: 24960
2026-01-28 13:41:19,724 | INFO | decoder input length: 38
2026-01-28 13:41:19,724 | INFO | max output length: 38
2026-01-28 13:41:19,724 | INFO | min output length: 3
2026-01-28 13:41:20,845 | INFO | end detected at 12
2026-01-28 13:41:20,847 | INFO |  -6.78 * 0.5 =  -3.39 for decoder
2026-01-28 13:41:20,847 | INFO | -12.18 * 0.5 =  -6.09 for ctc
2026-01-28 13:41:20,847 | INFO | total log probability: -9.48
2026-01-28 13:41:20,847 | INFO | normalized log probability: -1.90
2026-01-28 13:41:20,847 | INFO | total number of ended hypotheses: 187
2026-01-28 13:41:20,847 | INFO | best hypo: ▁c'est

2026-01-28 13:41:20,849 | INFO | speech length: 30880
2026-01-28 13:41:20,884 | INFO | decoder input length: 47
2026-01-28 13:41:20,884 | INFO | max output length: 47
2026-01-28 13:41:20,884 | INFO | min output length: 4
2026-01-28 13:41:22,948 | INFO | end detected at 25
2026-01-28 13:41:22,950 | INFO |  -1.49 * 0.5 =  -0.74 for decoder
2026-01-28 13:41:22,950 | INFO |  -7.53 * 0.5 =  -3.77 for ctc
2026-01-28 13:41:22,950 | INFO | total log probability: -4.51
2026-01-28 13:41:22,950 | INFO | normalized log probability: -0.30
2026-01-28 13:41:22,950 | INFO | total number of ended hypotheses: 220
2026-01-28 13:41:22,951 | INFO | best hypo: ▁alors▁ça▁varie▁beaucoup

2026-01-28 13:41:22,952 | INFO | speech length: 64000
2026-01-28 13:41:22,985 | INFO | decoder input length: 99
2026-01-28 13:41:22,985 | INFO | max output length: 99
2026-01-28 13:41:22,985 | INFO | min output length: 9
2026-01-28 13:41:25,480 | INFO | end detected at 27
2026-01-28 13:41:25,482 | INFO |  -5.12 * 0.5 =  -2.56 for decoder
2026-01-28 13:41:25,482 | INFO |  -4.17 * 0.5 =  -2.08 for ctc
2026-01-28 13:41:25,482 | INFO | total log probability: -4.64
2026-01-28 13:41:25,482 | INFO | normalized log probability: -0.26
2026-01-28 13:41:25,482 | INFO | total number of ended hypotheses: 217
2026-01-28 13:41:25,482 | INFO | best hypo: ▁il▁y▁a▁des▁gens▁de▁très▁bons▁amiens▁mais

2026-01-28 13:41:25,484 | INFO | speech length: 255200
2026-01-28 13:41:25,519 | INFO | decoder input length: 398
2026-01-28 13:41:25,519 | INFO | max output length: 398
2026-01-28 13:41:25,519 | INFO | min output length: 39
2026-01-28 13:41:41,404 | INFO | end detected at 134
2026-01-28 13:41:41,405 | INFO | -178.85 * 0.5 = -89.43 for decoder
2026-01-28 13:41:41,405 | INFO | -57.08 * 0.5 = -28.54 for ctc
2026-01-28 13:41:41,405 | INFO | total log probability: -117.97
2026-01-28 13:41:41,405 | INFO | normalized log probability: -0.92
2026-01-28 13:41:41,405 | INFO | total number of ended hypotheses: 166
2026-01-28 13:41:41,407 | INFO | best hypo: ▁par▁contre▁qu'on▁parle▁de▁ça▁ces▁passe▁mal▁qui▁veulent▁clairement▁une▁prémétine▁parce▁que▁çavait▁un▁métier▁stable▁un▁métier▁où▁gagneront▁de▁l'argent▁beauce▁et▁important▁et▁ils▁ont▁envie▁de▁faire▁la▁spécialité▁s▁qui▁rapportera▁le▁plus▁peu▁import▁un▁peu▁les▁aspect▁dont▁je▁vous▁ai▁parlé

2026-01-28 13:41:41,409 | INFO | speech length: 25760
2026-01-28 13:41:41,465 | INFO | decoder input length: 39
2026-01-28 13:41:41,465 | INFO | max output length: 39
2026-01-28 13:41:41,465 | INFO | min output length: 3
2026-01-28 13:41:43,440 | INFO | end detected at 21
2026-01-28 13:41:43,441 | INFO |  -1.48 * 0.5 =  -0.74 for decoder
2026-01-28 13:41:43,441 | INFO |  -2.56 * 0.5 =  -1.28 for ctc
2026-01-28 13:41:43,441 | INFO | total log probability: -2.02
2026-01-28 13:41:43,441 | INFO | normalized log probability: -0.13
2026-01-28 13:41:43,441 | INFO | total number of ended hypotheses: 166
2026-01-28 13:41:43,442 | INFO | best hypo: ▁il▁y▁en▁a▁d'autres▁au▁contraire

2026-01-28 13:41:43,444 | INFO | speech length: 35360
2026-01-28 13:41:43,474 | INFO | decoder input length: 54
2026-01-28 13:41:43,474 | INFO | max output length: 54
2026-01-28 13:41:43,474 | INFO | min output length: 5
2026-01-28 13:41:44,812 | INFO | end detected at 14
2026-01-28 13:41:44,813 | INFO |  -4.67 * 0.5 =  -2.33 for decoder
2026-01-28 13:41:44,813 | INFO |  -8.00 * 0.5 =  -4.00 for ctc
2026-01-28 13:41:44,814 | INFO | total log probability: -6.33
2026-01-28 13:41:44,814 | INFO | normalized log probability: -0.90
2026-01-28 13:41:44,814 | INFO | total number of ended hypotheses: 192
2026-01-28 13:41:44,814 | INFO | best hypo: ▁qui▁a▁qui▁on

2026-01-28 13:41:44,816 | INFO | speech length: 48320
2026-01-28 13:41:44,844 | INFO | decoder input length: 75
2026-01-28 13:41:44,844 | INFO | max output length: 75
2026-01-28 13:41:44,844 | INFO | min output length: 7
2026-01-28 13:41:47,286 | INFO | end detected at 25
2026-01-28 13:41:47,287 | INFO |  -2.38 * 0.5 =  -1.19 for decoder
2026-01-28 13:41:47,287 | INFO |  -1.18 * 0.5 =  -0.59 for ctc
2026-01-28 13:41:47,287 | INFO | total log probability: -1.78
2026-01-28 13:41:47,287 | INFO | normalized log probability: -0.08
2026-01-28 13:41:47,287 | INFO | total number of ended hypotheses: 156
2026-01-28 13:41:47,288 | INFO | best hypo: ▁qui▁recherchent▁vraiment▁ce▁contact▁encore▁plus

2026-01-28 13:41:47,289 | INFO | speech length: 11680
2026-01-28 13:41:47,319 | INFO | decoder input length: 17
2026-01-28 13:41:47,319 | INFO | max output length: 17
2026-01-28 13:41:47,319 | INFO | min output length: 1
2026-01-28 13:41:48,758 | INFO | adding <eos> in the last position in the loop
2026-01-28 13:41:48,769 | INFO | end detected at 16
2026-01-28 13:41:48,770 | INFO |  -5.47 * 0.5 =  -2.73 for decoder
2026-01-28 13:41:48,770 | INFO | -10.14 * 0.5 =  -5.07 for ctc
2026-01-28 13:41:48,770 | INFO | total log probability: -7.80
2026-01-28 13:41:48,770 | INFO | normalized log probability: -0.87
2026-01-28 13:41:48,770 | INFO | total number of ended hypotheses: 198
2026-01-28 13:41:48,770 | INFO | best hypo: ▁tu▁me▁fait▁aider

2026-01-28 13:41:48,772 | INFO | speech length: 53120
2026-01-28 13:41:48,807 | INFO | decoder input length: 82
2026-01-28 13:41:48,808 | INFO | max output length: 82
2026-01-28 13:41:48,808 | INFO | min output length: 8
2026-01-28 13:41:53,199 | INFO | end detected at 44
2026-01-28 13:41:53,201 | INFO | -10.30 * 0.5 =  -5.15 for decoder
2026-01-28 13:41:53,201 | INFO | -15.56 * 0.5 =  -7.78 for ctc
2026-01-28 13:41:53,201 | INFO | total log probability: -12.93
2026-01-28 13:41:53,201 | INFO | normalized log probability: -0.34
2026-01-28 13:41:53,201 | INFO | total number of ended hypotheses: 198
2026-01-28 13:41:53,201 | INFO | best hypo: ▁la▁psychiatrice▁au▁chose▁vraiment▁la▁psychiatrie▁fouait▁y▁fouettre

2026-01-28 13:41:53,203 | INFO | speech length: 118880
2026-01-28 13:41:53,239 | INFO | decoder input length: 185
2026-01-28 13:41:53,239 | INFO | max output length: 185
2026-01-28 13:41:53,239 | INFO | min output length: 18
2026-01-28 13:42:01,312 | INFO | end detected at 68
2026-01-28 13:42:01,313 | INFO | -18.17 * 0.5 =  -9.09 for decoder
2026-01-28 13:42:01,313 | INFO | -22.58 * 0.5 = -11.29 for ctc
2026-01-28 13:42:01,313 | INFO | total log probability: -20.37
2026-01-28 13:42:01,313 | INFO | normalized log probability: -0.33
2026-01-28 13:42:01,313 | INFO | total number of ended hypotheses: 174
2026-01-28 13:42:01,314 | INFO | best hypo: ▁c'est▁comme▁commondien▁il▁y▁a▁certains▁métiers▁où▁il▁faut▁être▁né▁ou▁est▁ce▁le▁dire▁une▁fois▁des▁psychiatres▁pour▁être▁psychiatre▁on▁peut▁pas

2026-01-28 13:42:01,316 | INFO | speech length: 176959
2026-01-28 13:42:01,354 | INFO | decoder input length: 275
2026-01-28 13:42:01,354 | INFO | max output length: 275
2026-01-28 13:42:01,354 | INFO | min output length: 27
2026-01-28 13:42:13,829 | INFO | end detected at 102
2026-01-28 13:42:13,830 | INFO | -53.16 * 0.5 = -26.58 for decoder
2026-01-28 13:42:13,831 | INFO | -14.80 * 0.5 =  -7.40 for ctc
2026-01-28 13:42:13,831 | INFO | total log probability: -33.98
2026-01-28 13:42:13,831 | INFO | normalized log probability: -0.36
2026-01-28 13:42:13,831 | INFO | total number of ended hypotheses: 180
2026-01-28 13:42:13,832 | INFO | best hypo: ▁il▁faut▁avoir▁un▁don▁spécial▁parce▁que▁la▁psychiatrice▁et▁sait▁quelque▁chose▁je▁pense▁que▁la▁chirurgie▁est▁pareille▁n'importe▁qui▁pourrait▁pas▁être▁chirurgien▁mais▁non▁les▁spécialités▁oayées▁à▁des▁spécialités▁qui

2026-01-28 13:42:13,834 | INFO | speech length: 122721
2026-01-28 13:42:13,872 | INFO | decoder input length: 191
2026-01-28 13:42:13,872 | INFO | max output length: 191
2026-01-28 13:42:13,872 | INFO | min output length: 19
2026-01-28 13:42:19,551 | INFO | end detected at 55
2026-01-28 13:42:19,553 | INFO |  -8.74 * 0.5 =  -4.37 for decoder
2026-01-28 13:42:19,553 | INFO | -15.73 * 0.5 =  -7.86 for ctc
2026-01-28 13:42:19,553 | INFO | total log probability: -12.24
2026-01-28 13:42:19,553 | INFO | normalized log probability: -0.25
2026-01-28 13:42:19,554 | INFO | total number of ended hypotheses: 180
2026-01-28 13:42:19,554 | INFO | best hypo: ▁pariguay▁une▁spécialité▁qui▁m'attire▁beaucoup▁et▁c'est▁vrai▁que▁elle▁n'est▁pas▁compatible▁avec▁eux

2026-01-28 13:42:19,556 | INFO | speech length: 20480
2026-01-28 13:42:19,591 | INFO | decoder input length: 31
2026-01-28 13:42:19,591 | INFO | max output length: 31
2026-01-28 13:42:19,591 | INFO | min output length: 3
2026-01-28 13:42:21,330 | INFO | end detected at 19
2026-01-28 13:42:21,331 | INFO |  -1.37 * 0.5 =  -0.69 for decoder
2026-01-28 13:42:21,331 | INFO |  -0.68 * 0.5 =  -0.34 for ctc
2026-01-28 13:42:21,331 | INFO | total log probability: -1.03
2026-01-28 13:42:21,331 | INFO | normalized log probability: -0.07
2026-01-28 13:42:21,331 | INFO | total number of ended hypotheses: 143
2026-01-28 13:42:21,331 | INFO | best hypo: ▁il▁a▁une▁vie▁de▁famille▁par▁exemple

2026-01-28 13:42:21,333 | INFO | speech length: 27360
2026-01-28 13:42:21,363 | INFO | decoder input length: 42
2026-01-28 13:42:21,364 | INFO | max output length: 42
2026-01-28 13:42:21,364 | INFO | min output length: 4
2026-01-28 13:42:22,774 | INFO | end detected at 15
2026-01-28 13:42:22,776 | INFO |  -5.07 * 0.5 =  -2.53 for decoder
2026-01-28 13:42:22,776 | INFO |  -2.85 * 0.5 =  -1.42 for ctc
2026-01-28 13:42:22,776 | INFO | total log probability: -3.96
2026-01-28 13:42:22,776 | INFO | normalized log probability: -0.66
2026-01-28 13:42:22,776 | INFO | total number of ended hypotheses: 185
2026-01-28 13:42:22,776 | INFO | best hypo: ▁réanimation

2026-01-28 13:42:22,778 | INFO | speech length: 34880
2026-01-28 13:42:22,811 | INFO | decoder input length: 54
2026-01-28 13:42:22,811 | INFO | max output length: 54
2026-01-28 13:42:22,811 | INFO | min output length: 5
2026-01-28 13:42:26,046 | INFO | end detected at 35
2026-01-28 13:42:26,047 | INFO |  -5.20 * 0.5 =  -2.60 for decoder
2026-01-28 13:42:26,047 | INFO |  -9.61 * 0.5 =  -4.80 for ctc
2026-01-28 13:42:26,047 | INFO | total log probability: -7.40
2026-01-28 13:42:26,047 | INFO | normalized log probability: -0.26
2026-01-28 13:42:26,047 | INFO | total number of ended hypotheses: 190
2026-01-28 13:42:26,048 | INFO | best hypo: ▁n'importe▁quand▁n'importe▁quelle▁heure▁est▁ce▁qui▁est▁moins▁payé

2026-01-28 13:42:26,050 | INFO | speech length: 83520
2026-01-28 13:42:26,084 | INFO | decoder input length: 130
2026-01-28 13:42:26,084 | INFO | max output length: 130
2026-01-28 13:42:26,084 | INFO | min output length: 13
2026-01-28 13:42:31,808 | INFO | end detected at 61
2026-01-28 13:42:31,809 | INFO | -18.88 * 0.5 =  -9.44 for decoder
2026-01-28 13:42:31,809 | INFO | -43.76 * 0.5 = -21.88 for ctc
2026-01-28 13:42:31,809 | INFO | total log probability: -31.32
2026-01-28 13:42:31,809 | INFO | normalized log probability: -0.58
2026-01-28 13:42:31,809 | INFO | total number of ended hypotheses: 183
2026-01-28 13:42:31,810 | INFO | best hypo: ▁avez▁passément▁en▁volumerai▁en▁fait▁et▁énormément▁nous▁fournira▁l'hôpital▁on▁sera▁pas▁payé▁en▁plus▁ou▁explosia

2026-01-28 13:42:31,812 | INFO | speech length: 30240
2026-01-28 13:42:31,847 | INFO | decoder input length: 46
2026-01-28 13:42:31,848 | INFO | max output length: 46
2026-01-28 13:42:31,848 | INFO | min output length: 4
2026-01-28 13:42:33,057 | INFO | end detected at 14
2026-01-28 13:42:33,059 | INFO |  -3.30 * 0.5 =  -1.65 for decoder
2026-01-28 13:42:33,059 | INFO |  -4.72 * 0.5 =  -2.36 for ctc
2026-01-28 13:42:33,059 | INFO | total log probability: -4.01
2026-01-28 13:42:33,059 | INFO | normalized log probability: -0.57
2026-01-28 13:42:33,059 | INFO | total number of ended hypotheses: 192
2026-01-28 13:42:33,059 | INFO | best hypo: ▁voilà▁heu

2026-01-28 13:42:33,061 | INFO | speech length: 58080
2026-01-28 13:42:33,094 | INFO | decoder input length: 90
2026-01-28 13:42:33,094 | INFO | max output length: 90
2026-01-28 13:42:33,094 | INFO | min output length: 9
2026-01-28 13:42:37,363 | INFO | end detected at 40
2026-01-28 13:42:37,365 | INFO |  -5.39 * 0.5 =  -2.69 for decoder
2026-01-28 13:42:37,365 | INFO | -11.88 * 0.5 =  -5.94 for ctc
2026-01-28 13:42:37,365 | INFO | total log probability: -8.63
2026-01-28 13:42:37,365 | INFO | normalized log probability: -0.32
2026-01-28 13:42:37,365 | INFO | total number of ended hypotheses: 172
2026-01-28 13:42:37,366 | INFO | best hypo: ▁et▁sinon▁les▁spécialités▁les▁un▁peu▁moins▁ce▁qui▁vous▁intéresse

2026-01-28 13:42:37,368 | INFO | speech length: 89280
2026-01-28 13:42:37,404 | INFO | decoder input length: 139
2026-01-28 13:42:37,404 | INFO | max output length: 139
2026-01-28 13:42:37,405 | INFO | min output length: 13
2026-01-28 13:42:43,883 | INFO | end detected at 60
2026-01-28 13:42:43,885 | INFO | -13.95 * 0.5 =  -6.98 for decoder
2026-01-28 13:42:43,885 | INFO | -15.04 * 0.5 =  -7.52 for ctc
2026-01-28 13:42:43,885 | INFO | total log probability: -14.50
2026-01-28 13:42:43,885 | INFO | normalized log probability: -0.27
2026-01-28 13:42:43,885 | INFO | total number of ended hypotheses: 182
2026-01-28 13:42:43,886 | INFO | best hypo: ▁il▁peut▁moins▁prise▁massager▁les▁spécialités▁à▁risques▁la▁gynécobstétrique▁par▁exemple▁la▁cancérologie

2026-01-28 13:42:43,888 | INFO | speech length: 76480
2026-01-28 13:42:43,927 | INFO | decoder input length: 119
2026-01-28 13:42:43,927 | INFO | max output length: 119
2026-01-28 13:42:43,927 | INFO | min output length: 11
2026-01-28 13:42:49,807 | INFO | end detected at 53
2026-01-28 13:42:49,808 | INFO |  -4.61 * 0.5 =  -2.31 for decoder
2026-01-28 13:42:49,808 | INFO | -13.08 * 0.5 =  -6.54 for ctc
2026-01-28 13:42:49,809 | INFO | total log probability: -8.85
2026-01-28 13:42:49,809 | INFO | normalized log probability: -0.20
2026-01-28 13:42:49,809 | INFO | total number of ended hypotheses: 175
2026-01-28 13:42:49,809 | INFO | best hypo: ▁ces▁spécialités▁sont▁moins▁en▁moins▁prises▁parce▁que▁dangereuses▁faut▁voir▁un▁bon▁avocat

2026-01-28 13:42:49,811 | INFO | speech length: 21920
2026-01-28 13:42:49,846 | INFO | decoder input length: 33
2026-01-28 13:42:49,847 | INFO | max output length: 33
2026-01-28 13:42:49,847 | INFO | min output length: 3
2026-01-28 13:42:51,536 | INFO | end detected at 16
2026-01-28 13:42:51,537 | INFO |  -0.92 * 0.5 =  -0.46 for decoder
2026-01-28 13:42:51,537 | INFO |  -0.36 * 0.5 =  -0.18 for ctc
2026-01-28 13:42:51,537 | INFO | total log probability: -0.64
2026-01-28 13:42:51,537 | INFO | normalized log probability: -0.06
2026-01-28 13:42:51,537 | INFO | total number of ended hypotheses: 161
2026-01-28 13:42:51,537 | INFO | best hypo: ▁pas▁de▁plus▁en▁plus▁important

2026-01-28 13:42:51,539 | INFO | speech length: 108640
2026-01-28 13:42:51,574 | INFO | decoder input length: 169
2026-01-28 13:42:51,574 | INFO | max output length: 169
2026-01-28 13:42:51,574 | INFO | min output length: 16
2026-01-28 13:42:56,572 | INFO | end detected at 44
2026-01-28 13:42:56,574 | INFO |  -5.90 * 0.5 =  -2.95 for decoder
2026-01-28 13:42:56,575 | INFO |  -4.48 * 0.5 =  -2.24 for ctc
2026-01-28 13:42:56,575 | INFO | total log probability: -5.19
2026-01-28 13:42:56,575 | INFO | normalized log probability: -0.14
2026-01-28 13:42:56,575 | INFO | total number of ended hypotheses: 200
2026-01-28 13:42:56,575 | INFO | best hypo: ▁parce▁que▁c'est▁pour▁moi▁c'est▁très▁important▁dans▁ces▁idéaux▁mais▁faut▁aussi▁être▁réaliste

2026-01-28 13:42:56,577 | INFO | speech length: 65920
2026-01-28 13:42:56,616 | INFO | decoder input length: 102
2026-01-28 13:42:56,616 | INFO | max output length: 102
2026-01-28 13:42:56,616 | INFO | min output length: 10
2026-01-28 13:42:59,463 | INFO | end detected at 26
2026-01-28 13:42:59,465 | INFO | -10.00 * 0.5 =  -5.00 for decoder
2026-01-28 13:42:59,465 | INFO |  -9.97 * 0.5 =  -4.98 for ctc
2026-01-28 13:42:59,465 | INFO | total log probability: -9.99
2026-01-28 13:42:59,465 | INFO | normalized log probability: -0.62
2026-01-28 13:42:59,465 | INFO | total number of ended hypotheses: 174
2026-01-28 13:42:59,465 | INFO | best hypo: ▁alors▁questions▁totalement▁du▁pas▁bon

2026-01-28 13:42:59,467 | INFO | speech length: 115520
2026-01-28 13:42:59,501 | INFO | decoder input length: 180
2026-01-28 13:42:59,501 | INFO | max output length: 180
2026-01-28 13:42:59,501 | INFO | min output length: 18
2026-01-28 13:43:08,342 | INFO | end detected at 77
2026-01-28 13:43:08,344 | INFO | -13.88 * 0.5 =  -6.94 for decoder
2026-01-28 13:43:08,344 | INFO | -14.96 * 0.5 =  -7.48 for ctc
2026-01-28 13:43:08,344 | INFO | total log probability: -14.42
2026-01-28 13:43:08,344 | INFO | normalized log probability: -0.21
2026-01-28 13:43:08,344 | INFO | total number of ended hypotheses: 208
2026-01-28 13:43:08,345 | INFO | best hypo: ▁▁sur▁l'équipement▁du▁quartier▁j'ai▁compris▁que▁ça▁allait▁que▁vous▁étiez▁content▁que▁il▁y▁avait▁suffisamment▁de▁d'espace▁vert▁de▁l'équipement

2026-01-28 13:43:08,347 | INFO | speech length: 18560
2026-01-28 13:43:08,386 | INFO | decoder input length: 28
2026-01-28 13:43:08,386 | INFO | max output length: 28
2026-01-28 13:43:08,386 | INFO | min output length: 2
2026-01-28 13:43:09,643 | INFO | end detected at 13
2026-01-28 13:43:09,644 | INFO |  -1.03 * 0.5 =  -0.51 for decoder
2026-01-28 13:43:09,644 | INFO |  -0.30 * 0.5 =  -0.15 for ctc
2026-01-28 13:43:09,644 | INFO | total log probability: -0.66
2026-01-28 13:43:09,644 | INFO | normalized log probability: -0.08
2026-01-28 13:43:09,644 | INFO | total number of ended hypotheses: 160
2026-01-28 13:43:09,644 | INFO | best hypo: ▁les▁animaux▁dans▁les

2026-01-28 13:43:09,646 | INFO | speech length: 14720
2026-01-28 13:43:09,674 | INFO | decoder input length: 22
2026-01-28 13:43:09,674 | INFO | max output length: 22
2026-01-28 13:43:09,674 | INFO | min output length: 2
2026-01-28 13:43:10,019 | INFO | end detected at 9
2026-01-28 13:43:10,020 | INFO |  -8.61 * 0.5 =  -4.31 for decoder
2026-01-28 13:43:10,020 | INFO |  -2.24 * 0.5 =  -1.12 for ctc
2026-01-28 13:43:10,020 | INFO | total log probability: -5.43
2026-01-28 13:43:10,020 | INFO | normalized log probability: -1.36
2026-01-28 13:43:10,020 | INFO | total number of ended hypotheses: 160
2026-01-28 13:43:10,020 | INFO | best hypo: ▁unme

2026-01-28 13:43:10,022 | INFO | speech length: 221920
2026-01-28 13:43:10,054 | INFO | decoder input length: 346
2026-01-28 13:43:10,054 | INFO | max output length: 346
2026-01-28 13:43:10,054 | INFO | min output length: 34
2026-01-28 13:43:19,202 | INFO | end detected at 89
2026-01-28 13:43:19,203 | INFO | -72.30 * 0.5 = -36.15 for decoder
2026-01-28 13:43:19,204 | INFO | -39.02 * 0.5 = -19.51 for ctc
2026-01-28 13:43:19,204 | INFO | total log probability: -55.66
2026-01-28 13:43:19,204 | INFO | normalized log probability: -0.70
2026-01-28 13:43:19,204 | INFO | total number of ended hypotheses: 179
2026-01-28 13:43:19,205 | INFO | best hypo: ▁alors▁un▁changement▁les▁crottes▁de▁chien▁seraient▁que▁depuis▁je▁sais▁pas▁si▁une▁loi▁qui▁a▁été▁instaurée▁mais▁de▁plus▁en▁plus▁voit▁des▁gens▁qui▁sont▁respectueux▁et▁qui▁ramass▁prennent▁leurs▁chiens

2026-01-28 13:43:19,207 | INFO | speech length: 137600
2026-01-28 13:43:19,244 | INFO | decoder input length: 214
2026-01-28 13:43:19,244 | INFO | max output length: 214
2026-01-28 13:43:19,244 | INFO | min output length: 21
2026-01-28 13:43:28,988 | INFO | end detected at 93
2026-01-28 13:43:28,989 | INFO | -54.04 * 0.5 = -27.02 for decoder
2026-01-28 13:43:28,989 | INFO | -45.75 * 0.5 = -22.88 for ctc
2026-01-28 13:43:28,989 | INFO | total log probability: -49.89
2026-01-28 13:43:28,989 | INFO | normalized log probability: -0.59
2026-01-28 13:43:28,989 | INFO | total number of ended hypotheses: 183
2026-01-28 13:43:28,990 | INFO | best hypo: ▁ils▁font▁faire▁leurs▁besoins▁n'en▁dans▁le▁trottoir▁pas▁sur▁le▁trottoir▁justement▁et▁moins▁personne▁j'aiseaucoup▁les▁animaux▁j'ai▁un▁contre▁chaussa▁sans▁pas▁d'en▁voir▁dans▁paris

2026-01-28 13:43:29,001 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 13:43:29,001 | INFO | Chunk: 1 | WER=100.000000 | S=1 D=1 I=0
2026-01-28 13:43:29,001 | INFO | Chunk: 2 | WER=100.000000 | S=1 D=0 I=0
2026-01-28 13:43:29,001 | INFO | Chunk: 3 | WER=133.333333 | S=2 D=0 I=2
2026-01-28 13:43:29,007 | INFO | Chunk: 4 | WER=34.862385 | S=21 D=12 I=5
2026-01-28 13:43:29,008 | INFO | Chunk: 5 | WER=20.000000 | S=5 D=4 I=1
2026-01-28 13:43:29,008 | INFO | Chunk: 6 | WER=66.666667 | S=4 D=2 I=0
2026-01-28 13:43:29,009 | INFO | Chunk: 7 | WER=20.000000 | S=1 D=1 I=1
2026-01-28 13:43:29,010 | INFO | Chunk: 8 | WER=51.515152 | S=11 D=6 I=0
2026-01-28 13:43:29,010 | INFO | Chunk: 9 | WER=63.636364 | S=2 D=5 I=0
2026-01-28 13:43:29,011 | INFO | Chunk: 10 | WER=43.333333 | S=5 D=8 I=0
2026-01-28 13:43:29,011 | INFO | Chunk: 11 | WER=100.000000 | S=1 D=2 I=0
2026-01-28 13:43:29,011 | INFO | Chunk: 12 | WER=0.000000 | S=1 D=0 I=0
2026-01-28 13:43:29,011 | INFO | Chunk: 13 | WER=39.130435 | S=4 D=5 I=0
2026-01-28 13:43:29,011 | INFO | Chunk: 14 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 13:43:29,012 | INFO | Chunk: 15 | WER=66.666667 | S=2 D=2 I=0
2026-01-28 13:43:29,012 | INFO | Chunk: 16 | WER=46.666667 | S=4 D=3 I=0
2026-01-28 13:43:29,017 | INFO | Chunk: 17 | WER=42.307692 | S=24 D=17 I=3
2026-01-28 13:43:29,017 | INFO | Chunk: 18 | WER=63.636364 | S=1 D=6 I=0
2026-01-28 13:43:29,019 | INFO | Chunk: 19 | WER=33.333333 | S=11 D=7 I=2
2026-01-28 13:43:29,020 | INFO | Chunk: 20 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 13:43:29,020 | INFO | Chunk: 21 | WER=33.333333 | S=2 D=1 I=0
2026-01-28 13:43:29,020 | INFO | Chunk: 22 | WER=53.333333 | S=1 D=7 I=0
2026-01-28 13:43:29,020 | INFO | Chunk: 23 | WER=100.000000 | S=2 D=4 I=0
2026-01-28 13:43:29,020 | INFO | Chunk: 24 | WER=42.857143 | S=0 D=3 I=0
2026-01-28 13:43:29,021 | INFO | Chunk: 25 | WER=30.769231 | S=1 D=3 I=0
2026-01-28 13:43:29,023 | INFO | Chunk: 26 | WER=29.032258 | S=9 D=8 I=1
2026-01-28 13:43:29,023 | INFO | Chunk: 27 | WER=11.111111 | S=0 D=1 I=0
2026-01-28 13:43:29,023 | INFO | Chunk: 28 | WER=66.666667 | S=1 D=0 I=1
2026-01-28 13:43:29,023 | INFO | Chunk: 29 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 13:43:29,024 | INFO | Chunk: 30 | WER=100.000000 | S=2 D=0 I=1
2026-01-28 13:43:29,024 | INFO | Chunk: 31 | WER=72.222222 | S=5 D=8 I=0
2026-01-28 13:43:29,025 | INFO | Chunk: 32 | WER=41.666667 | S=8 D=7 I=0
2026-01-28 13:43:29,026 | INFO | Chunk: 33 | WER=26.190476 | S=6 D=5 I=0
2026-01-28 13:43:29,027 | INFO | Chunk: 34 | WER=44.000000 | S=1 D=8 I=2
2026-01-28 13:43:29,027 | INFO | Chunk: 35 | WER=28.571429 | S=1 D=0 I=1
2026-01-28 13:43:29,027 | INFO | Chunk: 36 | WER=85.714286 | S=0 D=6 I=0
2026-01-28 13:43:29,027 | INFO | Chunk: 37 | WER=50.000000 | S=3 D=4 I=1
2026-01-28 13:43:29,028 | INFO | Chunk: 38 | WER=66.666667 | S=8 D=16 I=0
2026-01-28 13:43:29,028 | INFO | Chunk: 39 | WER=100.000000 | S=0 D=1 I=1
2026-01-28 13:43:29,029 | INFO | Chunk: 40 | WER=42.105263 | S=1 D=7 I=0
2026-01-28 13:43:29,029 | INFO | Chunk: 41 | WER=50.000000 | S=5 D=5 I=0
2026-01-28 13:43:29,030 | INFO | Chunk: 42 | WER=35.000000 | S=2 D=5 I=0
2026-01-28 13:43:29,030 | INFO | Chunk: 43 | WER=42.857143 | S=0 D=2 I=1
2026-01-28 13:43:29,030 | INFO | Chunk: 44 | WER=23.809524 | S=2 D=3 I=0
2026-01-28 13:43:29,031 | INFO | Chunk: 45 | WER=80.000000 | S=2 D=5 I=1
2026-01-28 13:43:29,031 | INFO | Chunk: 46 | WER=17.857143 | S=4 D=1 I=0
2026-01-28 13:43:29,032 | INFO | Chunk: 47 | WER=33.333333 | S=0 D=0 I=1
2026-01-28 13:43:29,032 | INFO | Chunk: 48 | WER=0.000000 | S=1 D=0 I=0
2026-01-28 13:43:29,033 | INFO | Chunk: 49 | WER=36.000000 | S=5 D=13 I=0
2026-01-28 13:43:29,034 | INFO | Chunk: 50 | WER=47.500000 | S=11 D=7 I=1
2026-01-28 13:43:29,469 | INFO | File: Rhap-D0006.wav | WER=40.096618 | S=188 D=205 I=22
2026-01-28 13:43:29,469 | INFO | ------------------------------
2026-01-28 13:43:29,469 | INFO | Conf cv Done!
2026-01-28 13:43:29,640 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 13:43:29,661 | INFO | Vocabulary size: 47
2026-01-28 13:43:30,189 | INFO | Gradient checkpoint layers: []
2026-01-28 13:43:31,098 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 13:43:31,102 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 13:43:31,103 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 13:43:31,103 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 13:43:31,106 | INFO | speech length: 18720
2026-01-28 13:43:31,144 | INFO | decoder input length: 28
2026-01-28 13:43:31,144 | INFO | max output length: 28
2026-01-28 13:43:31,144 | INFO | min output length: 2
2026-01-28 13:43:33,529 | INFO | adding <eos> in the last position in the loop
2026-01-28 13:43:33,539 | INFO | no hypothesis. Finish decoding.
2026-01-28 13:43:33,540 | INFO |  -2.04 * 0.5 =  -1.02 for decoder
2026-01-28 13:43:33,540 | INFO |  -1.73 * 0.5 =  -0.86 for ctc
2026-01-28 13:43:33,540 | INFO | total log probability: -1.88
2026-01-28 13:43:33,540 | INFO | normalized log probability: -0.07
2026-01-28 13:43:33,540 | INFO | total number of ended hypotheses: 123
2026-01-28 13:43:33,541 | INFO | best hypo: honnêtement<space>pas<space>vraiment

2026-01-28 13:43:33,542 | INFO | speech length: 8160
2026-01-28 13:43:33,566 | INFO | decoder input length: 12
2026-01-28 13:43:33,566 | INFO | max output length: 12
2026-01-28 13:43:33,566 | INFO | min output length: 1
2026-01-28 13:43:34,477 | INFO | end detected at 10
2026-01-28 13:43:34,478 | INFO |  -0.40 * 0.5 =  -0.20 for decoder
2026-01-28 13:43:34,478 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-28 13:43:34,478 | INFO | total log probability: -0.21
2026-01-28 13:43:34,478 | INFO | normalized log probability: -0.03
2026-01-28 13:43:34,478 | INFO | total number of ended hypotheses: 132
2026-01-28 13:43:34,478 | INFO | best hypo: j'ai

2026-01-28 13:43:34,480 | INFO | speech length: 10560
2026-01-28 13:43:34,513 | INFO | decoder input length: 16
2026-01-28 13:43:34,513 | INFO | max output length: 16
2026-01-28 13:43:34,513 | INFO | min output length: 1
2026-01-28 13:43:35,427 | INFO | end detected at 10
2026-01-28 13:43:35,429 | INFO |  -2.79 * 0.5 =  -1.40 for decoder
2026-01-28 13:43:35,429 | INFO |  -1.80 * 0.5 =  -0.90 for ctc
2026-01-28 13:43:35,429 | INFO | total log probability: -2.30
2026-01-28 13:43:35,429 | INFO | normalized log probability: -0.46
2026-01-28 13:43:35,429 | INFO | total number of ended hypotheses: 172
2026-01-28 13:43:35,429 | INFO | best hypo: peu

2026-01-28 13:43:35,431 | INFO | speech length: 14080
2026-01-28 13:43:35,463 | INFO | decoder input length: 21
2026-01-28 13:43:35,463 | INFO | max output length: 21
2026-01-28 13:43:35,463 | INFO | min output length: 2
2026-01-28 13:43:37,195 | INFO | adding <eos> in the last position in the loop
2026-01-28 13:43:37,203 | INFO | no hypothesis. Finish decoding.
2026-01-28 13:43:37,204 | INFO |  -4.67 * 0.5 =  -2.34 for decoder
2026-01-28 13:43:37,204 | INFO |  -7.74 * 0.5 =  -3.87 for ctc
2026-01-28 13:43:37,204 | INFO | total log probability: -6.21
2026-01-28 13:43:37,204 | INFO | normalized log probability: -0.30
2026-01-28 13:43:37,204 | INFO | total number of ended hypotheses: 92
2026-01-28 13:43:37,204 | INFO | best hypo: mais<space>on<space>n'avait<space>pas

2026-01-28 13:43:37,205 | INFO | speech length: 440800
2026-01-28 13:43:37,235 | INFO | decoder input length: 688
2026-01-28 13:43:37,235 | INFO | max output length: 688
2026-01-28 13:43:37,235 | INFO | min output length: 68
2026-01-28 13:44:51,084 | INFO | end detected at 515
2026-01-28 13:44:51,086 | INFO | -443.99 * 0.5 = -222.00 for decoder
2026-01-28 13:44:51,086 | INFO | -34.29 * 0.5 = -17.15 for ctc
2026-01-28 13:44:51,086 | INFO | total log probability: -239.14
2026-01-28 13:44:51,086 | INFO | normalized log probability: -0.47
2026-01-28 13:44:51,086 | INFO | total number of ended hypotheses: 182
2026-01-28 13:44:51,092 | INFO | best hypo: non<space>c'est<space>plus<space>au<space>niveau<space>de<space>la<space>fac<space>je<space>vois<space>moi<space>la<space>fac<space>ça<space>m'a<space>fait<space>beaucoup<space>de<space>bien<space>ça<space>m'a<space>sorti<space>un<space>petit<space>peu<space>de<space>mon<space>petit<space>cocon<space>du<space>septième<space>et<space>ça<space>m'a<space>fait<space>voir<space>même<space>si<space>c'est<space>quand<space>même<space>une<space>faille<space>dans<space>le<space>septième<space>enfin<space>s<space>y<space>a<space>des<space>gens<space>qui<space>viennent<space>de<space>partout<space>euh<space>et<space>euh<space>ça<space>m'a<space>frappé<space>de<space>voir<space>que<space>euh<space>j'avais<space>une<space>amie<space>euh<space>qui<space>vivait<space>à<space>vingt<space>ans<space>complètement<space>autonome<space>parce<space>que<space>ses<space>parents<space>sont<space>divorcés<space>e<space>n'ont<space>pas<space>les<space>moyens<space>de<space>faire<space>vivre<space>euh<space>des<space>d'autrs<space>gens<space>qui<space>venaient<space>de<space>qui<space>venaient<space>euh<space>de<space>l'étranger

2026-01-28 13:44:51,094 | INFO | speech length: 281600
2026-01-28 13:44:51,124 | INFO | decoder input length: 439
2026-01-28 13:44:51,124 | INFO | max output length: 439
2026-01-28 13:44:51,124 | INFO | min output length: 43
2026-01-28 13:45:23,842 | INFO | end detected at 294
2026-01-28 13:45:23,843 | INFO | -39.29 * 0.5 = -19.65 for decoder
2026-01-28 13:45:23,843 | INFO |  -1.25 * 0.5 =  -0.62 for ctc
2026-01-28 13:45:23,843 | INFO | total log probability: -20.27
2026-01-28 13:45:23,844 | INFO | normalized log probability: -0.07
2026-01-28 13:45:23,844 | INFO | total number of ended hypotheses: 168
2026-01-28 13:45:23,847 | INFO | best hypo: et<space>euh<space>qui<space>pour<space>pouvoir<space>se<space>payer<space>euh<space>la<space>petite<space>prépa<space>en<space>plus<space>pour<space>réussir<space>le<space>concours<space>euh<space>devait<space>travailler<space>le<space>week<space>end<space>euh<space>chez<space>monoprix<space>ça<space>c'est<space>des<space>petites<space>choses<space>euh<space>qui<space>ont<space>beaucoup<space>joué<space>et<space>euh<space>qui<space>m'ont<space>fait<space>euh<space>réaliser<space>euh<space>vraiment<space>ce<space>que<space>c'était<space>que<space>l'argent<space>et<space>euh<space>je<space>pense<space>que

2026-01-28 13:45:23,849 | INFO | speech length: 51040
2026-01-28 13:45:23,877 | INFO | decoder input length: 79
2026-01-28 13:45:23,877 | INFO | max output length: 79
2026-01-28 13:45:23,877 | INFO | min output length: 7
2026-01-28 13:45:28,885 | INFO | end detected at 57
2026-01-28 13:45:28,887 | INFO |  -4.31 * 0.5 =  -2.16 for decoder
2026-01-28 13:45:28,887 | INFO | -12.41 * 0.5 =  -6.21 for ctc
2026-01-28 13:45:28,887 | INFO | total log probability: -8.36
2026-01-28 13:45:28,887 | INFO | normalized log probability: -0.17
2026-01-28 13:45:28,887 | INFO | total number of ended hypotheses: 201
2026-01-28 13:45:28,888 | INFO | best hypo: ça<space>a<space>été<space>très<space>bénéficitaire<space>euh<space>c'est<space>l'argent

2026-01-28 13:45:28,890 | INFO | speech length: 77760
2026-01-28 13:45:28,919 | INFO | decoder input length: 121
2026-01-28 13:45:28,919 | INFO | max output length: 121
2026-01-28 13:45:28,919 | INFO | min output length: 12
2026-01-28 13:45:37,847 | INFO | end detected at 94
2026-01-28 13:45:37,848 | INFO | -10.50 * 0.5 =  -5.25 for decoder
2026-01-28 13:45:37,848 | INFO |  -2.94 * 0.5 =  -1.47 for ctc
2026-01-28 13:45:37,848 | INFO | total log probability: -6.72
2026-01-28 13:45:37,848 | INFO | normalized log probability: -0.08
2026-01-28 13:45:37,848 | INFO | total number of ended hypotheses: 178
2026-01-28 13:45:37,849 | INFO | best hypo: ben<space>non<space>fin<space>vraiment<space>cet<space>arg<space>avec<space>cet<space>argent<space>là<space>je<space>me<space>paye<space>maintenant<space>quasiment<space>tout

2026-01-28 13:45:37,851 | INFO | speech length: 148480
2026-01-28 13:45:37,881 | INFO | decoder input length: 231
2026-01-28 13:45:37,881 | INFO | max output length: 231
2026-01-28 13:45:37,881 | INFO | min output length: 23
2026-01-28 13:45:55,521 | INFO | end detected at 174
2026-01-28 13:45:55,524 | INFO | -17.07 * 0.5 =  -8.53 for decoder
2026-01-28 13:45:55,524 | INFO | -15.12 * 0.5 =  -7.56 for ctc
2026-01-28 13:45:55,524 | INFO | total log probability: -16.09
2026-01-28 13:45:55,524 | INFO | normalized log probability: -0.10
2026-01-28 13:45:55,524 | INFO | total number of ended hypotheses: 242
2026-01-28 13:45:55,526 | INFO | best hypo: à<space>part<space>la<space>nourriture<space>et<space>le<space>logement<space>voilà<space>mais<space>euh<space>ce<space>soit<space>les<space>habits<space>euh<space>les<space>vacances<space>euh<space>euh<space>la<space>jeune<space>fermée<space>des<space>vaccins<space>par<space>exemple<space>c'est<space>moi<space>qui<space>s'est<space>payé

2026-01-28 13:45:55,529 | INFO | speech length: 36800
2026-01-28 13:45:55,557 | INFO | decoder input length: 57
2026-01-28 13:45:55,558 | INFO | max output length: 57
2026-01-28 13:45:55,558 | INFO | min output length: 5
2026-01-28 13:45:59,292 | INFO | end detected at 46
2026-01-28 13:45:59,293 | INFO |  -3.11 * 0.5 =  -1.56 for decoder
2026-01-28 13:45:59,293 | INFO |  -0.10 * 0.5 =  -0.05 for ctc
2026-01-28 13:45:59,293 | INFO | total log probability: -1.61
2026-01-28 13:45:59,293 | INFO | normalized log probability: -0.04
2026-01-28 13:45:59,293 | INFO | total number of ended hypotheses: 204
2026-01-28 13:45:59,294 | INFO | best hypo: non<space>c'est<space>les<space>parents<space>je<space>pense<space>et<space>euh

2026-01-28 13:45:59,296 | INFO | speech length: 95680
2026-01-28 13:45:59,324 | INFO | decoder input length: 149
2026-01-28 13:45:59,324 | INFO | max output length: 149
2026-01-28 13:45:59,324 | INFO | min output length: 14
2026-01-28 13:46:10,703 | INFO | end detected at 125
2026-01-28 13:46:10,704 | INFO | -13.42 * 0.5 =  -6.71 for decoder
2026-01-28 13:46:10,705 | INFO |  -5.90 * 0.5 =  -2.95 for ctc
2026-01-28 13:46:10,705 | INFO | total log probability: -9.66
2026-01-28 13:46:10,705 | INFO | normalized log probability: -0.08
2026-01-28 13:46:10,705 | INFO | total number of ended hypotheses: 203
2026-01-28 13:46:10,706 | INFO | best hypo: et<space>f<space>le<space>le<space>fait<space>d'a<space>d'avoir<space>été<space>à<space>la<space>fac<space>euh<space>de<space>me<space>mettre<space>un<space>peu<space>sorti<space>de<space>d'un<space>petit<space>cocon<space>ça<space>m'a<space>renforcé<space>là<space>dedans

2026-01-28 13:46:10,708 | INFO | speech length: 9440
2026-01-28 13:46:10,735 | INFO | decoder input length: 14
2026-01-28 13:46:10,735 | INFO | max output length: 14
2026-01-28 13:46:10,735 | INFO | min output length: 1
2026-01-28 13:46:11,863 | INFO | end detected at 12
2026-01-28 13:46:11,864 | INFO |  -0.54 * 0.5 =  -0.27 for decoder
2026-01-28 13:46:11,864 | INFO |  -0.04 * 0.5 =  -0.02 for ctc
2026-01-28 13:46:11,864 | INFO | total log probability: -0.29
2026-01-28 13:46:11,864 | INFO | normalized log probability: -0.04
2026-01-28 13:46:11,864 | INFO | total number of ended hypotheses: 131
2026-01-28 13:46:11,864 | INFO | best hypo: ah<space>oui

2026-01-28 13:46:11,865 | INFO | speech length: 8640
2026-01-28 13:46:11,887 | INFO | decoder input length: 13
2026-01-28 13:46:11,887 | INFO | max output length: 13
2026-01-28 13:46:11,887 | INFO | min output length: 1
2026-01-28 13:46:12,611 | INFO | adding <eos> in the last position in the loop
2026-01-28 13:46:12,621 | INFO | end detected at 12
2026-01-28 13:46:12,622 | INFO |  -1.32 * 0.5 =  -0.66 for decoder
2026-01-28 13:46:12,623 | INFO |  -4.71 * 0.5 =  -2.36 for ctc
2026-01-28 13:46:12,623 | INFO | total log probability: -3.01
2026-01-28 13:46:12,623 | INFO | normalized log probability: -0.50
2026-01-28 13:46:12,623 | INFO | total number of ended hypotheses: 213
2026-01-28 13:46:12,623 | INFO | best hypo: bien

2026-01-28 13:46:12,625 | INFO | speech length: 85280
2026-01-28 13:46:12,653 | INFO | decoder input length: 132
2026-01-28 13:46:12,653 | INFO | max output length: 132
2026-01-28 13:46:12,653 | INFO | min output length: 13
2026-01-28 13:46:24,684 | INFO | end detected at 130
2026-01-28 13:46:24,685 | INFO | -10.92 * 0.5 =  -5.46 for decoder
2026-01-28 13:46:24,685 | INFO | -29.48 * 0.5 = -14.74 for ctc
2026-01-28 13:46:24,685 | INFO | total log probability: -20.20
2026-01-28 13:46:24,685 | INFO | normalized log probability: -0.17
2026-01-28 13:46:24,685 | INFO | total number of ended hypotheses: 191
2026-01-28 13:46:24,687 | INFO | best hypo: honnêtement<space>on<space>vit<space>vraiment<space>dans<space>un<space>petit<space>micro<space>fin<space>vraiment<space>dans<space>un<space>petit<space>microcosme<space>ça<space>fait<space>du<space>bien<space>de<space>sortir<space>un<space>peu

2026-01-28 13:46:24,689 | INFO | speech length: 28480
2026-01-28 13:46:24,716 | INFO | decoder input length: 44
2026-01-28 13:46:24,716 | INFO | max output length: 44
2026-01-28 13:46:24,716 | INFO | min output length: 4
2026-01-28 13:46:26,709 | INFO | end detected at 22
2026-01-28 13:46:26,710 | INFO |  -1.30 * 0.5 =  -0.65 for decoder
2026-01-28 13:46:26,710 | INFO |  -0.22 * 0.5 =  -0.11 for ctc
2026-01-28 13:46:26,710 | INFO | total log probability: -0.76
2026-01-28 13:46:26,710 | INFO | normalized log probability: -0.04
2026-01-28 13:46:26,710 | INFO | total number of ended hypotheses: 163
2026-01-28 13:46:26,711 | INFO | best hypo: type<space>de<space>travail

2026-01-28 13:46:26,712 | INFO | speech length: 31040
2026-01-28 13:46:26,744 | INFO | decoder input length: 48
2026-01-28 13:46:26,744 | INFO | max output length: 48
2026-01-28 13:46:26,744 | INFO | min output length: 4
2026-01-28 13:46:29,194 | INFO | end detected at 28
2026-01-28 13:46:29,195 | INFO |  -2.11 * 0.5 =  -1.05 for decoder
2026-01-28 13:46:29,195 | INFO |  -1.05 * 0.5 =  -0.52 for ctc
2026-01-28 13:46:29,195 | INFO | total log probability: -1.58
2026-01-28 13:46:29,195 | INFO | normalized log probability: -0.07
2026-01-28 13:46:29,195 | INFO | total number of ended hypotheses: 156
2026-01-28 13:46:29,196 | INFO | best hypo: médecin<space>en<space>france<space>euh

2026-01-28 13:46:29,198 | INFO | speech length: 48320
2026-01-28 13:46:29,234 | INFO | decoder input length: 75
2026-01-28 13:46:29,234 | INFO | max output length: 75
2026-01-28 13:46:29,234 | INFO | min output length: 7
2026-01-28 13:46:36,268 | INFO | adding <eos> in the last position in the loop
2026-01-28 13:46:36,277 | INFO | no hypothesis. Finish decoding.
2026-01-28 13:46:36,279 | INFO |  -5.84 * 0.5 =  -2.92 for decoder
2026-01-28 13:46:36,279 | INFO | -10.96 * 0.5 =  -5.48 for ctc
2026-01-28 13:46:36,279 | INFO | total log probability: -8.40
2026-01-28 13:46:36,279 | INFO | normalized log probability: -0.12
2026-01-28 13:46:36,279 | INFO | total number of ended hypotheses: 195
2026-01-28 13:46:36,280 | INFO | best hypo: alors<space>moi<space>déjà<space>je<space>veux<space>travailler<space>dans<space>le<space>public<space>hyper<space>dans<space>le<space>privé

2026-01-28 13:46:36,282 | INFO | speech length: 473120
2026-01-28 13:46:36,314 | INFO | decoder input length: 738
2026-01-28 13:46:36,314 | INFO | max output length: 738
2026-01-28 13:46:36,314 | INFO | min output length: 73
2026-01-28 13:47:50,698 | INFO | end detected at 562
2026-01-28 13:47:50,700 | INFO | -539.34 * 0.5 = -269.67 for decoder
2026-01-28 13:47:50,700 | INFO | -55.20 * 0.5 = -27.60 for ctc
2026-01-28 13:47:50,700 | INFO | total log probability: -297.27
2026-01-28 13:47:50,700 | INFO | normalized log probability: -0.54
2026-01-28 13:47:50,700 | INFO | total number of ended hypotheses: 178
2026-01-28 13:47:50,706 | INFO | best hypo: euh<space>je<space>pe<space>si<space>vous<space>voulez<space>savoir<space>pourquoi<space>euh<space>parce<space>que<space>euh<space>je<space>travaille<space>dans<space>le<space>pus<space>arriver<space>là<space>je<space>fais<space>des<space>petits<space>boulots<space>en<space>plus<space>et<space>euh<space>ça<space>n'en<space>regaine<space>pas<space>parce<space>que<space>c'est<space>vraiment<space>uniquement<space>tourné<space>vers<space>l'argent<space>y<space>a<space>cette<space>dimension<space>de<space>contact<space>avec<space>le<space>malade<space>qui<space>a<space>complètement<space>disparu<space>et<space>pour<space>le<space>coup<space>cette<space>dimension<space>euh<space>spécialité<space>de<space>la<space>médecine<space>qui<space>est<space>encore<space>plus<space>renforcée<space>parce<space>que<space>pour<space>gagner<space>encore<space>plus<space>il<space>paut<space>être<space>le<space>spécial<space>ste<space>du<space>spécialiste<space>du<space>spécialiste<space>donc<space>ça<space>c'est<space>un<space>mondroit<space>que<space>j'aime<space>pas<space>trop<space>euh<space>et<space>en<space>plus<space>euh<space>la<space>vraie<space>médecine

2026-01-28 13:47:50,708 | INFO | speech length: 35840
2026-01-28 13:47:50,736 | INFO | decoder input length: 55
2026-01-28 13:47:50,736 | INFO | max output length: 55
2026-01-28 13:47:50,736 | INFO | min output length: 5
2026-01-28 13:47:54,968 | INFO | end detected at 49
2026-01-28 13:47:54,969 | INFO |  -4.12 * 0.5 =  -2.06 for decoder
2026-01-28 13:47:54,969 | INFO |  -2.67 * 0.5 =  -1.33 for ctc
2026-01-28 13:47:54,969 | INFO | total log probability: -3.39
2026-01-28 13:47:54,969 | INFO | normalized log probability: -0.08
2026-01-28 13:47:54,969 | INFO | total number of ended hypotheses: 207
2026-01-28 13:47:54,970 | INFO | best hypo: euh<space>c'est<space>l'hôpital<space>qu'on<space>on<space>l'apprend

2026-01-28 13:47:54,972 | INFO | speech length: 232000
2026-01-28 13:47:55,002 | INFO | decoder input length: 362
2026-01-28 13:47:55,002 | INFO | max output length: 362
2026-01-28 13:47:55,002 | INFO | min output length: 36
2026-01-28 13:48:27,588 | INFO | end detected at 286
2026-01-28 13:48:27,590 | INFO | -40.06 * 0.5 = -20.03 for decoder
2026-01-28 13:48:27,591 | INFO | -25.48 * 0.5 = -12.74 for ctc
2026-01-28 13:48:27,591 | INFO | total log probability: -32.77
2026-01-28 13:48:27,591 | INFO | normalized log probability: -0.12
2026-01-28 13:48:27,591 | INFO | total number of ended hypotheses: 240
2026-01-28 13:48:27,594 | INFO | best hypo: ce<space>que<space>ce<space>que<space>j'admis<space>ce<space>que<space>j'appelle<space>la<space>vrai<space>médecine<space>c'est<space>c'est<space>les<space>les<space>cas<space>concrets<space>euh<space>quel<space>est<space>le<space>niveau<space>de<space>santé<space>de<space>la<space>population<space>euh<space>quels<space>sont<space>les<space>tel<space>tel<space>et<space>tel<space>cas<space>on<space>les<space>verrait<space>pas<space>on<space>ne<space>on<space>puisse<space>priver<space>les<space>classes<space>moyennes<space>et<space>les<space>ne<space>vont<space>pas<space>encline<space>privée

2026-01-28 13:48:27,596 | INFO | speech length: 26880
2026-01-28 13:48:27,625 | INFO | decoder input length: 41
2026-01-28 13:48:27,625 | INFO | max output length: 41
2026-01-28 13:48:27,625 | INFO | min output length: 4
2026-01-28 13:48:30,453 | INFO | end detected at 30
2026-01-28 13:48:30,454 | INFO |  -2.00 * 0.5 =  -1.00 for decoder
2026-01-28 13:48:30,454 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 13:48:30,454 | INFO | total log probability: -1.00
2026-01-28 13:48:30,454 | INFO | normalized log probability: -0.04
2026-01-28 13:48:30,454 | INFO | total number of ended hypotheses: 143
2026-01-28 13:48:30,454 | INFO | best hypo: euh<space>sinon<space>j'aimerais<space>euh

2026-01-28 13:48:30,456 | INFO | speech length: 25120
2026-01-28 13:48:30,486 | INFO | decoder input length: 38
2026-01-28 13:48:30,486 | INFO | max output length: 38
2026-01-28 13:48:30,486 | INFO | min output length: 3
2026-01-28 13:48:34,029 | INFO | adding <eos> in the last position in the loop
2026-01-28 13:48:34,035 | INFO | no hypothesis. Finish decoding.
2026-01-28 13:48:34,036 | INFO | -18.72 * 0.5 =  -9.36 for decoder
2026-01-28 13:48:34,036 | INFO | -41.68 * 0.5 = -20.84 for ctc
2026-01-28 13:48:34,036 | INFO | total log probability: -30.20
2026-01-28 13:48:34,036 | INFO | normalized log probability: -0.84
2026-01-28 13:48:34,036 | INFO | total number of ended hypotheses: 84
2026-01-28 13:48:34,036 | INFO | best hypo: comme<space>si<space>je<space>sais<space>je<space>peux<space>pas<space>venir

2026-01-28 13:48:34,037 | INFO | speech length: 35200
2026-01-28 13:48:34,073 | INFO | decoder input length: 54
2026-01-28 13:48:34,073 | INFO | max output length: 54
2026-01-28 13:48:34,073 | INFO | min output length: 5
2026-01-28 13:48:38,415 | INFO | adding <eos> in the last position in the loop
2026-01-28 13:48:38,422 | INFO | no hypothesis. Finish decoding.
2026-01-28 13:48:38,422 | INFO | -18.72 * 0.5 =  -9.36 for decoder
2026-01-28 13:48:38,422 | INFO | -32.96 * 0.5 = -16.48 for ctc
2026-01-28 13:48:38,422 | INFO | total log probability: -25.84
2026-01-28 13:48:38,422 | INFO | normalized log probability: -0.48
2026-01-28 13:48:38,422 | INFO | total number of ended hypotheses: 50
2026-01-28 13:48:38,423 | INFO | best hypo: mais<space>voilà<space>c'es<space>c'est<space>surtout<space>l'hôpital<space>public<space>média

2026-01-28 13:48:38,424 | INFO | speech length: 24960
2026-01-28 13:48:38,452 | INFO | decoder input length: 38
2026-01-28 13:48:38,452 | INFO | max output length: 38
2026-01-28 13:48:38,452 | INFO | min output length: 3
2026-01-28 13:48:40,940 | INFO | end detected at 30
2026-01-28 13:48:40,942 | INFO |  -6.16 * 0.5 =  -3.08 for decoder
2026-01-28 13:48:40,942 | INFO | -15.04 * 0.5 =  -7.52 for ctc
2026-01-28 13:48:40,942 | INFO | total log probability: -10.60
2026-01-28 13:48:40,943 | INFO | normalized log probability: -0.62
2026-01-28 13:48:40,943 | INFO | total number of ended hypotheses: 198
2026-01-28 13:48:40,943 | INFO | best hypo: sont<space>à<space>peu<space>près

2026-01-28 13:48:40,945 | INFO | speech length: 30880
2026-01-28 13:48:40,973 | INFO | decoder input length: 47
2026-01-28 13:48:40,973 | INFO | max output length: 47
2026-01-28 13:48:40,973 | INFO | min output length: 4
2026-01-28 13:48:44,695 | INFO | adding <eos> in the last position in the loop
2026-01-28 13:48:44,704 | INFO | end detected at 46
2026-01-28 13:48:44,704 | INFO |  -4.09 * 0.5 =  -2.04 for decoder
2026-01-28 13:48:44,704 | INFO |  -5.85 * 0.5 =  -2.93 for ctc
2026-01-28 13:48:44,705 | INFO | total log probability: -4.97
2026-01-28 13:48:44,705 | INFO | normalized log probability: -0.12
2026-01-28 13:48:44,705 | INFO | total number of ended hypotheses: 180
2026-01-28 13:48:44,705 | INFO | best hypo: alors<space>ça<space>dit<space>mais<space>ça<space>ça<space>m'arrive<space>beaucoup

2026-01-28 13:48:44,707 | INFO | speech length: 64000
2026-01-28 13:48:44,736 | INFO | decoder input length: 99
2026-01-28 13:48:44,736 | INFO | max output length: 99
2026-01-28 13:48:44,736 | INFO | min output length: 9
2026-01-28 13:48:50,096 | INFO | end detected at 59
2026-01-28 13:48:50,099 | INFO |  -5.10 * 0.5 =  -2.55 for decoder
2026-01-28 13:48:50,099 | INFO |  -5.69 * 0.5 =  -2.84 for ctc
2026-01-28 13:48:50,099 | INFO | total log probability: -5.39
2026-01-28 13:48:50,099 | INFO | normalized log probability: -0.11
2026-01-28 13:48:50,099 | INFO | total number of ended hypotheses: 254
2026-01-28 13:48:50,100 | INFO | best hypo: y<space>a<space>des<space>gens<space>euh<space>des<space>très<space>bons<space>gamins<space>mais<space>sur

2026-01-28 13:48:50,102 | INFO | speech length: 255200
2026-01-28 13:48:50,131 | INFO | decoder input length: 398
2026-01-28 13:48:50,131 | INFO | max output length: 398
2026-01-28 13:48:50,131 | INFO | min output length: 39
2026-01-28 13:49:23,543 | INFO | end detected at 319
2026-01-28 13:49:23,544 | INFO | -38.56 * 0.5 = -19.28 for decoder
2026-01-28 13:49:23,544 | INFO | -13.92 * 0.5 =  -6.96 for ctc
2026-01-28 13:49:23,544 | INFO | total log probability: -26.24
2026-01-28 13:49:23,544 | INFO | normalized log probability: -0.08
2026-01-28 13:49:23,544 | INFO | total number of ended hypotheses: 194
2026-01-28 13:49:23,548 | INFO | best hypo: par<space>contre<space>qu'on<space>parle<space>de<space>ça<space>ça<space>se<space>passe<space>mal<space>euh<space>qui<space>veulent<space>clairement<space>euh<space>i<space>s<space>ont<space>fait<space>médecine<space>parce<space>que<space>ça<space>va<space>être<space>un<space>métier<space>stable<space>un<space>métier<space>où<space>ils<space>gagneront<space>de<space>l'argent<space>pour<space>eux<space>c'est<space>important<space>et<space>ils<space>ont<space>envie<space>de<space>faire<space>la<space>fécilité<space>qui<space>rapportera<space>le<space>plus<space>peu<space>importe<space>un<space>peu<space>des<space>aspects<space>dont<space>j'ous<space>ai<space>parlé

2026-01-28 13:49:23,550 | INFO | speech length: 25760
2026-01-28 13:49:23,579 | INFO | decoder input length: 39
2026-01-28 13:49:23,579 | INFO | max output length: 39
2026-01-28 13:49:23,579 | INFO | min output length: 3
2026-01-28 13:49:26,825 | INFO | adding <eos> in the last position in the loop
2026-01-28 13:49:26,833 | INFO | no hypothesis. Finish decoding.
2026-01-28 13:49:26,834 | INFO |  -3.33 * 0.5 =  -1.66 for decoder
2026-01-28 13:49:26,834 | INFO |  -0.80 * 0.5 =  -0.40 for ctc
2026-01-28 13:49:26,834 | INFO | total log probability: -2.06
2026-01-28 13:49:26,834 | INFO | normalized log probability: -0.06
2026-01-28 13:49:26,834 | INFO | total number of ended hypotheses: 128
2026-01-28 13:49:26,835 | INFO | best hypo: et<space>y<space>en<space>a<space>d'autres<space>au<space>contraire<space>euh

2026-01-28 13:49:26,836 | INFO | speech length: 35360
2026-01-28 13:49:26,864 | INFO | decoder input length: 54
2026-01-28 13:49:26,864 | INFO | max output length: 54
2026-01-28 13:49:26,864 | INFO | min output length: 5
2026-01-28 13:49:29,043 | INFO | end detected at 22
2026-01-28 13:49:29,044 | INFO |  -1.30 * 0.5 =  -0.65 for decoder
2026-01-28 13:49:29,044 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-28 13:49:29,044 | INFO | total log probability: -0.65
2026-01-28 13:49:29,044 | INFO | normalized log probability: -0.04
2026-01-28 13:49:29,044 | INFO | total number of ended hypotheses: 167
2026-01-28 13:49:29,045 | INFO | best hypo: qui<space>euh<space>qui<space>ont

2026-01-28 13:49:29,046 | INFO | speech length: 48320
2026-01-28 13:49:29,075 | INFO | decoder input length: 75
2026-01-28 13:49:29,075 | INFO | max output length: 75
2026-01-28 13:49:29,075 | INFO | min output length: 7
2026-01-28 13:49:34,328 | INFO | end detected at 53
2026-01-28 13:49:34,329 | INFO |  -4.02 * 0.5 =  -2.01 for decoder
2026-01-28 13:49:34,332 | INFO |  -1.16 * 0.5 =  -0.58 for ctc
2026-01-28 13:49:34,332 | INFO | total log probability: -2.59
2026-01-28 13:49:34,332 | INFO | normalized log probability: -0.06
2026-01-28 13:49:34,332 | INFO | total number of ended hypotheses: 163
2026-01-28 13:49:34,333 | INFO | best hypo: qui<space>recherche<space>vraiment<space>ce<space>contact<space>encore<space>plus

2026-01-28 13:49:34,334 | INFO | speech length: 11680
2026-01-28 13:49:34,364 | INFO | decoder input length: 17
2026-01-28 13:49:34,364 | INFO | max output length: 17
2026-01-28 13:49:34,364 | INFO | min output length: 1
2026-01-28 13:49:35,901 | INFO | adding <eos> in the last position in the loop
2026-01-28 13:49:35,908 | INFO | no hypothesis. Finish decoding.
2026-01-28 13:49:35,908 | INFO |  -1.53 * 0.5 =  -0.76 for decoder
2026-01-28 13:49:35,908 | INFO |  -0.35 * 0.5 =  -0.17 for ctc
2026-01-28 13:49:35,908 | INFO | total log probability: -0.94
2026-01-28 13:49:35,908 | INFO | normalized log probability: -0.05
2026-01-28 13:49:35,908 | INFO | total number of ended hypotheses: 55
2026-01-28 13:49:35,909 | INFO | best hypo: qui<space>ont<space>fait<space>des<sos/eos>

2026-01-28 13:49:35,909 | WARNING | best hypo length: 17 == max output length: 17
2026-01-28 13:49:35,909 | WARNING | decoding may be stopped by the max output length limitation, please consider to increase the maxlenratio.
2026-01-28 13:49:35,909 | INFO | speech length: 53120
2026-01-28 13:49:35,937 | INFO | decoder input length: 82
2026-01-28 13:49:35,937 | INFO | max output length: 82
2026-01-28 13:49:35,937 | INFO | min output length: 8
2026-01-28 13:49:43,810 | INFO | adding <eos> in the last position in the loop
2026-01-28 13:49:43,820 | INFO | no hypothesis. Finish decoding.
2026-01-28 13:49:43,821 | INFO | -19.57 * 0.5 =  -9.78 for decoder
2026-01-28 13:49:43,821 | INFO | -27.14 * 0.5 = -13.57 for ctc
2026-01-28 13:49:43,821 | INFO | total log probability: -23.35
2026-01-28 13:49:43,821 | INFO | normalized log probability: -0.31
2026-01-28 13:49:43,821 | INFO | total number of ended hypotheses: 188
2026-01-28 13:49:43,822 | INFO | best hypo: non<space>la<space>psychatrice<space>et<space>la<space>chose<space>vraiment<space>la<space>psychatrie<space>faut<space>i<space>faut<space>être<space>euh

2026-01-28 13:49:43,824 | INFO | speech length: 118880
2026-01-28 13:49:43,859 | INFO | decoder input length: 185
2026-01-28 13:49:43,859 | INFO | max output length: 185
2026-01-28 13:49:43,859 | INFO | min output length: 18
2026-01-28 13:50:00,367 | INFO | end detected at 163
2026-01-28 13:50:00,368 | INFO | -21.79 * 0.5 = -10.89 for decoder
2026-01-28 13:50:00,368 | INFO | -13.67 * 0.5 =  -6.83 for ctc
2026-01-28 13:50:00,368 | INFO | total log probability: -17.73
2026-01-28 13:50:00,368 | INFO | normalized log probability: -0.11
2026-01-28 13:50:00,368 | INFO | total number of ended hypotheses: 195
2026-01-28 13:50:00,370 | INFO | best hypo: c'est<space>comme<space>euh<space>comment<space>dire<space>y<space>a<space>certains<space>métiers<space>où<space>i<space>f<space>i<space>faut<space>être<space>né<space>euh<space>ou<space>est<space>ce<space>veut<space>dire<space>i<space>faut<space>être<space>né<space>psychiatre<space>pour<space>être<space>psychiatre<space>on<space>peut<space>pas

2026-01-28 13:50:00,372 | INFO | speech length: 176959
2026-01-28 13:50:00,406 | INFO | decoder input length: 275
2026-01-28 13:50:00,406 | INFO | max output length: 275
2026-01-28 13:50:00,406 | INFO | min output length: 27
2026-01-28 13:50:26,194 | INFO | end detected at 237
2026-01-28 13:50:26,195 | INFO | -18.86 * 0.5 =  -9.43 for decoder
2026-01-28 13:50:26,195 | INFO |  -5.99 * 0.5 =  -3.00 for ctc
2026-01-28 13:50:26,195 | INFO | total log probability: -12.43
2026-01-28 13:50:26,196 | INFO | normalized log probability: -0.05
2026-01-28 13:50:26,196 | INFO | total number of ended hypotheses: 174
2026-01-28 13:50:26,198 | INFO | best hypo: i<space>faut<space>avoir<space>un<space>don<space>spécial<space>parce<space>que<space>la<space>psychiatrie<space>c'est<space>c'est<space>quelque<space>chose<space>euh<space>je<space>pense<space>que<space>la<space>chirurgie<space>c'est<space>pareil<space>hein<space>n'importe<space>qui<space>pourrait<space>pas<space>être<space>chirurgien<space>mais<space>euh<space>non<space>les<space>spécialités<space>oui<space>y<space>a<space>des<space>spécialités<space>qui<space>euh

2026-01-28 13:50:26,200 | INFO | speech length: 122721
2026-01-28 13:50:26,235 | INFO | decoder input length: 191
2026-01-28 13:50:26,235 | INFO | max output length: 191
2026-01-28 13:50:26,235 | INFO | min output length: 19
2026-01-28 13:50:31,971 | INFO | end detected at 130
2026-01-28 13:50:31,973 | INFO | -12.38 * 0.5 =  -6.19 for decoder
2026-01-28 13:50:31,973 | INFO |  -8.95 * 0.5 =  -4.47 for ctc
2026-01-28 13:50:31,973 | INFO | total log probability: -10.66
2026-01-28 13:50:31,973 | INFO | normalized log probability: -0.09
2026-01-28 13:50:31,973 | INFO | total number of ended hypotheses: 215
2026-01-28 13:50:31,975 | INFO | best hypo: paraît<space>que<space>oui<space>ben<space>euh<space>y<space>a<space>une<space>spécialité<space>qui<space>m'attire<space>beaucoup<space>et<space>euh<space>et<space>c'est<space>vrai<space>que<space>elle<space>était<space>pas<space>compatible<space>avec<space>euh

2026-01-28 13:50:31,977 | INFO | speech length: 20480
2026-01-28 13:50:32,004 | INFO | decoder input length: 31
2026-01-28 13:50:32,004 | INFO | max output length: 31
2026-01-28 13:50:32,004 | INFO | min output length: 3
2026-01-28 13:50:32,827 | INFO | adding <eos> in the last position in the loop
2026-01-28 13:50:32,834 | INFO | no hypothesis. Finish decoding.
2026-01-28 13:50:32,834 | INFO | -13.84 * 0.5 =  -6.92 for decoder
2026-01-28 13:50:32,834 | INFO | -26.06 * 0.5 = -13.03 for ctc
2026-01-28 13:50:32,834 | INFO | total log probability: -19.95
2026-01-28 13:50:32,835 | INFO | normalized log probability: -0.74
2026-01-28 13:50:32,835 | INFO | total number of ended hypotheses: 71
2026-01-28 13:50:32,835 | INFO | best hypo: la<space>vie<space>de<space>fmi<space>par<space>exemple

2026-01-28 13:50:32,836 | INFO | speech length: 27360
2026-01-28 13:50:32,862 | INFO | decoder input length: 42
2026-01-28 13:50:32,862 | INFO | max output length: 42
2026-01-28 13:50:32,862 | INFO | min output length: 4
2026-01-28 13:50:34,025 | INFO | adding <eos> in the last position in the loop
2026-01-28 13:50:34,033 | INFO | no hypothesis. Finish decoding.
2026-01-28 13:50:34,034 | INFO |  -3.74 * 0.5 =  -1.87 for decoder
2026-01-28 13:50:34,035 | INFO |  -4.96 * 0.5 =  -2.48 for ctc
2026-01-28 13:50:34,035 | INFO | total log probability: -4.35
2026-01-28 13:50:34,035 | INFO | normalized log probability: -0.11
2026-01-28 13:50:34,035 | INFO | total number of ended hypotheses: 178
2026-01-28 13:50:34,035 | INFO | best hypo: je<space>parle<space>de<space>la<space>de<space>la<space>réanimation<space>euh

2026-01-28 13:50:34,037 | INFO | speech length: 34880
2026-01-28 13:50:34,063 | INFO | decoder input length: 54
2026-01-28 13:50:34,063 | INFO | max output length: 54
2026-01-28 13:50:34,063 | INFO | min output length: 5
2026-01-28 13:50:35,590 | INFO | adding <eos> in the last position in the loop
2026-01-28 13:50:35,597 | INFO | no hypothesis. Finish decoding.
2026-01-28 13:50:35,598 | INFO | -23.38 * 0.5 = -11.69 for decoder
2026-01-28 13:50:35,598 | INFO | -50.94 * 0.5 = -25.47 for ctc
2026-01-28 13:50:35,598 | INFO | total log probability: -37.16
2026-01-28 13:50:35,598 | INFO | normalized log probability: -0.66
2026-01-28 13:50:35,598 | INFO | total number of ended hypotheses: 63
2026-01-28 13:50:35,599 | INFO | best hypo: n'importe<space>qui<space>n'importe<space>quel<space>heure<space>et<space>des<space>moins<space>payés<sos/eos>

2026-01-28 13:50:35,599 | WARNING | best hypo length: 54 == max output length: 54
2026-01-28 13:50:35,599 | WARNING | decoding may be stopped by the max output length limitation, please consider to increase the maxlenratio.
2026-01-28 13:50:35,600 | INFO | speech length: 83520
2026-01-28 13:50:35,627 | INFO | decoder input length: 130
2026-01-28 13:50:35,627 | INFO | max output length: 130
2026-01-28 13:50:35,627 | INFO | min output length: 13
2026-01-28 13:50:39,922 | INFO | adding <eos> in the last position in the loop
2026-01-28 13:50:39,930 | INFO | no hypothesis. Finish decoding.
2026-01-28 13:50:39,931 | INFO | -22.86 * 0.5 = -11.43 for decoder
2026-01-28 13:50:39,931 | INFO | -53.59 * 0.5 = -26.80 for ctc
2026-01-28 13:50:39,931 | INFO | total log probability: -38.22
2026-01-28 13:50:39,932 | INFO | normalized log probability: -0.31
2026-01-28 13:50:39,932 | INFO | total number of ended hypotheses: 163
2026-01-28 13:50:39,933 | INFO | best hypo: ah<space>oui<space>parce<space>on<space>voluerait<space>en<space>fait<space>y<space>a<space>énormément<space>de<space>fois<space>on<space>ira<space>à<space>l'hôpital<space>on<space>sera<space>pas<space>payé<space>en<space>plus<space>oui<space>ce<space>qui<space>veut<space>dire

2026-01-28 13:50:39,935 | INFO | speech length: 30240
2026-01-28 13:50:39,962 | INFO | decoder input length: 46
2026-01-28 13:50:39,962 | INFO | max output length: 46
2026-01-28 13:50:39,962 | INFO | min output length: 4
2026-01-28 13:50:40,435 | INFO | end detected at 15
2026-01-28 13:50:40,436 | INFO |  -0.81 * 0.5 =  -0.41 for decoder
2026-01-28 13:50:40,436 | INFO |  -0.04 * 0.5 =  -0.02 for ctc
2026-01-28 13:50:40,436 | INFO | total log probability: -0.42
2026-01-28 13:50:40,436 | INFO | normalized log probability: -0.04
2026-01-28 13:50:40,436 | INFO | total number of ended hypotheses: 130
2026-01-28 13:50:40,436 | INFO | best hypo: voilà<space>euh

2026-01-28 13:50:40,438 | INFO | speech length: 58080
2026-01-28 13:50:40,464 | INFO | decoder input length: 90
2026-01-28 13:50:40,464 | INFO | max output length: 90
2026-01-28 13:50:40,464 | INFO | min output length: 9
2026-01-28 13:50:43,196 | INFO | adding <eos> in the last position in the loop
2026-01-28 13:50:43,205 | INFO | no hypothesis. Finish decoding.
2026-01-28 13:50:43,206 | INFO |  -7.03 * 0.5 =  -3.51 for decoder
2026-01-28 13:50:43,206 | INFO |  -6.06 * 0.5 =  -3.03 for ctc
2026-01-28 13:50:43,206 | INFO | total log probability: -6.54
2026-01-28 13:50:43,206 | INFO | normalized log probability: -0.07
2026-01-28 13:50:43,206 | INFO | total number of ended hypotheses: 125
2026-01-28 13:50:43,207 | INFO | best hypo: et<space>sinon<space>les<space>spécialités<space>les<space>m<space>un<space>peu<space>moins<space>je<space>sais<space>pas<space>si<space>c'est<space>ça<space>qui<space>vous<space>intéresse

2026-01-28 13:50:43,209 | INFO | speech length: 89280
2026-01-28 13:50:43,238 | INFO | decoder input length: 139
2026-01-28 13:50:43,238 | INFO | max output length: 139
2026-01-28 13:50:43,239 | INFO | min output length: 13
2026-01-28 13:50:47,449 | INFO | end detected at 123
2026-01-28 13:50:47,450 | INFO | -11.87 * 0.5 =  -5.94 for decoder
2026-01-28 13:50:47,450 | INFO | -11.15 * 0.5 =  -5.58 for ctc
2026-01-28 13:50:47,450 | INFO | total log probability: -11.51
2026-01-28 13:50:47,450 | INFO | normalized log probability: -0.10
2026-01-28 13:50:47,450 | INFO | total number of ended hypotheses: 163
2026-01-28 13:50:47,452 | INFO | best hypo: un<space>petit<space>peu<space>moins<space>triste<space>bah<space>c'est<space>c'est<space>les<space>spécialités<space>à<space>risques<space>la<space>gynéco<space>obstétrique<space>par<space>exemple<space>la<space>cancérologie

2026-01-28 13:50:47,453 | INFO | speech length: 76480
2026-01-28 13:50:47,480 | INFO | decoder input length: 119
2026-01-28 13:50:47,480 | INFO | max output length: 119
2026-01-28 13:50:47,480 | INFO | min output length: 11
2026-01-28 13:50:51,373 | INFO | end detected at 112
2026-01-28 13:50:51,375 | INFO |  -8.84 * 0.5 =  -4.42 for decoder
2026-01-28 13:50:51,375 | INFO |  -3.47 * 0.5 =  -1.74 for ctc
2026-01-28 13:50:51,375 | INFO | total log probability: -6.15
2026-01-28 13:50:51,375 | INFO | normalized log probability: -0.06
2026-01-28 13:50:51,375 | INFO | total number of ended hypotheses: 195
2026-01-28 13:50:51,376 | INFO | best hypo: ça<space>ces<space>spécialités<space>euh<space>qui<space>sont<space>de<space>moins<space>en<space>moins<space>prises<space>parce<space>que<space>dangereuses<space>faut<space>avoir<space>un<space>bon<space>avocat

2026-01-28 13:50:51,378 | INFO | speech length: 21920
2026-01-28 13:50:51,404 | INFO | decoder input length: 33
2026-01-28 13:50:51,404 | INFO | max output length: 33
2026-01-28 13:50:51,404 | INFO | min output length: 3
2026-01-28 13:50:52,291 | INFO | adding <eos> in the last position in the loop
2026-01-28 13:50:52,299 | INFO | no hypothesis. Finish decoding.
2026-01-28 13:50:52,300 | INFO |  -2.62 * 0.5 =  -1.31 for decoder
2026-01-28 13:50:52,300 | INFO |  -1.20 * 0.5 =  -0.60 for ctc
2026-01-28 13:50:52,300 | INFO | total log probability: -1.91
2026-01-28 13:50:52,300 | INFO | normalized log probability: -0.06
2026-01-28 13:50:52,300 | INFO | total number of ended hypotheses: 114
2026-01-28 13:50:52,300 | INFO | best hypo: bah<space>de<space>plus<space>en<space>plus<space>important

2026-01-28 13:50:52,301 | INFO | speech length: 108640
2026-01-28 13:50:52,328 | INFO | decoder input length: 169
2026-01-28 13:50:52,328 | INFO | max output length: 169
2026-01-28 13:50:52,328 | INFO | min output length: 16
2026-01-28 13:50:56,894 | INFO | end detected at 122
2026-01-28 13:50:56,896 | INFO |  -9.72 * 0.5 =  -4.86 for decoder
2026-01-28 13:50:56,896 | INFO |  -1.74 * 0.5 =  -0.87 for ctc
2026-01-28 13:50:56,896 | INFO | total log probability: -5.73
2026-01-28 13:50:56,896 | INFO | normalized log probability: -0.05
2026-01-28 13:50:56,896 | INFO | total number of ended hypotheses: 180
2026-01-28 13:50:56,897 | INFO | best hypo: euh<space>parce<space>que<space>euh<space>s<space>c'est<space>s<space>pour<space>moi<space>c'est<space>très<space>important<space>d'avoir<space>ces<space>idéaux<space>mais<space>i<space>faut<space>aussi<space>être<space>réaliste<space>voilà

2026-01-28 13:50:56,899 | INFO | speech length: 65920
2026-01-28 13:50:56,925 | INFO | decoder input length: 102
2026-01-28 13:50:56,925 | INFO | max output length: 102
2026-01-28 13:50:56,925 | INFO | min output length: 10
2026-01-28 13:50:58,965 | INFO | end detected at 60
2026-01-28 13:50:58,967 | INFO |  -5.94 * 0.5 =  -2.97 for decoder
2026-01-28 13:50:58,967 | INFO | -14.66 * 0.5 =  -7.33 for ctc
2026-01-28 13:50:58,967 | INFO | total log probability: -10.30
2026-01-28 13:50:58,967 | INFO | normalized log probability: -0.19
2026-01-28 13:50:58,967 | INFO | total number of ended hypotheses: 221
2026-01-28 13:50:58,968 | INFO | best hypo: alors<space>j'ai<space>une<space>question<space>totalement<space>je<space>veux<space>pas<space>vous

2026-01-28 13:50:58,970 | INFO | speech length: 115520
2026-01-28 13:50:58,997 | INFO | decoder input length: 180
2026-01-28 13:50:58,997 | INFO | max output length: 180
2026-01-28 13:50:58,997 | INFO | min output length: 18
2026-01-28 13:51:04,771 | INFO | end detected at 153
2026-01-28 13:51:04,773 | INFO | -18.56 * 0.5 =  -9.28 for decoder
2026-01-28 13:51:04,773 | INFO | -17.61 * 0.5 =  -8.81 for ctc
2026-01-28 13:51:04,773 | INFO | total log probability: -18.09
2026-01-28 13:51:04,773 | INFO | normalized log probability: -0.13
2026-01-28 13:51:04,773 | INFO | total number of ended hypotheses: 210
2026-01-28 13:51:04,775 | INFO | best hypo: on<space>suit<space>en<space>équipement<space>du<space>quartier<space>j'ai<space>compris<space>que<space>ça<space>allé<space>que<space>vous<space>étiez<space>content<space>que<space>y<space>avait<space>suffisamment<space>de<space>d'espaces<space>verts<space>de<space>d'équipements

2026-01-28 13:51:04,776 | INFO | speech length: 18560
2026-01-28 13:51:04,803 | INFO | decoder input length: 28
2026-01-28 13:51:04,803 | INFO | max output length: 28
2026-01-28 13:51:04,803 | INFO | min output length: 2
2026-01-28 13:51:05,545 | INFO | adding <eos> in the last position in the loop
2026-01-28 13:51:05,555 | INFO | end detected at 27
2026-01-28 13:51:05,555 | INFO |  -1.69 * 0.5 =  -0.84 for decoder
2026-01-28 13:51:05,556 | INFO |  -0.03 * 0.5 =  -0.01 for ctc
2026-01-28 13:51:05,556 | INFO | total log probability: -0.86
2026-01-28 13:51:05,556 | INFO | normalized log probability: -0.04
2026-01-28 13:51:05,556 | INFO | total number of ended hypotheses: 186
2026-01-28 13:51:05,556 | INFO | best hypo: les<space>animaux<space>dans<space>les

2026-01-28 13:51:05,557 | INFO | speech length: 14720
2026-01-28 13:51:05,584 | INFO | decoder input length: 22
2026-01-28 13:51:05,584 | INFO | max output length: 22
2026-01-28 13:51:05,584 | INFO | min output length: 2
2026-01-28 13:51:05,898 | INFO | end detected at 10
2026-01-28 13:51:05,899 | INFO |  -0.35 * 0.5 =  -0.17 for decoder
2026-01-28 13:51:05,899 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-28 13:51:05,899 | INFO | total log probability: -0.19
2026-01-28 13:51:05,900 | INFO | normalized log probability: -0.04
2026-01-28 13:51:05,900 | INFO | total number of ended hypotheses: 162
2026-01-28 13:51:05,900 | INFO | best hypo: euh

2026-01-28 13:51:05,901 | INFO | speech length: 221920
2026-01-28 13:51:05,928 | INFO | decoder input length: 346
2026-01-28 13:51:05,928 | INFO | max output length: 346
2026-01-28 13:51:05,928 | INFO | min output length: 34
2026-01-28 13:51:18,844 | INFO | end detected at 259
2026-01-28 13:51:18,846 | INFO | -30.99 * 0.5 = -15.50 for decoder
2026-01-28 13:51:18,846 | INFO |  -5.91 * 0.5 =  -2.96 for ctc
2026-01-28 13:51:18,847 | INFO | total log probability: -18.45
2026-01-28 13:51:18,847 | INFO | normalized log probability: -0.07
2026-01-28 13:51:18,847 | INFO | total number of ended hypotheses: 214
2026-01-28 13:51:18,849 | INFO | best hypo: alors<space>un<space>changement<space>euh<space>les<space>crottes<space>de<space>chien<space>c'est<space>vrai<space>que<space>depuis<space>je<space>sais<space>pas<space>je<space>sais<space>pas<space>si<space>y<space>a<space>eu<space>une<space>loi<space>qui<space>a<space>été<space>instaurée<space>mais<space>euh<space>de<space>plus<space>en<space>plus<space>je<space>vois<space>des<space>gens<space>qui<space>sont<space>respectueux<space>et<space>qui<space>ramassent<space>euh<space>qui<space>qui<space>qui<space>prennent<space>leurs<space>chiens<space>qui

2026-01-28 13:51:18,852 | INFO | speech length: 137600
2026-01-28 13:51:18,879 | INFO | decoder input length: 214
2026-01-28 13:51:18,879 | INFO | max output length: 214
2026-01-28 13:51:18,879 | INFO | min output length: 21
2026-01-28 13:51:26,696 | INFO | end detected at 198
2026-01-28 13:51:26,697 | INFO | -23.14 * 0.5 = -11.57 for decoder
2026-01-28 13:51:26,697 | INFO | -29.80 * 0.5 = -14.90 for ctc
2026-01-28 13:51:26,697 | INFO | total log probability: -26.47
2026-01-28 13:51:26,697 | INFO | normalized log probability: -0.14
2026-01-28 13:51:26,697 | INFO | total number of ended hypotheses: 181
2026-01-28 13:51:26,699 | INFO | best hypo: i<space>font<space>faire<space>leurs<space>besoins<space>dans<space>dans<space>le<space>trottoir<space>pas<space>sur<space>le<space>trottoir<space>justement<space>et<space>euh<space>moi<space>personnement<space>j'aime<space>beaucoup<space>les<space>animaux<space>et<space>j'ai<space>rien<space>contre<space>je<space>trouve<space>ça<space>sempa<space>d'en<space>voir<space>dans<space>paris

2026-01-28 13:51:26,708 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 13:51:26,708 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 13:51:26,708 | INFO | Chunk: 2 | WER=100.000000 | S=1 D=0 I=0
2026-01-28 13:51:26,708 | INFO | Chunk: 3 | WER=133.333333 | S=2 D=0 I=2
2026-01-28 13:51:26,714 | INFO | Chunk: 4 | WER=12.844037 | S=6 D=2 I=6
2026-01-28 13:51:26,715 | INFO | Chunk: 5 | WER=20.000000 | S=1 D=0 I=9
2026-01-28 13:51:26,716 | INFO | Chunk: 6 | WER=22.222222 | S=1 D=0 I=1
2026-01-28 13:51:26,716 | INFO | Chunk: 7 | WER=26.666667 | S=3 D=0 I=1
2026-01-28 13:51:26,717 | INFO | Chunk: 8 | WER=36.363636 | S=10 D=1 I=1
2026-01-28 13:51:26,717 | INFO | Chunk: 9 | WER=36.363636 | S=0 D=3 I=1
2026-01-28 13:51:26,718 | INFO | Chunk: 10 | WER=23.333333 | S=4 D=1 I=2
2026-01-28 13:51:26,718 | INFO | Chunk: 11 | WER=33.333333 | S=0 D=1 I=0
2026-01-28 13:51:26,718 | INFO | Chunk: 12 | WER=0.000000 | S=1 D=0 I=0
2026-01-28 13:51:26,718 | INFO | Chunk: 13 | WER=8.695652 | S=1 D=1 I=0
2026-01-28 13:51:26,719 | INFO | Chunk: 14 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 13:51:26,719 | INFO | Chunk: 15 | WER=50.000000 | S=1 D=2 I=0
2026-01-28 13:51:26,719 | INFO | Chunk: 16 | WER=20.000000 | S=1 D=2 I=0
2026-01-28 13:51:26,724 | INFO | Chunk: 17 | WER=23.076923 | S=11 D=5 I=8
2026-01-28 13:51:26,724 | INFO | Chunk: 18 | WER=27.272727 | S=0 D=2 I=1
2026-01-28 13:51:26,726 | INFO | Chunk: 19 | WER=21.666667 | S=8 D=3 I=2
2026-01-28 13:51:26,727 | INFO | Chunk: 20 | WER=66.666667 | S=0 D=0 I=2
2026-01-28 13:51:26,727 | INFO | Chunk: 21 | WER=33.333333 | S=2 D=1 I=0
2026-01-28 13:51:26,727 | INFO | Chunk: 22 | WER=40.000000 | S=2 D=4 I=0
2026-01-28 13:51:26,727 | INFO | Chunk: 23 | WER=33.333333 | S=0 D=2 I=0
2026-01-28 13:51:26,727 | INFO | Chunk: 24 | WER=57.142857 | S=2 D=0 I=2
2026-01-28 13:51:26,728 | INFO | Chunk: 25 | WER=53.846154 | S=3 D=3 I=1
2026-01-28 13:51:26,730 | INFO | Chunk: 26 | WER=14.516129 | S=6 D=0 I=3
2026-01-28 13:51:26,730 | INFO | Chunk: 27 | WER=22.222222 | S=0 D=1 I=1
2026-01-28 13:51:26,730 | INFO | Chunk: 28 | WER=33.333333 | S=0 D=0 I=1
2026-01-28 13:51:26,730 | INFO | Chunk: 29 | WER=14.285714 | S=1 D=0 I=0
2026-01-28 13:51:26,730 | INFO | Chunk: 30 | WER=100.000000 | S=0 D=0 I=3
2026-01-28 13:51:26,731 | INFO | Chunk: 31 | WER=61.111111 | S=5 D=5 I=1
2026-01-28 13:51:26,732 | INFO | Chunk: 32 | WER=33.333333 | S=6 D=4 I=2
2026-01-28 13:51:26,733 | INFO | Chunk: 33 | WER=19.047619 | S=3 D=1 I=4
2026-01-28 13:51:26,733 | INFO | Chunk: 34 | WER=36.000000 | S=4 D=2 I=3
2026-01-28 13:51:26,734 | INFO | Chunk: 35 | WER=28.571429 | S=1 D=1 I=0
2026-01-28 13:51:26,734 | INFO | Chunk: 36 | WER=28.571429 | S=1 D=0 I=1
2026-01-28 13:51:26,734 | INFO | Chunk: 37 | WER=62.500000 | S=3 D=5 I=2
2026-01-28 13:51:26,735 | INFO | Chunk: 38 | WER=38.888889 | S=6 D=8 I=0
2026-01-28 13:51:26,735 | INFO | Chunk: 39 | WER=100.000000 | S=0 D=1 I=1
2026-01-28 13:51:26,735 | INFO | Chunk: 40 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 13:51:26,736 | INFO | Chunk: 41 | WER=20.000000 | S=1 D=1 I=2
2026-01-28 13:51:26,736 | INFO | Chunk: 42 | WER=20.000000 | S=1 D=2 I=1
2026-01-28 13:51:26,737 | INFO | Chunk: 43 | WER=42.857143 | S=0 D=2 I=1
2026-01-28 13:51:26,737 | INFO | Chunk: 44 | WER=28.571429 | S=2 D=0 I=4
2026-01-28 13:51:26,737 | INFO | Chunk: 45 | WER=30.000000 | S=1 D=1 I=1
2026-01-28 13:51:26,738 | INFO | Chunk: 46 | WER=17.857143 | S=4 D=1 I=0
2026-01-28 13:51:26,738 | INFO | Chunk: 47 | WER=33.333333 | S=0 D=0 I=1
2026-01-28 13:51:26,738 | INFO | Chunk: 48 | WER=0.000000 | S=1 D=0 I=0
2026-01-28 13:51:26,740 | INFO | Chunk: 49 | WER=14.000000 | S=2 D=1 I=4
2026-01-28 13:51:26,741 | INFO | Chunk: 50 | WER=25.000000 | S=5 D=4 I=1
2026-01-28 13:51:27,203 | INFO | File: Rhap-D0006.wav | WER=24.927536 | S=117 D=68 I=73
2026-01-28 13:51:27,203 | INFO | ------------------------------
2026-01-28 13:51:27,203 | INFO | Conf ester Done!
2026-01-28 13:56:11,698 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 13:56:11,699 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 13:56:11,699 | INFO | Chunk: 2 | WER=100.000000 | S=1 D=2 I=0
2026-01-28 13:56:11,699 | INFO | Chunk: 3 | WER=100.000000 | S=1 D=1 I=0
2026-01-28 13:56:11,699 | INFO | Chunk: 4 | WER=100.000000 | S=1 D=0 I=0
2026-01-28 13:56:11,699 | INFO | Chunk: 5 | WER=100.000000 | S=1 D=1 I=0
2026-01-28 13:56:11,699 | INFO | Chunk: 6 | WER=100.000000 | S=2 D=0 I=1
2026-01-28 13:56:11,706 | INFO | Chunk: 7 | WER=18.348624 | S=12 D=7 I=1
2026-01-28 13:56:11,707 | INFO | Chunk: 8 | WER=6.000000 | S=2 D=1 I=0
2026-01-28 13:56:11,708 | INFO | Chunk: 9 | WER=11.111111 | S=0 D=0 I=1
2026-01-28 13:56:11,708 | INFO | Chunk: 10 | WER=26.666667 | S=3 D=0 I=1
2026-01-28 13:56:11,709 | INFO | Chunk: 11 | WER=27.272727 | S=4 D=5 I=0
2026-01-28 13:56:11,709 | INFO | Chunk: 12 | WER=72.727273 | S=2 D=5 I=1
2026-01-28 13:56:11,710 | INFO | Chunk: 13 | WER=40.000000 | S=6 D=6 I=0
2026-01-28 13:56:11,710 | INFO | Chunk: 14 | WER=66.666667 | S=0 D=2 I=0
2026-01-28 13:56:11,710 | INFO | Chunk: 15 | WER=100.000000 | S=1 D=1 I=0
2026-01-28 13:56:11,710 | INFO | Chunk: 16 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 13:56:11,711 | INFO | Chunk: 17 | WER=30.434783 | S=0 D=7 I=0
2026-01-28 13:56:11,711 | INFO | Chunk: 18 | WER=33.333333 | S=0 D=0 I=1
2026-01-28 13:56:11,711 | INFO | Chunk: 19 | WER=66.666667 | S=0 D=4 I=0
2026-01-28 13:56:11,711 | INFO | Chunk: 20 | WER=66.666667 | S=5 D=5 I=0
2026-01-28 13:56:11,716 | INFO | Chunk: 21 | WER=20.192308 | S=11 D=10 I=0
2026-01-28 13:56:11,716 | INFO | Chunk: 22 | WER=81.818182 | S=1 D=7 I=1
2026-01-28 13:56:11,719 | INFO | Chunk: 23 | WER=26.666667 | S=10 D=4 I=2
2026-01-28 13:56:11,719 | INFO | Chunk: 24 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 13:56:11,719 | INFO | Chunk: 25 | WER=11.111111 | S=1 D=0 I=0
2026-01-28 13:56:11,719 | INFO | Chunk: 26 | WER=46.666667 | S=1 D=6 I=0
2026-01-28 13:56:11,720 | INFO | Chunk: 27 | WER=100.000000 | S=1 D=5 I=0
2026-01-28 13:56:11,720 | INFO | Chunk: 28 | WER=42.857143 | S=3 D=0 I=0
2026-01-28 13:56:11,720 | INFO | Chunk: 29 | WER=38.461538 | S=1 D=3 I=1
2026-01-28 13:56:11,722 | INFO | Chunk: 30 | WER=22.580645 | S=8 D=5 I=1
2026-01-28 13:56:11,722 | INFO | Chunk: 31 | WER=11.111111 | S=0 D=1 I=0
2026-01-28 13:56:11,722 | INFO | Chunk: 32 | WER=33.333333 | S=0 D=0 I=1
2026-01-28 13:56:11,723 | INFO | Chunk: 33 | WER=14.285714 | S=0 D=0 I=1
2026-01-28 13:56:11,723 | INFO | Chunk: 34 | WER=66.666667 | S=0 D=0 I=2
2026-01-28 13:56:11,723 | INFO | Chunk: 35 | WER=27.777778 | S=2 D=3 I=0
2026-01-28 13:56:11,724 | INFO | Chunk: 36 | WER=38.888889 | S=7 D=7 I=0
2026-01-28 13:56:11,725 | INFO | Chunk: 37 | WER=7.142857 | S=2 D=0 I=1
2026-01-28 13:56:11,726 | INFO | Chunk: 38 | WER=56.000000 | S=6 D=7 I=1
2026-01-28 13:56:11,726 | INFO | Chunk: 39 | WER=14.285714 | S=1 D=0 I=0
2026-01-28 13:56:11,726 | INFO | Chunk: 40 | WER=57.142857 | S=2 D=2 I=0
2026-01-28 13:56:11,727 | INFO | Chunk: 41 | WER=37.500000 | S=3 D=3 I=0
2026-01-28 13:56:11,727 | INFO | Chunk: 42 | WER=38.888889 | S=4 D=10 I=0
2026-01-28 13:56:11,727 | INFO | Chunk: 43 | WER=100.000000 | S=0 D=1 I=1
2026-01-28 13:56:11,728 | INFO | Chunk: 44 | WER=42.105263 | S=3 D=4 I=1
2026-01-28 13:56:11,728 | INFO | Chunk: 45 | WER=55.000000 | S=7 D=4 I=0
2026-01-28 13:56:11,729 | INFO | Chunk: 46 | WER=35.000000 | S=5 D=2 I=0
2026-01-28 13:56:11,729 | INFO | Chunk: 47 | WER=42.857143 | S=1 D=2 I=0
2026-01-28 13:56:11,729 | INFO | Chunk: 48 | WER=19.047619 | S=1 D=2 I=1
2026-01-28 13:56:11,730 | INFO | Chunk: 49 | WER=50.000000 | S=3 D=2 I=0
2026-01-28 13:56:11,730 | INFO | Chunk: 50 | WER=28.571429 | S=4 D=4 I=0
2026-01-28 13:56:11,731 | INFO | Chunk: 51 | WER=66.666667 | S=0 D=0 I=2
2026-01-28 13:56:11,731 | INFO | Chunk: 52 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 13:56:11,732 | INFO | Chunk: 53 | WER=22.000000 | S=5 D=4 I=2
2026-01-28 13:56:11,733 | INFO | Chunk: 54 | WER=32.500000 | S=9 D=4 I=0
2026-01-28 13:56:12,150 | INFO | File: Rhap-D0006.wav | WER=29.885057 | S=143 D=149 I=20
2026-01-28 13:56:12,150 | INFO | ------------------------------
2026-01-28 13:56:12,150 | INFO | hmm_tdnn Done!
2026-01-28 13:56:12,291 | INFO | ==================================Rhap-D0007.wav=========================================
2026-01-28 13:56:12,419 | INFO | Using rVAD model
2026-01-28 13:56:14,328 | INFO | Chunk: 0 | WER=45.945946 | S=19 D=12 I=3
2026-01-28 13:56:14,330 | INFO | Chunk: 1 | WER=16.326531 | S=3 D=2 I=3
2026-01-28 13:56:14,338 | INFO | File: Rhap-D0007.wav | WER=34.146341 | S=22 D=14 I=6
2026-01-28 13:56:14,338 | INFO | ------------------------------
2026-01-28 13:56:14,338 | INFO | w2vec vad chunk Done!
2026-01-28 13:56:16,301 | INFO | Chunk: 0 | WER=85.135135 | S=5 D=57 I=1
2026-01-28 13:56:16,302 | INFO | Chunk: 1 | WER=77.551020 | S=1 D=37 I=0
2026-01-28 13:56:16,305 | INFO | File: Rhap-D0007.wav | WER=82.113821 | S=6 D=94 I=1
2026-01-28 13:56:16,305 | INFO | ------------------------------
2026-01-28 13:56:16,305 | INFO | whisper med Done!
2026-01-28 13:56:21,896 | INFO | Chunk: 0 | WER=70.270270 | S=7 D=45 I=0
2026-01-28 13:56:21,897 | INFO | Chunk: 1 | WER=38.775510 | S=10 D=5 I=4
2026-01-28 13:56:21,903 | INFO | File: Rhap-D0007.wav | WER=57.723577 | S=17 D=50 I=4
2026-01-28 13:56:21,903 | INFO | ------------------------------
2026-01-28 13:56:21,903 | INFO | whisper large Done!
2026-01-28 13:56:22,062 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 13:56:22,093 | INFO | Vocabulary size: 350
2026-01-28 13:56:22,631 | INFO | Gradient checkpoint layers: []
2026-01-28 13:56:23,486 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 13:56:23,490 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 13:56:23,490 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 13:56:23,490 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 13:56:23,490 | INFO | speech length: 384800
2026-01-28 13:56:23,530 | INFO | decoder input length: 600
2026-01-28 13:56:23,530 | INFO | max output length: 600
2026-01-28 13:56:23,530 | INFO | min output length: 60
2026-01-28 13:56:47,100 | INFO | end detected at 163
2026-01-28 13:56:47,101 | INFO | -340.06 * 0.5 = -170.03 for decoder
2026-01-28 13:56:47,101 | INFO | -104.69 * 0.5 = -52.35 for ctc
2026-01-28 13:56:47,101 | INFO | total log probability: -222.38
2026-01-28 13:56:47,101 | INFO | normalized log probability: -1.42
2026-01-28 13:56:47,101 | INFO | total number of ended hypotheses: 172
2026-01-28 13:56:47,103 | INFO | best hypo: ▁ah▁nez▁savants▁bah▁là▁nous▁sommes▁au▁jardin▁de▁guy▁non▁pas▁tout▁près▁moi▁je▁conseillerai▁de▁d'aller▁de▁travers▁les▁jardins▁de▁ville▁vous▁tomber▁sur▁la▁par▁la▁pere▁grenette▁vous▁allez▁jusqu'au▁ligne▁de▁tram▁là▁vous▁continuer▁jusqu'à▁la▁rue▁hubert▁ubeaudoux▁ou▁toi▁vous▁pour▁contingueuer▁pourquoi▁vous▁tromper▁et▁vous▁suivez▁ligne▁de▁tram

2026-01-28 13:56:47,108 | INFO | speech length: 268800
2026-01-28 13:56:47,183 | INFO | decoder input length: 419
2026-01-28 13:56:47,183 | INFO | max output length: 419
2026-01-28 13:56:47,183 | INFO | min output length: 41
2026-01-28 13:57:02,744 | INFO | end detected at 121
2026-01-28 13:57:02,745 | INFO | -193.80 * 0.5 = -96.90 for decoder
2026-01-28 13:57:02,745 | INFO | -77.81 * 0.5 = -38.90 for ctc
2026-01-28 13:57:02,745 | INFO | total log probability: -135.80
2026-01-28 13:57:02,745 | INFO | normalized log probability: -1.21
2026-01-28 13:57:02,745 | INFO | total number of ended hypotheses: 168
2026-01-28 13:57:02,747 | INFO | best hypo: ▁du▁tram▁bleu▁soit▁alors▁vous▁tournez▁à▁droite▁juste▁après▁l'abbé▁hubert▁dubedou▁vous▁que▁vous▁remontez▁toujours▁tout▁droit▁vez▁tomber▁sur▁eux▁sur▁une▁église▁sur▁une▁place▁que▁là▁vous▁vous▁prenez▁à▁gauche▁et▁vous▁continuez▁sur▁deux▁cent▁mètres▁et▁vous▁vouche

2026-01-28 13:57:02,754 | INFO | Chunk: 0 | WER=55.405405 | S=28 D=11 I=2
2026-01-28 13:57:02,755 | INFO | Chunk: 1 | WER=24.489796 | S=6 D=3 I=3
2026-01-28 13:57:02,763 | INFO | File: Rhap-D0007.wav | WER=43.089431 | S=34 D=14 I=5
2026-01-28 13:57:02,763 | INFO | ------------------------------
2026-01-28 13:57:02,763 | INFO | Conf cv Done!
2026-01-28 13:57:02,926 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 13:57:02,965 | INFO | Vocabulary size: 47
2026-01-28 13:57:03,575 | INFO | Gradient checkpoint layers: []
2026-01-28 13:57:04,497 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 13:57:04,501 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 13:57:04,501 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 13:57:04,501 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 13:57:04,504 | INFO | speech length: 384800
2026-01-28 13:57:04,537 | INFO | decoder input length: 600
2026-01-28 13:57:04,537 | INFO | max output length: 600
2026-01-28 13:57:04,537 | INFO | min output length: 60
2026-01-28 13:58:01,971 | INFO | end detected at 402
2026-01-28 13:58:01,974 | INFO | -225.63 * 0.5 = -112.82 for decoder
2026-01-28 13:58:01,974 | INFO | -92.71 * 0.5 = -46.36 for ctc
2026-01-28 13:58:01,974 | INFO | total log probability: -159.17
2026-01-28 13:58:01,974 | INFO | normalized log probability: -0.41
2026-01-28 13:58:01,974 | INFO | total number of ended hypotheses: 244
2026-01-28 13:58:01,978 | INFO | best hypo: ah<space>né<space>chavant<space>ben<space>là<space>nous<space>sommes<space>au<space>germain<space>me<space>dit<space>donc<space>euh<space>ça<space>c'est<space>pas<space>tout<space>près<space>euh<space>moi<space>je<space>vous<space>conseillerais<space>de<space>d'aller<space>euh<space>de<space>traverser<space>javane<space>ville<space>vous<space>retomber<space>sur<space>la<space>sur<space>la<space>place<space>de<space>connette<space>ensuite<space>vous<space>allez<space>euh<space>jusqu'aux<space>lignes<space>de<space>trames<space>là<space>vous<space>continuez<space>jusqu'à<space>la<space>rue<space>ébert<space>du<space>dedoux<space>maintenant<space>soit<space>vous<space>pou<space>continuez<space>euh<space>pourquoi<space>vous<space>tromper<space>vous<space>suivez<space>la<space>ligne<space>de<space>trames

2026-01-28 13:58:01,982 | INFO | speech length: 268800
2026-01-28 13:58:02,013 | INFO | decoder input length: 419
2026-01-28 13:58:02,013 | INFO | max output length: 419
2026-01-28 13:58:02,013 | INFO | min output length: 41
2026-01-28 13:58:35,734 | INFO | end detected at 292
2026-01-28 13:58:35,736 | INFO | -44.47 * 0.5 = -22.24 for decoder
2026-01-28 13:58:35,736 | INFO | -26.30 * 0.5 = -13.15 for ctc
2026-01-28 13:58:35,736 | INFO | total log probability: -35.39
2026-01-28 13:58:35,736 | INFO | normalized log probability: -0.13
2026-01-28 13:58:35,736 | INFO | total number of ended hypotheses: 196
2026-01-28 13:58:35,739 | INFO | best hypo: du<space>train<space>bleu<space>soit<space>alors<space>vous<space>tournez<space>à<space>à<space>droite<space>juste<space>après<space>l'arrée<space>hubert<space>du<space>bedou<space>vous<space>vous<space>montez<space>toujours<space>tout<space>droit<space>vous<space>allez<space>tomber<space>sur<space>euh<space>sur<space>euh<space>une<space>église<space>sur<space>une<space>place<space>et<space>là<space>vous<space>vous<space>reprenez<space>à<space>gauche<space>vous<space>continuez<space>sur<space>euh<space>deux<space>cents<space>mètres<space>et<space>vous<space>êtes<space>à<space>la<space>nation

2026-01-28 13:58:35,747 | INFO | Chunk: 0 | WER=48.648649 | S=23 D=6 I=7
2026-01-28 13:58:35,749 | INFO | Chunk: 1 | WER=30.612245 | S=5 D=2 I=8
2026-01-28 13:58:35,757 | INFO | File: Rhap-D0007.wav | WER=41.463415 | S=28 D=8 I=15
2026-01-28 13:58:35,757 | INFO | ------------------------------
2026-01-28 13:58:35,758 | INFO | Conf ester Done!
2026-01-28 13:59:09,011 | INFO | Chunk: 0 | WER=64.864865 | S=25 D=21 I=2
2026-01-28 13:59:09,014 | INFO | Chunk: 1 | WER=32.653061 | S=11 D=4 I=1
2026-01-28 13:59:09,023 | INFO | File: Rhap-D0007.wav | WER=52.032520 | S=36 D=25 I=3
2026-01-28 13:59:09,023 | INFO | ------------------------------
2026-01-28 13:59:09,023 | INFO | hmm_tdnn Done!
2026-01-28 13:59:09,166 | INFO | ==================================Rhap-D0008.wav=========================================
2026-01-28 13:59:09,314 | INFO | Using rVAD model
2026-01-28 13:59:19,325 | INFO | Chunk: 0 | WER=15.789474 | S=1 D=0 I=2
2026-01-28 13:59:19,328 | INFO | Chunk: 1 | WER=34.285714 | S=10 D=11 I=3
2026-01-28 13:59:19,328 | INFO | Chunk: 2 | WER=100.000000 | S=2 D=1 I=0
2026-01-28 13:59:19,328 | INFO | Chunk: 3 | WER=40.000000 | S=0 D=1 I=1
2026-01-28 13:59:19,329 | INFO | Chunk: 4 | WER=66.666667 | S=10 D=12 I=4
2026-01-28 13:59:19,331 | INFO | Chunk: 5 | WER=31.372549 | S=3 D=7 I=6
2026-01-28 13:59:19,331 | INFO | Chunk: 6 | WER=65.000000 | S=3 D=4 I=6
2026-01-28 13:59:19,331 | INFO | Chunk: 7 | WER=66.666667 | S=4 D=0 I=0
2026-01-28 13:59:19,331 | INFO | Chunk: 8 | WER=140.000000 | S=0 D=2 I=5
2026-01-28 13:59:19,332 | INFO | Chunk: 9 | WER=40.000000 | S=1 D=8 I=5
2026-01-28 13:59:19,333 | INFO | Chunk: 10 | WER=23.529412 | S=0 D=3 I=1
2026-01-28 13:59:19,333 | INFO | Chunk: 11 | WER=31.818182 | S=2 D=3 I=2
2026-01-28 13:59:19,333 | INFO | Chunk: 12 | WER=11.111111 | S=0 D=0 I=1
2026-01-28 13:59:19,334 | INFO | Chunk: 13 | WER=51.724138 | S=2 D=8 I=5
2026-01-28 13:59:19,336 | INFO | Chunk: 14 | WER=42.187500 | S=10 D=13 I=4
2026-01-28 13:59:19,336 | INFO | Chunk: 15 | WER=114.285714 | S=2 D=2 I=4
2026-01-28 13:59:19,336 | INFO | Chunk: 16 | WER=400.000000 | S=1 D=0 I=3
2026-01-28 13:59:19,337 | INFO | Chunk: 17 | WER=150.000000 | S=2 D=1 I=3
2026-01-28 13:59:19,337 | INFO | Chunk: 18 | WER=100.000000 | S=3 D=0 I=0
2026-01-28 13:59:19,337 | INFO | Chunk: 19 | WER=42.857143 | S=0 D=2 I=1
2026-01-28 13:59:19,338 | INFO | Chunk: 20 | WER=50.000000 | S=1 D=4 I=5
2026-01-28 13:59:19,338 | INFO | Chunk: 21 | WER=70.000000 | S=2 D=8 I=4
2026-01-28 13:59:19,338 | INFO | Chunk: 22 | WER=100.000000 | S=1 D=4 I=0
2026-01-28 13:59:19,338 | INFO | Chunk: 23 | WER=100.000000 | S=6 D=3 I=1
2026-01-28 13:59:19,339 | INFO | Chunk: 24 | WER=33.333333 | S=0 D=3 I=4
2026-01-28 13:59:19,340 | INFO | Chunk: 25 | WER=47.826087 | S=4 D=16 I=2
2026-01-28 13:59:19,340 | INFO | Chunk: 26 | WER=110.000000 | S=10 D=0 I=1
2026-01-28 13:59:19,459 | INFO | File: Rhap-D0008.wav | WER=38.868613 | S=88 D=84 I=41
2026-01-28 13:59:19,459 | INFO | ------------------------------
2026-01-28 13:59:19,459 | INFO | w2vec vad chunk Done!
2026-01-28 13:59:43,611 | INFO | Chunk: 0 | WER=15.789474 | S=1 D=0 I=2
2026-01-28 13:59:43,612 | INFO | Chunk: 1 | WER=80.000000 | S=0 D=56 I=0
2026-01-28 13:59:43,612 | INFO | Chunk: 2 | WER=100.000000 | S=2 D=1 I=0
2026-01-28 13:59:43,613 | INFO | Chunk: 3 | WER=60.000000 | S=0 D=1 I=2
2026-01-28 13:59:43,614 | INFO | Chunk: 4 | WER=46.153846 | S=9 D=5 I=4
2026-01-28 13:59:43,615 | INFO | Chunk: 5 | WER=35.294118 | S=9 D=9 I=0
2026-01-28 13:59:43,615 | INFO | Chunk: 6 | WER=55.000000 | S=2 D=3 I=6
2026-01-28 13:59:43,616 | INFO | Chunk: 7 | WER=66.666667 | S=0 D=3 I=1
2026-01-28 13:59:43,616 | INFO | Chunk: 8 | WER=140.000000 | S=0 D=2 I=5
2026-01-28 13:59:43,617 | INFO | Chunk: 9 | WER=37.142857 | S=3 D=6 I=4
2026-01-28 13:59:43,617 | INFO | Chunk: 10 | WER=41.176471 | S=3 D=3 I=1
2026-01-28 13:59:43,617 | INFO | Chunk: 11 | WER=13.636364 | S=0 D=1 I=2
2026-01-28 13:59:43,618 | INFO | Chunk: 12 | WER=11.111111 | S=0 D=0 I=1
2026-01-28 13:59:43,618 | INFO | Chunk: 13 | WER=31.034483 | S=0 D=6 I=3
2026-01-28 13:59:43,619 | INFO | Chunk: 14 | WER=64.062500 | S=4 D=37 I=0
2026-01-28 13:59:43,620 | INFO | Chunk: 15 | WER=114.285714 | S=2 D=2 I=4
2026-01-28 13:59:43,620 | INFO | Chunk: 16 | WER=400.000000 | S=1 D=0 I=3
2026-01-28 13:59:43,620 | INFO | Chunk: 17 | WER=150.000000 | S=4 D=0 I=2
2026-01-28 13:59:43,620 | INFO | Chunk: 18 | WER=100.000000 | S=3 D=0 I=0
2026-01-28 13:59:43,620 | INFO | Chunk: 19 | WER=71.428571 | S=1 D=3 I=1
2026-01-28 13:59:43,621 | INFO | Chunk: 20 | WER=40.000000 | S=0 D=3 I=5
2026-01-28 13:59:43,621 | INFO | Chunk: 21 | WER=60.000000 | S=1 D=7 I=4
2026-01-28 13:59:43,621 | INFO | Chunk: 22 | WER=100.000000 | S=1 D=4 I=0
2026-01-28 13:59:43,622 | INFO | Chunk: 23 | WER=100.000000 | S=0 D=5 I=5
2026-01-28 13:59:43,622 | INFO | Chunk: 24 | WER=47.619048 | S=1 D=3 I=6
2026-01-28 13:59:43,623 | INFO | Chunk: 25 | WER=39.130435 | S=0 D=18 I=0
2026-01-28 13:59:43,623 | INFO | Chunk: 26 | WER=90.000000 | S=0 D=6 I=3
2026-01-28 13:59:43,722 | INFO | File: Rhap-D0008.wav | WER=43.795620 | S=62 D=149 I=29
2026-01-28 13:59:43,722 | INFO | ------------------------------
2026-01-28 13:59:43,722 | INFO | whisper med Done!
2026-01-28 14:00:19,474 | INFO | Chunk: 0 | WER=21.052632 | S=1 D=1 I=2
2026-01-28 14:00:19,476 | INFO | Chunk: 1 | WER=42.857143 | S=13 D=16 I=1
2026-01-28 14:00:19,476 | INFO | Chunk: 2 | WER=100.000000 | S=2 D=1 I=0
2026-01-28 14:00:19,477 | INFO | Chunk: 3 | WER=60.000000 | S=0 D=1 I=2
2026-01-28 14:00:19,477 | INFO | Chunk: 4 | WER=53.846154 | S=9 D=8 I=4
2026-01-28 14:00:19,479 | INFO | Chunk: 5 | WER=11.764706 | S=1 D=1 I=4
2026-01-28 14:00:19,479 | INFO | Chunk: 6 | WER=55.000000 | S=2 D=2 I=7
2026-01-28 14:00:19,480 | INFO | Chunk: 7 | WER=66.666667 | S=1 D=2 I=1
2026-01-28 14:00:19,480 | INFO | Chunk: 8 | WER=140.000000 | S=0 D=2 I=5
2026-01-28 14:00:19,481 | INFO | Chunk: 9 | WER=28.571429 | S=0 D=6 I=4
2026-01-28 14:00:19,481 | INFO | Chunk: 10 | WER=47.058824 | S=4 D=3 I=1
2026-01-28 14:00:19,482 | INFO | Chunk: 11 | WER=13.636364 | S=0 D=1 I=2
2026-01-28 14:00:19,482 | INFO | Chunk: 12 | WER=11.111111 | S=0 D=0 I=1
2026-01-28 14:00:19,483 | INFO | Chunk: 13 | WER=27.586207 | S=0 D=5 I=3
2026-01-28 14:00:19,484 | INFO | Chunk: 14 | WER=65.625000 | S=4 D=38 I=0
2026-01-28 14:00:19,484 | INFO | Chunk: 15 | WER=114.285714 | S=2 D=2 I=4
2026-01-28 14:00:19,484 | INFO | Chunk: 16 | WER=400.000000 | S=1 D=0 I=3
2026-01-28 14:00:19,484 | INFO | Chunk: 17 | WER=150.000000 | S=4 D=0 I=2
2026-01-28 14:00:19,484 | INFO | Chunk: 18 | WER=100.000000 | S=3 D=0 I=0
2026-01-28 14:00:19,485 | INFO | Chunk: 19 | WER=85.714286 | S=1 D=3 I=2
2026-01-28 14:00:19,485 | INFO | Chunk: 20 | WER=50.000000 | S=1 D=4 I=5
2026-01-28 14:00:19,486 | INFO | Chunk: 21 | WER=75.000000 | S=1 D=7 I=7
2026-01-28 14:00:19,486 | INFO | Chunk: 22 | WER=100.000000 | S=1 D=4 I=0
2026-01-28 14:00:19,486 | INFO | Chunk: 23 | WER=90.000000 | S=6 D=3 I=0
2026-01-28 14:00:19,487 | INFO | Chunk: 24 | WER=47.619048 | S=1 D=3 I=6
2026-01-28 14:00:19,487 | INFO | Chunk: 25 | WER=54.347826 | S=0 D=25 I=0
2026-01-28 14:00:19,488 | INFO | Chunk: 26 | WER=100.000000 | S=0 D=5 I=5
2026-01-28 14:00:19,598 | INFO | File: Rhap-D0008.wav | WER=37.956204 | S=88 D=96 I=24
2026-01-28 14:00:19,598 | INFO | ------------------------------
2026-01-28 14:00:19,598 | INFO | whisper large Done!
2026-01-28 14:00:19,756 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 14:00:19,786 | INFO | Vocabulary size: 350
2026-01-28 14:00:20,344 | INFO | Gradient checkpoint layers: []
2026-01-28 14:00:21,279 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 14:00:21,282 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 14:00:21,282 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 14:00:21,283 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 14:00:21,283 | INFO | speech length: 115840
2026-01-28 14:00:21,333 | INFO | decoder input length: 180
2026-01-28 14:00:21,333 | INFO | max output length: 180
2026-01-28 14:00:21,333 | INFO | min output length: 18
2026-01-28 14:00:27,952 | INFO | end detected at 58
2026-01-28 14:00:27,954 | INFO |  -4.81 * 0.5 =  -2.40 for decoder
2026-01-28 14:00:27,954 | INFO |  -3.93 * 0.5 =  -1.97 for ctc
2026-01-28 14:00:27,954 | INFO | total log probability: -4.37
2026-01-28 14:00:27,954 | INFO | normalized log probability: -0.08
2026-01-28 14:00:27,954 | INFO | total number of ended hypotheses: 159
2026-01-28 14:00:27,955 | INFO | best hypo: ▁voilà▁en▁fait▁je▁j'aurais▁voulu▁savoir▁comment▁je▁pouvais▁me▁rendre▁à▁pied▁au▁départ▁du▁téléphérique▁de▁grenoble

2026-01-28 14:00:27,958 | INFO | speech length: 385599
2026-01-28 14:00:27,995 | INFO | decoder input length: 601
2026-01-28 14:00:27,995 | INFO | max output length: 601
2026-01-28 14:00:27,995 | INFO | min output length: 60
2026-01-28 14:00:49,101 | INFO | end detected at 148
2026-01-28 14:00:49,102 | INFO | -292.58 * 0.5 = -146.29 for decoder
2026-01-28 14:00:49,102 | INFO | -35.70 * 0.5 = -17.85 for ctc
2026-01-28 14:00:49,102 | INFO | total log probability: -164.14
2026-01-28 14:00:49,103 | INFO | normalized log probability: -1.16
2026-01-28 14:00:49,103 | INFO | total number of ended hypotheses: 157
2026-01-28 14:00:49,104 | INFO | best hypo: ▁alors▁les▁bulles▁alors▁là▁en▁la▁nef▁avant▁alors▁s'est▁pas▁compliqué▁donc▁là▁vous▁partez▁dans▁sa▁direction▁toujours▁tout▁droit▁pendant▁dix▁moments▁où▁elle▁allait▁toujours▁aller▁tout▁droit▁vous▁allez▁passer▁devant▁la▁poste▁et▁sera▁votre▁droite▁et▁un▁peu▁plus▁loin▁chez▁pâques▁à▁niveau▁s'actement▁habita▁et▁la▁chambre▁de▁commerce▁à▁vote▁gauche

2026-01-28 14:00:49,106 | INFO | speech length: 17280
2026-01-28 14:00:49,141 | INFO | decoder input length: 26
2026-01-28 14:00:49,141 | INFO | max output length: 26
2026-01-28 14:00:49,141 | INFO | min output length: 2
2026-01-28 14:00:50,251 | INFO | end detected at 12
2026-01-28 14:00:50,251 | INFO |  -0.50 * 0.5 =  -0.25 for decoder
2026-01-28 14:00:50,252 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-28 14:00:50,252 | INFO | total log probability: -0.25
2026-01-28 14:00:50,252 | INFO | normalized log probability: -0.03
2026-01-28 14:00:50,252 | INFO | total number of ended hypotheses: 132
2026-01-28 14:00:50,252 | INFO | best hypo: ▁vous▁continuez

2026-01-28 14:00:50,253 | INFO | speech length: 41120
2026-01-28 14:00:50,296 | INFO | decoder input length: 63
2026-01-28 14:00:50,296 | INFO | max output length: 63
2026-01-28 14:00:50,296 | INFO | min output length: 6
2026-01-28 14:00:52,306 | INFO | end detected at 20
2026-01-28 14:00:52,306 | INFO |  -1.11 * 0.5 =  -0.56 for decoder
2026-01-28 14:00:52,307 | INFO |  -0.44 * 0.5 =  -0.22 for ctc
2026-01-28 14:00:52,307 | INFO | total log probability: -0.78
2026-01-28 14:00:52,307 | INFO | normalized log probability: -0.05
2026-01-28 14:00:52,307 | INFO | total number of ended hypotheses: 137
2026-01-28 14:00:52,307 | INFO | best hypo: ▁là▁vous▁remontez▁le▁boulevard

2026-01-28 14:00:52,308 | INFO | speech length: 185280
2026-01-28 14:00:52,341 | INFO | decoder input length: 289
2026-01-28 14:00:52,341 | INFO | max output length: 289
2026-01-28 14:00:52,341 | INFO | min output length: 28
2026-01-28 14:01:04,339 | INFO | end detected at 99
2026-01-28 14:01:04,341 | INFO | -100.32 * 0.5 = -50.16 for decoder
2026-01-28 14:01:04,341 | INFO | -46.21 * 0.5 = -23.11 for ctc
2026-01-28 14:01:04,341 | INFO | total log probability: -73.27
2026-01-28 14:01:04,341 | INFO | normalized log probability: -0.79
2026-01-28 14:01:04,341 | INFO | total number of ended hypotheses: 188
2026-01-28 14:01:04,342 | INFO | best hypo: ▁machin▁s'en▁bat▁chez▁pas▁comment▁il▁s'appelle▁son▁pas▁qui▁de▁chose▁à▁agutsomba▁ou▁à▁je▁crois▁si▁ça▁tout▁meilleur▁dans▁ce▁boulevard▁vous▁remonter▁et▁votre▁gauche▁vous▁allez▁passer▁devant▁de▁lycée▁champollion

2026-01-28 14:01:04,344 | INFO | speech length: 273920
2026-01-28 14:01:04,381 | INFO | decoder input length: 427
2026-01-28 14:01:04,381 | INFO | max output length: 427
2026-01-28 14:01:04,381 | INFO | min output length: 42
2026-01-28 14:01:20,230 | INFO | end detected at 127
2026-01-28 14:01:20,231 | INFO | -179.21 * 0.5 = -89.60 for decoder
2026-01-28 14:01:20,231 | INFO | -88.30 * 0.5 = -44.15 for ctc
2026-01-28 14:01:20,231 | INFO | total log probability: -133.75
2026-01-28 14:01:20,231 | INFO | normalized log probability: -1.09
2026-01-28 14:01:20,231 | INFO | total number of ended hypotheses: 134
2026-01-28 14:01:20,232 | INFO | best hypo: ▁heuh▁ces▁bons▁là▁vous▁avez▁compris▁jusqu'à▁présent▁oui▁paprobloyez▁alors▁à▁la▁fin▁de▁ce▁boulevard▁vous▁à▁l'arrivée▁alors▁au▁niveau▁du▁tram▁et▁vous▁allez▁rejoindre▁le▁tram▁la▁ligne▁ah▁oube▁fin▁les▁deux▁où▁elle▁passe▁et▁à▁ce▁nivolas▁vous▁allez▁vous▁tourner▁à▁votre▁droite

2026-01-28 14:01:20,234 | INFO | speech length: 82880
2026-01-28 14:01:20,268 | INFO | decoder input length: 129
2026-01-28 14:01:20,268 | INFO | max output length: 129
2026-01-28 14:01:20,268 | INFO | min output length: 12
2026-01-28 14:01:25,522 | INFO | end detected at 53
2026-01-28 14:01:25,523 | INFO |  -3.76 * 0.5 =  -1.88 for decoder
2026-01-28 14:01:25,523 | INFO |  -0.76 * 0.5 =  -0.38 for ctc
2026-01-28 14:01:25,523 | INFO | total log probability: -2.26
2026-01-28 14:01:25,523 | INFO | normalized log probability: -0.05
2026-01-28 14:01:25,523 | INFO | total number of ended hypotheses: 144
2026-01-28 14:01:25,524 | INFO | best hypo: ▁vous▁allez▁longer▁les▁quais▁vous▁avez▁passé▁devant▁une▁banque▁à▁l'angle▁y▁a▁une▁banque▁pour▁vous▁repérer▁les▁quais

2026-01-28 14:01:25,526 | INFO | speech length: 39840
2026-01-28 14:01:25,565 | INFO | decoder input length: 61
2026-01-28 14:01:25,565 | INFO | max output length: 61
2026-01-28 14:01:25,565 | INFO | min output length: 6
2026-01-28 14:01:27,537 | INFO | end detected at 20
2026-01-28 14:01:27,539 | INFO |  -3.59 * 0.5 =  -1.80 for decoder
2026-01-28 14:01:27,539 | INFO |  -6.47 * 0.5 =  -3.24 for ctc
2026-01-28 14:01:27,539 | INFO | total log probability: -5.03
2026-01-28 14:01:27,539 | INFO | normalized log probability: -0.50
2026-01-28 14:01:27,539 | INFO | total number of ended hypotheses: 213
2026-01-28 14:01:27,539 | INFO | best hypo: ▁les▁quais▁du▁tram▁ad

2026-01-28 14:01:27,541 | INFO | speech length: 37600
2026-01-28 14:01:27,570 | INFO | decoder input length: 58
2026-01-28 14:01:27,570 | INFO | max output length: 58
2026-01-28 14:01:27,570 | INFO | min output length: 5
2026-01-28 14:01:29,643 | INFO | end detected at 24
2026-01-28 14:01:29,644 | INFO |  -1.36 * 0.5 =  -0.68 for decoder
2026-01-28 14:01:29,644 | INFO |  -1.37 * 0.5 =  -0.69 for ctc
2026-01-28 14:01:29,644 | INFO | total log probability: -1.37
2026-01-28 14:01:29,644 | INFO | normalized log probability: -0.07
2026-01-28 14:01:29,644 | INFO | total number of ended hypotheses: 163
2026-01-28 14:01:29,644 | INFO | best hypo: ▁vous▁allez▁longer▁donc▁les▁rails▁du▁tram

2026-01-28 14:01:29,646 | INFO | speech length: 167040
2026-01-28 14:01:29,681 | INFO | decoder input length: 260
2026-01-28 14:01:29,681 | INFO | max output length: 260
2026-01-28 14:01:29,681 | INFO | min output length: 26
2026-01-28 14:01:37,803 | INFO | end detected at 75
2026-01-28 14:01:37,804 | INFO | -13.90 * 0.5 =  -6.95 for decoder
2026-01-28 14:01:37,804 | INFO | -15.53 * 0.5 =  -7.76 for ctc
2026-01-28 14:01:37,804 | INFO | total log probability: -14.71
2026-01-28 14:01:37,804 | INFO | normalized log probability: -0.21
2026-01-28 14:01:37,804 | INFO | total number of ended hypotheses: 144
2026-01-28 14:01:37,805 | INFO | best hypo: ▁aidé▁que▁vous▁voyez▁le▁magdo▁et▁la▁fnac▁ça▁seravote▁gauche▁vous▁pouvez▁traverser▁les▁rails▁à▁ce▁moment▁là▁parce▁que▁vous▁serez▁du▁bon▁côté▁pour▁eux▁plus▁tard

2026-01-28 14:01:37,807 | INFO | speech length: 106080
2026-01-28 14:01:37,842 | INFO | decoder input length: 165
2026-01-28 14:01:37,843 | INFO | max output length: 165
2026-01-28 14:01:37,843 | INFO | min output length: 16
2026-01-28 14:01:42,379 | INFO | end detected at 46
2026-01-28 14:01:42,380 | INFO |  -5.30 * 0.5 =  -2.65 for decoder
2026-01-28 14:01:42,380 | INFO |  -1.91 * 0.5 =  -0.95 for ctc
2026-01-28 14:01:42,380 | INFO | total log probability: -3.61
2026-01-28 14:01:42,380 | INFO | normalized log probability: -0.09
2026-01-28 14:01:42,380 | INFO | total number of ended hypotheses: 139
2026-01-28 14:01:42,381 | INFO | best hypo: ▁vous▁continuez▁vous▁passez▁devant▁un▁petit▁manège▁aussi▁vous▁longez▁les▁galeries▁la▁fayette▁oui

2026-01-28 14:01:42,382 | INFO | speech length: 134880
2026-01-28 14:01:42,417 | INFO | decoder input length: 210
2026-01-28 14:01:42,417 | INFO | max output length: 210
2026-01-28 14:01:42,417 | INFO | min output length: 21
2026-01-28 14:01:48,708 | INFO | end detected at 58
2026-01-28 14:01:48,710 | INFO | -10.17 * 0.5 =  -5.08 for decoder
2026-01-28 14:01:48,710 | INFO |  -9.21 * 0.5 =  -4.60 for ctc
2026-01-28 14:01:48,710 | INFO | total log probability: -9.69
2026-01-28 14:01:48,710 | INFO | normalized log probability: -0.19
2026-01-28 14:01:48,710 | INFO | total number of ended hypotheses: 187
2026-01-28 14:01:48,710 | INFO | best hypo: ▁là▁vous▁allez▁arriva▁à▁une▁place▁ou▁y▁a▁plein▁de▁café▁brasseries▁vous▁allez▁longer▁les▁terrasses▁de▁café

2026-01-28 14:01:48,712 | INFO | speech length: 67040
2026-01-28 14:01:48,749 | INFO | decoder input length: 104
2026-01-28 14:01:48,749 | INFO | max output length: 104
2026-01-28 14:01:48,749 | INFO | min output length: 10
2026-01-28 14:01:51,729 | INFO | end detected at 31
2026-01-28 14:01:51,729 | INFO |  -2.18 * 0.5 =  -1.09 for decoder
2026-01-28 14:01:51,729 | INFO |  -1.58 * 0.5 =  -0.79 for ctc
2026-01-28 14:01:51,729 | INFO | total log probability: -1.88
2026-01-28 14:01:51,729 | INFO | normalized log probability: -0.07
2026-01-28 14:01:51,730 | INFO | total number of ended hypotheses: 147
2026-01-28 14:01:51,730 | INFO | best hypo: ▁vous▁allez▁passer▁aussi▁pour▁vous▁repérer▁une▁boutique▁hagendas

2026-01-28 14:01:51,731 | INFO | speech length: 111200
2026-01-28 14:01:51,766 | INFO | decoder input length: 173
2026-01-28 14:01:51,767 | INFO | max output length: 173
2026-01-28 14:01:51,767 | INFO | min output length: 17
2026-01-28 14:01:57,856 | INFO | end detected at 64
2026-01-28 14:01:57,857 | INFO | -13.19 * 0.5 =  -6.60 for decoder
2026-01-28 14:01:57,858 | INFO |  -9.17 * 0.5 =  -4.58 for ctc
2026-01-28 14:01:57,858 | INFO | total log probability: -11.18
2026-01-28 14:01:57,858 | INFO | normalized log probability: -0.20
2026-01-28 14:01:57,858 | INFO | total number of ended hypotheses: 199
2026-01-28 14:01:57,858 | INFO | best hypo: ▁et▁là▁vous▁allez▁voir▁nous▁all▁arriver▁une▁autre▁place▁où▁y▁a▁une▁grande▁fontaine▁pouvez▁pas▁la▁louper▁et▁la▁boutique▁françoisir

2026-01-28 14:01:57,860 | INFO | speech length: 263360
2026-01-28 14:01:57,889 | INFO | decoder input length: 411
2026-01-28 14:01:57,890 | INFO | max output length: 411
2026-01-28 14:01:57,890 | INFO | min output length: 41
2026-01-28 14:02:13,969 | INFO | end detected at 134
2026-01-28 14:02:13,971 | INFO | -209.36 * 0.5 = -104.68 for decoder
2026-01-28 14:02:13,971 | INFO | -80.10 * 0.5 = -40.05 for ctc
2026-01-28 14:02:13,971 | INFO | total log probability: -144.73
2026-01-28 14:02:13,971 | INFO | normalized log probability: -1.13
2026-01-28 14:02:13,971 | INFO | total number of ended hypotheses: 173
2026-01-28 14:02:13,972 | INFO | best hypo: ▁et▁à▁ce▁nivolas▁si▁tu▁entres▁hagendas▁et▁françoisir▁y▁a▁une▁rue▁vous▁allez▁à▁traverser▁vous▁avez▁passer▁sous▁un▁petit▁porche▁on▁traverse▁la▁rue▁oui▁on▁traversezre▁cette▁rue▁et▁le▁porche▁onù▁pouvez▁pallouper▁les▁juste▁en▁face▁ou▁passer▁dessous▁et▁un▁petit▁kiosque▁à▁journo▁nalçu

2026-01-28 14:02:13,974 | INFO | speech length: 38080
2026-01-28 14:02:14,009 | INFO | decoder input length: 59
2026-01-28 14:02:14,009 | INFO | max output length: 59
2026-01-28 14:02:14,009 | INFO | min output length: 5
2026-01-28 14:02:15,960 | INFO | end detected at 23
2026-01-28 14:02:15,962 | INFO |  -2.92 * 0.5 =  -1.46 for decoder
2026-01-28 14:02:15,962 | INFO |  -6.88 * 0.5 =  -3.44 for ctc
2026-01-28 14:02:15,962 | INFO | total log probability: -4.90
2026-01-28 14:02:15,962 | INFO | normalized log probability: -0.27
2026-01-28 14:02:15,962 | INFO | total number of ended hypotheses: 176
2026-01-28 14:02:15,962 | INFO | best hypo: ▁et▁en▁passant▁sport▁je▁vous▁à▁l'arrivée

2026-01-28 14:02:15,964 | INFO | speech length: 15520
2026-01-28 14:02:16,007 | INFO | decoder input length: 23
2026-01-28 14:02:16,007 | INFO | max output length: 23
2026-01-28 14:02:16,007 | INFO | min output length: 2
2026-01-28 14:02:17,182 | INFO | end detected at 14
2026-01-28 14:02:17,183 | INFO |  -1.04 * 0.5 =  -0.52 for decoder
2026-01-28 14:02:17,183 | INFO |  -1.32 * 0.5 =  -0.66 for ctc
2026-01-28 14:02:17,184 | INFO | total log probability: -1.18
2026-01-28 14:02:17,184 | INFO | normalized log probability: -0.13
2026-01-28 14:02:17,184 | INFO | total number of ended hypotheses: 153
2026-01-28 14:02:17,184 | INFO | best hypo: ▁au▁jardin▁de▁ville

2026-01-28 14:02:17,185 | INFO | speech length: 26880
2026-01-28 14:02:17,213 | INFO | decoder input length: 41
2026-01-28 14:02:17,214 | INFO | max output length: 41
2026-01-28 14:02:17,214 | INFO | min output length: 4
2026-01-28 14:02:19,292 | INFO | end detected at 23
2026-01-28 14:02:19,293 | INFO |  -2.50 * 0.5 =  -1.25 for decoder
2026-01-28 14:02:19,293 | INFO |  -2.04 * 0.5 =  -1.02 for ctc
2026-01-28 14:02:19,293 | INFO | total log probability: -2.27
2026-01-28 14:02:19,293 | INFO | normalized log probability: -0.13
2026-01-28 14:02:19,294 | INFO | total number of ended hypotheses: 177
2026-01-28 14:02:19,294 | INFO | best hypo: ▁de▁grenoble▁on▁passe▁devant

2026-01-28 14:02:19,296 | INFO | speech length: 15200
2026-01-28 14:02:19,328 | INFO | decoder input length: 23
2026-01-28 14:02:19,328 | INFO | max output length: 23
2026-01-28 14:02:19,328 | INFO | min output length: 2
2026-01-28 14:02:20,611 | INFO | end detected at 15
2026-01-28 14:02:20,612 | INFO |  -2.19 * 0.5 =  -1.09 for decoder
2026-01-28 14:02:20,612 | INFO |  -7.99 * 0.5 =  -3.99 for ctc
2026-01-28 14:02:20,612 | INFO | total log probability: -5.09
2026-01-28 14:02:20,612 | INFO | normalized log probability: -0.57
2026-01-28 14:02:20,613 | INFO | total number of ended hypotheses: 155
2026-01-28 14:02:20,613 | INFO | best hypo: ▁oui▁passez▁donc

2026-01-28 14:02:20,614 | INFO | speech length: 41440
2026-01-28 14:02:20,646 | INFO | decoder input length: 64
2026-01-28 14:02:20,646 | INFO | max output length: 64
2026-01-28 14:02:20,646 | INFO | min output length: 6
2026-01-28 14:02:22,479 | INFO | end detected at 20
2026-01-28 14:02:22,481 | INFO |  -2.28 * 0.5 =  -1.14 for decoder
2026-01-28 14:02:22,481 | INFO |  -4.95 * 0.5 =  -2.48 for ctc
2026-01-28 14:02:22,481 | INFO | total log probability: -3.62
2026-01-28 14:02:22,481 | INFO | normalized log probability: -0.24
2026-01-28 14:02:22,481 | INFO | total number of ended hypotheses: 167
2026-01-28 14:02:22,481 | INFO | best hypo: ▁on▁vous▁arrivez▁dans▁le▁jardin

2026-01-28 14:02:22,483 | INFO | speech length: 105280
2026-01-28 14:02:22,512 | INFO | decoder input length: 164
2026-01-28 14:02:22,513 | INFO | max output length: 164
2026-01-28 14:02:22,513 | INFO | min output length: 16
2026-01-28 14:02:27,509 | INFO | end detected at 52
2026-01-28 14:02:27,510 | INFO |  -3.73 * 0.5 =  -1.86 for decoder
2026-01-28 14:02:27,510 | INFO |  -3.50 * 0.5 =  -1.75 for ctc
2026-01-28 14:02:27,510 | INFO | total log probability: -3.61
2026-01-28 14:02:27,510 | INFO | normalized log probability: -0.08
2026-01-28 14:02:27,510 | INFO | total number of ended hypotheses: 148
2026-01-28 14:02:27,511 | INFO | best hypo: ▁vous▁allez▁vous▁diriger▁vers▁le▁fond▁des▁jardins▁de▁ville▁en▁restant▁en▁y▁allant▁mais▁en▁restant▁sur▁la▁gauche

2026-01-28 14:02:27,513 | INFO | speech length: 85440
2026-01-28 14:02:27,548 | INFO | decoder input length: 133
2026-01-28 14:02:27,548 | INFO | max output length: 133
2026-01-28 14:02:27,548 | INFO | min output length: 13
2026-01-28 14:02:32,373 | INFO | end detected at 53
2026-01-28 14:02:32,375 | INFO | -14.05 * 0.5 =  -7.03 for decoder
2026-01-28 14:02:32,375 | INFO | -14.94 * 0.5 =  -7.47 for ctc
2026-01-28 14:02:32,375 | INFO | total log probability: -14.49
2026-01-28 14:02:32,375 | INFO | normalized log probability: -0.34
2026-01-28 14:02:32,375 | INFO | total number of ended hypotheses: 210
2026-01-28 14:02:32,376 | INFO | best hypo: ▁vous▁longez▁mais▁en▁a▁vous▁dirigeant▁vers▁la▁gauche▁vous▁allez▁passer▁aussi▁devant▁au▁milieu▁hiard

2026-01-28 14:02:32,378 | INFO | speech length: 36640
2026-01-28 14:02:32,412 | INFO | decoder input length: 56
2026-01-28 14:02:32,412 | INFO | max output length: 56
2026-01-28 14:02:32,412 | INFO | min output length: 5
2026-01-28 14:02:34,984 | INFO | end detected at 29
2026-01-28 14:02:34,986 | INFO |  -4.52 * 0.5 =  -2.26 for decoder
2026-01-28 14:02:34,986 | INFO |  -6.18 * 0.5 =  -3.09 for ctc
2026-01-28 14:02:34,986 | INFO | total log probability: -5.35
2026-01-28 14:02:34,986 | INFO | normalized log probability: -0.22
2026-01-28 14:02:34,986 | INFO | total number of ended hypotheses: 176
2026-01-28 14:02:34,986 | INFO | best hypo: ▁elle▁ça▁un▁kiosque▁aussi▁une▁petite▁scène▁couverte

2026-01-28 14:02:34,988 | INFO | speech length: 95840
2026-01-28 14:02:35,018 | INFO | decoder input length: 149
2026-01-28 14:02:35,018 | INFO | max output length: 149
2026-01-28 14:02:35,018 | INFO | min output length: 14
2026-01-28 14:02:41,566 | INFO | end detected at 64
2026-01-28 14:02:41,567 | INFO |  -4.98 * 0.5 =  -2.49 for decoder
2026-01-28 14:02:41,567 | INFO |  -5.79 * 0.5 =  -2.89 for ctc
2026-01-28 14:02:41,567 | INFO | total log probability: -5.38
2026-01-28 14:02:41,567 | INFO | normalized log probability: -0.09
2026-01-28 14:02:41,567 | INFO | total number of ended hypotheses: 159
2026-01-28 14:02:41,568 | INFO | best hypo: ▁vous▁passez▁devant▁ça▁vous▁continuez▁toujours▁en▁restant▁sur▁votre▁gauche▁et▁au▁fond▁du▁jardin▁vous▁allez▁voir▁et▁à▁une▁sortie

2026-01-28 14:02:41,570 | INFO | speech length: 217440
2026-01-28 14:02:41,605 | INFO | decoder input length: 339
2026-01-28 14:02:41,605 | INFO | max output length: 339
2026-01-28 14:02:41,605 | INFO | min output length: 33
2026-01-28 14:02:54,732 | INFO | end detected at 101
2026-01-28 14:02:54,733 | INFO | -113.42 * 0.5 = -56.71 for decoder
2026-01-28 14:02:54,733 | INFO | -41.88 * 0.5 = -20.94 for ctc
2026-01-28 14:02:54,733 | INFO | total log probability: -77.65
2026-01-28 14:02:54,733 | INFO | normalized log probability: -0.83
2026-01-28 14:02:54,733 | INFO | total number of ended hypotheses: 167
2026-01-28 14:02:54,735 | INFO | best hypo: ▁qui▁donnent▁sur▁les▁quais▁n'est▁quai▁de▁grenoble▁oui▁et▁là▁vous▁allez▁voir▁ya▁le▁hialibule▁est▁le▁bâtiment▁du▁départ▁des▁bulles▁de▁grenobles▁vers▁les▁bus▁oui▁vous▁pouvez▁pas▁vous▁pouvez▁pas▁louper

2026-01-28 14:02:54,737 | INFO | speech length: 46720
2026-01-28 14:02:54,779 | INFO | decoder input length: 72
2026-01-28 14:02:54,779 | INFO | max output length: 72
2026-01-28 14:02:54,779 | INFO | min output length: 7
2026-01-28 14:02:58,631 | INFO | end detected at 35
2026-01-28 14:02:58,632 | INFO |  -2.71 * 0.5 =  -1.36 for decoder
2026-01-28 14:02:58,632 | INFO |  -0.54 * 0.5 =  -0.27 for ctc
2026-01-28 14:02:58,632 | INFO | total log probability: -1.63
2026-01-28 14:02:58,632 | INFO | normalized log probability: -0.05
2026-01-28 14:02:58,632 | INFO | total number of ended hypotheses: 152
2026-01-28 14:02:58,632 | INFO | best hypo: ▁voilà▁mais▁je▁remercie▁beaucoup▁de▁votre▁attention▁à▁de▁rien

2026-01-28 14:02:58,639 | INFO | Chunk: 0 | WER=15.789474 | S=1 D=0 I=2
2026-01-28 14:02:58,641 | INFO | Chunk: 1 | WER=38.571429 | S=15 D=9 I=3
2026-01-28 14:02:58,642 | INFO | Chunk: 2 | WER=100.000000 | S=2 D=1 I=0
2026-01-28 14:02:58,642 | INFO | Chunk: 3 | WER=40.000000 | S=0 D=1 I=1
2026-01-28 14:02:58,643 | INFO | Chunk: 4 | WER=64.102564 | S=12 D=6 I=7
2026-01-28 14:02:58,644 | INFO | Chunk: 5 | WER=50.980392 | S=12 D=5 I=9
2026-01-28 14:02:58,645 | INFO | Chunk: 6 | WER=55.000000 | S=2 D=3 I=6
2026-01-28 14:02:58,645 | INFO | Chunk: 7 | WER=66.666667 | S=3 D=1 I=0
2026-01-28 14:02:58,645 | INFO | Chunk: 8 | WER=140.000000 | S=0 D=2 I=5
2026-01-28 14:02:58,646 | INFO | Chunk: 9 | WER=45.714286 | S=3 D=8 I=5
2026-01-28 14:02:58,646 | INFO | Chunk: 10 | WER=35.294118 | S=1 D=3 I=2
2026-01-28 14:02:58,647 | INFO | Chunk: 11 | WER=31.818182 | S=2 D=3 I=2
2026-01-28 14:02:58,647 | INFO | Chunk: 12 | WER=11.111111 | S=0 D=0 I=1
2026-01-28 14:02:58,648 | INFO | Chunk: 13 | WER=41.379310 | S=2 D=7 I=3
2026-01-28 14:02:58,650 | INFO | Chunk: 14 | WER=51.562500 | S=19 D=12 I=2
2026-01-28 14:02:58,650 | INFO | Chunk: 15 | WER=114.285714 | S=2 D=2 I=4
2026-01-28 14:02:58,650 | INFO | Chunk: 16 | WER=400.000000 | S=1 D=0 I=3
2026-01-28 14:02:58,650 | INFO | Chunk: 17 | WER=125.000000 | S=2 D=1 I=2
2026-01-28 14:02:58,651 | INFO | Chunk: 18 | WER=100.000000 | S=3 D=0 I=0
2026-01-28 14:02:58,651 | INFO | Chunk: 19 | WER=57.142857 | S=1 D=2 I=1
2026-01-28 14:02:58,651 | INFO | Chunk: 20 | WER=40.000000 | S=0 D=3 I=5
2026-01-28 14:02:58,652 | INFO | Chunk: 21 | WER=50.000000 | S=0 D=6 I=4
2026-01-28 14:02:58,652 | INFO | Chunk: 22 | WER=100.000000 | S=9 D=1 I=0
2026-01-28 14:02:58,652 | INFO | Chunk: 23 | WER=42.857143 | S=0 D=3 I=6
2026-01-28 14:02:58,654 | INFO | Chunk: 24 | WER=54.347826 | S=14 D=9 I=2
2026-01-28 14:02:58,654 | INFO | Chunk: 25 | WER=110.000000 | S=10 D=0 I=1
2026-01-28 14:02:58,777 | INFO | File: Rhap-D0008.wav | WER=41.988950 | S=108 D=66 I=54
2026-01-28 14:02:58,777 | INFO | ------------------------------
2026-01-28 14:02:58,777 | INFO | Conf cv Done!
2026-01-28 14:02:58,938 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 14:02:58,956 | INFO | Vocabulary size: 47
2026-01-28 14:02:59,470 | INFO | Gradient checkpoint layers: []
2026-01-28 14:03:00,404 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 14:03:00,407 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 14:03:00,407 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 14:03:00,408 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 14:03:00,410 | INFO | speech length: 115840
2026-01-28 14:03:00,442 | INFO | decoder input length: 180
2026-01-28 14:03:00,442 | INFO | max output length: 180
2026-01-28 14:03:00,442 | INFO | min output length: 18
2026-01-28 14:03:13,370 | INFO | end detected at 119
2026-01-28 14:03:13,371 | INFO |  -9.05 * 0.5 =  -4.52 for decoder
2026-01-28 14:03:13,371 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-28 14:03:13,372 | INFO | total log probability: -4.53
2026-01-28 14:03:13,372 | INFO | normalized log probability: -0.04
2026-01-28 14:03:13,372 | INFO | total number of ended hypotheses: 179
2026-01-28 14:03:13,373 | INFO | best hypo: voilà<space>en<space>fait<space>je<space>j'aurais<space>voulu<space>savoir<space>comment<space>je<space>pouvais<space>me<space>rendre<space>à<space>pied<space>au<space>départ<space>du<space>téléphérique<space>de<space>grenoble

2026-01-28 14:03:13,375 | INFO | speech length: 385599
2026-01-28 14:03:13,402 | INFO | decoder input length: 601
2026-01-28 14:03:13,402 | INFO | max output length: 601
2026-01-28 14:03:13,402 | INFO | min output length: 60
2026-01-28 14:04:06,642 | INFO | end detected at 389
2026-01-28 14:04:06,644 | INFO | -162.60 * 0.5 = -81.30 for decoder
2026-01-28 14:04:06,644 | INFO | -14.87 * 0.5 =  -7.43 for ctc
2026-01-28 14:04:06,644 | INFO | total log probability: -88.73
2026-01-28 14:04:06,644 | INFO | normalized log probability: -0.23
2026-01-28 14:04:06,644 | INFO | total number of ended hypotheses: 180
2026-01-28 14:04:06,648 | INFO | best hypo: alors<space>euh<space>les<space>bulles<space>alors<space>là<space>on<space>est<space>où<space>euh<space>la<space>nef<space>avant<space>alors<space>c'est<space>pas<space>compliqué<space>donc<space>là<space>vous<space>partez<space>dans<space>cette<space>direction<space>toujours<space>tout<space>droit<space>pendant<space>un<space>petit<space>moment<space>vous<space>allez<space>toujours<space>aller<space>tout<space>droit<space>vous<space>allez<space>passer<space>devant<space>la<space>poste<space>qui<space>sera<space>votre<space>droite<space>et<space>un<space>peu<space>plus<space>loin<space>euh<space>je<space>sais<space>pas<space>quel<space>niveau<space>c'est<space>exactement<space>habitat<space>et<space>euh<space>la<space>chambre<space>de<space>de<space>commerce<space>à<space>votre<space>gauche

2026-01-28 14:04:06,650 | INFO | speech length: 17280
2026-01-28 14:04:06,680 | INFO | decoder input length: 26
2026-01-28 14:04:06,680 | INFO | max output length: 26
2026-01-28 14:04:06,680 | INFO | min output length: 2
2026-01-28 14:04:08,468 | INFO | end detected at 20
2026-01-28 14:04:08,469 | INFO |  -1.21 * 0.5 =  -0.61 for decoder
2026-01-28 14:04:08,469 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-28 14:04:08,469 | INFO | total log probability: -0.61
2026-01-28 14:04:08,469 | INFO | normalized log probability: -0.04
2026-01-28 14:04:08,469 | INFO | total number of ended hypotheses: 135
2026-01-28 14:04:08,469 | INFO | best hypo: vous<space>continuez

2026-01-28 14:04:08,471 | INFO | speech length: 41120
2026-01-28 14:04:08,498 | INFO | decoder input length: 63
2026-01-28 14:04:08,498 | INFO | max output length: 63
2026-01-28 14:04:08,498 | INFO | min output length: 6
2026-01-28 14:04:12,049 | INFO | end detected at 39
2026-01-28 14:04:12,050 | INFO |  -2.73 * 0.5 =  -1.36 for decoder
2026-01-28 14:04:12,050 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 14:04:12,050 | INFO | total log probability: -1.36
2026-01-28 14:04:12,050 | INFO | normalized log probability: -0.04
2026-01-28 14:04:12,050 | INFO | total number of ended hypotheses: 136
2026-01-28 14:04:12,051 | INFO | best hypo: là<space>vous<space>remontez<space>le<space>boulevard<space>euh

2026-01-28 14:04:12,053 | INFO | speech length: 185280
2026-01-28 14:04:12,085 | INFO | decoder input length: 289
2026-01-28 14:04:12,085 | INFO | max output length: 289
2026-01-28 14:04:12,085 | INFO | min output length: 28
2026-01-28 14:04:36,193 | INFO | end detected at 230
2026-01-28 14:04:36,195 | INFO | -34.88 * 0.5 = -17.44 for decoder
2026-01-28 14:04:36,195 | INFO | -18.14 * 0.5 =  -9.07 for ctc
2026-01-28 14:04:36,195 | INFO | total log probability: -26.51
2026-01-28 14:04:36,195 | INFO | normalized log probability: -0.12
2026-01-28 14:04:36,195 | INFO | total number of ended hypotheses: 200
2026-01-28 14:04:36,199 | INFO | best hypo: machin<space>sanba<space>je<space>sais<space>pas<space>comment<space>i<space>s'appelle<space>sans<space>pas<space>quelque<space>chose<space>a<space>agutsanba<space>oui<space>je<space>crois<space>que<space>c'est<space>ça<space>tout<space>manière<space>dans<space>ce<space>boulevard<space>vous<space>remontez<space>et<space>euh<space>votre<space>gauche<space>vous<space>avez<space>passé<space>euh<space>devant<space>le<space>lycée<space>champaulion

2026-01-28 14:04:36,203 | INFO | speech length: 273920
2026-01-28 14:04:36,235 | INFO | decoder input length: 427
2026-01-28 14:04:36,235 | INFO | max output length: 427
2026-01-28 14:04:36,235 | INFO | min output length: 42
2026-01-28 14:05:08,067 | INFO | end detected at 283
2026-01-28 14:05:08,069 | INFO | -45.24 * 0.5 = -22.62 for decoder
2026-01-28 14:05:08,069 | INFO | -22.91 * 0.5 = -11.45 for ctc
2026-01-28 14:05:08,069 | INFO | total log probability: -34.07
2026-01-28 14:05:08,069 | INFO | normalized log probability: -0.12
2026-01-28 14:05:08,069 | INFO | total number of ended hypotheses: 179
2026-01-28 14:05:08,072 | INFO | best hypo: euh<space>c'est<space>bon<space>là<space>vous<space>avez<space>compris<space>jusqu'à<space>présent<space>oui<space>ben<space>de<space>problèmerie<space>alors<space>à<space>la<space>fin<space>de<space>ce<space>boulevard<space>vous<space>allez<space>arriver<space>euh<space>au<space>niveau<space>du<space>trame<space>vous<space>allez<space>rejoindre<space>le<space>trame<space>la<space>ligne<space>euh<space>a<space>ou<space>b<space>fin<space>les<space>deux<space>elle<space>passe<space>et<space>à<space>ce<space>niveau<space>là<space>vous<space>allez<space>tourner<space>à<space>votre<space>droite

2026-01-28 14:05:08,074 | INFO | speech length: 82880
2026-01-28 14:05:08,104 | INFO | decoder input length: 129
2026-01-28 14:05:08,104 | INFO | max output length: 129
2026-01-28 14:05:08,104 | INFO | min output length: 12
2026-01-28 14:05:20,042 | INFO | end detected at 125
2026-01-28 14:05:20,044 | INFO | -22.75 * 0.5 = -11.38 for decoder
2026-01-28 14:05:20,044 | INFO | -15.60 * 0.5 =  -7.80 for ctc
2026-01-28 14:05:20,044 | INFO | total log probability: -19.17
2026-01-28 14:05:20,044 | INFO | normalized log probability: -0.16
2026-01-28 14:05:20,044 | INFO | total number of ended hypotheses: 188
2026-01-28 14:05:20,045 | INFO | best hypo: vous<space>allez<space>longer<space>les<space>pieds<space>vous<space>allez<space>passer<space>devant<space>u<space>une<space>banque<space>à<space>'angle<space>y<space>a<space>une<space>banque<space>pour<space>vous<space>repérer<space>le<space>quai

2026-01-28 14:05:20,048 | INFO | speech length: 39840
2026-01-28 14:05:20,078 | INFO | decoder input length: 61
2026-01-28 14:05:20,078 | INFO | max output length: 61
2026-01-28 14:05:20,078 | INFO | min output length: 6
2026-01-28 14:05:23,805 | INFO | end detected at 42
2026-01-28 14:05:23,806 | INFO |  -4.40 * 0.5 =  -2.20 for decoder
2026-01-28 14:05:23,806 | INFO |  -8.38 * 0.5 =  -4.19 for ctc
2026-01-28 14:05:23,806 | INFO | total log probability: -6.39
2026-01-28 14:05:23,806 | INFO | normalized log probability: -0.18
2026-01-28 14:05:23,806 | INFO | total number of ended hypotheses: 183
2026-01-28 14:05:23,807 | INFO | best hypo: euh<space>les<space>quais<space>du<space>drame<space>à<space>d'accord

2026-01-28 14:05:23,809 | INFO | speech length: 37600
2026-01-28 14:05:23,841 | INFO | decoder input length: 58
2026-01-28 14:05:23,841 | INFO | max output length: 58
2026-01-28 14:05:23,841 | INFO | min output length: 5
2026-01-28 14:05:28,732 | INFO | end detected at 54
2026-01-28 14:05:28,734 | INFO |  -5.46 * 0.5 =  -2.73 for decoder
2026-01-28 14:05:28,735 | INFO |  -3.01 * 0.5 =  -1.51 for ctc
2026-01-28 14:05:28,735 | INFO | total log probability: -4.24
2026-01-28 14:05:28,735 | INFO | normalized log probability: -0.09
2026-01-28 14:05:28,735 | INFO | total number of ended hypotheses: 200
2026-01-28 14:05:28,735 | INFO | best hypo: vous<space>allez<space>longer<space>euh<space>donc<space>les<space>rails<space>du<space>drame

2026-01-28 14:05:28,738 | INFO | speech length: 167040
2026-01-28 14:05:28,769 | INFO | decoder input length: 260
2026-01-28 14:05:28,770 | INFO | max output length: 260
2026-01-28 14:05:28,770 | INFO | min output length: 26
2026-01-28 14:05:46,619 | INFO | end detected at 182
2026-01-28 14:05:46,621 | INFO | -14.67 * 0.5 =  -7.33 for decoder
2026-01-28 14:05:46,621 | INFO |  -2.71 * 0.5 =  -1.36 for ctc
2026-01-28 14:05:46,621 | INFO | total log probability: -8.69
2026-01-28 14:05:46,621 | INFO | normalized log probability: -0.05
2026-01-28 14:05:46,621 | INFO | total number of ended hypotheses: 240
2026-01-28 14:05:46,623 | INFO | best hypo: et<space>dès<space>que<space>vous<space>voyez<space>euh<space>le<space>mac<space>do<space>et<space>la<space>fnac<space>ça<space>sera<space>votre<space>gauche<space>vous<space>pouvez<space>traverser<space>euh<space>les<space>rails<space>à<space>s<space>à<space>ce<space>moment<space>là<space>parce<space>que<space>vous<space>serez<space>du<space>bon<space>côté<space>pour<space>plus<space>tard

2026-01-28 14:05:46,626 | INFO | speech length: 106080
2026-01-28 14:05:46,656 | INFO | decoder input length: 165
2026-01-28 14:05:46,656 | INFO | max output length: 165
2026-01-28 14:05:46,656 | INFO | min output length: 16
2026-01-28 14:05:56,217 | INFO | end detected at 106
2026-01-28 14:05:56,219 | INFO |  -8.88 * 0.5 =  -4.44 for decoder
2026-01-28 14:05:56,219 | INFO |  -4.57 * 0.5 =  -2.28 for ctc
2026-01-28 14:05:56,219 | INFO | total log probability: -6.72
2026-01-28 14:05:56,219 | INFO | normalized log probability: -0.07
2026-01-28 14:05:56,219 | INFO | total number of ended hypotheses: 165
2026-01-28 14:05:56,220 | INFO | best hypo: vous<space>continuez<space>vous<space>passez<space>euh<space>devant<space>un<space>petit<space>manège<space>aussi<space>vous<space>longez<space>le<space>galerie<space>la<space>faillette<space>oui

2026-01-28 14:05:56,222 | INFO | speech length: 134880
2026-01-28 14:05:56,251 | INFO | decoder input length: 210
2026-01-28 14:05:56,251 | INFO | max output length: 210
2026-01-28 14:05:56,251 | INFO | min output length: 21
2026-01-28 14:06:08,616 | INFO | end detected at 134
2026-01-28 14:06:08,618 | INFO | -12.07 * 0.5 =  -6.04 for decoder
2026-01-28 14:06:08,618 | INFO |  -5.72 * 0.5 =  -2.86 for ctc
2026-01-28 14:06:08,618 | INFO | total log probability: -8.89
2026-01-28 14:06:08,618 | INFO | normalized log probability: -0.07
2026-01-28 14:06:08,618 | INFO | total number of ended hypotheses: 198
2026-01-28 14:06:08,620 | INFO | best hypo: là<space>vous<space>allez<space>arriver<space>à<space>une<space>place<space>euh<space>où<space>y<space>a<space>plein<space>de<space>cafés<space>de<space>brasserie<space>euh<space>vous<space>allez<space>longer<space>euh<space>les<space>terrasses<space>de<space>café<space>oui

2026-01-28 14:06:08,621 | INFO | speech length: 67040
2026-01-28 14:06:08,650 | INFO | decoder input length: 104
2026-01-28 14:06:08,651 | INFO | max output length: 104
2026-01-28 14:06:08,651 | INFO | min output length: 10
2026-01-28 14:06:15,736 | INFO | end detected at 78
2026-01-28 14:06:15,737 | INFO |  -7.64 * 0.5 =  -3.82 for decoder
2026-01-28 14:06:15,737 | INFO |  -1.21 * 0.5 =  -0.60 for ctc
2026-01-28 14:06:15,737 | INFO | total log probability: -4.42
2026-01-28 14:06:15,737 | INFO | normalized log probability: -0.06
2026-01-28 14:06:15,737 | INFO | total number of ended hypotheses: 176
2026-01-28 14:06:15,738 | INFO | best hypo: vous<space>allez<space>passer<space>aussi<space>pour<space>vous<space>repérer<space>euh<space>une<space>boutique<space>euh<space>agendas

2026-01-28 14:06:15,740 | INFO | speech length: 111200
2026-01-28 14:06:15,769 | INFO | decoder input length: 173
2026-01-28 14:06:15,769 | INFO | max output length: 173
2026-01-28 14:06:15,769 | INFO | min output length: 17
2026-01-28 14:06:29,907 | INFO | end detected at 150
2026-01-28 14:06:29,908 | INFO | -12.95 * 0.5 =  -6.48 for decoder
2026-01-28 14:06:29,908 | INFO | -10.75 * 0.5 =  -5.37 for ctc
2026-01-28 14:06:29,908 | INFO | total log probability: -11.85
2026-01-28 14:06:29,908 | INFO | normalized log probability: -0.08
2026-01-28 14:06:29,908 | INFO | total number of ended hypotheses: 183
2026-01-28 14:06:29,910 | INFO | best hypo: et<space>là<space>vous<space>allez<space>voir<space>vous<space>allez<space>arriver<space>une<space>autre<space>place<space>où<space>il<space>y<space>a<space>une<space>grande<space>fontaine<space>vous<space>pouvez<space>pas<space>la<space>louper<space>et<space>euh<space>la<space>boutique<space>françoisir

2026-01-28 14:06:29,912 | INFO | speech length: 263360
2026-01-28 14:06:29,941 | INFO | decoder input length: 411
2026-01-28 14:06:29,941 | INFO | max output length: 411
2026-01-28 14:06:29,941 | INFO | min output length: 41
2026-01-28 14:07:04,743 | INFO | end detected at 318
2026-01-28 14:07:04,746 | INFO | -39.16 * 0.5 = -19.58 for decoder
2026-01-28 14:07:04,746 | INFO | -21.62 * 0.5 = -10.81 for ctc
2026-01-28 14:07:04,746 | INFO | total log probability: -30.39
2026-01-28 14:07:04,746 | INFO | normalized log probability: -0.10
2026-01-28 14:07:04,746 | INFO | total number of ended hypotheses: 224
2026-01-28 14:07:04,750 | INFO | best hypo: et<space>à<space>ce<space>niveau<space>là<space>ça<space>se<space>situe<space>euh<space>entre<space>agenda<space>c'est<space>françoisir<space>y<space>a<space>une<space>rue<space>vous<space>allez<space>la<space>traverser<space>vous<space>avez<space>passé<space>sous<space>un<space>petit<space>porche<space>on<space>traverse<space>la<space>rue<space>oui<space>vous<space>traversez<space>cette<space>rue<space>et<space>le<space>le<space>porte<space>vous<space>pouvez<space>pas<space>louper<space>il<space>est<space>juste<space>en<space>face<space>ou<space>passer<space>dessous<space>il<space>y<space>a<space>un<space>petit<space>kiosque<space>à<space>journaux<space>na<space>le<space>sous

2026-01-28 14:07:04,753 | INFO | speech length: 38080
2026-01-28 14:07:04,797 | INFO | decoder input length: 59
2026-01-28 14:07:04,797 | INFO | max output length: 59
2026-01-28 14:07:04,797 | INFO | min output length: 5
2026-01-28 14:07:09,212 | INFO | end detected at 53
2026-01-28 14:07:09,214 | INFO |  -4.43 * 0.5 =  -2.22 for decoder
2026-01-28 14:07:09,214 | INFO |  -7.89 * 0.5 =  -3.95 for ctc
2026-01-28 14:07:09,214 | INFO | total log probability: -6.16
2026-01-28 14:07:09,214 | INFO | normalized log probability: -0.13
2026-01-28 14:07:09,214 | INFO | total number of ended hypotheses: 165
2026-01-28 14:07:09,215 | INFO | best hypo: et<space>en<space>passant<space>sport<space>je<space>vous<space>ai<space>à<space>l'arrivée<space>euh

2026-01-28 14:07:09,217 | INFO | speech length: 15520
2026-01-28 14:07:09,252 | INFO | decoder input length: 23
2026-01-28 14:07:09,252 | INFO | max output length: 23
2026-01-28 14:07:09,252 | INFO | min output length: 2
2026-01-28 14:07:11,074 | INFO | adding <eos> in the last position in the loop
2026-01-28 14:07:11,081 | INFO | no hypothesis. Finish decoding.
2026-01-28 14:07:11,082 | INFO |  -1.90 * 0.5 =  -0.95 for decoder
2026-01-28 14:07:11,082 | INFO |  -0.06 * 0.5 =  -0.03 for ctc
2026-01-28 14:07:11,082 | INFO | total log probability: -0.98
2026-01-28 14:07:11,082 | INFO | normalized log probability: -0.04
2026-01-28 14:07:11,082 | INFO | total number of ended hypotheses: 60
2026-01-28 14:07:11,083 | INFO | best hypo: euh<space>au<space>jardin<space>de<space>ville<sos/eos>

2026-01-28 14:07:11,083 | WARNING | best hypo length: 23 == max output length: 23
2026-01-28 14:07:11,083 | WARNING | decoding may be stopped by the max output length limitation, please consider to increase the maxlenratio.
2026-01-28 14:07:11,084 | INFO | speech length: 26880
2026-01-28 14:07:11,119 | INFO | decoder input length: 41
2026-01-28 14:07:11,119 | INFO | max output length: 41
2026-01-28 14:07:11,119 | INFO | min output length: 4
2026-01-28 14:07:14,226 | INFO | end detected at 37
2026-01-28 14:07:14,228 | INFO |  -4.53 * 0.5 =  -2.27 for decoder
2026-01-28 14:07:14,228 | INFO |  -1.63 * 0.5 =  -0.82 for ctc
2026-01-28 14:07:14,228 | INFO | total log probability: -3.08
2026-01-28 14:07:14,228 | INFO | normalized log probability: -0.11
2026-01-28 14:07:14,228 | INFO | total number of ended hypotheses: 211
2026-01-28 14:07:14,228 | INFO | best hypo: de<space>grenoble<space>on<space>basse<space>de<space>vol

2026-01-28 14:07:14,231 | INFO | speech length: 15200
2026-01-28 14:07:14,260 | INFO | decoder input length: 23
2026-01-28 14:07:14,260 | INFO | max output length: 23
2026-01-28 14:07:14,260 | INFO | min output length: 2
2026-01-28 14:07:16,065 | INFO | adding <eos> in the last position in the loop
2026-01-28 14:07:16,074 | INFO | no hypothesis. Finish decoding.
2026-01-28 14:07:16,075 | INFO |  -2.86 * 0.5 =  -1.43 for decoder
2026-01-28 14:07:16,075 | INFO |  -4.96 * 0.5 =  -2.48 for ctc
2026-01-28 14:07:16,075 | INFO | total log probability: -3.91
2026-01-28 14:07:16,075 | INFO | normalized log probability: -0.21
2026-01-28 14:07:16,075 | INFO | total number of ended hypotheses: 156
2026-01-28 14:07:16,076 | INFO | best hypo: oui<space>passez<space>devant

2026-01-28 14:07:16,078 | INFO | speech length: 41440
2026-01-28 14:07:16,113 | INFO | decoder input length: 64
2026-01-28 14:07:16,113 | INFO | max output length: 64
2026-01-28 14:07:16,113 | INFO | min output length: 6
2026-01-28 14:07:19,665 | INFO | end detected at 42
2026-01-28 14:07:19,667 | INFO |  -3.03 * 0.5 =  -1.51 for decoder
2026-01-28 14:07:19,667 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-28 14:07:19,667 | INFO | total log probability: -1.52
2026-01-28 14:07:19,667 | INFO | normalized log probability: -0.04
2026-01-28 14:07:19,667 | INFO | total number of ended hypotheses: 161
2026-01-28 14:07:19,667 | INFO | best hypo: donc<space>vous<space>arrivez<space>dans<space>le<space>le<space>jardin

2026-01-28 14:07:19,669 | INFO | speech length: 105280
2026-01-28 14:07:19,700 | INFO | decoder input length: 164
2026-01-28 14:07:19,700 | INFO | max output length: 164
2026-01-28 14:07:19,700 | INFO | min output length: 16
2026-01-28 14:07:31,150 | INFO | end detected at 125
2026-01-28 14:07:31,152 | INFO | -14.70 * 0.5 =  -7.35 for decoder
2026-01-28 14:07:31,152 | INFO |  -1.54 * 0.5 =  -0.77 for ctc
2026-01-28 14:07:31,152 | INFO | total log probability: -8.12
2026-01-28 14:07:31,152 | INFO | normalized log probability: -0.07
2026-01-28 14:07:31,152 | INFO | total number of ended hypotheses: 193
2026-01-28 14:07:31,153 | INFO | best hypo: vous<space>allez<space>vous<space>dioger<space>vers<space>le<space>fond<space>des<space>jardins<space>de<space>ville<space>en<space>restant<space>euh<space>en<space>en<space>y<space>allant<space>mais<space>en<space>restant<space>sur<space>la<space>gauche

2026-01-28 14:07:31,155 | INFO | speech length: 85440
2026-01-28 14:07:31,185 | INFO | decoder input length: 133
2026-01-28 14:07:31,185 | INFO | max output length: 133
2026-01-28 14:07:31,185 | INFO | min output length: 13
2026-01-28 14:07:40,954 | INFO | end detected at 116
2026-01-28 14:07:40,956 | INFO | -16.76 * 0.5 =  -8.38 for decoder
2026-01-28 14:07:40,956 | INFO | -10.03 * 0.5 =  -5.02 for ctc
2026-01-28 14:07:40,956 | INFO | total log probability: -13.40
2026-01-28 14:07:40,956 | INFO | normalized log probability: -0.12
2026-01-28 14:07:40,956 | INFO | total number of ended hypotheses: 197
2026-01-28 14:07:40,957 | INFO | best hypo: euh<space>vous<space>longez<space>mais<space>en<space>en<space>a<space>vous<space>dire<space>un<space>gens<space>vers<space>la<space>gauche<space>vous<space>allez<space>passer<space>d'aussi<space>devant<space>au<space>milieu<space>y<space>a

2026-01-28 14:07:40,959 | INFO | speech length: 36640
2026-01-28 14:07:40,989 | INFO | decoder input length: 56
2026-01-28 14:07:40,989 | INFO | max output length: 56
2026-01-28 14:07:40,989 | INFO | min output length: 5
2026-01-28 14:07:45,594 | INFO | adding <eos> in the last position in the loop
2026-01-28 14:07:45,603 | INFO | no hypothesis. Finish decoding.
2026-01-28 14:07:45,605 | INFO |  -6.40 * 0.5 =  -3.20 for decoder
2026-01-28 14:07:45,605 | INFO |  -7.92 * 0.5 =  -3.96 for ctc
2026-01-28 14:07:45,605 | INFO | total log probability: -7.16
2026-01-28 14:07:45,605 | INFO | normalized log probability: -0.14
2026-01-28 14:07:45,605 | INFO | total number of ended hypotheses: 178
2026-01-28 14:07:45,606 | INFO | best hypo: euh<space>ça<space>un<space>kiosque<space>aussi<space>une<space>petite<space>scène<space>couverte

2026-01-28 14:07:45,607 | INFO | speech length: 95840
2026-01-28 14:07:45,637 | INFO | decoder input length: 149
2026-01-28 14:07:45,638 | INFO | max output length: 149
2026-01-28 14:07:45,638 | INFO | min output length: 14
2026-01-28 14:07:57,672 | INFO | end detected at 132
2026-01-28 14:07:57,674 | INFO | -11.12 * 0.5 =  -5.56 for decoder
2026-01-28 14:07:57,674 | INFO |  -0.58 * 0.5 =  -0.29 for ctc
2026-01-28 14:07:57,674 | INFO | total log probability: -5.85
2026-01-28 14:07:57,674 | INFO | normalized log probability: -0.05
2026-01-28 14:07:57,674 | INFO | total number of ended hypotheses: 182
2026-01-28 14:07:57,675 | INFO | best hypo: vous<space>passez<space>devant<space>ça<space>vous<space>continuez<space>toujours<space>en<space>restant<space>sur<space>votre<space>cauche<space>et<space>au<space>fond<space>du<space>jardin<space>vous<space>allez<space>voir<space>y<space>a<space>une<space>sortie

2026-01-28 14:07:57,678 | INFO | speech length: 217440
2026-01-28 14:07:57,712 | INFO | decoder input length: 339
2026-01-28 14:07:57,713 | INFO | max output length: 339
2026-01-28 14:07:57,713 | INFO | min output length: 33
2026-01-28 14:08:23,712 | INFO | end detected at 236
2026-01-28 14:08:23,714 | INFO | -30.13 * 0.5 = -15.06 for decoder
2026-01-28 14:08:23,714 | INFO | -23.75 * 0.5 = -11.88 for ctc
2026-01-28 14:08:23,714 | INFO | total log probability: -26.94
2026-01-28 14:08:23,714 | INFO | normalized log probability: -0.12
2026-01-28 14:08:23,714 | INFO | total number of ended hypotheses: 212
2026-01-28 14:08:23,717 | INFO | best hypo: qui<space>donne<space>euh<space>sur<space>les<space>quais<space>l'équipe<space>de<space>grenoble<space>oui<space>et<space>là<space>euh<space>vous<space>allez<space>voir<space>y<space>a<space>le<space>y<space>a<space>les<space>bulles<space>y<space>a<space>le<space>bâtiment<space>euh<space>du<space>départ<space>des<space>bulles<space>euh<space>de<space>grenoble<space>on<space>verra<space>les<space>buts<space>oui<space>vous<space>pouvez<space>pas<space>vous<space>pouvez<space>pas<space>louper<space>euh

2026-01-28 14:08:23,720 | INFO | speech length: 46720
2026-01-28 14:08:23,753 | INFO | decoder input length: 72
2026-01-28 14:08:23,753 | INFO | max output length: 72
2026-01-28 14:08:23,753 | INFO | min output length: 7
2026-01-28 14:08:30,276 | INFO | end detected at 70
2026-01-28 14:08:30,277 | INFO |  -6.95 * 0.5 =  -3.48 for decoder
2026-01-28 14:08:30,277 | INFO |  -0.76 * 0.5 =  -0.38 for ctc
2026-01-28 14:08:30,277 | INFO | total log probability: -3.86
2026-01-28 14:08:30,277 | INFO | normalized log probability: -0.06
2026-01-28 14:08:30,277 | INFO | total number of ended hypotheses: 163
2026-01-28 14:08:30,278 | INFO | best hypo: voilà<space>bah<space>je<space>vous<space>remercie<space>beaucoup<space>de<space>votre<space>attention<space>à<space>de<space>rien

2026-01-28 14:08:30,286 | INFO | Chunk: 0 | WER=15.789474 | S=1 D=0 I=2
2026-01-28 14:08:30,289 | INFO | Chunk: 1 | WER=15.714286 | S=2 D=3 I=6
2026-01-28 14:08:30,289 | INFO | Chunk: 2 | WER=100.000000 | S=2 D=1 I=0
2026-01-28 14:08:30,289 | INFO | Chunk: 3 | WER=60.000000 | S=0 D=1 I=2
2026-01-28 14:08:30,290 | INFO | Chunk: 4 | WER=48.717949 | S=9 D=4 I=6
2026-01-28 14:08:30,292 | INFO | Chunk: 5 | WER=29.411765 | S=7 D=1 I=7
2026-01-28 14:08:30,292 | INFO | Chunk: 6 | WER=50.000000 | S=1 D=3 I=6
2026-01-28 14:08:30,293 | INFO | Chunk: 7 | WER=100.000000 | S=4 D=0 I=2
2026-01-28 14:08:30,293 | INFO | Chunk: 8 | WER=160.000000 | S=0 D=2 I=6
2026-01-28 14:08:30,294 | INFO | Chunk: 9 | WER=37.142857 | S=2 D=4 I=7
2026-01-28 14:08:30,294 | INFO | Chunk: 10 | WER=52.941176 | S=3 D=3 I=3
2026-01-28 14:08:30,295 | INFO | Chunk: 11 | WER=40.909091 | S=1 D=2 I=6
2026-01-28 14:08:30,295 | INFO | Chunk: 12 | WER=33.333333 | S=0 D=0 I=3
2026-01-28 14:08:30,296 | INFO | Chunk: 13 | WER=31.034483 | S=0 D=5 I=4
2026-01-28 14:08:30,298 | INFO | Chunk: 14 | WER=32.812500 | S=10 D=5 I=6
2026-01-28 14:08:30,299 | INFO | Chunk: 15 | WER=142.857143 | S=2 D=2 I=6
2026-01-28 14:08:30,299 | INFO | Chunk: 16 | WER=700.000000 | S=1 D=0 I=6
2026-01-28 14:08:30,299 | INFO | Chunk: 17 | WER=125.000000 | S=1 D=1 I=3
2026-01-28 14:08:30,299 | INFO | Chunk: 18 | WER=100.000000 | S=3 D=0 I=0
2026-01-28 14:08:30,299 | INFO | Chunk: 19 | WER=57.142857 | S=0 D=2 I=2
2026-01-28 14:08:30,300 | INFO | Chunk: 20 | WER=45.000000 | S=1 D=2 I=6
2026-01-28 14:08:30,300 | INFO | Chunk: 21 | WER=70.000000 | S=2 D=4 I=8
2026-01-28 14:08:30,301 | INFO | Chunk: 22 | WER=100.000000 | S=9 D=1 I=0
2026-01-28 14:08:30,301 | INFO | Chunk: 23 | WER=47.619048 | S=1 D=3 I=6
2026-01-28 14:08:30,303 | INFO | Chunk: 24 | WER=39.130435 | S=3 D=6 I=9
2026-01-28 14:08:30,303 | INFO | Chunk: 25 | WER=120.000000 | S=0 D=5 I=7
2026-01-28 14:08:30,470 | INFO | File: Rhap-D0008.wav | WER=33.517495 | S=77 D=23 I=82
2026-01-28 14:08:30,470 | INFO | ------------------------------
2026-01-28 14:08:30,470 | INFO | Conf ester Done!
2026-01-28 14:11:28,682 | INFO | Chunk: 0 | WER=15.789474 | S=1 D=0 I=2
2026-01-28 14:11:28,696 | INFO | Chunk: 1 | WER=25.714286 | S=5 D=8 I=5
2026-01-28 14:11:28,697 | INFO | Chunk: 2 | WER=100.000000 | S=2 D=1 I=0
2026-01-28 14:11:28,697 | INFO | Chunk: 3 | WER=100.000000 | S=1 D=1 I=3
2026-01-28 14:11:28,698 | INFO | Chunk: 4 | WER=66.666667 | S=14 D=3 I=9
2026-01-28 14:11:28,700 | INFO | Chunk: 5 | WER=33.333333 | S=7 D=6 I=4
2026-01-28 14:11:28,701 | INFO | Chunk: 6 | WER=65.000000 | S=3 D=4 I=6
2026-01-28 14:11:28,701 | INFO | Chunk: 7 | WER=100.000000 | S=3 D=2 I=1
2026-01-28 14:11:28,701 | INFO | Chunk: 8 | WER=160.000000 | S=0 D=2 I=6
2026-01-28 14:11:28,702 | INFO | Chunk: 9 | WER=45.714286 | S=3 D=7 I=6
2026-01-28 14:11:28,703 | INFO | Chunk: 10 | WER=29.411765 | S=1 D=3 I=1
2026-01-28 14:11:28,703 | INFO | Chunk: 11 | WER=40.909091 | S=5 D=2 I=2
2026-01-28 14:11:28,703 | INFO | Chunk: 12 | WER=33.333333 | S=1 D=0 I=2
2026-01-28 14:11:28,704 | INFO | Chunk: 13 | WER=41.379310 | S=3 D=5 I=4
2026-01-28 14:11:28,707 | INFO | Chunk: 14 | WER=34.375000 | S=11 D=6 I=5
2026-01-28 14:11:28,707 | INFO | Chunk: 15 | WER=114.285714 | S=2 D=2 I=4
2026-01-28 14:11:28,707 | INFO | Chunk: 16 | WER=400.000000 | S=1 D=0 I=3
2026-01-28 14:11:28,707 | INFO | Chunk: 17 | WER=125.000000 | S=1 D=1 I=3
2026-01-28 14:11:28,707 | INFO | Chunk: 18 | WER=200.000000 | S=3 D=0 I=3
2026-01-28 14:11:28,708 | INFO | Chunk: 19 | WER=42.857143 | S=0 D=2 I=1
2026-01-28 14:11:28,708 | INFO | Chunk: 20 | WER=40.000000 | S=0 D=3 I=5
2026-01-28 14:11:28,709 | INFO | Chunk: 21 | WER=80.000000 | S=3 D=6 I=7
2026-01-28 14:11:28,709 | INFO | Chunk: 22 | WER=100.000000 | S=1 D=4 I=0
2026-01-28 14:11:28,709 | INFO | Chunk: 23 | WER=100.000000 | S=6 D=3 I=1
2026-01-28 14:11:28,710 | INFO | Chunk: 24 | WER=57.142857 | S=2 D=3 I=7
2026-01-28 14:11:28,711 | INFO | Chunk: 25 | WER=32.608696 | S=5 D=6 I=4
2026-01-28 14:11:28,711 | INFO | Chunk: 26 | WER=130.000000 | S=0 D=5 I=8
2026-01-28 14:11:28,860 | INFO | File: Rhap-D0008.wav | WER=38.138686 | S=94 D=49 I=66
2026-01-28 14:11:28,861 | INFO | ------------------------------
2026-01-28 14:11:28,861 | INFO | hmm_tdnn Done!
2026-01-28 14:11:29,088 | INFO | ==================================Rhap-D0009.wav=========================================
2026-01-28 14:11:29,535 | INFO | Using rVAD model
2026-01-28 14:11:49,785 | INFO | Chunk: 0 | WER=47.619048 | S=1 D=1 I=8
2026-01-28 14:11:49,785 | INFO | Chunk: 1 | WER=166.666667 | S=3 D=0 I=2
2026-01-28 14:11:49,785 | INFO | Chunk: 2 | WER=93.750000 | S=2 D=7 I=6
2026-01-28 14:11:49,786 | INFO | Chunk: 3 | WER=60.000000 | S=11 D=7 I=0
2026-01-28 14:11:49,786 | INFO | Chunk: 4 | WER=100.000000 | S=3 D=4 I=0
2026-01-28 14:11:49,786 | INFO | Chunk: 5 | WER=300.000000 | S=1 D=0 I=2
2026-01-28 14:11:49,787 | INFO | Chunk: 6 | WER=100.000000 | S=13 D=1 I=0
2026-01-28 14:11:49,787 | INFO | Chunk: 7 | WER=112.500000 | S=8 D=0 I=1
2026-01-28 14:11:49,789 | INFO | Chunk: 8 | WER=44.776119 | S=5 D=21 I=4
2026-01-28 14:11:49,790 | INFO | Chunk: 9 | WER=75.000000 | S=10 D=8 I=3
2026-01-28 14:11:49,790 | INFO | Chunk: 10 | WER=71.428571 | S=10 D=9 I=6
2026-01-28 14:11:49,791 | INFO | Chunk: 11 | WER=171.428571 | S=7 D=0 I=5
2026-01-28 14:11:49,791 | INFO | Chunk: 12 | WER=80.952381 | S=6 D=9 I=2
2026-01-28 14:11:49,791 | INFO | Chunk: 13 | WER=94.444444 | S=6 D=10 I=1
2026-01-28 14:11:49,792 | INFO | Chunk: 14 | WER=81.250000 | S=0 D=11 I=2
2026-01-28 14:11:49,793 | INFO | Chunk: 15 | WER=44.230769 | S=5 D=11 I=7
2026-01-28 14:11:49,794 | INFO | Chunk: 16 | WER=62.745098 | S=11 D=16 I=5
2026-01-28 14:11:49,795 | INFO | Chunk: 17 | WER=85.714286 | S=7 D=11 I=0
2026-01-28 14:11:49,795 | INFO | Chunk: 18 | WER=80.769231 | S=10 D=10 I=1
2026-01-28 14:11:49,796 | INFO | Chunk: 19 | WER=71.111111 | S=16 D=15 I=1
2026-01-28 14:11:49,797 | INFO | Chunk: 20 | WER=68.965517 | S=3 D=13 I=4
2026-01-28 14:11:49,797 | INFO | Chunk: 21 | WER=94.117647 | S=13 D=3 I=0
2026-01-28 14:11:49,798 | INFO | Chunk: 22 | WER=100.000000 | S=10 D=1 I=0
2026-01-28 14:11:49,798 | INFO | Chunk: 23 | WER=93.548387 | S=9 D=16 I=4
2026-01-28 14:11:49,798 | INFO | Chunk: 24 | WER=100.000000 | S=4 D=3 I=0
2026-01-28 14:11:49,798 | INFO | Chunk: 25 | WER=100.000000 | S=3 D=1 I=0
2026-01-28 14:11:49,799 | INFO | Chunk: 26 | WER=39.130435 | S=0 D=6 I=3
2026-01-28 14:11:49,799 | INFO | Chunk: 27 | WER=90.909091 | S=4 D=5 I=11
2026-01-28 14:11:49,800 | INFO | Chunk: 28 | WER=171.428571 | S=7 D=0 I=5
2026-01-28 14:11:49,801 | INFO | Chunk: 29 | WER=60.000000 | S=10 D=14 I=3
2026-01-28 14:11:49,801 | INFO | Chunk: 30 | WER=142.857143 | S=5 D=0 I=5
2026-01-28 14:11:49,802 | INFO | Chunk: 31 | WER=61.904762 | S=8 D=15 I=3
2026-01-28 14:11:49,802 | INFO | Chunk: 32 | WER=100.000000 | S=5 D=1 I=0
2026-01-28 14:11:49,802 | INFO | Chunk: 33 | WER=68.421053 | S=2 D=9 I=2
2026-01-28 14:11:49,803 | INFO | Chunk: 34 | WER=100.000000 | S=5 D=0 I=0
2026-01-28 14:11:49,803 | INFO | Chunk: 35 | WER=0.000000 | S=1 D=0 I=0
2026-01-28 14:11:49,803 | INFO | Chunk: 36 | WER=34.285714 | S=2 D=9 I=1
2026-01-28 14:11:49,804 | INFO | Chunk: 37 | WER=100.000000 | S=5 D=0 I=0
2026-01-28 14:11:49,804 | INFO | Chunk: 38 | WER=107.692308 | S=13 D=0 I=1
2026-01-28 14:11:49,805 | INFO | Chunk: 39 | WER=40.000000 | S=2 D=8 I=4
2026-01-28 14:11:49,805 | INFO | Chunk: 40 | WER=200.000000 | S=4 D=0 I=4
2026-01-28 14:11:49,805 | INFO | Chunk: 41 | WER=100.000000 | S=17 D=4 I=0
2026-01-28 14:11:49,806 | INFO | Chunk: 42 | WER=78.571429 | S=2 D=16 I=4
2026-01-28 14:11:49,806 | INFO | Chunk: 43 | WER=100.000000 | S=1 D=2 I=0
2026-01-28 14:11:49,806 | INFO | Chunk: 44 | WER=100.000000 | S=12 D=1 I=0
2026-01-28 14:11:49,807 | INFO | Chunk: 45 | WER=130.000000 | S=0 D=3 I=10
2026-01-28 14:11:49,807 | INFO | Chunk: 46 | WER=200.000000 | S=5 D=0 I=5
2026-01-28 14:11:49,807 | INFO | Chunk: 47 | WER=87.500000 | S=5 D=2 I=0
2026-01-28 14:11:49,808 | INFO | Chunk: 48 | WER=96.551724 | S=23 D=5 I=0
2026-01-28 14:11:49,808 | INFO | Chunk: 49 | WER=100.000000 | S=0 D=6 I=6
2026-01-28 14:11:49,808 | INFO | Chunk: 50 | WER=175.000000 | S=4 D=0 I=3
2026-01-28 14:11:49,808 | INFO | Chunk: 51 | WER=250.000000 | S=4 D=0 I=6
2026-01-28 14:11:49,808 | INFO | Chunk: 52 | WER=100.000000 | S=1 D=4 I=0
2026-01-28 14:11:49,808 | INFO | Chunk: 53 | WER=100.000000 | S=0 D=2 I=2
2026-01-28 14:11:50,177 | INFO | File: Rhap-D0009.wav | WER=54.400000 | S=269 D=216 I=59
2026-01-28 14:11:50,177 | INFO | ------------------------------
2026-01-28 14:11:50,177 | INFO | w2vec vad chunk Done!
2026-01-28 14:12:25,071 | INFO | Chunk: 0 | WER=42.857143 | S=0 D=1 I=8
2026-01-28 14:12:25,071 | INFO | Chunk: 1 | WER=133.333333 | S=3 D=0 I=1
2026-01-28 14:12:25,072 | INFO | Chunk: 2 | WER=68.750000 | S=1 D=9 I=1
2026-01-28 14:12:25,072 | INFO | Chunk: 3 | WER=43.333333 | S=5 D=6 I=2
2026-01-28 14:12:25,073 | INFO | Chunk: 4 | WER=100.000000 | S=2 D=5 I=0
2026-01-28 14:12:25,073 | INFO | Chunk: 5 | WER=400.000000 | S=1 D=0 I=3
2026-01-28 14:12:25,073 | INFO | Chunk: 6 | WER=121.428571 | S=1 D=5 I=11
2026-01-28 14:12:25,073 | INFO | Chunk: 7 | WER=112.500000 | S=8 D=0 I=1
2026-01-28 14:12:25,074 | INFO | Chunk: 8 | WER=80.597015 | S=0 D=54 I=0
2026-01-28 14:12:25,075 | INFO | Chunk: 9 | WER=46.428571 | S=4 D=7 I=2
2026-01-28 14:12:25,075 | INFO | Chunk: 10 | WER=60.000000 | S=4 D=17 I=0
2026-01-28 14:12:25,076 | INFO | Chunk: 11 | WER=171.428571 | S=7 D=0 I=5
2026-01-28 14:12:25,076 | INFO | Chunk: 12 | WER=128.571429 | S=7 D=6 I=14
2026-01-28 14:12:25,077 | INFO | Chunk: 13 | WER=94.444444 | S=12 D=3 I=2
2026-01-28 14:12:25,077 | INFO | Chunk: 14 | WER=93.750000 | S=1 D=10 I=4
2026-01-28 14:12:25,078 | INFO | Chunk: 15 | WER=40.384615 | S=1 D=20 I=0
2026-01-28 14:12:25,079 | INFO | Chunk: 16 | WER=68.627451 | S=6 D=29 I=0
2026-01-28 14:12:25,079 | INFO | Chunk: 17 | WER=71.428571 | S=6 D=9 I=0
2026-01-28 14:12:25,080 | INFO | Chunk: 18 | WER=76.923077 | S=10 D=8 I=2
2026-01-28 14:12:25,080 | INFO | Chunk: 19 | WER=95.555556 | S=1 D=41 I=1
2026-01-28 14:12:25,081 | INFO | Chunk: 20 | WER=65.517241 | S=2 D=11 I=6
2026-01-28 14:12:25,081 | INFO | Chunk: 21 | WER=94.117647 | S=13 D=3 I=0
2026-01-28 14:12:25,082 | INFO | Chunk: 22 | WER=100.000000 | S=11 D=0 I=0
2026-01-28 14:12:25,082 | INFO | Chunk: 23 | WER=64.516129 | S=6 D=13 I=1
2026-01-28 14:12:25,082 | INFO | Chunk: 24 | WER=100.000000 | S=3 D=4 I=0
2026-01-28 14:12:25,083 | INFO | Chunk: 25 | WER=100.000000 | S=4 D=0 I=0
2026-01-28 14:12:25,083 | INFO | Chunk: 26 | WER=69.565217 | S=0 D=13 I=3
2026-01-28 14:12:25,084 | INFO | Chunk: 27 | WER=81.818182 | S=1 D=7 I=10
2026-01-28 14:12:25,084 | INFO | Chunk: 28 | WER=157.142857 | S=7 D=0 I=4
2026-01-28 14:12:25,084 | INFO | Chunk: 29 | WER=73.333333 | S=3 D=30 I=0
2026-01-28 14:12:25,085 | INFO | Chunk: 30 | WER=42.857143 | S=0 D=3 I=0
2026-01-28 14:12:25,085 | INFO | Chunk: 31 | WER=69.047619 | S=1 D=28 I=0
2026-01-28 14:12:25,085 | INFO | Chunk: 32 | WER=100.000000 | S=6 D=0 I=0
2026-01-28 14:12:25,086 | INFO | Chunk: 33 | WER=73.684211 | S=4 D=10 I=0
2026-01-28 14:12:25,086 | INFO | Chunk: 34 | WER=120.000000 | S=5 D=0 I=1
2026-01-28 14:12:25,086 | INFO | Chunk: 35 | WER=0.000000 | S=1 D=0 I=0
2026-01-28 14:12:25,087 | INFO | Chunk: 36 | WER=37.142857 | S=2 D=9 I=2
2026-01-28 14:12:25,087 | INFO | Chunk: 37 | WER=80.000000 | S=4 D=0 I=0
2026-01-28 14:12:25,087 | INFO | Chunk: 38 | WER=100.000000 | S=13 D=0 I=0
2026-01-28 14:12:25,088 | INFO | Chunk: 39 | WER=74.285714 | S=5 D=21 I=0
2026-01-28 14:12:25,088 | INFO | Chunk: 40 | WER=150.000000 | S=4 D=0 I=2
2026-01-28 14:12:25,088 | INFO | Chunk: 41 | WER=100.000000 | S=19 D=2 I=0
2026-01-28 14:12:25,089 | INFO | Chunk: 42 | WER=75.000000 | S=0 D=18 I=3
2026-01-28 14:12:25,089 | INFO | Chunk: 43 | WER=100.000000 | S=3 D=0 I=0
2026-01-28 14:12:25,089 | INFO | Chunk: 44 | WER=100.000000 | S=12 D=1 I=0
2026-01-28 14:12:25,089 | INFO | Chunk: 45 | WER=110.000000 | S=0 D=3 I=8
2026-01-28 14:12:25,090 | INFO | Chunk: 46 | WER=220.000000 | S=5 D=0 I=6
2026-01-28 14:12:25,090 | INFO | Chunk: 47 | WER=100.000000 | S=7 D=1 I=0
2026-01-28 14:12:25,091 | INFO | Chunk: 48 | WER=96.551724 | S=25 D=2 I=1
2026-01-28 14:12:25,091 | INFO | Chunk: 49 | WER=108.333333 | S=12 D=0 I=1
2026-01-28 14:12:25,091 | INFO | Chunk: 50 | WER=175.000000 | S=4 D=0 I=3
2026-01-28 14:12:25,091 | INFO | Chunk: 51 | WER=225.000000 | S=4 D=0 I=5
2026-01-28 14:12:25,091 | INFO | Chunk: 52 | WER=100.000000 | S=1 D=4 I=0
2026-01-28 14:12:25,091 | INFO | Chunk: 53 | WER=100.000000 | S=2 D=2 I=0
2026-01-28 14:12:25,400 | INFO | File: Rhap-D0009.wav | WER=61.800000 | S=185 D=367 I=66
2026-01-28 14:12:25,400 | INFO | ------------------------------
2026-01-28 14:12:25,400 | INFO | whisper med Done!
2026-01-28 14:13:15,403 | INFO | Chunk: 0 | WER=42.857143 | S=0 D=1 I=8
2026-01-28 14:13:15,403 | INFO | Chunk: 1 | WER=166.666667 | S=3 D=0 I=2
2026-01-28 14:13:15,403 | INFO | Chunk: 2 | WER=100.000000 | S=16 D=0 I=0
2026-01-28 14:13:15,404 | INFO | Chunk: 3 | WER=56.666667 | S=11 D=6 I=0
2026-01-28 14:13:15,404 | INFO | Chunk: 4 | WER=100.000000 | S=5 D=1 I=1
2026-01-28 14:13:15,404 | INFO | Chunk: 5 | WER=500.000000 | S=1 D=0 I=4
2026-01-28 14:13:15,405 | INFO | Chunk: 6 | WER=121.428571 | S=13 D=0 I=4
2026-01-28 14:13:15,405 | INFO | Chunk: 7 | WER=112.500000 | S=8 D=0 I=1
2026-01-28 14:13:15,406 | INFO | Chunk: 8 | WER=82.089552 | S=10 D=45 I=0
2026-01-28 14:13:15,407 | INFO | Chunk: 9 | WER=60.714286 | S=8 D=8 I=1
2026-01-28 14:13:15,407 | INFO | Chunk: 10 | WER=57.142857 | S=3 D=17 I=0
2026-01-28 14:13:15,407 | INFO | Chunk: 11 | WER=171.428571 | S=7 D=0 I=5
2026-01-28 14:13:15,408 | INFO | Chunk: 12 | WER=104.761905 | S=6 D=6 I=10
2026-01-28 14:13:15,408 | INFO | Chunk: 13 | WER=94.444444 | S=11 D=5 I=1
2026-01-28 14:13:15,409 | INFO | Chunk: 14 | WER=93.750000 | S=10 D=5 I=0
2026-01-28 14:13:15,410 | INFO | Chunk: 15 | WER=46.153846 | S=2 D=22 I=0
2026-01-28 14:13:15,411 | INFO | Chunk: 16 | WER=68.627451 | S=8 D=27 I=0
2026-01-28 14:13:15,411 | INFO | Chunk: 17 | WER=85.714286 | S=2 D=11 I=5
2026-01-28 14:13:15,412 | INFO | Chunk: 18 | WER=76.923077 | S=9 D=9 I=2
2026-01-28 14:13:15,412 | INFO | Chunk: 19 | WER=95.555556 | S=1 D=41 I=1
2026-01-28 14:13:15,413 | INFO | Chunk: 20 | WER=65.517241 | S=2 D=11 I=6
2026-01-28 14:13:15,413 | INFO | Chunk: 21 | WER=94.117647 | S=11 D=4 I=1
2026-01-28 14:13:15,413 | INFO | Chunk: 22 | WER=100.000000 | S=10 D=1 I=0
2026-01-28 14:13:15,414 | INFO | Chunk: 23 | WER=54.838710 | S=6 D=10 I=1
2026-01-28 14:13:15,414 | INFO | Chunk: 24 | WER=100.000000 | S=3 D=4 I=0
2026-01-28 14:13:15,414 | INFO | Chunk: 25 | WER=100.000000 | S=4 D=0 I=0
2026-01-28 14:13:15,415 | INFO | Chunk: 26 | WER=56.521739 | S=1 D=9 I=3
2026-01-28 14:13:15,415 | INFO | Chunk: 27 | WER=86.363636 | S=3 D=6 I=10
2026-01-28 14:13:15,415 | INFO | Chunk: 28 | WER=157.142857 | S=7 D=0 I=4
2026-01-28 14:13:15,416 | INFO | Chunk: 29 | WER=73.333333 | S=2 D=31 I=0
2026-01-28 14:13:15,416 | INFO | Chunk: 30 | WER=42.857143 | S=0 D=3 I=0
2026-01-28 14:13:15,417 | INFO | Chunk: 31 | WER=59.523810 | S=4 D=16 I=5
2026-01-28 14:13:15,417 | INFO | Chunk: 32 | WER=100.000000 | S=6 D=0 I=0
2026-01-28 14:13:15,418 | INFO | Chunk: 33 | WER=57.894737 | S=3 D=8 I=0
2026-01-28 14:13:15,418 | INFO | Chunk: 34 | WER=120.000000 | S=5 D=0 I=1
2026-01-28 14:13:15,418 | INFO | Chunk: 35 | WER=0.000000 | S=1 D=0 I=0
2026-01-28 14:13:15,419 | INFO | Chunk: 36 | WER=42.857143 | S=3 D=10 I=2
2026-01-28 14:13:15,419 | INFO | Chunk: 37 | WER=80.000000 | S=4 D=0 I=0
2026-01-28 14:13:15,419 | INFO | Chunk: 38 | WER=100.000000 | S=13 D=0 I=0
2026-01-28 14:13:15,420 | INFO | Chunk: 39 | WER=74.285714 | S=7 D=19 I=0
2026-01-28 14:13:15,420 | INFO | Chunk: 40 | WER=125.000000 | S=4 D=0 I=1
2026-01-28 14:13:15,420 | INFO | Chunk: 41 | WER=100.000000 | S=19 D=2 I=0
2026-01-28 14:13:15,421 | INFO | Chunk: 42 | WER=71.428571 | S=0 D=14 I=6
2026-01-28 14:13:15,421 | INFO | Chunk: 43 | WER=166.666667 | S=3 D=0 I=2
2026-01-28 14:13:15,421 | INFO | Chunk: 44 | WER=100.000000 | S=12 D=1 I=0
2026-01-28 14:13:15,422 | INFO | Chunk: 45 | WER=140.000000 | S=0 D=3 I=11
2026-01-28 14:13:15,422 | INFO | Chunk: 46 | WER=220.000000 | S=5 D=0 I=6
2026-01-28 14:13:15,422 | INFO | Chunk: 47 | WER=100.000000 | S=5 D=3 I=0
2026-01-28 14:13:15,423 | INFO | Chunk: 48 | WER=89.655172 | S=8 D=7 I=11
2026-01-28 14:13:15,423 | INFO | Chunk: 49 | WER=108.333333 | S=0 D=6 I=7
2026-01-28 14:13:15,423 | INFO | Chunk: 50 | WER=175.000000 | S=4 D=0 I=3
2026-01-28 14:13:15,424 | INFO | Chunk: 51 | WER=250.000000 | S=4 D=0 I=6
2026-01-28 14:13:15,424 | INFO | Chunk: 52 | WER=100.000000 | S=1 D=4 I=0
2026-01-28 14:13:15,424 | INFO | Chunk: 53 | WER=100.000000 | S=3 D=1 I=0
2026-01-28 14:13:15,762 | INFO | File: Rhap-D0009.wav | WER=60.400000 | S=223 D=313 I=68
2026-01-28 14:13:15,762 | INFO | ------------------------------
2026-01-28 14:13:15,762 | INFO | whisper large Done!
2026-01-28 14:13:15,922 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 14:13:15,952 | INFO | Vocabulary size: 350
2026-01-28 14:13:16,533 | INFO | Gradient checkpoint layers: []
2026-01-28 14:13:17,168 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 14:13:17,171 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 14:13:17,172 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 14:13:17,172 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 14:13:17,172 | INFO | speech length: 142560
2026-01-28 14:13:17,209 | INFO | decoder input length: 222
2026-01-28 14:13:17,209 | INFO | max output length: 222
2026-01-28 14:13:17,209 | INFO | min output length: 22
2026-01-28 14:13:20,852 | INFO | end detected at 73
2026-01-28 14:13:20,853 | INFO | -11.76 * 0.5 =  -5.88 for decoder
2026-01-28 14:13:20,853 | INFO |  -8.98 * 0.5 =  -4.49 for ctc
2026-01-28 14:13:20,853 | INFO | total log probability: -10.37
2026-01-28 14:13:20,853 | INFO | normalized log probability: -0.15
2026-01-28 14:13:20,853 | INFO | total number of ended hypotheses: 169
2026-01-28 14:13:20,854 | INFO | best hypo: ▁bon▁autre▁chose▁on▁âme▁oh▁nos▁chaises▁qui▁ont▁besoin▁de▁refaire▁est▁ce▁que▁tu▁connais▁des▁un▁endroit▁où▁je▁pourrais▁les▁laisser▁pour▁le▁rempaillage

2026-01-28 14:13:20,857 | INFO | speech length: 15840
2026-01-28 14:13:20,888 | INFO | decoder input length: 24
2026-01-28 14:13:20,888 | INFO | max output length: 24
2026-01-28 14:13:20,888 | INFO | min output length: 2
2026-01-28 14:13:21,418 | INFO | end detected at 17
2026-01-28 14:13:21,420 | INFO |  -2.81 * 0.5 =  -1.40 for decoder
2026-01-28 14:13:21,420 | INFO |  -5.81 * 0.5 =  -2.90 for ctc
2026-01-28 14:13:21,420 | INFO | total log probability: -4.31
2026-01-28 14:13:21,420 | INFO | normalized log probability: -0.39
2026-01-28 14:13:21,420 | INFO | total number of ended hypotheses: 179
2026-01-28 14:13:21,421 | INFO | best hypo: ▁mais▁les▁chaises▁il▁faut

2026-01-28 14:13:21,422 | INFO | speech length: 61920
2026-01-28 14:13:21,457 | INFO | decoder input length: 96
2026-01-28 14:13:21,457 | INFO | max output length: 96
2026-01-28 14:13:21,457 | INFO | min output length: 9
2026-01-28 14:13:23,259 | INFO | end detected at 47
2026-01-28 14:13:23,260 | INFO | -13.51 * 0.5 =  -6.76 for decoder
2026-01-28 14:13:23,260 | INFO | -32.44 * 0.5 = -16.22 for ctc
2026-01-28 14:13:23,261 | INFO | total log probability: -22.98
2026-01-28 14:13:23,261 | INFO | normalized log probability: -0.60
2026-01-28 14:13:23,261 | INFO | total number of ended hypotheses: 178
2026-01-28 14:13:23,261 | INFO | best hypo: ▁ah▁oui▁t'as▁quelqu'un▁qui▁peut▁emmager▁quelqu'un▁qui▁le▁s'insoler▁dans▁un

2026-01-28 14:13:23,263 | INFO | speech length: 105760
2026-01-28 14:13:23,298 | INFO | decoder input length: 164
2026-01-28 14:13:23,298 | INFO | max output length: 164
2026-01-28 14:13:23,298 | INFO | min output length: 16
2026-01-28 14:13:25,722 | INFO | end detected at 55
2026-01-28 14:13:25,723 | INFO | -13.01 * 0.5 =  -6.51 for decoder
2026-01-28 14:13:25,723 | INFO | -25.34 * 0.5 = -12.67 for ctc
2026-01-28 14:13:25,723 | INFO | total log probability: -19.18
2026-01-28 14:13:25,723 | INFO | normalized log probability: -0.42
2026-01-28 14:13:25,723 | INFO | total number of ended hypotheses: 176
2026-01-28 14:13:25,724 | INFO | best hypo: ▁ah▁c'est▁bien▁il▁faut▁conter▁à▁peu▁près▁combien▁par▁chaises▁ce▁sont▁de▁ces▁léchelles▁de▁notre▁côté▁là

2026-01-28 14:13:25,726 | INFO | speech length: 27840
2026-01-28 14:13:25,758 | INFO | decoder input length: 43
2026-01-28 14:13:25,758 | INFO | max output length: 43
2026-01-28 14:13:25,758 | INFO | min output length: 4
2026-01-28 14:13:26,179 | INFO | end detected at 12
2026-01-28 14:13:26,181 | INFO |  -8.47 * 0.5 =  -4.23 for decoder
2026-01-28 14:13:26,181 | INFO |  -4.04 * 0.5 =  -2.02 for ctc
2026-01-28 14:13:26,181 | INFO | total log probability: -6.25
2026-01-28 14:13:26,181 | INFO | normalized log probability: -1.04
2026-01-28 14:13:26,181 | INFO | total number of ended hypotheses: 177
2026-01-28 14:13:26,181 | INFO | best hypo: ▁comptetez

2026-01-28 14:13:26,183 | INFO | speech length: 63840
2026-01-28 14:13:26,214 | INFO | decoder input length: 99
2026-01-28 14:13:26,214 | INFO | max output length: 99
2026-01-28 14:13:26,214 | INFO | min output length: 9
2026-01-28 14:13:27,780 | INFO | end detected at 39
2026-01-28 14:13:27,782 | INFO |  -9.58 * 0.5 =  -4.79 for decoder
2026-01-28 14:13:27,782 | INFO | -19.23 * 0.5 =  -9.62 for ctc
2026-01-28 14:13:27,782 | INFO | total log probability: -14.41
2026-01-28 14:13:27,782 | INFO | normalized log probability: -0.44
2026-01-28 14:13:27,782 | INFO | total number of ended hypotheses: 172
2026-01-28 14:13:27,782 | INFO | best hypo: ▁oh▁oui▁je▁pensais▁plus▁que▁cela▁encore▁encore▁et▁est▁ce▁qui▁font▁aussi▁le▁canage

2026-01-28 14:13:27,784 | INFO | speech length: 42880
2026-01-28 14:13:27,817 | INFO | decoder input length: 66
2026-01-28 14:13:27,817 | INFO | max output length: 66
2026-01-28 14:13:27,817 | INFO | min output length: 6
2026-01-28 14:13:28,806 | INFO | end detected at 27
2026-01-28 14:13:28,807 | INFO |  -2.18 * 0.5 =  -1.09 for decoder
2026-01-28 14:13:28,807 | INFO |  -5.80 * 0.5 =  -2.90 for ctc
2026-01-28 14:13:28,807 | INFO | total log probability: -3.99
2026-01-28 14:13:28,807 | INFO | normalized log probability: -0.18
2026-01-28 14:13:28,807 | INFO | total number of ended hypotheses: 174
2026-01-28 14:13:28,808 | INFO | best hypo: ▁pareil▁là▁par▁contre▁ça▁doit▁être▁plus▁onéreux

2026-01-28 14:13:28,809 | INFO | speech length: 338880
2026-01-28 14:13:28,844 | INFO | decoder input length: 529
2026-01-28 14:13:28,844 | INFO | max output length: 529
2026-01-28 14:13:28,844 | INFO | min output length: 52
2026-01-28 14:13:39,497 | INFO | end detected at 129
2026-01-28 14:13:39,499 | INFO | -373.04 * 0.5 = -186.52 for decoder
2026-01-28 14:13:39,499 | INFO | -136.30 * 0.5 = -68.15 for ctc
2026-01-28 14:13:39,499 | INFO | total log probability: -254.67
2026-01-28 14:13:39,499 | INFO | normalized log probability: -2.07
2026-01-28 14:13:39,499 | INFO | total number of ended hypotheses: 169
2026-01-28 14:13:39,500 | INFO | best hypo: ▁hein▁oui▁et▁innoullés▁ils▁te▁les▁prennent▁et▁il▁faut▁compter▁combien▁de▁temps▁après▁pour▁très▁rapidement▁ah▁oui▁et▁sais▁tu▁que▁la▁paine▁a▁été▁abîmée▁dans▁cesds▁de▁la▁maisons▁làins▁elle▁a▁été▁abîmée▁à▁parce▁que▁desshu▁avait▁des▁'▁et▁pas▁des▁coussins▁c'étaient▁des▁carrés▁de▁laine

2026-01-28 14:13:39,502 | INFO | speech length: 125760
2026-01-28 14:13:39,532 | INFO | decoder input length: 196
2026-01-28 14:13:39,532 | INFO | max output length: 196
2026-01-28 14:13:39,532 | INFO | min output length: 19
2026-01-28 14:13:43,128 | INFO | end detected at 78
2026-01-28 14:13:43,130 | INFO | -31.78 * 0.5 = -15.89 for decoder
2026-01-28 14:13:43,130 | INFO | -58.41 * 0.5 = -29.21 for ctc
2026-01-28 14:13:43,130 | INFO | total log probability: -45.09
2026-01-28 14:13:43,130 | INFO | normalized log probability: -0.64
2026-01-28 14:13:43,130 | INFO | total number of ended hypotheses: 190
2026-01-28 14:13:43,131 | INFO | best hypo: ▁qui▁sait▁qu'on▁avait▁eu▁en▁tunisie▁il▁qu'avait▁émis▁de▁son▁morceau▁de▁rue▁et▁vraiment▁c'est▁ces▁poids▁uniques▁n'aient▁pas▁joués▁impacts▁ah▁oui

2026-01-28 14:13:43,133 | INFO | speech length: 133280
2026-01-28 14:13:43,177 | INFO | decoder input length: 207
2026-01-28 14:13:43,177 | INFO | max output length: 207
2026-01-28 14:13:43,177 | INFO | min output length: 20
2026-01-28 14:13:47,542 | INFO | end detected at 82
2026-01-28 14:13:47,543 | INFO | -19.98 * 0.5 =  -9.99 for decoder
2026-01-28 14:13:47,543 | INFO | -33.25 * 0.5 = -16.62 for ctc
2026-01-28 14:13:47,543 | INFO | total log probability: -26.62
2026-01-28 14:13:47,543 | INFO | normalized log probability: -0.36
2026-01-28 14:13:47,543 | INFO | total number of ended hypotheses: 176
2026-01-28 14:13:47,544 | INFO | best hypo: ▁mais▁même▁parfois▁je▁me▁suis▁demandé▁si▁c'étaient▁pas▁des▁dévers▁qui▁s'étaient▁introduits▁dans▁un▁an▁moment▁que▁ce▁sont▁les▁impacts▁des▁coussachons▁qui▁vont▁pénétrer▁dans▁la▁page

2026-01-28 14:13:47,546 | INFO | speech length: 36320
2026-01-28 14:13:47,578 | INFO | decoder input length: 56
2026-01-28 14:13:47,578 | INFO | max output length: 56
2026-01-28 14:13:47,578 | INFO | min output length: 5
2026-01-28 14:13:48,669 | INFO | end detected at 27
2026-01-28 14:13:48,670 | INFO |  -2.65 * 0.5 =  -1.32 for decoder
2026-01-28 14:13:48,670 | INFO |  -2.94 * 0.5 =  -1.47 for ctc
2026-01-28 14:13:48,670 | INFO | total log probability: -2.79
2026-01-28 14:13:48,670 | INFO | normalized log probability: -0.12
2026-01-28 14:13:48,670 | INFO | total number of ended hypotheses: 154
2026-01-28 14:13:48,670 | INFO | best hypo: ▁pas▁c'est▁dommage▁parce▁qu'on▁a▁tout▁perdu▁comme▁ça

2026-01-28 14:13:48,672 | INFO | speech length: 63040
2026-01-28 14:13:48,708 | INFO | decoder input length: 98
2026-01-28 14:13:48,708 | INFO | max output length: 98
2026-01-28 14:13:48,708 | INFO | min output length: 9
2026-01-28 14:13:50,912 | INFO | end detected at 50
2026-01-28 14:13:50,914 | INFO | -17.34 * 0.5 =  -8.67 for decoder
2026-01-28 14:13:50,914 | INFO | -30.97 * 0.5 = -15.48 for ctc
2026-01-28 14:13:50,914 | INFO | total log probability: -24.16
2026-01-28 14:13:50,914 | INFO | normalized log probability: -0.56
2026-01-28 14:13:50,914 | INFO | total number of ended hypotheses: 179
2026-01-28 14:13:50,914 | INFO | best hypo: ▁non▁monsieur▁myrle▁n'a▁pas▁problème▁vivant▁ce▁monsieur▁larise▁d'un▁en▁plus▁de▁travail▁que▁là

2026-01-28 14:13:50,916 | INFO | speech length: 60160
2026-01-28 14:13:50,951 | INFO | decoder input length: 93
2026-01-28 14:13:50,952 | INFO | max output length: 93
2026-01-28 14:13:50,952 | INFO | min output length: 9
2026-01-28 14:13:54,568 | INFO | end detected at 33
2026-01-28 14:13:54,569 | INFO | -15.32 * 0.5 =  -7.66 for decoder
2026-01-28 14:13:54,569 | INFO | -11.79 * 0.5 =  -5.90 for ctc
2026-01-28 14:13:54,569 | INFO | total log probability: -13.56
2026-01-28 14:13:54,569 | INFO | normalized log probability: -0.52
2026-01-28 14:13:54,569 | INFO | total number of ended hypotheses: 176
2026-01-28 14:13:54,570 | INFO | best hypo: ▁la▁véritable▁pareille▁coin▁ou▁pas▁d'apeil▁synthétique

2026-01-28 14:13:54,572 | INFO | speech length: 57440
2026-01-28 14:13:54,610 | INFO | decoder input length: 89
2026-01-28 14:13:54,610 | INFO | max output length: 89
2026-01-28 14:13:54,610 | INFO | min output length: 8
2026-01-28 14:13:58,369 | INFO | end detected at 34
2026-01-28 14:13:58,371 | INFO | -10.66 * 0.5 =  -5.33 for decoder
2026-01-28 14:13:58,371 | INFO | -19.52 * 0.5 =  -9.76 for ctc
2026-01-28 14:13:58,371 | INFO | total log probability: -15.09
2026-01-28 14:13:58,371 | INFO | normalized log probability: -0.58
2026-01-28 14:13:58,371 | INFO | total number of ended hypotheses: 199
2026-01-28 14:13:58,371 | INFO | best hypo: ▁et▁prendre▁une▁semaine▁où▁deux▁saillies▁cé▁sont▁livrées

2026-01-28 14:13:58,373 | INFO | speech length: 229120
2026-01-28 14:13:58,417 | INFO | decoder input length: 357
2026-01-28 14:13:58,417 | INFO | max output length: 357
2026-01-28 14:13:58,417 | INFO | min output length: 35
2026-01-28 14:14:15,045 | INFO | end detected at 134
2026-01-28 14:14:15,047 | INFO | -151.10 * 0.5 = -75.55 for decoder
2026-01-28 14:14:15,047 | INFO | -73.30 * 0.5 = -36.65 for ctc
2026-01-28 14:14:15,047 | INFO | total log probability: -112.20
2026-01-28 14:14:15,047 | INFO | normalized log probability: -0.88
2026-01-28 14:14:15,047 | INFO | total number of ended hypotheses: 184
2026-01-28 14:14:15,049 | INFO | best hypo: ▁voilà▁le▁petit▁fauteuil▁que▁j'ai▁là▁à▁côté▁que▁je▁veux▁rabiller▁a▁toujours▁été▁appelé▁mes▁parents▁fauteuils▁crap▁oh▁moi▁est▁ce▁que▁ça▁n'est▁un▁vrai▁rhome▁te▁fauteugo▁et▁pourquoi▁on▁appe▁crapo▁pas▁parce▁que▁ceux▁gange▁mette▁vraiment▁dans▁le▁fondé

2026-01-28 14:14:15,053 | INFO | speech length: 171200
2026-01-28 14:14:15,107 | INFO | decoder input length: 267
2026-01-28 14:14:15,107 | INFO | max output length: 267
2026-01-28 14:14:15,107 | INFO | min output length: 26
2026-01-28 14:14:28,277 | INFO | end detected at 110
2026-01-28 14:14:28,279 | INFO | -113.78 * 0.5 = -56.89 for decoder
2026-01-28 14:14:28,279 | INFO | -84.45 * 0.5 = -42.22 for ctc
2026-01-28 14:14:28,279 | INFO | total log probability: -99.11
2026-01-28 14:14:28,280 | INFO | normalized log probability: -1.01
2026-01-28 14:14:28,280 | INFO | total number of ended hypotheses: 236
2026-01-28 14:14:28,281 | INFO | best hypo: ▁disait▁loin▁comme▁ça▁et▁il▁s'était▁vraiment▁▁très▁rasard▁très▁bas▁donc▁tu▁pourrais▁donner▁l'expression▁d'autres▁chicongards▁sa▁date▁de▁camp▁peu▁près▁ce▁fauteililar▁sous▁apollon▁fin▁dix▁neufième▁sièc

2026-01-28 14:14:28,284 | INFO | speech length: 71999
2026-01-28 14:14:28,315 | INFO | decoder input length: 111
2026-01-28 14:14:28,315 | INFO | max output length: 111
2026-01-28 14:14:28,315 | INFO | min output length: 11
2026-01-28 14:14:33,184 | INFO | end detected at 47
2026-01-28 14:14:33,186 | INFO | -14.86 * 0.5 =  -7.43 for decoder
2026-01-28 14:14:33,186 | INFO | -25.00 * 0.5 = -12.50 for ctc
2026-01-28 14:14:33,186 | INFO | total log probability: -19.93
2026-01-28 14:14:33,186 | INFO | normalized log probability: -0.51
2026-01-28 14:14:33,186 | INFO | total number of ended hypotheses: 182
2026-01-28 14:14:33,186 | INFO | best hypo: ▁les▁pieds▁croît▁y▁sont▁noirs▁quoi▁tu▁sais▁dit▁bu▁typiquement▁napollon▁trois

2026-01-28 14:14:33,188 | INFO | speech length: 68000
2026-01-28 14:14:33,220 | INFO | decoder input length: 105
2026-01-28 14:14:33,220 | INFO | max output length: 105
2026-01-28 14:14:33,220 | INFO | min output length: 10
2026-01-28 14:14:37,720 | INFO | end detected at 44
2026-01-28 14:14:37,722 | INFO | -13.11 * 0.5 =  -6.56 for decoder
2026-01-28 14:14:37,722 | INFO | -19.62 * 0.5 =  -9.81 for ctc
2026-01-28 14:14:37,722 | INFO | total log probability: -16.36
2026-01-28 14:14:37,722 | INFO | normalized log probability: -0.45
2026-01-28 14:14:37,722 | INFO | total number of ended hypotheses: 190
2026-01-28 14:14:37,722 | INFO | best hypo: ▁oxer▁quel▁bois▁je▁du▁bois▁de▁hêtre▁normalement▁ce▁du▁bois▁de▁hêtre▁dessus

2026-01-28 14:14:37,724 | INFO | speech length: 181120
2026-01-28 14:14:37,761 | INFO | decoder input length: 282
2026-01-28 14:14:37,761 | INFO | max output length: 282
2026-01-28 14:14:37,761 | INFO | min output length: 28
2026-01-28 14:14:47,700 | INFO | end detected at 95
2026-01-28 14:14:47,702 | INFO | -91.55 * 0.5 = -45.78 for decoder
2026-01-28 14:14:47,702 | INFO | -90.51 * 0.5 = -45.26 for ctc
2026-01-28 14:14:47,702 | INFO | total log probability: -91.03
2026-01-28 14:14:47,702 | INFO | normalized log probability: -1.05
2026-01-28 14:14:47,702 | INFO | total number of ended hypotheses: 201
2026-01-28 14:14:47,703 | INFO | best hypo: ▁le▁traquier▁la▁quais▁noir▁n'accord▁a▁pour▁le▁refaire▁tu▁me▁dis▁qu'il▁faudrait▁j'aimerrisini▁non▁du▁sai▁ou▁il▁fallait▁initialement▁les▁étatsinibotes▁sur▁les▁gens▁souris▁mais▁sur▁ta▁du▁sel

2026-01-28 14:14:47,706 | INFO | speech length: 97440
2026-01-28 14:14:47,745 | INFO | decoder input length: 151
2026-01-28 14:14:47,745 | INFO | max output length: 151
2026-01-28 14:14:47,746 | INFO | min output length: 15
2026-01-28 14:14:54,157 | INFO | end detected at 58
2026-01-28 14:14:54,158 | INFO | -20.87 * 0.5 = -10.44 for decoder
2026-01-28 14:14:54,158 | INFO | -33.14 * 0.5 = -16.57 for ctc
2026-01-28 14:14:54,158 | INFO | total log probability: -27.01
2026-01-28 14:14:54,158 | INFO | normalized log probability: -0.53
2026-01-28 14:14:54,159 | INFO | total number of ended hypotheses: 151
2026-01-28 14:14:54,159 | INFO | best hypo: ▁alors▁imbrefrey▁l'assise▁mais▁ce▁qui▁four▁que▁je▁lui▁dise▁aussi▁les▁accoudoirs▁t'as▁vu▁commerçon▁à▁haut

2026-01-28 14:14:54,161 | INFO | speech length: 56320
2026-01-28 14:14:54,197 | INFO | decoder input length: 87
2026-01-28 14:14:54,197 | INFO | max output length: 87
2026-01-28 14:14:54,197 | INFO | min output length: 8
2026-01-28 14:14:58,295 | INFO | end detected at 38
2026-01-28 14:14:58,297 | INFO |  -5.60 * 0.5 =  -2.80 for decoder
2026-01-28 14:14:58,297 | INFO |  -9.75 * 0.5 =  -4.88 for ctc
2026-01-28 14:14:58,297 | INFO | total log probability: -7.68
2026-01-28 14:14:58,297 | INFO | normalized log probability: -0.24
2026-01-28 14:14:58,297 | INFO | total number of ended hypotheses: 184
2026-01-28 14:14:58,298 | INFO | best hypo: ▁reprendre▁tout▁à▁faire▁le▁travail▁faut▁faire▁un▁assis▁et▁les▁accoudoirs

2026-01-28 14:14:58,300 | INFO | speech length: 38560
2026-01-28 14:14:58,335 | INFO | decoder input length: 59
2026-01-28 14:14:58,335 | INFO | max output length: 59
2026-01-28 14:14:58,335 | INFO | min output length: 5
2026-01-28 14:15:00,978 | INFO | end detected at 25
2026-01-28 14:15:00,980 | INFO |  -1.96 * 0.5 =  -0.98 for decoder
2026-01-28 14:15:00,980 | INFO |  -4.90 * 0.5 =  -2.45 for ctc
2026-01-28 14:15:00,980 | INFO | total log probability: -3.43
2026-01-28 14:15:00,980 | INFO | normalized log probability: -0.17
2026-01-28 14:15:00,980 | INFO | total number of ended hypotheses: 164
2026-01-28 14:15:00,981 | INFO | best hypo: ▁est▁ce▁que▁lui▁peut▁me▁leur▁couvrir▁en▁même▁temps

2026-01-28 14:15:00,982 | INFO | speech length: 148800
2026-01-28 14:15:01,022 | INFO | decoder input length: 232
2026-01-28 14:15:01,022 | INFO | max output length: 232
2026-01-28 14:15:01,022 | INFO | min output length: 23
2026-01-28 14:15:09,507 | INFO | end detected at 72
2026-01-28 14:15:09,509 | INFO | -28.27 * 0.5 = -14.14 for decoder
2026-01-28 14:15:09,509 | INFO | -40.52 * 0.5 = -20.26 for ctc
2026-01-28 14:15:09,510 | INFO | total log probability: -34.40
2026-01-28 14:15:09,510 | INFO | normalized log probability: -0.55
2026-01-28 14:15:09,510 | INFO | total number of ended hypotheses: 184
2026-01-28 14:15:09,510 | INFO | best hypo: ▁en▁effet▁pas▁que▁nous▁leur▁emmènes▁de▁le▁restaurer▁au▁moment▁où▁passer▁sa▁défaite▁de▁lire▁tout▁est▁relatif▁tout▁est▁relatif▁et▁ses▁déserts

2026-01-28 14:15:09,512 | INFO | speech length: 11840
2026-01-28 14:15:09,549 | INFO | decoder input length: 18
2026-01-28 14:15:09,549 | INFO | max output length: 18
2026-01-28 14:15:09,549 | INFO | min output length: 1
2026-01-28 14:15:11,200 | INFO | adding <eos> in the last position in the loop
2026-01-28 14:15:11,209 | INFO | no hypothesis. Finish decoding.
2026-01-28 14:15:11,211 | INFO |  -5.34 * 0.5 =  -2.67 for decoder
2026-01-28 14:15:11,211 | INFO | -11.17 * 0.5 =  -5.59 for ctc
2026-01-28 14:15:11,211 | INFO | total log probability: -8.26
2026-01-28 14:15:11,211 | INFO | normalized log probability: -1.03
2026-01-28 14:15:11,211 | INFO | total number of ended hypotheses: 196
2026-01-28 14:15:11,211 | INFO | best hypo: ▁oui▁monsieur

2026-01-28 14:15:11,213 | INFO | speech length: 20800
2026-01-28 14:15:11,253 | INFO | decoder input length: 32
2026-01-28 14:15:11,254 | INFO | max output length: 32
2026-01-28 14:15:11,254 | INFO | min output length: 3
2026-01-28 14:15:12,886 | INFO | end detected at 16
2026-01-28 14:15:12,887 | INFO |  -2.93 * 0.5 =  -1.46 for decoder
2026-01-28 14:15:12,887 | INFO |  -6.67 * 0.5 =  -3.33 for ctc
2026-01-28 14:15:12,887 | INFO | total log probability: -4.80
2026-01-28 14:15:12,887 | INFO | normalized log probability: -0.44
2026-01-28 14:15:12,887 | INFO | total number of ended hypotheses: 183
2026-01-28 14:15:12,887 | INFO | best hypo: ▁la▁voix▁en▁sellerie

2026-01-28 14:15:12,889 | INFO | speech length: 85280
2026-01-28 14:15:12,922 | INFO | decoder input length: 132
2026-01-28 14:15:12,923 | INFO | max output length: 132
2026-01-28 14:15:12,923 | INFO | min output length: 13
2026-01-28 14:15:17,167 | INFO | end detected at 38
2026-01-28 14:15:17,168 | INFO |  -7.99 * 0.5 =  -4.00 for decoder
2026-01-28 14:15:17,168 | INFO | -10.47 * 0.5 =  -5.23 for ctc
2026-01-28 14:15:17,168 | INFO | total log probability: -9.23
2026-01-28 14:15:17,168 | INFO | normalized log probability: -0.29
2026-01-28 14:15:17,169 | INFO | total number of ended hypotheses: 172
2026-01-28 14:15:17,169 | INFO | best hypo: ▁est▁ce▁que▁c'est▁est▁ce▁que▁c'est▁moi▁qui▁est▁ce▁que▁je▁peux▁sur▁lui▁amer▁tissu

2026-01-28 14:15:17,171 | INFO | speech length: 90720
2026-01-28 14:15:17,216 | INFO | decoder input length: 141
2026-01-28 14:15:17,216 | INFO | max output length: 141
2026-01-28 14:15:17,216 | INFO | min output length: 14
2026-01-28 14:15:24,985 | INFO | end detected at 69
2026-01-28 14:15:24,986 | INFO | -12.79 * 0.5 =  -6.39 for decoder
2026-01-28 14:15:24,986 | INFO | -11.58 * 0.5 =  -5.79 for ctc
2026-01-28 14:15:24,986 | INFO | total log probability: -12.18
2026-01-28 14:15:24,986 | INFO | normalized log probability: -0.19
2026-01-28 14:15:24,986 | INFO | total number of ended hypotheses: 174
2026-01-28 14:15:24,987 | INFO | best hypo: ▁est▁ce▁que▁j'ai▁un▁rouleautissu▁de▁velours▁beige▁alors▁où▁il▁faudra▁que▁tu▁me▁dises▁aussi▁l'éteinte▁que▁je▁peux▁mettre▁là▁dessus

2026-01-28 14:15:24,989 | INFO | speech length: 58240
2026-01-28 14:15:25,028 | INFO | decoder input length: 90
2026-01-28 14:15:25,028 | INFO | max output length: 90
2026-01-28 14:15:25,028 | INFO | min output length: 9
2026-01-28 14:15:28,215 | INFO | end detected at 30
2026-01-28 14:15:28,217 | INFO |  -2.79 * 0.5 =  -1.39 for decoder
2026-01-28 14:15:28,217 | INFO |  -5.63 * 0.5 =  -2.82 for ctc
2026-01-28 14:15:28,217 | INFO | total log probability: -4.21
2026-01-28 14:15:28,218 | INFO | normalized log probability: -0.17
2026-01-28 14:15:28,218 | INFO | total number of ended hypotheses: 173
2026-01-28 14:15:28,218 | INFO | best hypo: ▁déjà▁les▁teintes▁vont▁se▁savoir▁que▁tu▁as▁une▁base

2026-01-28 14:15:28,220 | INFO | speech length: 181920
2026-01-28 14:15:28,264 | INFO | decoder input length: 283
2026-01-28 14:15:28,264 | INFO | max output length: 283
2026-01-28 14:15:28,264 | INFO | min output length: 28
2026-01-28 14:15:40,202 | INFO | end detected at 97
2026-01-28 14:15:40,203 | INFO | -46.88 * 0.5 = -23.44 for decoder
2026-01-28 14:15:40,203 | INFO | -55.55 * 0.5 = -27.77 for ctc
2026-01-28 14:15:40,203 | INFO | total log probability: -51.21
2026-01-28 14:15:40,203 | INFO | normalized log probability: -0.58
2026-01-28 14:15:40,203 | INFO | total number of ended hypotheses: 185
2026-01-28 14:15:40,204 | INFO | best hypo: ▁nous▁autres▁en▁fonction▁de▁ta▁pièce▁ce▁genre▁si▁tu▁as▁bien▁mis▁si▁je▁me▁gardais▁l'étanne▁de▁l'époque▁non▁fait▁pour▁possible▁m'obliger▁tes▁bonne▁obligation▁jai▁je▁bêche▁clair

2026-01-28 14:15:40,207 | INFO | speech length: 54880
2026-01-28 14:15:40,244 | INFO | decoder input length: 85
2026-01-28 14:15:40,244 | INFO | max output length: 85
2026-01-28 14:15:40,244 | INFO | min output length: 8
2026-01-28 14:15:44,936 | INFO | end detected at 46
2026-01-28 14:15:44,937 | INFO | -20.58 * 0.5 = -10.29 for decoder
2026-01-28 14:15:44,937 | INFO | -41.31 * 0.5 = -20.66 for ctc
2026-01-28 14:15:44,937 | INFO | total log probability: -30.95
2026-01-28 14:15:44,937 | INFO | normalized log probability: -0.77
2026-01-28 14:15:44,937 | INFO | total number of ended hypotheses: 164
2026-01-28 14:15:44,938 | INFO | best hypo: ▁un▁velours▁mèche▁clair▁loin▁ma▁femme▁monsque▁je▁vous▁suis▁se▁promène▁dans▁la▁pièce

2026-01-28 14:15:44,940 | INFO | speech length: 129600
2026-01-28 14:15:44,974 | INFO | decoder input length: 202
2026-01-28 14:15:44,974 | INFO | max output length: 202
2026-01-28 14:15:44,975 | INFO | min output length: 20
2026-01-28 14:15:53,801 | INFO | end detected at 81
2026-01-28 14:15:53,803 | INFO | -26.12 * 0.5 = -13.06 for decoder
2026-01-28 14:15:53,803 | INFO | -25.12 * 0.5 = -12.56 for ctc
2026-01-28 14:15:53,803 | INFO | total log probability: -25.62
2026-01-28 14:15:53,803 | INFO | normalized log probability: -0.35
2026-01-28 14:15:53,803 | INFO | total number of ended hypotheses: 192
2026-01-28 14:15:53,804 | INFO | best hypo: ▁comme▁j'aurais▁mis▁des▁coussins▁à▁fond▁roulis▁je▁commagé▁sur▁l'hon▁amérindienne▁du▁fait▁que▁tu▁as▁déjà▁quelque▁chose▁à▁fond▁clair▁oui▁je▁crains▁que▁bagé

2026-01-28 14:15:53,806 | INFO | speech length: 19520
2026-01-28 14:15:53,844 | INFO | decoder input length: 30
2026-01-28 14:15:53,844 | INFO | max output length: 30
2026-01-28 14:15:53,844 | INFO | min output length: 3
2026-01-28 14:15:55,463 | INFO | end detected at 18
2026-01-28 14:15:55,464 | INFO |  -4.98 * 0.5 =  -2.49 for decoder
2026-01-28 14:15:55,465 | INFO |  -6.95 * 0.5 =  -3.47 for ctc
2026-01-28 14:15:55,465 | INFO | total log probability: -5.96
2026-01-28 14:15:55,465 | INFO | normalized log probability: -0.66
2026-01-28 14:15:55,465 | INFO | total number of ended hypotheses: 200
2026-01-28 14:15:55,465 | INFO | best hypo: ▁jaunâtre▁anti▁peu

2026-01-28 14:15:55,467 | INFO | speech length: 88480
2026-01-28 14:15:55,504 | INFO | decoder input length: 137
2026-01-28 14:15:55,504 | INFO | max output length: 137
2026-01-28 14:15:55,504 | INFO | min output length: 13
2026-01-28 14:15:59,935 | INFO | end detected at 46
2026-01-28 14:15:59,937 | INFO | -11.54 * 0.5 =  -5.77 for decoder
2026-01-28 14:15:59,937 | INFO | -23.90 * 0.5 = -11.95 for ctc
2026-01-28 14:15:59,937 | INFO | total log probability: -17.72
2026-01-28 14:15:59,937 | INFO | normalized log probability: -0.45
2026-01-28 14:15:59,937 | INFO | total number of ended hypotheses: 179
2026-01-28 14:15:59,938 | INFO | best hypo: ▁sous▁mon▁tissu▁latte▁coussin▁est▁quand▁même▁foncé▁rouge▁rojor▁madras▁sainte▁louis

2026-01-28 14:15:59,940 | INFO | speech length: 14240
2026-01-28 14:16:00,001 | INFO | decoder input length: 21
2026-01-28 14:16:00,001 | INFO | max output length: 21
2026-01-28 14:16:00,001 | INFO | min output length: 2
2026-01-28 14:16:01,581 | INFO | end detected at 17
2026-01-28 14:16:01,582 | INFO |  -4.20 * 0.5 =  -2.10 for decoder
2026-01-28 14:16:01,582 | INFO |  -7.62 * 0.5 =  -3.81 for ctc
2026-01-28 14:16:01,582 | INFO | total log probability: -5.91
2026-01-28 14:16:01,583 | INFO | normalized log probability: -0.49
2026-01-28 14:16:01,583 | INFO | total number of ended hypotheses: 174
2026-01-28 14:16:01,583 | INFO | best hypo: ▁quoi▁est▁ce▁assez▁bon

2026-01-28 14:16:01,585 | INFO | speech length: 8160
2026-01-28 14:16:01,613 | INFO | decoder input length: 12
2026-01-28 14:16:01,613 | INFO | max output length: 12
2026-01-28 14:16:01,613 | INFO | min output length: 1
2026-01-28 14:16:02,429 | INFO | end detected at 8
2026-01-28 14:16:02,431 | INFO |  -0.97 * 0.5 =  -0.48 for decoder
2026-01-28 14:16:02,431 | INFO |  -0.71 * 0.5 =  -0.36 for ctc
2026-01-28 14:16:02,431 | INFO | total log probability: -0.84
2026-01-28 14:16:02,431 | INFO | normalized log probability: -0.21
2026-01-28 14:16:02,431 | INFO | total number of ended hypotheses: 157
2026-01-28 14:16:02,431 | INFO | best hypo: ▁oui

2026-01-28 14:16:02,433 | INFO | speech length: 126560
2026-01-28 14:16:02,479 | INFO | decoder input length: 197
2026-01-28 14:16:02,479 | INFO | max output length: 197
2026-01-28 14:16:02,480 | INFO | min output length: 19
2026-01-28 14:16:08,577 | INFO | end detected at 75
2026-01-28 14:16:08,578 | INFO | -18.52 * 0.5 =  -9.26 for decoder
2026-01-28 14:16:08,578 | INFO | -29.83 * 0.5 = -14.92 for ctc
2026-01-28 14:16:08,578 | INFO | total log probability: -24.18
2026-01-28 14:16:08,578 | INFO | normalized log probability: -0.36
2026-01-28 14:16:08,578 | INFO | total number of ended hypotheses: 176
2026-01-28 14:16:08,579 | INFO | best hypo: ▁est▁ce▁qu'il▁faut▁que▁j'ai▁à▁ce▁que▁j'aime▁pas▁trop▁la▁franche▁tout▁autour▁encore▁faut▁c'éter▁une▁frange▁vint▁trendre▁je▁peux▁l'oublier

2026-01-28 14:16:08,581 | INFO | speech length: 10240
2026-01-28 14:16:08,609 | INFO | decoder input length: 15
2026-01-28 14:16:08,609 | INFO | max output length: 15
2026-01-28 14:16:08,609 | INFO | min output length: 1
2026-01-28 14:16:09,022 | INFO | adding <eos> in the last position in the loop
2026-01-28 14:16:09,030 | INFO | no hypothesis. Finish decoding.
2026-01-28 14:16:09,031 | INFO |  -1.77 * 0.5 =  -0.88 for decoder
2026-01-28 14:16:09,031 | INFO |  -6.41 * 0.5 =  -3.21 for ctc
2026-01-28 14:16:09,031 | INFO | total log probability: -4.09
2026-01-28 14:16:09,031 | INFO | normalized log probability: -0.45
2026-01-28 14:16:09,031 | INFO | total number of ended hypotheses: 170
2026-01-28 14:16:09,031 | INFO | best hypo: ▁c'est▁en▁voiture

2026-01-28 14:16:09,033 | INFO | speech length: 44640
2026-01-28 14:16:09,060 | INFO | decoder input length: 69
2026-01-28 14:16:09,060 | INFO | max output length: 69
2026-01-28 14:16:09,060 | INFO | min output length: 6
2026-01-28 14:16:10,298 | INFO | end detected at 35
2026-01-28 14:16:10,299 | INFO |  -3.20 * 0.5 =  -1.60 for decoder
2026-01-28 14:16:10,300 | INFO |  -2.73 * 0.5 =  -1.37 for ctc
2026-01-28 14:16:10,300 | INFO | total log probability: -2.97
2026-01-28 14:16:10,300 | INFO | normalized log probability: -0.10
2026-01-28 14:16:10,300 | INFO | total number of ended hypotheses: 163
2026-01-28 14:16:10,300 | INFO | best hypo: ▁j'ai▁est▁ce▁que▁faut▁que▁les▁pieds▁soient▁quand▁même▁me▁cachés

2026-01-28 14:16:10,302 | INFO | speech length: 136640
2026-01-28 14:16:10,336 | INFO | decoder input length: 213
2026-01-28 14:16:10,336 | INFO | max output length: 213
2026-01-28 14:16:10,336 | INFO | min output length: 21
2026-01-28 14:16:14,068 | INFO | end detected at 74
2026-01-28 14:16:14,069 | INFO | -24.41 * 0.5 = -12.21 for decoder
2026-01-28 14:16:14,070 | INFO | -21.92 * 0.5 = -10.96 for ctc
2026-01-28 14:16:14,070 | INFO | total log probability: -23.17
2026-01-28 14:16:14,070 | INFO | normalized log probability: -0.34
2026-01-28 14:16:14,070 | INFO | total number of ended hypotheses: 162
2026-01-28 14:16:14,070 | INFO | best hypo: ▁dans▁la▁mesure▁du▁remède▁de▁france▁une▁panne▁à▁ce▁que▁j'ai▁regardé▁sur▁un▁catalogue▁et▁j'ai▁vu▁qu'en▁fait▁le▁tissu▁parfois▁descendait▁jusqu'au▁bas

2026-01-28 14:16:14,072 | INFO | speech length: 17761
2026-01-28 14:16:14,104 | INFO | decoder input length: 27
2026-01-28 14:16:14,104 | INFO | max output length: 27
2026-01-28 14:16:14,104 | INFO | min output length: 2
2026-01-28 14:16:14,804 | INFO | end detected at 23
2026-01-28 14:16:14,806 | INFO |  -6.91 * 0.5 =  -3.46 for decoder
2026-01-28 14:16:14,806 | INFO | -12.19 * 0.5 =  -6.10 for ctc
2026-01-28 14:16:14,806 | INFO | total log probability: -9.55
2026-01-28 14:16:14,806 | INFO | normalized log probability: -0.64
2026-01-28 14:16:14,806 | INFO | total number of ended hypotheses: 190
2026-01-28 14:16:14,806 | INFO | best hypo: ▁soit▁c'était▁une▁franche▁ma

2026-01-28 14:16:14,808 | INFO | speech length: 65760
2026-01-28 14:16:14,840 | INFO | decoder input length: 102
2026-01-28 14:16:14,840 | INFO | max output length: 102
2026-01-28 14:16:14,840 | INFO | min output length: 10
2026-01-28 14:16:16,378 | INFO | end detected at 39
2026-01-28 14:16:16,380 | INFO |  -7.03 * 0.5 =  -3.52 for decoder
2026-01-28 14:16:16,380 | INFO |  -9.32 * 0.5 =  -4.66 for ctc
2026-01-28 14:16:16,380 | INFO | total log probability: -8.17
2026-01-28 14:16:16,380 | INFO | normalized log probability: -0.25
2026-01-28 14:16:16,380 | INFO | total number of ended hypotheses: 177
2026-01-28 14:16:16,381 | INFO | best hypo: ▁ouh▁mais▁ça▁ne▁déguise▁pas▁du▁fait▁que▁c'est▁un▁fauteuil▁à▁une▁écorne

2026-01-28 14:16:16,382 | INFO | speech length: 77440
2026-01-28 14:16:16,416 | INFO | decoder input length: 120
2026-01-28 14:16:16,416 | INFO | max output length: 120
2026-01-28 14:16:16,416 | INFO | min output length: 12
2026-01-28 14:16:18,484 | INFO | end detected at 52
2026-01-28 14:16:18,485 | INFO |  -9.68 * 0.5 =  -4.84 for decoder
2026-01-28 14:16:18,485 | INFO | -22.82 * 0.5 = -11.41 for ctc
2026-01-28 14:16:18,485 | INFO | total log probability: -16.25
2026-01-28 14:16:18,485 | INFO | normalized log probability: -0.37
2026-01-28 14:16:18,486 | INFO | total number of ended hypotheses: 168
2026-01-28 14:16:18,486 | INFO | best hypo: ▁je▁venais▁de▁c'est▁ce▁que▁ça▁donne▁un▁aspect▁quand▁même▁plus▁moderne▁si▁tu▁mets▁une▁orange

2026-01-28 14:16:18,488 | INFO | speech length: 15040
2026-01-28 14:16:18,516 | INFO | decoder input length: 23
2026-01-28 14:16:18,516 | INFO | max output length: 23
2026-01-28 14:16:18,516 | INFO | min output length: 2
2026-01-28 14:16:18,977 | INFO | end detected at 14
2026-01-28 14:16:18,978 | INFO |  -2.50 * 0.5 =  -1.25 for decoder
2026-01-28 14:16:18,978 | INFO |  -7.17 * 0.5 =  -3.58 for ctc
2026-01-28 14:16:18,979 | INFO | total log probability: -4.84
2026-01-28 14:16:18,979 | INFO | normalized log probability: -0.69
2026-01-28 14:16:18,979 | INFO | total number of ended hypotheses: 186
2026-01-28 14:16:18,979 | INFO | best hypo: ▁pas▁gênant

2026-01-28 14:16:18,980 | INFO | speech length: 44160
2026-01-28 14:16:19,014 | INFO | decoder input length: 68
2026-01-28 14:16:19,014 | INFO | max output length: 68
2026-01-28 14:16:19,014 | INFO | min output length: 6
2026-01-28 14:16:20,184 | INFO | end detected at 32
2026-01-28 14:16:20,185 | INFO |  -5.52 * 0.5 =  -2.76 for decoder
2026-01-28 14:16:20,185 | INFO |  -8.37 * 0.5 =  -4.18 for ctc
2026-01-28 14:16:20,185 | INFO | total log probability: -6.95
2026-01-28 14:16:20,185 | INFO | normalized log probability: -0.26
2026-01-28 14:16:20,185 | INFO | total number of ended hypotheses: 176
2026-01-28 14:16:20,185 | INFO | best hypo: ▁enfin▁je▁l'ai▁gardé▁la▁frange▁mais▁c'est▁inutilisable

2026-01-28 14:16:20,187 | INFO | speech length: 68800
2026-01-28 14:16:20,216 | INFO | decoder input length: 107
2026-01-28 14:16:20,216 | INFO | max output length: 107
2026-01-28 14:16:20,216 | INFO | min output length: 10
2026-01-28 14:16:21,740 | INFO | end detected at 39
2026-01-28 14:16:21,741 | INFO | -10.28 * 0.5 =  -5.14 for decoder
2026-01-28 14:16:21,741 | INFO |  -9.79 * 0.5 =  -4.90 for ctc
2026-01-28 14:16:21,741 | INFO | total log probability: -10.04
2026-01-28 14:16:21,741 | INFO | normalized log probability: -0.30
2026-01-28 14:16:21,741 | INFO | total number of ended hypotheses: 183
2026-01-28 14:16:21,741 | INFO | best hypo: ▁ça▁a▁pris▁la▁poussière▁c'est▁même▁au▁nettoyage▁quand▁que▁ce▁pola▁benne

2026-01-28 14:16:21,743 | INFO | speech length: 27680
2026-01-28 14:16:21,773 | INFO | decoder input length: 42
2026-01-28 14:16:21,773 | INFO | max output length: 42
2026-01-28 14:16:21,773 | INFO | min output length: 4
2026-01-28 14:16:22,605 | INFO | end detected at 26
2026-01-28 14:16:22,606 | INFO |  -8.19 * 0.5 =  -4.09 for decoder
2026-01-28 14:16:22,606 | INFO | -17.73 * 0.5 =  -8.86 for ctc
2026-01-28 14:16:22,606 | INFO | total log probability: -12.96
2026-01-28 14:16:22,607 | INFO | normalized log probability: -0.72
2026-01-28 14:16:22,607 | INFO | total number of ended hypotheses: 181
2026-01-28 14:16:22,607 | INFO | best hypo: ▁alors▁que▁tu▁peux▁faire▁comme▁star▁une▁venture

2026-01-28 14:16:22,608 | INFO | speech length: 13920
2026-01-28 14:16:22,638 | INFO | decoder input length: 21
2026-01-28 14:16:22,639 | INFO | max output length: 21
2026-01-28 14:16:22,639 | INFO | min output length: 2
2026-01-28 14:16:23,182 | INFO | end detected at 17
2026-01-28 14:16:23,184 | INFO |  -3.49 * 0.5 =  -1.74 for decoder
2026-01-28 14:16:23,184 | INFO |  -3.34 * 0.5 =  -1.67 for ctc
2026-01-28 14:16:23,184 | INFO | total log probability: -3.42
2026-01-28 14:16:23,184 | INFO | normalized log probability: -0.28
2026-01-28 14:16:23,184 | INFO | total number of ended hypotheses: 157
2026-01-28 14:16:23,184 | INFO | best hypo: ▁je▁vois▁avec▁monsieur▁là

2026-01-28 14:16:23,186 | INFO | speech length: 94080
2026-01-28 14:16:23,220 | INFO | decoder input length: 146
2026-01-28 14:16:23,220 | INFO | max output length: 146
2026-01-28 14:16:23,220 | INFO | min output length: 14
2026-01-28 14:16:25,979 | INFO | end detected at 64
2026-01-28 14:16:25,980 | INFO | -21.20 * 0.5 = -10.60 for decoder
2026-01-28 14:16:25,980 | INFO | -35.41 * 0.5 = -17.71 for ctc
2026-01-28 14:16:25,980 | INFO | total log probability: -28.31
2026-01-28 14:16:25,980 | INFO | normalized log probability: -0.49
2026-01-28 14:16:25,980 | INFO | total number of ended hypotheses: 168
2026-01-28 14:16:25,981 | INFO | best hypo: ▁mon▁bien▁que▁tu▁jugeaisrais▁chaleur▁et▁votre▁problème▁je▁me▁disais▁j'irais▁peut▁être▁à▁vir▁parce▁qu'on▁était▁allé▁le▁voir▁ce▁monsieur▁là

2026-01-28 14:16:25,983 | INFO | speech length: 59360
2026-01-28 14:16:26,016 | INFO | decoder input length: 92
2026-01-28 14:16:26,016 | INFO | max output length: 92
2026-01-28 14:16:26,016 | INFO | min output length: 9
2026-01-28 14:16:27,269 | INFO | end detected at 31
2026-01-28 14:16:27,270 | INFO | -11.54 * 0.5 =  -5.77 for decoder
2026-01-28 14:16:27,270 | INFO |  -8.86 * 0.5 =  -4.43 for ctc
2026-01-28 14:16:27,270 | INFO | total log probability: -10.20
2026-01-28 14:16:27,270 | INFO | normalized log probability: -0.39
2026-01-28 14:16:27,270 | INFO | total number of ended hypotheses: 171
2026-01-28 14:16:27,271 | INFO | best hypo: ▁il▁était▁très▁simpa▁et▁puis▁cher▁collabors▁allant▁du▁tout

2026-01-28 14:16:27,273 | INFO | speech length: 32640
2026-01-28 14:16:27,302 | INFO | decoder input length: 50
2026-01-28 14:16:27,303 | INFO | max output length: 50
2026-01-28 14:16:27,303 | INFO | min output length: 5
2026-01-28 14:16:28,257 | INFO | end detected at 23
2026-01-28 14:16:28,257 | INFO |  -1.46 * 0.5 =  -0.73 for decoder
2026-01-28 14:16:28,258 | INFO |  -0.03 * 0.5 =  -0.01 for ctc
2026-01-28 14:16:28,258 | INFO | total log probability: -0.74
2026-01-28 14:16:28,258 | INFO | normalized log probability: -0.04
2026-01-28 14:16:28,258 | INFO | total number of ended hypotheses: 152
2026-01-28 14:16:28,258 | INFO | best hypo: ▁sa▁femme▁est▁originaire▁de▁la▁région

2026-01-28 14:16:28,259 | INFO | speech length: 26080
2026-01-28 14:16:28,292 | INFO | decoder input length: 40
2026-01-28 14:16:28,292 | INFO | max output length: 40
2026-01-28 14:16:28,292 | INFO | min output length: 4
2026-01-28 14:16:29,210 | INFO | end detected at 27
2026-01-28 14:16:29,211 | INFO |  -6.95 * 0.5 =  -3.48 for decoder
2026-01-28 14:16:29,211 | INFO | -14.32 * 0.5 =  -7.16 for ctc
2026-01-28 14:16:29,211 | INFO | total log probability: -10.64
2026-01-28 14:16:29,211 | INFO | normalized log probability: -0.59
2026-01-28 14:16:29,211 | INFO | total number of ended hypotheses: 169
2026-01-28 14:16:29,211 | INFO | best hypo: ▁vraiment▁je▁travaille▁avec▁les▁maisons▁morales

2026-01-28 14:16:29,213 | INFO | speech length: 11520
2026-01-28 14:16:29,243 | INFO | decoder input length: 17
2026-01-28 14:16:29,243 | INFO | max output length: 17
2026-01-28 14:16:29,243 | INFO | min output length: 1
2026-01-28 14:16:29,540 | INFO | end detected at 8
2026-01-28 14:16:29,542 | INFO |  -1.14 * 0.5 =  -0.57 for decoder
2026-01-28 14:16:29,542 | INFO |  -1.27 * 0.5 =  -0.64 for ctc
2026-01-28 14:16:29,542 | INFO | total log probability: -1.21
2026-01-28 14:16:29,542 | INFO | normalized log probability: -0.30
2026-01-28 14:16:29,542 | INFO | total number of ended hypotheses: 157
2026-01-28 14:16:29,542 | INFO | best hypo: ▁non

2026-01-28 14:16:29,551 | INFO | Chunk: 0 | WER=47.619048 | S=2 D=0 I=8
2026-01-28 14:16:29,552 | INFO | Chunk: 1 | WER=166.666667 | S=3 D=0 I=2
2026-01-28 14:16:29,552 | INFO | Chunk: 2 | WER=106.250000 | S=16 D=0 I=1
2026-01-28 14:16:29,553 | INFO | Chunk: 3 | WER=63.333333 | S=7 D=10 I=2
2026-01-28 14:16:29,553 | INFO | Chunk: 4 | WER=100.000000 | S=1 D=6 I=0
2026-01-28 14:16:29,553 | INFO | Chunk: 5 | WER=121.428571 | S=14 D=0 I=3
2026-01-28 14:16:29,554 | INFO | Chunk: 6 | WER=112.500000 | S=8 D=0 I=1
2026-01-28 14:16:29,556 | INFO | Chunk: 7 | WER=49.253731 | S=10 D=17 I=6
2026-01-28 14:16:29,556 | INFO | Chunk: 8 | WER=85.714286 | S=13 D=4 I=7
2026-01-28 14:16:29,557 | INFO | Chunk: 9 | WER=71.428571 | S=8 D=9 I=8
2026-01-28 14:16:29,558 | INFO | Chunk: 10 | WER=171.428571 | S=7 D=0 I=5
2026-01-28 14:16:29,558 | INFO | Chunk: 11 | WER=95.238095 | S=18 D=2 I=0
2026-01-28 14:16:29,559 | INFO | Chunk: 12 | WER=94.444444 | S=6 D=10 I=1
2026-01-28 14:16:29,559 | INFO | Chunk: 13 | WER=93.750000 | S=1 D=10 I=4
2026-01-28 14:16:29,560 | INFO | Chunk: 14 | WER=59.615385 | S=13 D=10 I=8
2026-01-28 14:16:29,562 | INFO | Chunk: 15 | WER=72.549020 | S=16 D=18 I=3
2026-01-28 14:16:29,562 | INFO | Chunk: 16 | WER=90.476190 | S=6 D=10 I=3
2026-01-28 14:16:29,563 | INFO | Chunk: 17 | WER=84.615385 | S=9 D=12 I=1
2026-01-28 14:16:29,564 | INFO | Chunk: 18 | WER=80.000000 | S=20 D=12 I=4
2026-01-28 14:16:29,565 | INFO | Chunk: 19 | WER=79.310345 | S=3 D=14 I=6
2026-01-28 14:16:29,565 | INFO | Chunk: 20 | WER=94.117647 | S=12 D=4 I=0
2026-01-28 14:16:29,565 | INFO | Chunk: 21 | WER=100.000000 | S=11 D=0 I=0
2026-01-28 14:16:29,566 | INFO | Chunk: 22 | WER=90.322581 | S=16 D=8 I=4
2026-01-28 14:16:29,566 | INFO | Chunk: 23 | WER=100.000000 | S=2 D=5 I=0
2026-01-28 14:16:29,567 | INFO | Chunk: 24 | WER=100.000000 | S=4 D=0 I=0
2026-01-28 14:16:29,567 | INFO | Chunk: 25 | WER=30.434783 | S=1 D=4 I=2
2026-01-28 14:16:29,568 | INFO | Chunk: 26 | WER=109.090909 | S=5 D=7 I=12
2026-01-28 14:16:29,568 | INFO | Chunk: 27 | WER=157.142857 | S=7 D=0 I=4
2026-01-28 14:16:29,569 | INFO | Chunk: 28 | WER=68.888889 | S=18 D=11 I=2
2026-01-28 14:16:29,569 | INFO | Chunk: 29 | WER=214.285714 | S=6 D=0 I=9
2026-01-28 14:16:29,570 | INFO | Chunk: 30 | WER=69.047619 | S=12 D=14 I=3
2026-01-28 14:16:29,571 | INFO | Chunk: 31 | WER=100.000000 | S=3 D=3 I=0
2026-01-28 14:16:29,571 | INFO | Chunk: 32 | WER=84.210526 | S=3 D=9 I=4
2026-01-28 14:16:29,571 | INFO | Chunk: 33 | WER=100.000000 | S=5 D=0 I=0
2026-01-28 14:16:29,571 | INFO | Chunk: 34 | WER=0.000000 | S=1 D=0 I=0
2026-01-28 14:16:29,572 | INFO | Chunk: 35 | WER=62.857143 | S=6 D=10 I=6
2026-01-28 14:16:29,572 | INFO | Chunk: 36 | WER=60.000000 | S=2 D=1 I=0
2026-01-28 14:16:29,573 | INFO | Chunk: 37 | WER=107.692308 | S=13 D=0 I=1
2026-01-28 14:16:29,574 | INFO | Chunk: 38 | WER=51.428571 | S=5 D=8 I=5
2026-01-28 14:16:29,574 | INFO | Chunk: 39 | WER=150.000000 | S=4 D=0 I=2
2026-01-28 14:16:29,574 | INFO | Chunk: 40 | WER=100.000000 | S=16 D=5 I=0
2026-01-28 14:16:29,575 | INFO | Chunk: 41 | WER=85.714286 | S=6 D=13 I=5
2026-01-28 14:16:29,575 | INFO | Chunk: 42 | WER=100.000000 | S=2 D=1 I=0
2026-01-28 14:16:29,575 | INFO | Chunk: 43 | WER=100.000000 | S=11 D=2 I=0
2026-01-28 14:16:29,576 | INFO | Chunk: 44 | WER=110.000000 | S=0 D=3 I=8
2026-01-28 14:16:29,576 | INFO | Chunk: 45 | WER=180.000000 | S=5 D=0 I=4
2026-01-28 14:16:29,576 | INFO | Chunk: 46 | WER=100.000000 | S=5 D=3 I=0
2026-01-28 14:16:29,577 | INFO | Chunk: 47 | WER=93.103448 | S=24 D=2 I=1
2026-01-28 14:16:29,577 | INFO | Chunk: 48 | WER=100.000000 | S=1 D=6 I=5
2026-01-28 14:16:29,577 | INFO | Chunk: 49 | WER=175.000000 | S=4 D=0 I=3
2026-01-28 14:16:29,577 | INFO | Chunk: 50 | WER=175.000000 | S=4 D=0 I=3
2026-01-28 14:16:29,578 | INFO | Chunk: 51 | WER=100.000000 | S=1 D=4 I=0
2026-01-28 14:16:29,974 | INFO | File: Rhap-D0009.wav | WER=60.703518 | S=356 D=179 I=69
2026-01-28 14:16:29,974 | INFO | ------------------------------
2026-01-28 14:16:29,974 | INFO | Conf cv Done!
2026-01-28 14:16:30,114 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 14:16:30,133 | INFO | Vocabulary size: 47
2026-01-28 14:16:30,651 | INFO | Gradient checkpoint layers: []
2026-01-28 14:16:31,297 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 14:16:31,302 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 14:16:31,302 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 14:16:31,303 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 14:16:31,305 | INFO | speech length: 142560
2026-01-28 14:16:31,338 | INFO | decoder input length: 222
2026-01-28 14:16:31,338 | INFO | max output length: 222
2026-01-28 14:16:31,338 | INFO | min output length: 22
2026-01-28 14:16:38,192 | INFO | end detected at 152
2026-01-28 14:16:38,193 | INFO | -13.37 * 0.5 =  -6.68 for decoder
2026-01-28 14:16:38,193 | INFO |  -2.24 * 0.5 =  -1.12 for ctc
2026-01-28 14:16:38,193 | INFO | total log probability: -7.81
2026-01-28 14:16:38,193 | INFO | normalized log probability: -0.05
2026-01-28 14:16:38,193 | INFO | total number of ended hypotheses: 145
2026-01-28 14:16:38,195 | INFO | best hypo: bon<space>autre<space>chose<space>on<space>a<space>nos<space>nos<space>chaises<space>qui<space>ont<space>besoin<space>de<space>refaire<space>est<space>ce<space>que<space>tu<space>connais<space>des<space>un<space>endroit<space>où<space>je<space>pourrais<space>les<space>laisser<space>pour<space>le<space>rempaillage

2026-01-28 14:16:38,197 | INFO | speech length: 15840
2026-01-28 14:16:38,225 | INFO | decoder input length: 24
2026-01-28 14:16:38,225 | INFO | max output length: 24
2026-01-28 14:16:38,225 | INFO | min output length: 2
2026-01-28 14:16:38,890 | INFO | adding <eos> in the last position in the loop
2026-01-28 14:16:38,898 | INFO | no hypothesis. Finish decoding.
2026-01-28 14:16:38,898 | INFO |  -2.20 * 0.5 =  -1.10 for decoder
2026-01-28 14:16:38,898 | INFO |  -6.58 * 0.5 =  -3.29 for ctc
2026-01-28 14:16:38,898 | INFO | total log probability: -4.39
2026-01-28 14:16:38,899 | INFO | normalized log probability: -0.18
2026-01-28 14:16:38,899 | INFO | total number of ended hypotheses: 103
2026-01-28 14:16:38,899 | INFO | best hypo: bah<space>les<space>chaises<space>i<space>faut

2026-01-28 14:16:38,900 | INFO | speech length: 61920
2026-01-28 14:16:38,927 | INFO | decoder input length: 96
2026-01-28 14:16:38,927 | INFO | max output length: 96
2026-01-28 14:16:38,927 | INFO | min output length: 9
2026-01-28 14:16:42,059 | INFO | end detected at 94
2026-01-28 14:16:42,060 | INFO | -18.93 * 0.5 =  -9.46 for decoder
2026-01-28 14:16:42,060 | INFO | -38.64 * 0.5 = -19.32 for ctc
2026-01-28 14:16:42,060 | INFO | total log probability: -28.78
2026-01-28 14:16:42,060 | INFO | normalized log probability: -0.35
2026-01-28 14:16:42,061 | INFO | total number of ended hypotheses: 184
2026-01-28 14:16:42,062 | INFO | best hypo: c'est<space>ah<space>oui<space>à<space>quelqu'un<space>qui<space>peut<space>euh<space>je<space>quelqu'un<space>qui<space>est<space>de<space>ça<space>sur<space>le<space>lendemain

2026-01-28 14:16:42,064 | INFO | speech length: 105760
2026-01-28 14:16:42,092 | INFO | decoder input length: 164
2026-01-28 14:16:42,092 | INFO | max output length: 164
2026-01-28 14:16:42,092 | INFO | min output length: 16
2026-01-28 14:16:47,786 | INFO | end detected at 151
2026-01-28 14:16:47,788 | INFO | -27.02 * 0.5 = -13.51 for decoder
2026-01-28 14:16:47,788 | INFO | -35.10 * 0.5 = -17.55 for ctc
2026-01-28 14:16:47,788 | INFO | total log probability: -31.06
2026-01-28 14:16:47,788 | INFO | normalized log probability: -0.23
2026-01-28 14:16:47,788 | INFO | total number of ended hypotheses: 197
2026-01-28 14:16:47,790 | INFO | best hypo: ah<space>c'est<space>bien<space>et<space>il<space>faut<space>compter<space>à<space>peu<space>près<space>combien<space>par<space>chasse<space>quoi<space>ce<space>sont<space>des<space>laissés<space>d'échelle<space>de<space>un<space>autre<space>côté<space>là<space>quoi<space>c'est<space>est<space>pas

2026-01-28 14:16:47,792 | INFO | speech length: 27840
2026-01-28 14:16:47,820 | INFO | decoder input length: 43
2026-01-28 14:16:47,820 | INFO | max output length: 43
2026-01-28 14:16:47,820 | INFO | min output length: 4
2026-01-28 14:16:48,395 | INFO | end detected at 17
2026-01-28 14:16:48,396 | INFO |  -1.13 * 0.5 =  -0.57 for decoder
2026-01-28 14:16:48,396 | INFO |  -1.59 * 0.5 =  -0.79 for ctc
2026-01-28 14:16:48,396 | INFO | total log probability: -1.36
2026-01-28 14:16:48,396 | INFO | normalized log probability: -0.10
2026-01-28 14:16:48,396 | INFO | total number of ended hypotheses: 148
2026-01-28 14:16:48,396 | INFO | best hypo: compter<space>euh

2026-01-28 14:16:48,398 | INFO | speech length: 63840
2026-01-28 14:16:48,424 | INFO | decoder input length: 99
2026-01-28 14:16:48,424 | INFO | max output length: 99
2026-01-28 14:16:48,424 | INFO | min output length: 9
2026-01-28 14:16:51,564 | INFO | end detected at 95
2026-01-28 14:16:51,566 | INFO | -12.44 * 0.5 =  -6.22 for decoder
2026-01-28 14:16:51,566 | INFO |  -6.74 * 0.5 =  -3.37 for ctc
2026-01-28 14:16:51,566 | INFO | total log probability: -9.59
2026-01-28 14:16:51,566 | INFO | normalized log probability: -0.11
2026-01-28 14:16:51,566 | INFO | total number of ended hypotheses: 194
2026-01-28 14:16:51,567 | INFO | best hypo: ah<space>oui<space>moi<space>je<space>pense<space>sais<space>plus<space>que<space>ça<space>quarante<space>ans<space>et<space>est<space>ce<space>qu'i<space>font<space>aussi<space>le<space>canage

2026-01-28 14:16:51,569 | INFO | speech length: 42880
2026-01-28 14:16:51,595 | INFO | decoder input length: 66
2026-01-28 14:16:51,595 | INFO | max output length: 66
2026-01-28 14:16:51,595 | INFO | min output length: 6
2026-01-28 14:16:53,249 | INFO | end detected at 54
2026-01-28 14:16:53,251 | INFO |  -8.13 * 0.5 =  -4.06 for decoder
2026-01-28 14:16:53,251 | INFO |  -2.78 * 0.5 =  -1.39 for ctc
2026-01-28 14:16:53,251 | INFO | total log probability: -5.46
2026-01-28 14:16:53,251 | INFO | normalized log probability: -0.11
2026-01-28 14:16:53,251 | INFO | total number of ended hypotheses: 178
2026-01-28 14:16:53,252 | INFO | best hypo: pareil<space>là<space>par<space>contre<space>ça<space>doit<space>être<space>plus<space>oréreux

2026-01-28 14:16:53,253 | INFO | speech length: 338880
2026-01-28 14:16:53,280 | INFO | decoder input length: 529
2026-01-28 14:16:53,281 | INFO | max output length: 529
2026-01-28 14:16:53,281 | INFO | min output length: 52
2026-01-28 14:17:31,834 | INFO | end detected at 303
2026-01-28 14:17:31,837 | INFO | -70.71 * 0.5 = -35.36 for decoder
2026-01-28 14:17:31,837 | INFO | -31.86 * 0.5 = -15.93 for ctc
2026-01-28 14:17:31,838 | INFO | total log probability: -51.28
2026-01-28 14:17:31,838 | INFO | normalized log probability: -0.18
2026-01-28 14:17:31,838 | INFO | total number of ended hypotheses: 232
2026-01-28 14:17:31,842 | INFO | best hypo: ah<space>oui<space>et<space>ils<space>nous<space>les<space>i<space>te<space>les<space>prennent<space>et<space>i<space>faut<space>compter<space>combien<space>de<space>temps<space>après<space>très<space>rapidement<space>ah<space>oui<space>un<space>jour<space>mais<space>c'est<space>tu<space>que<space>la<space>paille<space>a<space>été<space>abîmée<space>dans<space>ces<space>deux<space>de<space>la<space>maison<space>là<space>elle<space>a<space>été<space>abîmée<space>parce<space>que<space>euh<space>dessus<space>y<space>avait<space>des<space>c'est<space>pas<space>des<space>cousins<space>c'était<space>des<space>des<space>carrés<space>de<space>laines

2026-01-28 14:17:31,847 | INFO | speech length: 125760
2026-01-28 14:17:31,906 | INFO | decoder input length: 196
2026-01-28 14:17:31,907 | INFO | max output length: 196
2026-01-28 14:17:31,907 | INFO | min output length: 19
2026-01-28 14:17:47,298 | INFO | end detected at 134
2026-01-28 14:17:47,301 | INFO | -24.53 * 0.5 = -12.27 for decoder
2026-01-28 14:17:47,301 | INFO | -37.60 * 0.5 = -18.80 for ctc
2026-01-28 14:17:47,301 | INFO | total log probability: -31.07
2026-01-28 14:17:47,301 | INFO | normalized log probability: -0.25
2026-01-28 14:17:47,301 | INFO | total number of ended hypotheses: 196
2026-01-28 14:17:47,303 | INFO | best hypo: ti<space>sait<space>qu'on<space>avait<space>eu<space>en<space>tunisie<space>ce<space>qui<space>avait<space>émis<space>dessus<space>et<space>vraiment<space>c'est<space>c'est<space>pas<space>unique<space>c'est<space>pas<space>chez<space>homme<space>ah<space>oui

2026-01-28 14:17:47,305 | INFO | speech length: 133280
2026-01-28 14:17:47,334 | INFO | decoder input length: 207
2026-01-28 14:17:47,334 | INFO | max output length: 207
2026-01-28 14:17:47,334 | INFO | min output length: 20
2026-01-28 14:18:07,853 | INFO | end detected at 199
2026-01-28 14:18:07,854 | INFO | -21.50 * 0.5 = -10.75 for decoder
2026-01-28 14:18:07,855 | INFO | -31.63 * 0.5 = -15.82 for ctc
2026-01-28 14:18:07,855 | INFO | total log probability: -26.57
2026-01-28 14:18:07,855 | INFO | normalized log probability: -0.14
2026-01-28 14:18:07,855 | INFO | total number of ended hypotheses: 200
2026-01-28 14:18:07,857 | INFO | best hypo: mais<space>même<space>parfois<space>je<space>me<space>suis<space>demandé<space>si<space>c'était<space>pas<space>des<space>des<space>airs<space>qui<space>s'étaient<space>introduits<space>dans<space>un<space>an<space>bon<space>non<space>mais<space>ce<space>sont<space>les<space>les<space>impacts<space>de<space>tout<space>ça<space>qui<space>ont<space>qui<space>ont<space>pénètre<space>dans<space>la<space>page

2026-01-28 14:18:07,859 | INFO | speech length: 36320
2026-01-28 14:18:07,889 | INFO | decoder input length: 56
2026-01-28 14:18:07,889 | INFO | max output length: 56
2026-01-28 14:18:07,889 | INFO | min output length: 5
2026-01-28 14:18:11,537 | INFO | adding <eos> in the last position in the loop
2026-01-28 14:18:11,546 | INFO | no hypothesis. Finish decoding.
2026-01-28 14:18:11,547 | INFO |  -4.26 * 0.5 =  -2.13 for decoder
2026-01-28 14:18:11,547 | INFO |  -0.31 * 0.5 =  -0.16 for ctc
2026-01-28 14:18:11,547 | INFO | total log probability: -2.29
2026-01-28 14:18:11,547 | INFO | normalized log probability: -0.04
2026-01-28 14:18:11,548 | INFO | total number of ended hypotheses: 145
2026-01-28 14:18:11,548 | INFO | best hypo: bah<space>c'est<space>dommage<space>parce<space>qu'on<space>a<space>tout<space>perdu<space>comme<space>ça

2026-01-28 14:18:11,550 | INFO | speech length: 63040
2026-01-28 14:18:11,584 | INFO | decoder input length: 98
2026-01-28 14:18:11,585 | INFO | max output length: 98
2026-01-28 14:18:11,585 | INFO | min output length: 9
2026-01-28 14:18:20,556 | INFO | adding <eos> in the last position in the loop
2026-01-28 14:18:20,567 | INFO | no hypothesis. Finish decoding.
2026-01-28 14:18:20,568 | INFO | -35.02 * 0.5 = -17.51 for decoder
2026-01-28 14:18:20,568 | INFO | -114.72 * 0.5 = -57.36 for ctc
2026-01-28 14:18:20,568 | INFO | total log probability: -74.87
2026-01-28 14:18:20,568 | INFO | normalized log probability: -0.80
2026-01-28 14:18:20,568 | INFO | total number of ended hypotheses: 119
2026-01-28 14:18:20,569 | INFO | best hypo: non<space>mais<space>tu<space>me<space>rdr<space>y<space>a<space>pas<space>de<space>problème<space>puisqubon<space>ce<space>monsieur<space>là<space>i<space>f<space>en<space>plus<space>i<space>travai<space>avec<space>la

2026-01-28 14:18:20,571 | INFO | speech length: 60160
2026-01-28 14:18:20,598 | INFO | decoder input length: 93
2026-01-28 14:18:20,598 | INFO | max output length: 93
2026-01-28 14:18:20,598 | INFO | min output length: 9
2026-01-28 14:18:28,948 | INFO | end detected at 85
2026-01-28 14:18:28,950 | INFO | -20.25 * 0.5 = -10.13 for decoder
2026-01-28 14:18:28,950 | INFO | -42.27 * 0.5 = -21.13 for ctc
2026-01-28 14:18:28,950 | INFO | total log probability: -31.26
2026-01-28 14:18:28,950 | INFO | normalized log probability: -0.42
2026-01-28 14:18:28,950 | INFO | total number of ended hypotheses: 206
2026-01-28 14:18:28,951 | INFO | best hypo: la<space>la<space>la<space>véritable<space>pari<space>quoi<space>ni<space>pas<space>la<space>faille<space>synthétique<space>n'est<space>pas<space>mouet

2026-01-28 14:18:28,953 | INFO | speech length: 57440
2026-01-28 14:18:28,984 | INFO | decoder input length: 89
2026-01-28 14:18:28,984 | INFO | max output length: 89
2026-01-28 14:18:28,984 | INFO | min output length: 8
2026-01-28 14:18:36,334 | INFO | end detected at 76
2026-01-28 14:18:36,336 | INFO | -16.34 * 0.5 =  -8.17 for decoder
2026-01-28 14:18:36,336 | INFO | -18.41 * 0.5 =  -9.21 for ctc
2026-01-28 14:18:36,336 | INFO | total log probability: -17.38
2026-01-28 14:18:36,336 | INFO | normalized log probability: -0.26
2026-01-28 14:18:36,336 | INFO | total number of ended hypotheses: 117
2026-01-28 14:18:36,337 | INFO | best hypo: et<space>en<space>une<space>semaine<space>ou<space>deux<space>ça<space>y<space>est<space>c'est<space>dans<space>ces<space>on<space>livre<space>ouais

2026-01-28 14:18:36,339 | INFO | speech length: 229120
2026-01-28 14:18:36,367 | INFO | decoder input length: 357
2026-01-28 14:18:36,368 | INFO | max output length: 357
2026-01-28 14:18:36,368 | INFO | min output length: 35
2026-01-28 14:19:07,539 | INFO | end detected at 279
2026-01-28 14:19:07,541 | INFO | -42.80 * 0.5 = -21.40 for decoder
2026-01-28 14:19:07,541 | INFO | -27.38 * 0.5 = -13.69 for ctc
2026-01-28 14:19:07,541 | INFO | total log probability: -35.09
2026-01-28 14:19:07,541 | INFO | normalized log probability: -0.13
2026-01-28 14:19:07,541 | INFO | total number of ended hypotheses: 210
2026-01-28 14:19:07,544 | INFO | best hypo: bon<space>alors<space>le<space>petit<space>fauteuil<space>que<space>j'ai<space>là<space>à<space>côté<space>que<space>je<space>veux<space>rhabiller<space>a<space>toujours<space>été<space>appelé<space>par<space>mes<space>parents<space>faut<space>elle<space>crapeau<space>hein<space>est<space>ce<space>que<space>ça<space>n'est<space>un<space>vrai<space>hein<space>c'est<space>un<space>et<space>pourquoi<space>on<space>appelle<space>crapeau<space>pas<space>parce<space>que<space>ce<space>les<space>les<space>gens<space>se<space>mettaient<space>vraiment<space>dans<space>le<space>fondé

2026-01-28 14:19:07,547 | INFO | speech length: 171200
2026-01-28 14:19:07,577 | INFO | decoder input length: 267
2026-01-28 14:19:07,577 | INFO | max output length: 267
2026-01-28 14:19:07,577 | INFO | min output length: 26
2026-01-28 14:19:32,879 | INFO | end detected at 257
2026-01-28 14:19:32,881 | INFO | -45.10 * 0.5 = -22.55 for decoder
2026-01-28 14:19:32,881 | INFO | -111.90 * 0.5 = -55.95 for ctc
2026-01-28 14:19:32,881 | INFO | total log probability: -78.50
2026-01-28 14:19:32,881 | INFO | normalized log probability: -0.32
2026-01-28 14:19:32,881 | INFO | total number of ended hypotheses: 229
2026-01-28 14:19:32,884 | INFO | best hypo: qui<space>est<space>là<space>lors<space>de<space>la<space>chose<space>comme<space>ça<space>et<space>c'était<space>vraiment<space>très<space>très<space>résart<space>très<space>bas<space>donc<space>ce<space>qui<space>pourrait<space>donner<space>l'expression<space>de<space>de<space>trafic<space>anti<space>alors<space>ça<space>dette<space>de<space>quand<space>à<space>peu<space>près<space>ces<space>fauteils<space>là<space>c'est<space>sous<space>la<space>polon<space>trois<space>fin<space>dix<space>neuvième<space>siècle

2026-01-28 14:19:32,887 | INFO | speech length: 71999
2026-01-28 14:19:32,917 | INFO | decoder input length: 111
2026-01-28 14:19:32,917 | INFO | max output length: 111
2026-01-28 14:19:32,917 | INFO | min output length: 11
2026-01-28 14:19:41,909 | INFO | end detected at 101
2026-01-28 14:19:41,911 | INFO | -24.38 * 0.5 = -12.19 for decoder
2026-01-28 14:19:41,911 | INFO | -47.39 * 0.5 = -23.70 for ctc
2026-01-28 14:19:41,911 | INFO | total log probability: -35.89
2026-01-28 14:19:41,911 | INFO | normalized log probability: -0.40
2026-01-28 14:19:41,911 | INFO | total number of ended hypotheses: 204
2026-01-28 14:19:41,912 | INFO | best hypo: j'ai<space>les<space>pieds<space>quoi<space>qu'ils<space>sont<space>noirs<space>pas<space>tuxés<space>sont<space>dip<space>typiquement<space>non<space>pas<space>l'on<space>croit

2026-01-28 14:19:41,915 | INFO | speech length: 68000
2026-01-28 14:19:41,947 | INFO | decoder input length: 105
2026-01-28 14:19:41,947 | INFO | max output length: 105
2026-01-28 14:19:41,947 | INFO | min output length: 10
2026-01-28 14:19:50,998 | INFO | end detected at 97
2026-01-28 14:19:51,000 | INFO | -17.07 * 0.5 =  -8.53 for decoder
2026-01-28 14:19:51,000 | INFO | -20.78 * 0.5 = -10.39 for ctc
2026-01-28 14:19:51,000 | INFO | total log probability: -18.92
2026-01-28 14:19:51,000 | INFO | normalized log probability: -0.21
2026-01-28 14:19:51,000 | INFO | total number of ended hypotheses: 213
2026-01-28 14:19:51,001 | INFO | best hypo: donc<space>c'est<space>quelle<space>bois<space>je<space>suis<space>du<space>bois<space>de<space>être<space>euh<space>normalement<space>c'est<space>du<space>bois<space>de<space>être<space>de

2026-01-28 14:19:51,004 | INFO | speech length: 181120
2026-01-28 14:19:51,040 | INFO | decoder input length: 282
2026-01-28 14:19:51,040 | INFO | max output length: 282
2026-01-28 14:19:51,040 | INFO | min output length: 28
2026-01-28 14:20:13,859 | INFO | end detected at 214
2026-01-28 14:20:13,862 | INFO | -42.47 * 0.5 = -21.24 for decoder
2026-01-28 14:20:13,862 | INFO | -62.80 * 0.5 = -31.40 for ctc
2026-01-28 14:20:13,862 | INFO | total log probability: -52.64
2026-01-28 14:20:13,862 | INFO | normalized log probability: -0.26
2026-01-28 14:20:13,862 | INFO | total number of ended hypotheses: 216
2026-01-28 14:20:13,864 | INFO | best hypo: qui<space>est<space>là<space>qui<space>est<space>noir<space>d'accord<space>alors<space>pour<space>le<space>refaire<space>qui<space>me<space>dit<space>qu'il<space>faudrait<space>j'aille<space>verser<space>une<space>du<space>sait<space>du<space>oui<space>enfin<space>initialement<space>aux<space>états<space>unis<space>battre<space>sur<space>les<space>gens<space>saut<space>rues<space>mais<space>i<space>sont<space>tendus<space>sept

2026-01-28 14:20:13,867 | INFO | speech length: 97440
2026-01-28 14:20:13,901 | INFO | decoder input length: 151
2026-01-28 14:20:13,902 | INFO | max output length: 151
2026-01-28 14:20:13,902 | INFO | min output length: 15
2026-01-28 14:20:25,934 | INFO | end detected at 127
2026-01-28 14:20:25,937 | INFO | -23.47 * 0.5 = -11.74 for decoder
2026-01-28 14:20:25,937 | INFO | -30.24 * 0.5 = -15.12 for ctc
2026-01-28 14:20:25,937 | INFO | total log probability: -26.86
2026-01-28 14:20:25,937 | INFO | normalized log probability: -0.24
2026-01-28 14:20:25,937 | INFO | total number of ended hypotheses: 183
2026-01-28 14:20:25,939 | INFO | best hypo: alors<space>i<space>me<space>refraient<space>assis<space>oui<space>mais<space>est<space>ce<space>qu'i<space>faut<space>que<space>je<space>lui<space>dise<space>aussi<space>les<space>accoudoirs<space>ta<space>vus<space>comme<space>i<space>sont<space>oh

2026-01-28 14:20:25,943 | INFO | speech length: 56320
2026-01-28 14:20:26,053 | INFO | decoder input length: 87
2026-01-28 14:20:26,053 | INFO | max output length: 87
2026-01-28 14:20:26,053 | INFO | min output length: 8
2026-01-28 14:20:33,580 | INFO | adding <eos> in the last position in the loop
2026-01-28 14:20:33,589 | INFO | no hypothesis. Finish decoding.
2026-01-28 14:20:33,591 | INFO | -16.91 * 0.5 =  -8.45 for decoder
2026-01-28 14:20:33,591 | INFO | -21.32 * 0.5 = -10.66 for ctc
2026-01-28 14:20:33,591 | INFO | total log probability: -19.12
2026-01-28 14:20:33,591 | INFO | normalized log probability: -0.24
2026-01-28 14:20:33,591 | INFO | total number of ended hypotheses: 177
2026-01-28 14:20:33,592 | INFO | best hypo: reprendre<space>hein<space>quit<space>à<space>faire<space>le<space>travail<space>i<space>faut<space>faire<space>un<space>aciste<space>et<space>les<space>accouloirs

2026-01-28 14:20:33,594 | INFO | speech length: 38560
2026-01-28 14:20:33,627 | INFO | decoder input length: 59
2026-01-28 14:20:33,627 | INFO | max output length: 59
2026-01-28 14:20:33,627 | INFO | min output length: 5
2026-01-28 14:20:38,875 | INFO | end detected at 57
2026-01-28 14:20:38,876 | INFO |  -7.97 * 0.5 =  -3.99 for decoder
2026-01-28 14:20:38,876 | INFO |  -4.57 * 0.5 =  -2.29 for ctc
2026-01-28 14:20:38,876 | INFO | total log probability: -6.27
2026-01-28 14:20:38,876 | INFO | normalized log probability: -0.12
2026-01-28 14:20:38,876 | INFO | total number of ended hypotheses: 167
2026-01-28 14:20:38,877 | INFO | best hypo: est<space>ce<space>que<space>le<space>vie<space>peut<space>me<space>leur<space>couvrir<space>en<space>même<space>temps

2026-01-28 14:20:38,879 | INFO | speech length: 148800
2026-01-28 14:20:38,910 | INFO | decoder input length: 232
2026-01-28 14:20:38,910 | INFO | max output length: 232
2026-01-28 14:20:38,910 | INFO | min output length: 23
2026-01-28 14:20:53,353 | INFO | end detected at 155
2026-01-28 14:20:53,356 | INFO | -20.89 * 0.5 = -10.45 for decoder
2026-01-28 14:20:53,356 | INFO | -27.18 * 0.5 = -13.59 for ctc
2026-01-28 14:20:53,356 | INFO | total log probability: -24.04
2026-01-28 14:20:53,356 | INFO | normalized log probability: -0.16
2026-01-28 14:20:53,356 | INFO | total number of ended hypotheses: 213
2026-01-28 14:20:53,358 | INFO | best hypo: ah<space>oui<space>c'est<space>peur<space>que<space>nous<space>leur<space>mettent<space>de<space>de<space>restaurer<space>où<space>ben<space>ça<space>ça<space>doit<space>fait<space>tenir<space>or<space>tout<space>est<space>relatif<space>tout<space>est<space>relatif<space>parce<space>que<space>c'est<space>désert

2026-01-28 14:20:53,361 | INFO | speech length: 11840
2026-01-28 14:20:53,401 | INFO | decoder input length: 18
2026-01-28 14:20:53,401 | INFO | max output length: 18
2026-01-28 14:20:53,401 | INFO | min output length: 1
2026-01-28 14:20:54,874 | INFO | adding <eos> in the last position in the loop
2026-01-28 14:20:54,881 | INFO | no hypothesis. Finish decoding.
2026-01-28 14:20:54,882 | INFO |  -3.12 * 0.5 =  -1.56 for decoder
2026-01-28 14:20:54,882 | INFO |  -0.93 * 0.5 =  -0.46 for ctc
2026-01-28 14:20:54,882 | INFO | total log probability: -2.02
2026-01-28 14:20:54,882 | INFO | normalized log probability: -0.10
2026-01-28 14:20:54,882 | INFO | total number of ended hypotheses: 57
2026-01-28 14:20:54,882 | INFO | best hypo: oui<space>mais<space>je<space>pense<sos/eos>

2026-01-28 14:20:54,882 | WARNING | best hypo length: 18 == max output length: 18
2026-01-28 14:20:54,882 | WARNING | decoding may be stopped by the max output length limitation, please consider to increase the maxlenratio.
2026-01-28 14:20:54,883 | INFO | speech length: 20800
2026-01-28 14:20:54,914 | INFO | decoder input length: 32
2026-01-28 14:20:54,914 | INFO | max output length: 32
2026-01-28 14:20:54,914 | INFO | min output length: 3
2026-01-28 14:20:57,515 | INFO | adding <eos> in the last position in the loop
2026-01-28 14:20:57,525 | INFO | end detected at 31
2026-01-28 14:20:57,526 | INFO |  -2.17 * 0.5 =  -1.08 for decoder
2026-01-28 14:20:57,526 | INFO |  -1.72 * 0.5 =  -0.86 for ctc
2026-01-28 14:20:57,526 | INFO | total log probability: -1.94
2026-01-28 14:20:57,526 | INFO | normalized log probability: -0.07
2026-01-28 14:20:57,526 | INFO | total number of ended hypotheses: 175
2026-01-28 14:20:57,526 | INFO | best hypo: très<space>loin<space>en<space>cellerie<space>euh

2026-01-28 14:20:57,528 | INFO | speech length: 85280
2026-01-28 14:20:57,560 | INFO | decoder input length: 132
2026-01-28 14:20:57,560 | INFO | max output length: 132
2026-01-28 14:20:57,560 | INFO | min output length: 13
2026-01-28 14:21:06,738 | INFO | end detected at 95
2026-01-28 14:21:06,740 | INFO |  -7.50 * 0.5 =  -3.75 for decoder
2026-01-28 14:21:06,740 | INFO |  -1.56 * 0.5 =  -0.78 for ctc
2026-01-28 14:21:06,740 | INFO | total log probability: -4.53
2026-01-28 14:21:06,740 | INFO | normalized log probability: -0.05
2026-01-28 14:21:06,740 | INFO | total number of ended hypotheses: 166
2026-01-28 14:21:06,741 | INFO | best hypo: est<space>ce<space>que<space>c'est<space>est<space>ce<space>que<space>c'est<space>moi<space>qui<space>est<space>ce<space>que<space>je<space>peux<space>se<space>lui<space>amener<space>le<space>tissu<space>hein

2026-01-28 14:21:06,743 | INFO | speech length: 90720
2026-01-28 14:21:06,775 | INFO | decoder input length: 141
2026-01-28 14:21:06,775 | INFO | max output length: 141
2026-01-28 14:21:06,775 | INFO | min output length: 14
2026-01-28 14:21:20,848 | INFO | end detected at 138
2026-01-28 14:21:20,850 | INFO | -26.28 * 0.5 = -13.14 for decoder
2026-01-28 14:21:20,850 | INFO | -84.03 * 0.5 = -42.01 for ctc
2026-01-28 14:21:20,850 | INFO | total log probability: -55.15
2026-01-28 14:21:20,850 | INFO | normalized log probability: -0.42
2026-01-28 14:21:20,850 | INFO | total number of ended hypotheses: 150
2026-01-28 14:21:20,852 | INFO | best hypo: est<space>ce<space>que<space>j'ai<space>un<space>rouleau<space>de<space>tissus<space>de<space>velours<space>belges<space>alors<space>oui<space>faudrait<space>qutu<space>me<space>dises<space>aussi<space>les<space>tins<space>que<space>je<space>peu<space>mettre<space>là<space>dessus

2026-01-28 14:21:20,854 | INFO | speech length: 58240
2026-01-28 14:21:20,886 | INFO | decoder input length: 90
2026-01-28 14:21:20,887 | INFO | max output length: 90
2026-01-28 14:21:20,887 | INFO | min output length: 9
2026-01-28 14:21:27,032 | INFO | end detected at 62
2026-01-28 14:21:27,035 | INFO |  -6.17 * 0.5 =  -3.08 for decoder
2026-01-28 14:21:27,035 | INFO | -13.06 * 0.5 =  -6.53 for ctc
2026-01-28 14:21:27,035 | INFO | total log probability: -9.61
2026-01-28 14:21:27,035 | INFO | normalized log probability: -0.18
2026-01-28 14:21:27,035 | INFO | total number of ended hypotheses: 211
2026-01-28 14:21:27,036 | INFO | best hypo: déjà<space>l'éteinte<space>bon<space>i<space>faut<space>savoir<space>que<space>tu<space>as<space>une<space>base

2026-01-28 14:21:27,038 | INFO | speech length: 181920
2026-01-28 14:21:27,070 | INFO | decoder input length: 283
2026-01-28 14:21:27,070 | INFO | max output length: 283
2026-01-28 14:21:27,070 | INFO | min output length: 28
2026-01-28 14:21:50,889 | INFO | end detected at 211
2026-01-28 14:21:50,891 | INFO | -47.59 * 0.5 = -23.80 for decoder
2026-01-28 14:21:50,891 | INFO | -35.98 * 0.5 = -17.99 for ctc
2026-01-28 14:21:50,891 | INFO | total log probability: -41.79
2026-01-28 14:21:50,891 | INFO | normalized log probability: -0.21
2026-01-28 14:21:50,891 | INFO | total number of ended hypotheses: 232
2026-01-28 14:21:50,894 | INFO | best hypo: nous<space>êtes<space>en<space>fonction<space>de<space>ta<space>pièce<space>déjà<space>me<space>situelle<space>bien<space>oui<space>mais<space>si<space>je<space>veux<space>gardais<space>l'étane<space>de<space>l'époque<space>non<space>c'est<space>pas<space>possible<space>me<space>m'obliger<space>je<space>suis<space>pas<space>une<space>obligation<space>j'ai<space>un<space>beige<space>euh<space>baisse<space>clair

2026-01-28 14:21:50,896 | INFO | speech length: 54880
2026-01-28 14:21:50,934 | INFO | decoder input length: 85
2026-01-28 14:21:50,934 | INFO | max output length: 85
2026-01-28 14:21:50,934 | INFO | min output length: 8
2026-01-28 14:21:58,665 | INFO | adding <eos> in the last position in the loop
2026-01-28 14:21:58,675 | INFO | no hypothesis. Finish decoding.
2026-01-28 14:21:58,676 | INFO | -24.40 * 0.5 = -12.20 for decoder
2026-01-28 14:21:58,676 | INFO | -45.13 * 0.5 = -22.56 for ctc
2026-01-28 14:21:58,676 | INFO | total log probability: -34.77
2026-01-28 14:21:58,676 | INFO | normalized log probability: -0.45
2026-01-28 14:21:58,676 | INFO | total number of ended hypotheses: 165
2026-01-28 14:21:58,677 | INFO | best hypo: un<space>veu<space>lourd<space>mais<space>hein<space>oui<space>mais<space>enfin<space>bon<space>si<space>c'est<space>pour<space>metre<space>dans<space>la<space>pièce

2026-01-28 14:21:58,679 | INFO | speech length: 129600
2026-01-28 14:21:58,731 | INFO | decoder input length: 202
2026-01-28 14:21:58,731 | INFO | max output length: 202
2026-01-28 14:21:58,731 | INFO | min output length: 20
2026-01-28 14:22:16,527 | INFO | end detected at 179
2026-01-28 14:22:16,529 | INFO | -31.75 * 0.5 = -15.87 for decoder
2026-01-28 14:22:16,529 | INFO | -80.06 * 0.5 = -40.03 for ctc
2026-01-28 14:22:16,529 | INFO | total log probability: -55.90
2026-01-28 14:22:16,529 | INFO | normalized log probability: -0.33
2026-01-28 14:22:16,529 | INFO | total number of ended hypotheses: 199
2026-01-28 14:22:16,531 | INFO | best hypo: comme<space>j'aurais<space>mis<space>des<space>cousins<space>à<space>fond<space>euh<space>roi<space>je<space>faut<space>manger<space>sur<space>le<space>s<space>sa<space>porte<space>méridienne<space>du<space>fait<space>que<space>tu<space>as<space>déjà<space>quelque<space>chose<space>d'un<space>d'un<space>son<space>clair<space>je<space>crois<space>que<space>belge<space>est

2026-01-28 14:22:16,534 | INFO | speech length: 19520
2026-01-28 14:22:16,565 | INFO | decoder input length: 30
2026-01-28 14:22:16,565 | INFO | max output length: 30
2026-01-28 14:22:16,565 | INFO | min output length: 3
2026-01-28 14:22:18,995 | INFO | adding <eos> in the last position in the loop
2026-01-28 14:22:19,002 | INFO | no hypothesis. Finish decoding.
2026-01-28 14:22:19,003 | INFO | -13.19 * 0.5 =  -6.59 for decoder
2026-01-28 14:22:19,003 | INFO |  -5.68 * 0.5 =  -2.84 for ctc
2026-01-28 14:22:19,003 | INFO | total log probability: -9.44
2026-01-28 14:22:19,003 | INFO | normalized log probability: -0.31
2026-01-28 14:22:19,003 | INFO | total number of ended hypotheses: 64
2026-01-28 14:22:19,004 | INFO | best hypo: je<space>journatre<space>un<space>petit<space>peu<space>ça

2026-01-28 14:22:19,005 | INFO | speech length: 88480
2026-01-28 14:22:19,036 | INFO | decoder input length: 137
2026-01-28 14:22:19,036 | INFO | max output length: 137
2026-01-28 14:22:19,036 | INFO | min output length: 13
2026-01-28 14:22:29,153 | INFO | end detected at 110
2026-01-28 14:22:29,155 | INFO | -19.10 * 0.5 =  -9.55 for decoder
2026-01-28 14:22:29,155 | INFO | -26.31 * 0.5 = -13.16 for ctc
2026-01-28 14:22:29,155 | INFO | total log probability: -22.70
2026-01-28 14:22:29,155 | INFO | normalized log probability: -0.22
2026-01-28 14:22:29,155 | INFO | total number of ended hypotheses: 200
2026-01-28 14:22:29,156 | INFO | best hypo: parce<space>que<space>mo<space>tissu<space>là<space>de<space>coussin<space>est<space>quand<space>même<space>euh<space>au<space>fond<space>c'est<space>rouge<space>moi<space>je<space>remaderasse<space>un<space>peu<space>oui

2026-01-28 14:22:29,158 | INFO | speech length: 14240
2026-01-28 14:22:29,190 | INFO | decoder input length: 21
2026-01-28 14:22:29,191 | INFO | max output length: 21
2026-01-28 14:22:29,191 | INFO | min output length: 2
2026-01-28 14:22:31,059 | INFO | adding <eos> in the last position in the loop
2026-01-28 14:22:31,068 | INFO | no hypothesis. Finish decoding.
2026-01-28 14:22:31,069 | INFO |  -1.39 * 0.5 =  -0.69 for decoder
2026-01-28 14:22:31,069 | INFO |  -0.09 * 0.5 =  -0.04 for ctc
2026-01-28 14:22:31,069 | INFO | total log probability: -0.74
2026-01-28 14:22:31,069 | INFO | normalized log probability: -0.04
2026-01-28 14:22:31,069 | INFO | total number of ended hypotheses: 127
2026-01-28 14:22:31,069 | INFO | best hypo: oui<space>ça<space>c'est<space>bon

2026-01-28 14:22:31,071 | INFO | speech length: 8160
2026-01-28 14:22:31,098 | INFO | decoder input length: 12
2026-01-28 14:22:31,098 | INFO | max output length: 12
2026-01-28 14:22:31,098 | INFO | min output length: 1
2026-01-28 14:22:32,061 | INFO | end detected at 10
2026-01-28 14:22:32,062 | INFO |  -0.33 * 0.5 =  -0.17 for decoder
2026-01-28 14:22:32,063 | INFO |  -0.11 * 0.5 =  -0.05 for ctc
2026-01-28 14:22:32,063 | INFO | total log probability: -0.22
2026-01-28 14:22:32,063 | INFO | normalized log probability: -0.04
2026-01-28 14:22:32,063 | INFO | total number of ended hypotheses: 165
2026-01-28 14:22:32,063 | INFO | best hypo: oui

2026-01-28 14:22:32,065 | INFO | speech length: 126560
2026-01-28 14:22:32,095 | INFO | decoder input length: 197
2026-01-28 14:22:32,095 | INFO | max output length: 197
2026-01-28 14:22:32,095 | INFO | min output length: 19
2026-01-28 14:22:48,565 | INFO | end detected at 159
2026-01-28 14:22:48,568 | INFO | -18.84 * 0.5 =  -9.42 for decoder
2026-01-28 14:22:48,568 | INFO | -13.89 * 0.5 =  -6.94 for ctc
2026-01-28 14:22:48,568 | INFO | total log probability: -16.36
2026-01-28 14:22:48,568 | INFO | normalized log probability: -0.11
2026-01-28 14:22:48,568 | INFO | total number of ended hypotheses: 219
2026-01-28 14:22:48,570 | INFO | best hypo: et<space>est<space>ce<space>qu'il<space>faut<space>que<space>j'ai<space>parce<space>que<space>j'aime<space>pas<space>trop<space>la<space>franche<space>tout<space>autour<space>encore<space>que<space>c'était<space>une<space>franche<space>euh<space>vingt<space>trente<space>je<space>sais<space>pas<space>noble

2026-01-28 14:22:48,573 | INFO | speech length: 10240
2026-01-28 14:22:48,605 | INFO | decoder input length: 15
2026-01-28 14:22:48,605 | INFO | max output length: 15
2026-01-28 14:22:48,605 | INFO | min output length: 1
2026-01-28 14:22:49,854 | INFO | adding <eos> in the last position in the loop
2026-01-28 14:22:49,861 | INFO | no hypothesis. Finish decoding.
2026-01-28 14:22:49,862 | INFO |  -8.99 * 0.5 =  -4.49 for decoder
2026-01-28 14:22:49,862 | INFO | -17.88 * 0.5 =  -8.94 for ctc
2026-01-28 14:22:49,862 | INFO | total log probability: -13.44
2026-01-28 14:22:49,862 | INFO | normalized log probability: -1.12
2026-01-28 14:22:49,862 | INFO | total number of ended hypotheses: 61
2026-01-28 14:22:49,862 | INFO | best hypo: cet<space>embête

2026-01-28 14:22:49,863 | INFO | speech length: 44640
2026-01-28 14:22:49,892 | INFO | decoder input length: 69
2026-01-28 14:22:49,892 | INFO | max output length: 69
2026-01-28 14:22:49,892 | INFO | min output length: 6
2026-01-28 14:22:55,923 | INFO | adding <eos> in the last position in the loop
2026-01-28 14:22:55,934 | INFO | no hypothesis. Finish decoding.
2026-01-28 14:22:55,936 | INFO |  -6.55 * 0.5 =  -3.27 for decoder
2026-01-28 14:22:55,936 | INFO | -24.60 * 0.5 = -12.30 for ctc
2026-01-28 14:22:55,937 | INFO | total log probability: -15.57
2026-01-28 14:22:55,937 | INFO | normalized log probability: -0.26
2026-01-28 14:22:55,937 | INFO | total number of ended hypotheses: 211
2026-01-28 14:22:55,937 | INFO | best hypo: j'ai<space>est<space>ce<space>que<space>pour<space>que<space>les<space>pieds<space>soient<space>quand<space>même<space>cachés

2026-01-28 14:22:55,940 | INFO | speech length: 136640
2026-01-28 14:22:55,972 | INFO | decoder input length: 213
2026-01-28 14:22:55,972 | INFO | max output length: 213
2026-01-28 14:22:55,972 | INFO | min output length: 21
2026-01-28 14:23:13,483 | INFO | end detected at 184
2026-01-28 14:23:13,485 | INFO | -23.71 * 0.5 = -11.85 for decoder
2026-01-28 14:23:13,485 | INFO | -55.51 * 0.5 = -27.75 for ctc
2026-01-28 14:23:13,485 | INFO | total log probability: -39.61
2026-01-28 14:23:13,485 | INFO | normalized log probability: -0.23
2026-01-28 14:23:13,485 | INFO | total number of ended hypotheses: 218
2026-01-28 14:23:13,487 | INFO | best hypo: dans<space>la<space>mesure<space>où<space>tu<space>vas<space>pas<space>mal<space>de<space>france<space>une<space>surtout<space>pompe<space>la<space>homme<space>parce<space>que<space>j'ai<space>regardé<space>sur<space>un<space>catalogue<space>et<space>j'ai<space>vu<space>qu'en<space>fait<space>le<space>tissu<space>parfois<space>descendait<space>jusqu'au<space>bar

2026-01-28 14:23:13,490 | INFO | speech length: 17761
2026-01-28 14:23:13,523 | INFO | decoder input length: 27
2026-01-28 14:23:13,523 | INFO | max output length: 27
2026-01-28 14:23:13,523 | INFO | min output length: 2
2026-01-28 14:23:15,746 | INFO | adding <eos> in the last position in the loop
2026-01-28 14:23:15,753 | INFO | no hypothesis. Finish decoding.
2026-01-28 14:23:15,754 | INFO | -13.00 * 0.5 =  -6.50 for decoder
2026-01-28 14:23:15,754 | INFO | -10.74 * 0.5 =  -5.37 for ctc
2026-01-28 14:23:15,754 | INFO | total log probability: -11.87
2026-01-28 14:23:15,754 | INFO | normalized log probability: -0.41
2026-01-28 14:23:15,754 | INFO | total number of ended hypotheses: 41
2026-01-28 14:23:15,754 | INFO | best hypo: soit<space>c'état<space>une<space>france<space>mati

2026-01-28 14:23:15,755 | WARNING | best hypo length: 27 == max output length: 27
2026-01-28 14:23:15,755 | WARNING | decoding may be stopped by the max output length limitation, please consider to increase the maxlenratio.
2026-01-28 14:23:15,755 | INFO | speech length: 65760
2026-01-28 14:23:15,786 | INFO | decoder input length: 102
2026-01-28 14:23:15,786 | INFO | max output length: 102
2026-01-28 14:23:15,786 | INFO | min output length: 10
2026-01-28 14:23:22,897 | INFO | end detected at 81
2026-01-28 14:23:22,898 | INFO |  -5.94 * 0.5 =  -2.97 for decoder
2026-01-28 14:23:22,898 | INFO |  -4.34 * 0.5 =  -2.17 for ctc
2026-01-28 14:23:22,898 | INFO | total log probability: -5.14
2026-01-28 14:23:22,898 | INFO | normalized log probability: -0.07
2026-01-28 14:23:22,898 | INFO | total number of ended hypotheses: 170
2026-01-28 14:23:22,899 | INFO | best hypo: oui<space>mais<space>ça<space>ne<space>déguise<space>pas<space>du<space>fait<space>que<space>c'est<space>un<space>fauteuil<space>qu'à<space>une<space>époque

2026-01-28 14:23:22,901 | INFO | speech length: 77440
2026-01-28 14:23:22,932 | INFO | decoder input length: 120
2026-01-28 14:23:22,932 | INFO | max output length: 120
2026-01-28 14:23:22,932 | INFO | min output length: 12
2026-01-28 14:23:33,652 | INFO | end detected at 114
2026-01-28 14:23:33,654 | INFO | -19.44 * 0.5 =  -9.72 for decoder
2026-01-28 14:23:33,654 | INFO | -39.93 * 0.5 = -19.96 for ctc
2026-01-28 14:23:33,654 | INFO | total log probability: -29.68
2026-01-28 14:23:33,654 | INFO | normalized log probability: -0.27
2026-01-28 14:23:33,654 | INFO | total number of ended hypotheses: 180
2026-01-28 14:23:33,655 | INFO | best hypo: je<space>vais<space>les<space>deux<space>est<space>ce<space>que<space>ça<space>donne<space>un<space>aspect<space>quand<space>même<space>plus<space>moderne<space>c'est<space>tu<space>médecine<space>je<space>crois<space>bien<space>sûr

2026-01-28 14:23:33,657 | INFO | speech length: 15040
2026-01-28 14:23:33,688 | INFO | decoder input length: 23
2026-01-28 14:23:33,688 | INFO | max output length: 23
2026-01-28 14:23:33,689 | INFO | min output length: 2
2026-01-28 14:23:35,588 | INFO | adding <eos> in the last position in the loop
2026-01-28 14:23:35,597 | INFO | no hypothesis. Finish decoding.
2026-01-28 14:23:35,597 | INFO |  -2.88 * 0.5 =  -1.44 for decoder
2026-01-28 14:23:35,598 | INFO |  -7.16 * 0.5 =  -3.58 for ctc
2026-01-28 14:23:35,598 | INFO | total log probability: -5.02
2026-01-28 14:23:35,598 | INFO | normalized log probability: -0.23
2026-01-28 14:23:35,598 | INFO | total number of ended hypotheses: 106
2026-01-28 14:23:35,598 | INFO | best hypo: ça<space>c'est<space>quoi<space>gênant

2026-01-28 14:23:35,599 | INFO | speech length: 44160
2026-01-28 14:23:35,637 | INFO | decoder input length: 68
2026-01-28 14:23:35,637 | INFO | max output length: 68
2026-01-28 14:23:35,637 | INFO | min output length: 6
2026-01-28 14:23:41,316 | INFO | end detected at 64
2026-01-28 14:23:41,317 | INFO |  -4.99 * 0.5 =  -2.49 for decoder
2026-01-28 14:23:41,317 | INFO |  -1.93 * 0.5 =  -0.97 for ctc
2026-01-28 14:23:41,317 | INFO | total log probability: -3.46
2026-01-28 14:23:41,318 | INFO | normalized log probability: -0.06
2026-01-28 14:23:41,318 | INFO | total number of ended hypotheses: 146
2026-01-28 14:23:41,318 | INFO | best hypo: par<space>contre<space>je<space>l'ai<space>gardé<space>la<space>frange<space>mais<space>c'est<space>inutilisable

2026-01-28 14:23:41,320 | INFO | speech length: 68800
2026-01-28 14:23:41,351 | INFO | decoder input length: 107
2026-01-28 14:23:41,351 | INFO | max output length: 107
2026-01-28 14:23:41,351 | INFO | min output length: 10
2026-01-28 14:23:49,816 | INFO | end detected at 94
2026-01-28 14:23:49,818 | INFO |  -9.29 * 0.5 =  -4.65 for decoder
2026-01-28 14:23:49,818 | INFO | -12.20 * 0.5 =  -6.10 for ctc
2026-01-28 14:23:49,818 | INFO | total log probability: -10.74
2026-01-28 14:23:49,818 | INFO | normalized log probability: -0.13
2026-01-28 14:23:49,818 | INFO | total number of ended hypotheses: 229
2026-01-28 14:23:49,819 | INFO | best hypo: ça<space>a<space>pris<space>la<space>poussière<space>c'est<space>même<space>au<space>nettoyage<space>je<space>crois<space>que<space>c'est<space>pas<space>la<space>peine<space>hein

2026-01-28 14:23:49,821 | INFO | speech length: 27680
2026-01-28 14:23:49,851 | INFO | decoder input length: 42
2026-01-28 14:23:49,851 | INFO | max output length: 42
2026-01-28 14:23:49,851 | INFO | min output length: 4
2026-01-28 14:23:53,403 | INFO | adding <eos> in the last position in the loop
2026-01-28 14:23:53,411 | INFO | no hypothesis. Finish decoding.
2026-01-28 14:23:53,412 | INFO | -21.41 * 0.5 = -10.70 for decoder
2026-01-28 14:23:53,412 | INFO | -65.54 * 0.5 = -32.77 for ctc
2026-01-28 14:23:53,412 | INFO | total log probability: -43.48
2026-01-28 14:23:53,412 | INFO | normalized log probability: -0.99
2026-01-28 14:23:53,412 | INFO | total number of ended hypotheses: 44
2026-01-28 14:23:53,413 | INFO | best hypo: non<space>non<space>si<space>tu<space>peux<space>faire<space>com<space>ça<space>tu<space>te<space>vend

2026-01-28 14:23:53,414 | WARNING | best hypo length: 42 == max output length: 42
2026-01-28 14:23:53,414 | WARNING | decoding may be stopped by the max output length limitation, please consider to increase the maxlenratio.
2026-01-28 14:23:53,417 | INFO | speech length: 13920
2026-01-28 14:23:53,452 | INFO | decoder input length: 21
2026-01-28 14:23:53,452 | INFO | max output length: 21
2026-01-28 14:23:53,452 | INFO | min output length: 2
2026-01-28 14:23:55,131 | INFO | adding <eos> in the last position in the loop
2026-01-28 14:23:55,139 | INFO | no hypothesis. Finish decoding.
2026-01-28 14:23:55,140 | INFO |  -7.05 * 0.5 =  -3.53 for decoder
2026-01-28 14:23:55,140 | INFO | -12.09 * 0.5 =  -6.04 for ctc
2026-01-28 14:23:55,140 | INFO | total log probability: -9.57
2026-01-28 14:23:55,140 | INFO | normalized log probability: -0.46
2026-01-28 14:23:55,140 | INFO | total number of ended hypotheses: 62
2026-01-28 14:23:55,140 | INFO | best hypo: du<space>voin<space>ce<space>matin<space>là

2026-01-28 14:23:55,141 | INFO | speech length: 94080
2026-01-28 14:23:55,172 | INFO | decoder input length: 146
2026-01-28 14:23:55,172 | INFO | max output length: 146
2026-01-28 14:23:55,172 | INFO | min output length: 14
2026-01-28 14:24:07,876 | INFO | end detected at 142
2026-01-28 14:24:07,878 | INFO | -24.50 * 0.5 = -12.25 for decoder
2026-01-28 14:24:07,878 | INFO | -45.73 * 0.5 = -22.86 for ctc
2026-01-28 14:24:07,878 | INFO | total log probability: -35.11
2026-01-28 14:24:07,878 | INFO | normalized log probability: -0.26
2026-01-28 14:24:07,878 | INFO | total number of ended hypotheses: 180
2026-01-28 14:24:07,880 | INFO | best hypo: bon<space>bah<space>écoutez<space>j'ai<space>ça<space>à<space>l'heure<space>y<space>a<space>pas<space>problème<space>je<space>me<space>disais<space>j'irais<space>peut<space>être<space>à<space>vire<space>parce<space>qu'on<space>était<space>allé<space>le<space>voir<space>ce<space>monsieur<space>là

2026-01-28 14:24:07,882 | INFO | speech length: 59360
2026-01-28 14:24:07,915 | INFO | decoder input length: 92
2026-01-28 14:24:07,915 | INFO | max output length: 92
2026-01-28 14:24:07,915 | INFO | min output length: 9
2026-01-28 14:24:14,125 | INFO | end detected at 66
2026-01-28 14:24:14,128 | INFO |  -4.76 * 0.5 =  -2.38 for decoder
2026-01-28 14:24:14,128 | INFO |  -1.31 * 0.5 =  -0.66 for ctc
2026-01-28 14:24:14,128 | INFO | total log probability: -3.04
2026-01-28 14:24:14,128 | INFO | normalized log probability: -0.05
2026-01-28 14:24:14,128 | INFO | total number of ended hypotheses: 174
2026-01-28 14:24:14,129 | INFO | best hypo: il<space>était<space>très<space>sympa<space>et<space>puis<space>je<space>connais<space>pas<space>ah<space>non<space>du<space>tout

2026-01-28 14:24:14,131 | INFO | speech length: 32640
2026-01-28 14:24:14,161 | INFO | decoder input length: 50
2026-01-28 14:24:14,161 | INFO | max output length: 50
2026-01-28 14:24:14,162 | INFO | min output length: 5
2026-01-28 14:24:18,391 | INFO | adding <eos> in the last position in the loop
2026-01-28 14:24:18,400 | INFO | no hypothesis. Finish decoding.
2026-01-28 14:24:18,401 | INFO |  -6.15 * 0.5 =  -3.07 for decoder
2026-01-28 14:24:18,401 | INFO | -12.87 * 0.5 =  -6.43 for ctc
2026-01-28 14:24:18,401 | INFO | total log probability: -9.51
2026-01-28 14:24:18,401 | INFO | normalized log probability: -0.21
2026-01-28 14:24:18,402 | INFO | total number of ended hypotheses: 134
2026-01-28 14:24:18,402 | INFO | best hypo: moi<space>je<space>suis<space>femme<space>et<space>originaire<space>de<space>la<space>région

2026-01-28 14:24:18,404 | INFO | speech length: 26080
2026-01-28 14:24:18,434 | INFO | decoder input length: 40
2026-01-28 14:24:18,434 | INFO | max output length: 40
2026-01-28 14:24:18,434 | INFO | min output length: 4
2026-01-28 14:24:21,644 | INFO | adding <eos> in the last position in the loop
2026-01-28 14:24:21,651 | INFO | no hypothesis. Finish decoding.
2026-01-28 14:24:21,651 | INFO | -18.76 * 0.5 =  -9.38 for decoder
2026-01-28 14:24:21,651 | INFO | -43.56 * 0.5 = -21.78 for ctc
2026-01-28 14:24:21,651 | INFO | total log probability: -31.16
2026-01-28 14:24:21,651 | INFO | normalized log probability: -0.82
2026-01-28 14:24:21,651 | INFO | total number of ended hypotheses: 47
2026-01-28 14:24:21,652 | INFO | best hypo: oui<space>moi<space>je<space>travail<space>avec<space>les<space>omorales

2026-01-28 14:24:21,653 | INFO | speech length: 11520
2026-01-28 14:24:21,684 | INFO | decoder input length: 17
2026-01-28 14:24:21,684 | INFO | max output length: 17
2026-01-28 14:24:21,684 | INFO | min output length: 1
2026-01-28 14:24:22,721 | INFO | end detected at 11
2026-01-28 14:24:22,723 | INFO |  -1.17 * 0.5 =  -0.59 for decoder
2026-01-28 14:24:22,723 | INFO |  -1.62 * 0.5 =  -0.81 for ctc
2026-01-28 14:24:22,723 | INFO | total log probability: -1.39
2026-01-28 14:24:22,723 | INFO | normalized log probability: -0.28
2026-01-28 14:24:22,723 | INFO | total number of ended hypotheses: 190
2026-01-28 14:24:22,724 | INFO | best hypo: euh

2026-01-28 14:24:22,734 | INFO | Chunk: 0 | WER=38.095238 | S=0 D=0 I=8
2026-01-28 14:24:22,734 | INFO | Chunk: 1 | WER=166.666667 | S=3 D=0 I=2
2026-01-28 14:24:22,735 | INFO | Chunk: 2 | WER=118.750000 | S=11 D=2 I=6
2026-01-28 14:24:22,736 | INFO | Chunk: 3 | WER=66.666667 | S=3 D=8 I=9
2026-01-28 14:24:22,736 | INFO | Chunk: 4 | WER=100.000000 | S=2 D=5 I=0
2026-01-28 14:24:22,736 | INFO | Chunk: 5 | WER=128.571429 | S=2 D=5 I=11
2026-01-28 14:24:22,736 | INFO | Chunk: 6 | WER=112.500000 | S=8 D=0 I=1
2026-01-28 14:24:22,739 | INFO | Chunk: 7 | WER=38.805970 | S=11 D=9 I=6
2026-01-28 14:24:22,740 | INFO | Chunk: 8 | WER=71.428571 | S=6 D=7 I=7
2026-01-28 14:24:22,741 | INFO | Chunk: 9 | WER=65.714286 | S=6 D=6 I=11
2026-01-28 14:24:22,741 | INFO | Chunk: 10 | WER=171.428571 | S=7 D=0 I=5
2026-01-28 14:24:22,742 | INFO | Chunk: 11 | WER=95.238095 | S=5 D=7 I=8
2026-01-28 14:24:22,742 | INFO | Chunk: 12 | WER=88.888889 | S=11 D=4 I=1
2026-01-28 14:24:22,742 | INFO | Chunk: 13 | WER=100.000000 | S=16 D=0 I=0
2026-01-28 14:24:22,744 | INFO | Chunk: 14 | WER=50.000000 | S=8 D=7 I=11
2026-01-28 14:24:22,746 | INFO | Chunk: 15 | WER=60.784314 | S=14 D=9 I=8
2026-01-28 14:24:22,747 | INFO | Chunk: 16 | WER=100.000000 | S=9 D=7 I=5
2026-01-28 14:24:22,747 | INFO | Chunk: 17 | WER=76.923077 | S=12 D=7 I=1
2026-01-28 14:24:22,749 | INFO | Chunk: 18 | WER=73.333333 | S=14 D=11 I=8
2026-01-28 14:24:22,750 | INFO | Chunk: 19 | WER=68.965517 | S=4 D=10 I=6
2026-01-28 14:24:22,750 | INFO | Chunk: 20 | WER=94.117647 | S=14 D=2 I=0
2026-01-28 14:24:22,750 | INFO | Chunk: 21 | WER=109.090909 | S=11 D=0 I=1
2026-01-28 14:24:22,751 | INFO | Chunk: 22 | WER=93.548387 | S=13 D=8 I=8
2026-01-28 14:24:22,752 | INFO | Chunk: 23 | WER=100.000000 | S=6 D=1 I=0
2026-01-28 14:24:22,752 | INFO | Chunk: 24 | WER=125.000000 | S=4 D=0 I=1
2026-01-28 14:24:22,752 | INFO | Chunk: 25 | WER=39.130435 | S=1 D=4 I=4
2026-01-28 14:24:22,753 | INFO | Chunk: 26 | WER=90.909091 | S=5 D=5 I=10
2026-01-28 14:24:22,753 | INFO | Chunk: 27 | WER=171.428571 | S=7 D=0 I=5
2026-01-28 14:24:22,754 | INFO | Chunk: 28 | WER=53.333333 | S=13 D=7 I=4
2026-01-28 14:24:22,755 | INFO | Chunk: 29 | WER=242.857143 | S=7 D=0 I=10
2026-01-28 14:24:22,756 | INFO | Chunk: 30 | WER=71.428571 | S=8 D=13 I=9
2026-01-28 14:24:22,756 | INFO | Chunk: 31 | WER=100.000000 | S=6 D=0 I=0
2026-01-28 14:24:22,756 | INFO | Chunk: 32 | WER=110.526316 | S=18 D=0 I=3
2026-01-28 14:24:22,757 | INFO | Chunk: 33 | WER=100.000000 | S=5 D=0 I=0
2026-01-28 14:24:22,757 | INFO | Chunk: 34 | WER=0.000000 | S=1 D=0 I=0
2026-01-28 14:24:22,758 | INFO | Chunk: 35 | WER=54.285714 | S=4 D=9 I=6
2026-01-28 14:24:22,758 | INFO | Chunk: 36 | WER=100.000000 | S=2 D=3 I=0
2026-01-28 14:24:22,758 | INFO | Chunk: 37 | WER=100.000000 | S=13 D=0 I=0
2026-01-28 14:24:22,759 | INFO | Chunk: 38 | WER=40.000000 | S=6 D=3 I=5
2026-01-28 14:24:22,759 | INFO | Chunk: 39 | WER=150.000000 | S=4 D=0 I=2
2026-01-28 14:24:22,760 | INFO | Chunk: 40 | WER=100.000000 | S=17 D=4 I=0
2026-01-28 14:24:22,760 | INFO | Chunk: 41 | WER=92.857143 | S=3 D=14 I=9
2026-01-28 14:24:22,761 | INFO | Chunk: 42 | WER=166.666667 | S=3 D=0 I=2
2026-01-28 14:24:22,761 | INFO | Chunk: 43 | WER=100.000000 | S=12 D=1 I=0
2026-01-28 14:24:22,761 | INFO | Chunk: 44 | WER=150.000000 | S=0 D=3 I=12
2026-01-28 14:24:22,762 | INFO | Chunk: 45 | WER=220.000000 | S=5 D=0 I=6
2026-01-28 14:24:22,762 | INFO | Chunk: 46 | WER=100.000000 | S=5 D=3 I=0
2026-01-28 14:24:22,763 | INFO | Chunk: 47 | WER=93.103448 | S=10 D=7 I=10
2026-01-28 14:24:22,763 | INFO | Chunk: 48 | WER=108.333333 | S=0 D=6 I=7
2026-01-28 14:24:22,763 | INFO | Chunk: 49 | WER=200.000000 | S=3 D=0 I=5
2026-01-28 14:24:22,763 | INFO | Chunk: 50 | WER=175.000000 | S=4 D=0 I=3
2026-01-28 14:24:22,764 | INFO | Chunk: 51 | WER=100.000000 | S=1 D=4 I=0
2026-01-28 14:24:23,282 | INFO | File: Rhap-D0009.wav | WER=53.668342 | S=314 D=92 I=128
2026-01-28 14:24:23,282 | INFO | ------------------------------
2026-01-28 14:24:23,282 | INFO | Conf ester Done!
2026-01-28 14:30:59,286 | INFO | Chunk: 0 | WER=57.142857 | S=3 D=1 I=8
2026-01-28 14:30:59,299 | INFO | Chunk: 1 | WER=100.000000 | S=3 D=0 I=0
2026-01-28 14:30:59,300 | INFO | Chunk: 2 | WER=112.500000 | S=16 D=0 I=2
2026-01-28 14:30:59,301 | INFO | Chunk: 3 | WER=63.333333 | S=6 D=10 I=3
2026-01-28 14:30:59,302 | INFO | Chunk: 4 | WER=100.000000 | S=2 D=5 I=0
2026-01-28 14:30:59,302 | INFO | Chunk: 5 | WER=300.000000 | S=1 D=0 I=2
2026-01-28 14:30:59,302 | INFO | Chunk: 6 | WER=100.000000 | S=1 D=6 I=7
2026-01-28 14:30:59,303 | INFO | Chunk: 7 | WER=125.000000 | S=8 D=0 I=2
2026-01-28 14:30:59,307 | INFO | Chunk: 8 | WER=41.791045 | S=9 D=14 I=5
2026-01-28 14:30:59,309 | INFO | Chunk: 9 | WER=67.857143 | S=6 D=6 I=7
2026-01-28 14:30:59,310 | INFO | Chunk: 10 | WER=71.428571 | S=7 D=9 I=9
2026-01-28 14:30:59,311 | INFO | Chunk: 11 | WER=171.428571 | S=7 D=0 I=5
2026-01-28 14:30:59,312 | INFO | Chunk: 12 | WER=95.238095 | S=2 D=9 I=9
2026-01-28 14:30:59,313 | INFO | Chunk: 13 | WER=88.888889 | S=10 D=5 I=1
2026-01-28 14:30:59,313 | INFO | Chunk: 14 | WER=93.750000 | S=12 D=3 I=0
2026-01-28 14:30:59,317 | INFO | Chunk: 15 | WER=53.846154 | S=10 D=8 I=10
2026-01-28 14:30:59,319 | INFO | Chunk: 16 | WER=70.588235 | S=18 D=15 I=3
2026-01-28 14:30:59,320 | INFO | Chunk: 17 | WER=85.714286 | S=7 D=9 I=2
2026-01-28 14:30:59,321 | INFO | Chunk: 18 | WER=80.769231 | S=10 D=9 I=2
2026-01-28 14:30:59,323 | INFO | Chunk: 19 | WER=80.000000 | S=16 D=14 I=6
2026-01-28 14:30:59,324 | INFO | Chunk: 20 | WER=100.000000 | S=24 D=5 I=0
2026-01-28 14:30:59,325 | INFO | Chunk: 21 | WER=94.117647 | S=12 D=3 I=1
2026-01-28 14:30:59,325 | INFO | Chunk: 22 | WER=100.000000 | S=10 D=1 I=0
2026-01-28 14:30:59,326 | INFO | Chunk: 23 | WER=93.548387 | S=13 D=9 I=7
2026-01-28 14:30:59,327 | INFO | Chunk: 24 | WER=100.000000 | S=4 D=3 I=0
2026-01-28 14:30:59,327 | INFO | Chunk: 25 | WER=100.000000 | S=4 D=0 I=0
2026-01-28 14:30:59,327 | INFO | Chunk: 26 | WER=60.869565 | S=4 D=7 I=3
2026-01-28 14:30:59,328 | INFO | Chunk: 27 | WER=81.818182 | S=2 D=6 I=10
2026-01-28 14:30:59,328 | INFO | Chunk: 28 | WER=142.857143 | S=7 D=0 I=3
2026-01-28 14:30:59,330 | INFO | Chunk: 29 | WER=77.777778 | S=16 D=18 I=1
2026-01-28 14:30:59,330 | INFO | Chunk: 30 | WER=157.142857 | S=5 D=0 I=6
2026-01-28 14:30:59,331 | INFO | Chunk: 31 | WER=71.428571 | S=10 D=14 I=6
2026-01-28 14:30:59,331 | INFO | Chunk: 32 | WER=116.666667 | S=6 D=0 I=1
2026-01-28 14:30:59,332 | INFO | Chunk: 33 | WER=100.000000 | S=6 D=9 I=4
2026-01-28 14:30:59,332 | INFO | Chunk: 34 | WER=100.000000 | S=5 D=0 I=0
2026-01-28 14:30:59,332 | INFO | Chunk: 35 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 14:30:59,333 | INFO | Chunk: 36 | WER=57.142857 | S=3 D=11 I=6
2026-01-28 14:30:59,333 | INFO | Chunk: 37 | WER=100.000000 | S=3 D=2 I=0
2026-01-28 14:30:59,334 | INFO | Chunk: 38 | WER=100.000000 | S=11 D=2 I=0
2026-01-28 14:30:59,335 | INFO | Chunk: 39 | WER=42.857143 | S=6 D=3 I=6
2026-01-28 14:30:59,335 | INFO | Chunk: 40 | WER=150.000000 | S=4 D=0 I=2
2026-01-28 14:30:59,336 | INFO | Chunk: 41 | WER=100.000000 | S=19 D=2 I=0
2026-01-28 14:30:59,336 | INFO | Chunk: 42 | WER=78.571429 | S=2 D=13 I=7
2026-01-28 14:30:59,336 | INFO | Chunk: 43 | WER=133.333333 | S=3 D=0 I=1
2026-01-28 14:30:59,337 | INFO | Chunk: 44 | WER=92.307692 | S=1 D=7 I=4
2026-01-28 14:30:59,337 | INFO | Chunk: 45 | WER=130.000000 | S=0 D=3 I=10
2026-01-28 14:30:59,337 | INFO | Chunk: 46 | WER=180.000000 | S=5 D=0 I=4
2026-01-28 14:30:59,338 | INFO | Chunk: 47 | WER=100.000000 | S=5 D=3 I=0
2026-01-28 14:30:59,338 | INFO | Chunk: 48 | WER=93.103448 | S=18 D=9 I=0
2026-01-28 14:30:59,339 | INFO | Chunk: 49 | WER=108.333333 | S=0 D=6 I=7
2026-01-28 14:30:59,339 | INFO | Chunk: 50 | WER=175.000000 | S=3 D=0 I=4
2026-01-28 14:30:59,339 | INFO | Chunk: 51 | WER=200.000000 | S=4 D=0 I=4
2026-01-28 14:30:59,339 | INFO | Chunk: 52 | WER=100.000000 | S=1 D=4 I=0
2026-01-28 14:30:59,339 | INFO | Chunk: 53 | WER=100.000000 | S=1 D=3 I=0
2026-01-28 14:30:59,847 | INFO | File: Rhap-D0009.wav | WER=55.600000 | S=332 D=155 I=69
2026-01-28 14:30:59,847 | INFO | ------------------------------
2026-01-28 14:30:59,847 | INFO | hmm_tdnn Done!
2026-01-28 14:31:00,024 | INFO | ==================================Rhap-D0017.wav=========================================
2026-01-28 14:31:00,834 | INFO | Using rVAD model
2026-01-28 14:31:04,482 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 14:31:04,482 | INFO | Chunk: 1 | WER=25.000000 | S=0 D=1 I=0
2026-01-28 14:31:04,482 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 14:31:04,483 | INFO | Chunk: 3 | WER=27.272727 | S=2 D=1 I=0
2026-01-28 14:31:04,483 | INFO | Chunk: 4 | WER=66.666667 | S=3 D=1 I=0
2026-01-28 14:31:04,483 | INFO | Chunk: 5 | WER=11.111111 | S=1 D=0 I=0
2026-01-28 14:31:04,483 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 14:31:04,483 | INFO | Chunk: 7 | WER=55.555556 | S=3 D=2 I=0
2026-01-28 14:31:04,484 | INFO | Chunk: 8 | WER=33.333333 | S=2 D=2 I=0
2026-01-28 14:31:04,485 | INFO | Chunk: 9 | WER=7.317073 | S=2 D=0 I=1
2026-01-28 14:31:04,493 | INFO | File: Rhap-D0017.wav | WER=19.047619 | S=14 D=6 I=0
2026-01-28 14:31:04,493 | INFO | ------------------------------
2026-01-28 14:31:04,493 | INFO | w2vec vad chunk Done!
2026-01-28 14:31:09,118 | INFO | Chunk: 0 | WER=100.000000 | S=0 D=0 I=1
2026-01-28 14:31:09,119 | INFO | Chunk: 1 | WER=25.000000 | S=0 D=1 I=0
2026-01-28 14:31:09,119 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 14:31:09,119 | INFO | Chunk: 3 | WER=27.272727 | S=1 D=2 I=0
2026-01-28 14:31:09,119 | INFO | Chunk: 4 | WER=83.333333 | S=2 D=3 I=0
2026-01-28 14:31:09,119 | INFO | Chunk: 5 | WER=11.111111 | S=1 D=0 I=0
2026-01-28 14:31:09,120 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 14:31:09,120 | INFO | Chunk: 7 | WER=33.333333 | S=0 D=3 I=0
2026-01-28 14:31:09,120 | INFO | Chunk: 8 | WER=50.000000 | S=3 D=3 I=0
2026-01-28 14:31:09,121 | INFO | Chunk: 9 | WER=85.365854 | S=0 D=35 I=0
2026-01-28 14:31:09,126 | INFO | File: Rhap-D0017.wav | WER=52.380952 | S=7 D=47 I=1
2026-01-28 14:31:09,126 | INFO | ------------------------------
2026-01-28 14:31:09,126 | INFO | whisper med Done!
2026-01-28 14:31:18,697 | INFO | Chunk: 0 | WER=100.000000 | S=0 D=0 I=1
2026-01-28 14:31:18,697 | INFO | Chunk: 1 | WER=25.000000 | S=1 D=0 I=0
2026-01-28 14:31:18,698 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 14:31:18,698 | INFO | Chunk: 3 | WER=27.272727 | S=1 D=2 I=0
2026-01-28 14:31:18,698 | INFO | Chunk: 4 | WER=100.000000 | S=2 D=4 I=0
2026-01-28 14:31:18,698 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 14:31:18,699 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 14:31:18,699 | INFO | Chunk: 7 | WER=11.111111 | S=1 D=0 I=0
2026-01-28 14:31:18,699 | INFO | Chunk: 8 | WER=33.333333 | S=3 D=1 I=0
2026-01-28 14:31:18,701 | INFO | Chunk: 9 | WER=14.634146 | S=0 D=5 I=1
2026-01-28 14:31:18,708 | INFO | File: Rhap-D0017.wav | WER=20.952381 | S=8 D=12 I=2
2026-01-28 14:31:18,708 | INFO | ------------------------------
2026-01-28 14:31:18,708 | INFO | whisper large Done!
2026-01-28 14:31:18,841 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 14:31:18,878 | INFO | Vocabulary size: 350
2026-01-28 14:31:19,563 | INFO | Gradient checkpoint layers: []
2026-01-28 14:31:20,496 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 14:31:20,500 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 14:31:20,500 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 14:31:20,500 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 14:31:20,501 | INFO | speech length: 15680
2026-01-28 14:31:20,539 | INFO | decoder input length: 24
2026-01-28 14:31:20,539 | INFO | max output length: 24
2026-01-28 14:31:20,539 | INFO | min output length: 2
2026-01-28 14:31:21,360 | INFO | end detected at 8
2026-01-28 14:31:21,361 | INFO |  -0.24 * 0.5 =  -0.12 for decoder
2026-01-28 14:31:21,362 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 14:31:21,362 | INFO | total log probability: -0.12
2026-01-28 14:31:21,362 | INFO | normalized log probability: -0.03
2026-01-28 14:31:21,362 | INFO | total number of ended hypotheses: 140
2026-01-28 14:31:21,362 | INFO | best hypo: ▁oui

2026-01-28 14:31:21,365 | INFO | speech length: 14080
2026-01-28 14:31:21,395 | INFO | decoder input length: 21
2026-01-28 14:31:21,395 | INFO | max output length: 21
2026-01-28 14:31:21,395 | INFO | min output length: 2
2026-01-28 14:31:22,869 | INFO | end detected at 14
2026-01-28 14:31:22,870 | INFO |  -0.73 * 0.5 =  -0.36 for decoder
2026-01-28 14:31:22,870 | INFO |  -0.10 * 0.5 =  -0.05 for ctc
2026-01-28 14:31:22,870 | INFO | total log probability: -0.42
2026-01-28 14:31:22,870 | INFO | normalized log probability: -0.04
2026-01-28 14:31:22,870 | INFO | total number of ended hypotheses: 144
2026-01-28 14:31:22,870 | INFO | best hypo: ▁prenez▁à▁droite

2026-01-28 14:31:22,872 | INFO | speech length: 13600
2026-01-28 14:31:22,909 | INFO | decoder input length: 20
2026-01-28 14:31:22,909 | INFO | max output length: 20
2026-01-28 14:31:22,909 | INFO | min output length: 2
2026-01-28 14:31:24,016 | INFO | end detected at 11
2026-01-28 14:31:24,016 | INFO |  -1.48 * 0.5 =  -0.74 for decoder
2026-01-28 14:31:24,017 | INFO |  -0.66 * 0.5 =  -0.33 for ctc
2026-01-28 14:31:24,017 | INFO | total log probability: -1.07
2026-01-28 14:31:24,017 | INFO | normalized log probability: -0.18
2026-01-28 14:31:24,017 | INFO | total number of ended hypotheses: 160
2026-01-28 14:31:24,017 | INFO | best hypo: ▁la▁droite

2026-01-28 14:31:24,019 | INFO | speech length: 51200
2026-01-28 14:31:24,055 | INFO | decoder input length: 79
2026-01-28 14:31:24,055 | INFO | max output length: 79
2026-01-28 14:31:24,055 | INFO | min output length: 7
2026-01-28 14:31:27,451 | INFO | end detected at 32
2026-01-28 14:31:27,452 | INFO |  -2.09 * 0.5 =  -1.04 for decoder
2026-01-28 14:31:27,453 | INFO |  -6.57 * 0.5 =  -3.29 for ctc
2026-01-28 14:31:27,453 | INFO | total log probability: -4.33
2026-01-28 14:31:27,453 | INFO | normalized log probability: -0.15
2026-01-28 14:31:27,453 | INFO | total number of ended hypotheses: 166
2026-01-28 14:31:27,453 | INFO | best hypo: ▁et▁vous▁allez▁tout▁droit▁jusqu'au▁prochain▁arrêt▁de▁tram

2026-01-28 14:31:27,455 | INFO | speech length: 23680
2026-01-28 14:31:27,492 | INFO | decoder input length: 36
2026-01-28 14:31:27,492 | INFO | max output length: 36
2026-01-28 14:31:27,492 | INFO | min output length: 3
2026-01-28 14:31:29,000 | INFO | end detected at 15
2026-01-28 14:31:29,002 | INFO |  -6.10 * 0.5 =  -3.05 for decoder
2026-01-28 14:31:29,002 | INFO |  -1.32 * 0.5 =  -0.66 for ctc
2026-01-28 14:31:29,002 | INFO | total log probability: -3.71
2026-01-28 14:31:29,002 | INFO | normalized log probability: -0.37
2026-01-28 14:31:29,002 | INFO | total number of ended hypotheses: 167
2026-01-28 14:31:29,002 | INFO | best hypo: ▁il▁y▁a▁neuf▁chavons

2026-01-28 14:31:29,004 | INFO | speech length: 44800
2026-01-28 14:31:29,040 | INFO | decoder input length: 69
2026-01-28 14:31:29,040 | INFO | max output length: 69
2026-01-28 14:31:29,040 | INFO | min output length: 6
2026-01-28 14:31:31,826 | INFO | end detected at 28
2026-01-28 14:31:31,827 | INFO |  -1.64 * 0.5 =  -0.82 for decoder
2026-01-28 14:31:31,827 | INFO |  -0.48 * 0.5 =  -0.24 for ctc
2026-01-28 14:31:31,827 | INFO | total log probability: -1.06
2026-01-28 14:31:31,827 | INFO | normalized log probability: -0.04
2026-01-28 14:31:31,827 | INFO | total number of ended hypotheses: 149
2026-01-28 14:31:31,827 | INFO | best hypo: ▁et▁vous▁continuez▁tout▁droit▁jusqu'au▁deuxième▁arrêt

2026-01-28 14:31:31,829 | INFO | speech length: 61120
2026-01-28 14:31:31,867 | INFO | decoder input length: 95
2026-01-28 14:31:31,867 | INFO | max output length: 95
2026-01-28 14:31:31,867 | INFO | min output length: 9
2026-01-28 14:31:34,648 | INFO | end detected at 27
2026-01-28 14:31:34,649 | INFO |  -1.71 * 0.5 =  -0.86 for decoder
2026-01-28 14:31:34,649 | INFO |  -0.24 * 0.5 =  -0.12 for ctc
2026-01-28 14:31:34,649 | INFO | total log probability: -0.98
2026-01-28 14:31:34,649 | INFO | normalized log probability: -0.04
2026-01-28 14:31:34,649 | INFO | total number of ended hypotheses: 144
2026-01-28 14:31:34,649 | INFO | best hypo: ▁et▁avant▁d'arriver▁au▁troisième▁vous▁prenez▁à▁droite

2026-01-28 14:31:34,651 | INFO | speech length: 42400
2026-01-28 14:31:34,690 | INFO | decoder input length: 65
2026-01-28 14:31:34,690 | INFO | max output length: 65
2026-01-28 14:31:34,690 | INFO | min output length: 6
2026-01-28 14:31:37,519 | INFO | end detected at 29
2026-01-28 14:31:37,520 | INFO | -13.83 * 0.5 =  -6.92 for decoder
2026-01-28 14:31:37,520 | INFO | -13.25 * 0.5 =  -6.63 for ctc
2026-01-28 14:31:37,520 | INFO | total log probability: -13.54
2026-01-28 14:31:37,520 | INFO | normalized log probability: -0.68
2026-01-28 14:31:37,520 | INFO | total number of ended hypotheses: 173
2026-01-28 14:31:37,521 | INFO | best hypo: ▁lào▁ya▁les▁interctions▁les▁intersecte

2026-01-28 14:31:37,523 | INFO | speech length: 39680
2026-01-28 14:31:37,556 | INFO | decoder input length: 61
2026-01-28 14:31:37,556 | INFO | max output length: 61
2026-01-28 14:31:37,556 | INFO | min output length: 6
2026-01-28 14:31:40,063 | INFO | end detected at 26
2026-01-28 14:31:40,066 | INFO |  -3.32 * 0.5 =  -1.66 for decoder
2026-01-28 14:31:40,066 | INFO |  -7.00 * 0.5 =  -3.50 for ctc
2026-01-28 14:31:40,066 | INFO | total log probability: -5.16
2026-01-28 14:31:40,066 | INFO | normalized log probability: -0.27
2026-01-28 14:31:40,067 | INFO | total number of ended hypotheses: 196
2026-01-28 14:31:40,067 | INFO | best hypo: ▁et▁vous▁marchez▁sur▁deux▁cents▁mètres▁et▁vous▁ir

2026-01-28 14:31:40,070 | INFO | speech length: 299840
2026-01-28 14:31:40,142 | INFO | decoder input length: 468
2026-01-28 14:31:40,142 | INFO | max output length: 468
2026-01-28 14:31:40,142 | INFO | min output length: 46
2026-01-28 14:31:55,133 | INFO | end detected at 105
2026-01-28 14:31:55,135 | INFO | -99.35 * 0.5 = -49.67 for decoder
2026-01-28 14:31:55,135 | INFO | -19.41 * 0.5 =  -9.70 for ctc
2026-01-28 14:31:55,135 | INFO | total log probability: -59.38
2026-01-28 14:31:55,135 | INFO | normalized log probability: -0.61
2026-01-28 14:31:55,136 | INFO | total number of ended hypotheses: 185
2026-01-28 14:31:55,137 | INFO | best hypo: ▁mais▁oh▁au▁prochain▁arrêt▁je▁continue▁au▁prochain▁arrêt▁oui▁vous▁continuez▁jusqu'au▁deuxième▁et▁avant▁d'arriver▁au▁troisième▁il▁y▁aura▁une▁intersection▁des▁voies▁et▁là▁vous▁prenez▁droit▁jusqu'à▁voir▁à▁la▁cathédrale

2026-01-28 14:31:55,146 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 14:31:55,146 | INFO | Chunk: 1 | WER=25.000000 | S=0 D=1 I=0
2026-01-28 14:31:55,146 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 14:31:55,147 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 14:31:55,147 | INFO | Chunk: 4 | WER=50.000000 | S=2 D=1 I=0
2026-01-28 14:31:55,148 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 14:31:55,148 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 14:31:55,149 | INFO | Chunk: 7 | WER=77.777778 | S=4 D=3 I=0
2026-01-28 14:31:55,149 | INFO | Chunk: 8 | WER=33.333333 | S=2 D=2 I=0
2026-01-28 14:31:55,152 | INFO | Chunk: 9 | WER=12.195122 | S=2 D=2 I=1
2026-01-28 14:31:55,166 | INFO | File: Rhap-D0017.wav | WER=19.047619 | S=10 D=9 I=1
2026-01-28 14:31:55,166 | INFO | ------------------------------
2026-01-28 14:31:55,166 | INFO | Conf cv Done!
2026-01-28 14:31:55,495 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 14:31:55,517 | INFO | Vocabulary size: 47
2026-01-28 14:31:56,168 | INFO | Gradient checkpoint layers: []
2026-01-28 14:31:57,093 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 14:31:57,101 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 14:31:57,101 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 14:31:57,102 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 14:31:57,106 | INFO | speech length: 15680
2026-01-28 14:31:57,150 | INFO | decoder input length: 24
2026-01-28 14:31:57,150 | INFO | max output length: 24
2026-01-28 14:31:57,150 | INFO | min output length: 2
2026-01-28 14:31:58,343 | INFO | end detected at 14
2026-01-28 14:31:58,344 | INFO |  -0.64 * 0.5 =  -0.32 for decoder
2026-01-28 14:31:58,344 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 14:31:58,344 | INFO | total log probability: -0.32
2026-01-28 14:31:58,344 | INFO | normalized log probability: -0.04
2026-01-28 14:31:58,344 | INFO | total number of ended hypotheses: 165
2026-01-28 14:31:58,344 | INFO | best hypo: euh<space>oui

2026-01-28 14:31:58,346 | INFO | speech length: 14080
2026-01-28 14:31:58,379 | INFO | decoder input length: 21
2026-01-28 14:31:58,379 | INFO | max output length: 21
2026-01-28 14:31:58,379 | INFO | min output length: 2
2026-01-28 14:32:00,082 | INFO | adding <eos> in the last position in the loop
2026-01-28 14:32:00,092 | INFO | no hypothesis. Finish decoding.
2026-01-28 14:32:00,093 | INFO |  -2.22 * 0.5 =  -1.11 for decoder
2026-01-28 14:32:00,093 | INFO |  -1.87 * 0.5 =  -0.94 for ctc
2026-01-28 14:32:00,093 | INFO | total log probability: -2.05
2026-01-28 14:32:00,093 | INFO | normalized log probability: -0.11
2026-01-28 14:32:00,093 | INFO | total number of ended hypotheses: 134
2026-01-28 14:32:00,093 | INFO | best hypo: prenait<space>à<space>droite

2026-01-28 14:32:00,095 | INFO | speech length: 13600
2026-01-28 14:32:00,127 | INFO | decoder input length: 20
2026-01-28 14:32:00,127 | INFO | max output length: 20
2026-01-28 14:32:00,127 | INFO | min output length: 2
2026-01-28 14:32:01,636 | INFO | end detected at 17
2026-01-28 14:32:01,637 | INFO |  -0.82 * 0.5 =  -0.41 for decoder
2026-01-28 14:32:01,637 | INFO |  -0.07 * 0.5 =  -0.04 for ctc
2026-01-28 14:32:01,637 | INFO | total log probability: -0.45
2026-01-28 14:32:01,637 | INFO | normalized log probability: -0.04
2026-01-28 14:32:01,637 | INFO | total number of ended hypotheses: 167
2026-01-28 14:32:01,638 | INFO | best hypo: la<space>droite

2026-01-28 14:32:01,640 | INFO | speech length: 51200
2026-01-28 14:32:01,677 | INFO | decoder input length: 79
2026-01-28 14:32:01,677 | INFO | max output length: 79
2026-01-28 14:32:01,677 | INFO | min output length: 7
2026-01-28 14:32:07,746 | INFO | end detected at 66
2026-01-28 14:32:07,749 | INFO |  -5.61 * 0.5 =  -2.80 for decoder
2026-01-28 14:32:07,750 | INFO |  -7.85 * 0.5 =  -3.93 for ctc
2026-01-28 14:32:07,750 | INFO | total log probability: -6.73
2026-01-28 14:32:07,750 | INFO | normalized log probability: -0.11
2026-01-28 14:32:07,750 | INFO | total number of ended hypotheses: 244
2026-01-28 14:32:07,751 | INFO | best hypo: et<space>vous<space>allez<space>tout<space>droit<space>jusqu'au<space>prochain<space>arrêt<space>de<space>trame

2026-01-28 14:32:07,754 | INFO | speech length: 23680
2026-01-28 14:32:07,792 | INFO | decoder input length: 36
2026-01-28 14:32:07,792 | INFO | max output length: 36
2026-01-28 14:32:07,792 | INFO | min output length: 3
2026-01-28 14:32:10,535 | INFO | end detected at 30
2026-01-28 14:32:10,537 | INFO |  -2.13 * 0.5 =  -1.06 for decoder
2026-01-28 14:32:10,537 | INFO |  -1.47 * 0.5 =  -0.74 for ctc
2026-01-28 14:32:10,537 | INFO | total log probability: -1.80
2026-01-28 14:32:10,537 | INFO | normalized log probability: -0.08
2026-01-28 14:32:10,538 | INFO | total number of ended hypotheses: 189
2026-01-28 14:32:10,538 | INFO | best hypo: oui<space>y<space>a<space>neuf<space>chavants

2026-01-28 14:32:10,541 | INFO | speech length: 44800
2026-01-28 14:32:10,578 | INFO | decoder input length: 69
2026-01-28 14:32:10,578 | INFO | max output length: 69
2026-01-28 14:32:10,579 | INFO | min output length: 6
2026-01-28 14:32:16,148 | INFO | end detected at 59
2026-01-28 14:32:16,150 | INFO |  -4.27 * 0.5 =  -2.14 for decoder
2026-01-28 14:32:16,150 | INFO |  -2.81 * 0.5 =  -1.41 for ctc
2026-01-28 14:32:16,150 | INFO | total log probability: -3.54
2026-01-28 14:32:16,150 | INFO | normalized log probability: -0.07
2026-01-28 14:32:16,150 | INFO | total number of ended hypotheses: 173
2026-01-28 14:32:16,151 | INFO | best hypo: et<space>vous<space>continuez<space>tout<space>droit<space>jusqu'au<space>deuxième<space>arrêt

2026-01-28 14:32:16,153 | INFO | speech length: 61120
2026-01-28 14:32:16,187 | INFO | decoder input length: 95
2026-01-28 14:32:16,187 | INFO | max output length: 95
2026-01-28 14:32:16,187 | INFO | min output length: 9
2026-01-28 14:32:22,435 | INFO | end detected at 66
2026-01-28 14:32:22,438 | INFO |  -6.49 * 0.5 =  -3.24 for decoder
2026-01-28 14:32:22,438 | INFO |  -5.00 * 0.5 =  -2.50 for ctc
2026-01-28 14:32:22,438 | INFO | total log probability: -5.74
2026-01-28 14:32:22,438 | INFO | normalized log probability: -0.11
2026-01-28 14:32:22,438 | INFO | total number of ended hypotheses: 214
2026-01-28 14:32:22,439 | INFO | best hypo: et<space>avant<space>d'arriver<space>au<space>troisième<space>vous<space>prenez<space>à<space>droite

2026-01-28 14:32:22,442 | INFO | speech length: 42400
2026-01-28 14:32:22,474 | INFO | decoder input length: 65
2026-01-28 14:32:22,474 | INFO | max output length: 65
2026-01-28 14:32:22,475 | INFO | min output length: 6
2026-01-28 14:32:27,145 | INFO | end detected at 51
2026-01-28 14:32:27,147 | INFO | -14.29 * 0.5 =  -7.15 for decoder
2026-01-28 14:32:27,147 | INFO |  -7.43 * 0.5 =  -3.72 for ctc
2026-01-28 14:32:27,147 | INFO | total log probability: -10.86
2026-01-28 14:32:27,147 | INFO | normalized log probability: -0.25
2026-01-28 14:32:27,147 | INFO | total number of ended hypotheses: 199
2026-01-28 14:32:27,148 | INFO | best hypo: laouiya<space>les<space>uns<space>incarçon<space>les<space>interfections

2026-01-28 14:32:27,150 | INFO | speech length: 39680
2026-01-28 14:32:27,180 | INFO | decoder input length: 61
2026-01-28 14:32:27,180 | INFO | max output length: 61
2026-01-28 14:32:27,180 | INFO | min output length: 6
2026-01-28 14:32:32,246 | INFO | adding <eos> in the last position in the loop
2026-01-28 14:32:32,257 | INFO | end detected at 60
2026-01-28 14:32:32,259 | INFO |  -4.71 * 0.5 =  -2.36 for decoder
2026-01-28 14:32:32,259 | INFO |  -8.31 * 0.5 =  -4.15 for ctc
2026-01-28 14:32:32,259 | INFO | total log probability: -6.51
2026-01-28 14:32:32,259 | INFO | normalized log probability: -0.12
2026-01-28 14:32:32,259 | INFO | total number of ended hypotheses: 226
2026-01-28 14:32:32,260 | INFO | best hypo: et<space>vous<space>marchez<space>sur<space>deux<space>cents<space>mètres<space>et<space>vous<space>y<space>êtes

2026-01-28 14:32:32,262 | INFO | speech length: 299840
2026-01-28 14:32:32,294 | INFO | decoder input length: 468
2026-01-28 14:32:32,294 | INFO | max output length: 468
2026-01-28 14:32:32,294 | INFO | min output length: 46
2026-01-28 14:33:02,157 | INFO | end detected at 235
2026-01-28 14:33:02,159 | INFO | -22.11 * 0.5 = -11.06 for decoder
2026-01-28 14:33:02,160 | INFO |  -8.06 * 0.5 =  -4.03 for ctc
2026-01-28 14:33:02,160 | INFO | total log probability: -15.09
2026-01-28 14:33:02,160 | INFO | normalized log probability: -0.07
2026-01-28 14:33:02,160 | INFO | total number of ended hypotheses: 213
2026-01-28 14:33:02,163 | INFO | best hypo: euh<space>mais<space>au<space>au<space>prochain<space>arrêt<space>je<space>je<space>continue<space>euh<space>au<space>prochain<space>arrêt<space>oui<space>vous<space>continuez<space>jusqu'au<space>deuxième<space>et<space>avant<space>d'arriver<space>au<space>troisième<space>il<space>y<space>aura<space>une<space>interception<space>des<space>voix<space>et<space>là<space>vous<space>prenez<space>un<space>doigt<space>jusqu'à<space>voir<space>la<space>cathédrale

2026-01-28 14:33:02,169 | INFO | Chunk: 0 | WER=100.000000 | S=0 D=0 I=1
2026-01-28 14:33:02,169 | INFO | Chunk: 1 | WER=50.000000 | S=1 D=1 I=0
2026-01-28 14:33:02,169 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 14:33:02,169 | INFO | Chunk: 3 | WER=9.090909 | S=1 D=0 I=0
2026-01-28 14:33:02,169 | INFO | Chunk: 4 | WER=66.666667 | S=3 D=1 I=0
2026-01-28 14:33:02,170 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 14:33:02,170 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 14:33:02,170 | INFO | Chunk: 7 | WER=88.888889 | S=5 D=3 I=0
2026-01-28 14:33:02,171 | INFO | Chunk: 8 | WER=16.666667 | S=1 D=1 I=0
2026-01-28 14:33:02,172 | INFO | Chunk: 9 | WER=14.634146 | S=4 D=0 I=2
2026-01-28 14:33:02,179 | INFO | File: Rhap-D0017.wav | WER=22.857143 | S=15 D=6 I=3
2026-01-28 14:33:02,179 | INFO | ------------------------------
2026-01-28 14:33:02,179 | INFO | Conf ester Done!
2026-01-28 14:34:00,615 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 14:34:00,615 | INFO | Chunk: 1 | WER=25.000000 | S=0 D=1 I=0
2026-01-28 14:34:00,615 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 14:34:00,616 | INFO | Chunk: 3 | WER=9.090909 | S=1 D=0 I=0
2026-01-28 14:34:00,616 | INFO | Chunk: 4 | WER=66.666667 | S=3 D=1 I=0
2026-01-28 14:34:00,616 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 14:34:00,617 | INFO | Chunk: 6 | WER=20.000000 | S=2 D=0 I=0
2026-01-28 14:34:00,617 | INFO | Chunk: 7 | WER=33.333333 | S=2 D=0 I=1
2026-01-28 14:34:00,617 | INFO | Chunk: 8 | WER=50.000000 | S=2 D=3 I=1
2026-01-28 14:34:00,619 | INFO | Chunk: 9 | WER=26.829268 | S=9 D=0 I=2
2026-01-28 14:34:00,630 | INFO | File: Rhap-D0017.wav | WER=26.666667 | S=19 D=5 I=4
2026-01-28 14:34:00,630 | INFO | ------------------------------
2026-01-28 14:34:00,630 | INFO | hmm_tdnn Done!
2026-01-28 14:34:00,812 | INFO | ==================================Rhap-D0020.wav=========================================
2026-01-28 14:34:00,992 | INFO | Using rVAD model
2026-01-28 14:34:03,061 | INFO | Chunk: 0 | WER=14.864865 | S=7 D=4 I=0
2026-01-28 14:34:03,062 | INFO | Chunk: 1 | WER=38.461538 | S=2 D=2 I=1
2026-01-28 14:34:03,062 | INFO | Chunk: 2 | WER=28.571429 | S=1 D=1 I=0
2026-01-28 14:34:03,062 | INFO | Chunk: 3 | WER=20.000000 | S=0 D=0 I=1
2026-01-28 14:34:03,069 | INFO | File: Rhap-D0020.wav | WER=19.191919 | S=10 D=7 I=2
2026-01-28 14:34:03,069 | INFO | ------------------------------
2026-01-28 14:34:03,069 | INFO | w2vec vad chunk Done!
2026-01-28 14:34:06,731 | INFO | Chunk: 0 | WER=68.918919 | S=4 D=46 I=1
2026-01-28 14:34:06,732 | INFO | Chunk: 1 | WER=23.076923 | S=1 D=0 I=2
2026-01-28 14:34:06,732 | INFO | Chunk: 2 | WER=28.571429 | S=0 D=2 I=0
2026-01-28 14:34:06,732 | INFO | Chunk: 3 | WER=20.000000 | S=0 D=0 I=1
2026-01-28 14:34:06,737 | INFO | File: Rhap-D0020.wav | WER=56.565657 | S=6 D=47 I=3
2026-01-28 14:34:06,737 | INFO | ------------------------------
2026-01-28 14:34:06,738 | INFO | whisper med Done!
2026-01-28 14:34:13,004 | INFO | Chunk: 0 | WER=41.891892 | S=5 D=26 I=0
2026-01-28 14:34:13,005 | INFO | Chunk: 1 | WER=15.384615 | S=1 D=0 I=1
2026-01-28 14:34:13,005 | INFO | Chunk: 2 | WER=28.571429 | S=0 D=2 I=0
2026-01-28 14:34:13,005 | INFO | Chunk: 3 | WER=20.000000 | S=0 D=0 I=1
2026-01-28 14:34:13,011 | INFO | File: Rhap-D0020.wav | WER=36.363636 | S=6 D=28 I=2
2026-01-28 14:34:13,011 | INFO | ------------------------------
2026-01-28 14:34:13,011 | INFO | whisper large Done!
2026-01-28 14:34:13,188 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 14:34:13,225 | INFO | Vocabulary size: 350
2026-01-28 14:34:13,865 | INFO | Gradient checkpoint layers: []
2026-01-28 14:34:14,721 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 14:34:14,725 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 14:34:14,725 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 14:34:14,725 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 14:34:14,726 | INFO | speech length: 347200
2026-01-28 14:34:14,773 | INFO | decoder input length: 542
2026-01-28 14:34:14,773 | INFO | max output length: 542
2026-01-28 14:34:14,773 | INFO | min output length: 54
2026-01-28 14:34:39,134 | INFO | end detected at 166
2026-01-28 14:34:39,137 | INFO | -370.20 * 0.5 = -185.10 for decoder
2026-01-28 14:34:39,137 | INFO | -102.26 * 0.5 = -51.13 for ctc
2026-01-28 14:34:39,137 | INFO | total log probability: -236.23
2026-01-28 14:34:39,137 | INFO | normalized log probability: -1.48
2026-01-28 14:34:39,137 | INFO | total number of ended hypotheses: 191
2026-01-28 14:34:39,139 | INFO | best hypo: ▁oh▁oui▁à▁plein▁d'autres▁itinéraires▁possibles▁après▁salpêtre▁par▁la▁rue▁lacanale▁donc▁on▁traverse▁la▁place▁un▁type▁en▁face▁et▁puis▁l'on▁arrive▁sur▁la▁place▁victor▁hugo▁et▁traverse▁les▁rues▁piétonnes▁et▁apprès▁létèrement▁sur▁la▁droite▁mais▁il▁semble▁est▁le▁plus▁géographiquement▁le▁plus▁simple▁de▁de▁faire▁s'tend▁le▁droit▁de▁la▁par▁la▁place

2026-01-28 14:34:39,143 | INFO | speech length: 68800
2026-01-28 14:34:39,175 | INFO | decoder input length: 107
2026-01-28 14:34:39,175 | INFO | max output length: 107
2026-01-28 14:34:39,175 | INFO | min output length: 10
2026-01-28 14:34:42,692 | INFO | end detected at 35
2026-01-28 14:34:42,693 | INFO | -12.09 * 0.5 =  -6.04 for decoder
2026-01-28 14:34:42,693 | INFO |  -6.68 * 0.5 =  -3.34 for ctc
2026-01-28 14:34:42,694 | INFO | total log probability: -9.38
2026-01-28 14:34:42,694 | INFO | normalized log probability: -0.31
2026-01-28 14:34:42,694 | INFO | total number of ended hypotheses: 161
2026-01-28 14:34:42,694 | INFO | best hypo: ▁voace▁de▁la▁préfecture▁c'est▁la▁place▁je▁suppuquois▁verdun

2026-01-28 14:34:42,696 | INFO | speech length: 41600
2026-01-28 14:34:42,728 | INFO | decoder input length: 64
2026-01-28 14:34:42,728 | INFO | max output length: 64
2026-01-28 14:34:42,728 | INFO | min output length: 6
2026-01-28 14:34:44,696 | INFO | end detected at 21
2026-01-28 14:34:44,697 | INFO |  -1.89 * 0.5 =  -0.94 for decoder
2026-01-28 14:34:44,697 | INFO |  -7.83 * 0.5 =  -3.91 for ctc
2026-01-28 14:34:44,697 | INFO | total log probability: -4.86
2026-01-28 14:34:44,697 | INFO | normalized log probability: -0.35
2026-01-28 14:34:44,697 | INFO | total number of ended hypotheses: 170
2026-01-28 14:34:44,697 | INFO | best hypo: ▁il▁est▁après▁légèrement▁à▁gauche

2026-01-28 14:34:44,699 | INFO | speech length: 40320
2026-01-28 14:34:44,738 | INFO | decoder input length: 62
2026-01-28 14:34:44,738 | INFO | max output length: 62
2026-01-28 14:34:44,738 | INFO | min output length: 6
2026-01-28 14:34:45,927 | INFO | end detected at 13
2026-01-28 14:34:45,928 | INFO |  -2.60 * 0.5 =  -1.30 for decoder
2026-01-28 14:34:45,928 | INFO |  -5.95 * 0.5 =  -2.98 for ctc
2026-01-28 14:34:45,928 | INFO | total log probability: -4.28
2026-01-28 14:34:45,928 | INFO | normalized log probability: -0.48
2026-01-28 14:34:45,928 | INFO | total number of ended hypotheses: 150
2026-01-28 14:34:45,928 | INFO | best hypo: ▁et▁que▁ça▁vous▁va

2026-01-28 14:34:45,937 | INFO | Chunk: 0 | WER=29.729730 | S=12 D=9 I=1
2026-01-28 14:34:45,937 | INFO | Chunk: 1 | WER=30.769231 | S=2 D=2 I=0
2026-01-28 14:34:45,937 | INFO | Chunk: 2 | WER=42.857143 | S=2 D=1 I=0
2026-01-28 14:34:45,937 | INFO | Chunk: 3 | WER=60.000000 | S=1 D=1 I=1
2026-01-28 14:34:45,944 | INFO | File: Rhap-D0020.wav | WER=32.323232 | S=17 D=13 I=2
2026-01-28 14:34:45,944 | INFO | ------------------------------
2026-01-28 14:34:45,945 | INFO | Conf cv Done!
2026-01-28 14:34:46,133 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 14:34:46,156 | INFO | Vocabulary size: 47
2026-01-28 14:34:46,805 | INFO | Gradient checkpoint layers: []
2026-01-28 14:34:47,647 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 14:34:47,651 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 14:34:47,651 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 14:34:47,652 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 14:34:47,654 | INFO | speech length: 347200
2026-01-28 14:34:47,690 | INFO | decoder input length: 542
2026-01-28 14:34:47,690 | INFO | max output length: 542
2026-01-28 14:34:47,690 | INFO | min output length: 54
2026-01-28 14:35:41,959 | INFO | end detected at 407
2026-01-28 14:35:41,962 | INFO | -58.41 * 0.5 = -29.21 for decoder
2026-01-28 14:35:41,962 | INFO | -14.39 * 0.5 =  -7.19 for ctc
2026-01-28 14:35:41,962 | INFO | total log probability: -36.40
2026-01-28 14:35:41,962 | INFO | normalized log probability: -0.09
2026-01-28 14:35:41,962 | INFO | total number of ended hypotheses: 215
2026-01-28 14:35:41,967 | INFO | best hypo: ah<space>oui<space>y<space>a<space>plein<space>d'autres<space>itinéraires<space>possibles<space>après<space>ça<space>peut<space>être<space>euh<space>par<space>la<space>rue<space>la<space>canale<space>donc<space>euh<space>on<space>traverse<space>la<space>place<space>un<space>petit<space>peu<space>en<space>face<space>et<space>puis<space>là<space>on<space>arrive<space>sur<space>la<space>place<space>victor<space>hugo<space>et<space>traverser<space>les<space>rues<space>piétonnes<space>et<space>après<space>légèrement<space>sur<space>la<space>droite<space>mais<space>i<space>me<space>semble<space>que<space>le<space>plus<space>géographiquement<space>le<space>plus<space>simple<space>c'est<space>de<space>de<space>de<space>faire<space>cet<space>endre<space>le<space>droit<space>par<space>la<space>par<space>la<space>place<space>pour<space>cent<space>hesitation

2026-01-28 14:35:41,970 | INFO | speech length: 68800
2026-01-28 14:35:42,003 | INFO | decoder input length: 107
2026-01-28 14:35:42,003 | INFO | max output length: 107
2026-01-28 14:35:42,003 | INFO | min output length: 10
2026-01-28 14:35:48,992 | INFO | end detected at 74
2026-01-28 14:35:48,994 | INFO |  -8.06 * 0.5 =  -4.03 for decoder
2026-01-28 14:35:48,994 | INFO |  -4.99 * 0.5 =  -2.49 for ctc
2026-01-28 14:35:48,994 | INFO | total log probability: -6.52
2026-01-28 14:35:48,994 | INFO | normalized log probability: -0.10
2026-01-28 14:35:48,994 | INFO | total number of ended hypotheses: 169
2026-01-28 14:35:48,995 | INFO | best hypo: face<space>de<space>la<space>préfecture<space>c'est<space>la<space>place<space>euh<space>je<space>sais<space>plus<space>quoi<space>verdin

2026-01-28 14:35:48,997 | INFO | speech length: 41600
2026-01-28 14:35:49,028 | INFO | decoder input length: 64
2026-01-28 14:35:49,028 | INFO | max output length: 64
2026-01-28 14:35:49,028 | INFO | min output length: 6
2026-01-28 14:35:53,173 | INFO | end detected at 45
2026-01-28 14:35:53,175 | INFO |  -3.20 * 0.5 =  -1.60 for decoder
2026-01-28 14:35:53,175 | INFO |  -0.24 * 0.5 =  -0.12 for ctc
2026-01-28 14:35:53,175 | INFO | total log probability: -1.72
2026-01-28 14:35:53,175 | INFO | normalized log probability: -0.04
2026-01-28 14:35:53,175 | INFO | total number of ended hypotheses: 176
2026-01-28 14:35:53,176 | INFO | best hypo: ouais<space>et<space>après<space>euh<space>légèrement<space>à<space>gauche

2026-01-28 14:35:53,178 | INFO | speech length: 40320
2026-01-28 14:35:53,216 | INFO | decoder input length: 62
2026-01-28 14:35:53,217 | INFO | max output length: 62
2026-01-28 14:35:53,217 | INFO | min output length: 6
2026-01-28 14:35:56,590 | INFO | end detected at 36
2026-01-28 14:35:56,592 | INFO |  -2.78 * 0.5 =  -1.39 for decoder
2026-01-28 14:35:56,592 | INFO |  -1.64 * 0.5 =  -0.82 for ctc
2026-01-28 14:35:56,592 | INFO | total log probability: -2.21
2026-01-28 14:35:56,593 | INFO | normalized log probability: -0.08
2026-01-28 14:35:56,593 | INFO | total number of ended hypotheses: 215
2026-01-28 14:35:56,593 | INFO | best hypo: qu'est<space>ce<space>que<space>c'est<space>vous<space>va

2026-01-28 14:35:56,603 | INFO | Chunk: 0 | WER=18.918919 | S=6 D=1 I=7
2026-01-28 14:35:56,603 | INFO | Chunk: 1 | WER=23.076923 | S=2 D=0 I=1
2026-01-28 14:35:56,604 | INFO | Chunk: 2 | WER=28.571429 | S=0 D=1 I=1
2026-01-28 14:35:56,604 | INFO | Chunk: 3 | WER=80.000000 | S=1 D=0 I=3
2026-01-28 14:35:56,612 | INFO | File: Rhap-D0020.wav | WER=23.232323 | S=9 D=2 I=12
2026-01-28 14:35:56,612 | INFO | ------------------------------
2026-01-28 14:35:56,612 | INFO | Conf ester Done!
2026-01-28 14:36:28,226 | INFO | Chunk: 0 | WER=17.567568 | S=9 D=2 I=2
2026-01-28 14:36:28,227 | INFO | Chunk: 1 | WER=46.153846 | S=3 D=0 I=3
2026-01-28 14:36:28,227 | INFO | Chunk: 2 | WER=28.571429 | S=1 D=1 I=0
2026-01-28 14:36:28,227 | INFO | Chunk: 3 | WER=100.000000 | S=0 D=3 I=2
2026-01-28 14:36:28,234 | INFO | File: Rhap-D0020.wav | WER=25.252525 | S=14 D=5 I=6
2026-01-28 14:36:28,235 | INFO | ------------------------------
2026-01-28 14:36:28,235 | INFO | hmm_tdnn Done!
2026-01-28 14:36:28,394 | INFO | ==================================Rhap-D1001.wav=========================================
2026-01-28 14:36:28,636 | INFO | Using rVAD model
2026-01-28 14:37:01,043 | INFO | Chunk: 0 | WER=100.000000 | S=0 D=0 I=4
2026-01-28 14:37:01,044 | INFO | Chunk: 1 | WER=160.000000 | S=4 D=0 I=4
2026-01-28 14:37:01,044 | INFO | Chunk: 2 | WER=83.333333 | S=1 D=0 I=4
2026-01-28 14:37:01,045 | INFO | Chunk: 3 | WER=90.000000 | S=9 D=0 I=0
2026-01-28 14:37:01,045 | INFO | Chunk: 4 | WER=85.714286 | S=1 D=2 I=3
2026-01-28 14:37:01,045 | INFO | Chunk: 5 | WER=83.333333 | S=4 D=1 I=0
2026-01-28 14:37:01,046 | INFO | Chunk: 6 | WER=166.666667 | S=6 D=0 I=4
2026-01-28 14:37:01,046 | INFO | Chunk: 7 | WER=142.857143 | S=7 D=0 I=3
2026-01-28 14:37:01,047 | INFO | Chunk: 8 | WER=100.000000 | S=8 D=0 I=2
2026-01-28 14:37:01,048 | INFO | Chunk: 9 | WER=92.307692 | S=3 D=12 I=9
2026-01-28 14:37:01,048 | INFO | Chunk: 10 | WER=100.000000 | S=4 D=4 I=1
2026-01-28 14:37:01,049 | INFO | Chunk: 11 | WER=100.000000 | S=4 D=2 I=0
2026-01-28 14:37:01,049 | INFO | Chunk: 12 | WER=75.000000 | S=3 D=0 I=0
2026-01-28 14:37:01,049 | INFO | Chunk: 13 | WER=100.000000 | S=2 D=0 I=0
2026-01-28 14:37:01,050 | INFO | Chunk: 14 | WER=46.428571 | S=2 D=6 I=5
2026-01-28 14:37:01,051 | INFO | Chunk: 15 | WER=118.181818 | S=9 D=0 I=4
2026-01-28 14:37:01,051 | INFO | Chunk: 16 | WER=100.000000 | S=6 D=2 I=0
2026-01-28 14:37:01,051 | INFO | Chunk: 17 | WER=0.000000 | S=1 D=0 I=2
2026-01-28 14:37:01,052 | INFO | Chunk: 18 | WER=175.000000 | S=4 D=0 I=3
2026-01-28 14:37:01,052 | INFO | Chunk: 19 | WER=100.000000 | S=7 D=2 I=0
2026-01-28 14:37:01,053 | INFO | Chunk: 20 | WER=128.571429 | S=7 D=0 I=2
2026-01-28 14:37:01,053 | INFO | Chunk: 21 | WER=94.117647 | S=1 D=8 I=7
2026-01-28 14:37:01,054 | INFO | Chunk: 22 | WER=137.500000 | S=7 D=0 I=4
2026-01-28 14:37:01,054 | INFO | Chunk: 23 | WER=150.000000 | S=10 D=0 I=5
2026-01-28 14:37:01,055 | INFO | Chunk: 24 | WER=100.000000 | S=7 D=4 I=0
2026-01-28 14:37:01,055 | INFO | Chunk: 25 | WER=162.500000 | S=8 D=0 I=5
2026-01-28 14:37:01,056 | INFO | Chunk: 26 | WER=112.500000 | S=8 D=0 I=1
2026-01-28 14:37:01,056 | INFO | Chunk: 27 | WER=125.000000 | S=7 D=0 I=3
2026-01-28 14:37:01,057 | INFO | Chunk: 28 | WER=72.727273 | S=2 D=8 I=6
2026-01-28 14:37:01,057 | INFO | Chunk: 29 | WER=150.000000 | S=5 D=0 I=4
2026-01-28 14:37:01,058 | INFO | Chunk: 30 | WER=100.000000 | S=1 D=6 I=0
2026-01-28 14:37:01,058 | INFO | Chunk: 31 | WER=225.000000 | S=4 D=0 I=5
2026-01-28 14:37:01,058 | INFO | Chunk: 32 | WER=100.000000 | S=0 D=2 I=1
2026-01-28 14:37:01,058 | INFO | Chunk: 33 | WER=66.666667 | S=1 D=1 I=0
2026-01-28 14:37:01,058 | INFO | Chunk: 34 | WER=100.000000 | S=2 D=0 I=0
2026-01-28 14:37:01,059 | INFO | Chunk: 35 | WER=90.909091 | S=1 D=1 I=8
2026-01-28 14:37:01,060 | INFO | Chunk: 36 | WER=171.428571 | S=6 D=0 I=6
2026-01-28 14:37:01,060 | INFO | Chunk: 37 | WER=66.666667 | S=0 D=2 I=2
2026-01-28 14:37:01,060 | INFO | Chunk: 38 | WER=200.000000 | S=2 D=0 I=2
2026-01-28 14:37:01,060 | INFO | Chunk: 39 | WER=100.000000 | S=2 D=2 I=0
2026-01-28 14:37:01,061 | INFO | Chunk: 40 | WER=300.000000 | S=2 D=0 I=4
2026-01-28 14:37:01,061 | INFO | Chunk: 41 | WER=140.000000 | S=10 D=0 I=4
2026-01-28 14:37:01,062 | INFO | Chunk: 42 | WER=100.000000 | S=8 D=2 I=0
2026-01-28 14:37:01,063 | INFO | Chunk: 43 | WER=59.090909 | S=0 D=8 I=5
2026-01-28 14:37:01,063 | INFO | Chunk: 44 | WER=100.000000 | S=3 D=0 I=0
2026-01-28 14:37:01,064 | INFO | Chunk: 45 | WER=93.333333 | S=0 D=3 I=11
2026-01-28 14:37:01,065 | INFO | Chunk: 46 | WER=100.000000 | S=1 D=11 I=8
2026-01-28 14:37:01,065 | INFO | Chunk: 47 | WER=128.571429 | S=7 D=0 I=2
2026-01-28 14:37:01,065 | INFO | Chunk: 48 | WER=100.000000 | S=1 D=1 I=0
2026-01-28 14:37:01,065 | INFO | Chunk: 49 | WER=300.000000 | S=1 D=0 I=2
2026-01-28 14:37:01,066 | INFO | Chunk: 50 | WER=100.000000 | S=4 D=0 I=0
2026-01-28 14:37:01,066 | INFO | Chunk: 51 | WER=300.000000 | S=1 D=0 I=2
2026-01-28 14:37:01,067 | INFO | Chunk: 52 | WER=105.882353 | S=16 D=0 I=2
2026-01-28 14:37:01,067 | INFO | Chunk: 53 | WER=100.000000 | S=2 D=0 I=0
2026-01-28 14:37:01,068 | INFO | Chunk: 54 | WER=29.629630 | S=0 D=1 I=7
2026-01-28 14:37:01,069 | INFO | Chunk: 55 | WER=114.285714 | S=0 D=6 I=10
2026-01-28 14:37:01,072 | INFO | Chunk: 56 | WER=63.461538 | S=4 D=16 I=13
2026-01-28 14:37:01,073 | INFO | Chunk: 57 | WER=120.000000 | S=0 D=5 I=13
2026-01-28 14:37:01,073 | INFO | Chunk: 58 | WER=91.666667 | S=6 D=2 I=3
2026-01-28 14:37:01,074 | INFO | Chunk: 59 | WER=84.000000 | S=0 D=12 I=9
2026-01-28 14:37:01,074 | INFO | Chunk: 60 | WER=85.714286 | S=12 D=3 I=3
2026-01-28 14:37:01,075 | INFO | Chunk: 61 | WER=142.857143 | S=7 D=0 I=3
2026-01-28 14:37:01,075 | INFO | Chunk: 62 | WER=87.500000 | S=12 D=1 I=1
2026-01-28 14:37:01,075 | INFO | Chunk: 63 | WER=100.000000 | S=8 D=1 I=0
2026-01-28 14:37:01,076 | INFO | Chunk: 64 | WER=100.000000 | S=1 D=3 I=1
2026-01-28 14:37:01,076 | INFO | Chunk: 65 | WER=200.000000 | S=2 D=0 I=2
2026-01-28 14:37:01,076 | INFO | Chunk: 66 | WER=150.000000 | S=4 D=0 I=2
2026-01-28 14:37:01,077 | INFO | Chunk: 67 | WER=117.647059 | S=1 D=6 I=13
2026-01-28 14:37:01,077 | INFO | Chunk: 68 | WER=100.000000 | S=4 D=0 I=0
2026-01-28 14:37:01,078 | INFO | Chunk: 69 | WER=60.526316 | S=1 D=11 I=11
2026-01-28 14:37:01,078 | INFO | Chunk: 70 | WER=100.000000 | S=6 D=0 I=0
2026-01-28 14:37:01,079 | INFO | Chunk: 71 | WER=150.000000 | S=6 D=0 I=3
2026-01-28 14:37:01,079 | INFO | Chunk: 72 | WER=120.000000 | S=5 D=0 I=1
2026-01-28 14:37:01,082 | INFO | Chunk: 73 | WER=46.153846 | S=13 D=9 I=8
2026-01-28 14:37:01,082 | INFO | Chunk: 74 | WER=118.181818 | S=11 D=0 I=2
2026-01-28 14:37:01,490 | INFO | File: Rhap-D1001.wav | WER=30.287860 | S=83 D=35 I=124
2026-01-28 14:37:01,491 | INFO | ------------------------------
2026-01-28 14:37:01,491 | INFO | w2vec vad chunk Done!
2026-01-28 14:38:03,538 | INFO | Chunk: 0 | WER=100.000000 | S=0 D=0 I=4
2026-01-28 14:38:03,539 | INFO | Chunk: 1 | WER=160.000000 | S=4 D=0 I=4
2026-01-28 14:38:03,539 | INFO | Chunk: 2 | WER=100.000000 | S=4 D=0 I=2
2026-01-28 14:38:03,540 | INFO | Chunk: 3 | WER=90.000000 | S=9 D=0 I=0
2026-01-28 14:38:03,540 | INFO | Chunk: 4 | WER=85.714286 | S=1 D=2 I=3
2026-01-28 14:38:03,540 | INFO | Chunk: 5 | WER=83.333333 | S=4 D=1 I=0
2026-01-28 14:38:03,540 | INFO | Chunk: 6 | WER=150.000000 | S=6 D=0 I=3
2026-01-28 14:38:03,541 | INFO | Chunk: 7 | WER=142.857143 | S=7 D=0 I=3
2026-01-28 14:38:03,541 | INFO | Chunk: 8 | WER=100.000000 | S=8 D=0 I=2
2026-01-28 14:38:03,542 | INFO | Chunk: 9 | WER=92.307692 | S=3 D=13 I=8
2026-01-28 14:38:03,542 | INFO | Chunk: 10 | WER=100.000000 | S=4 D=4 I=1
2026-01-28 14:38:03,543 | INFO | Chunk: 11 | WER=100.000000 | S=4 D=2 I=0
2026-01-28 14:38:03,543 | INFO | Chunk: 12 | WER=75.000000 | S=3 D=0 I=0
2026-01-28 14:38:03,543 | INFO | Chunk: 13 | WER=200.000000 | S=2 D=0 I=2
2026-01-28 14:38:03,544 | INFO | Chunk: 14 | WER=39.285714 | S=1 D=5 I=5
2026-01-28 14:38:03,545 | INFO | Chunk: 15 | WER=109.090909 | S=9 D=0 I=3
2026-01-28 14:38:03,545 | INFO | Chunk: 16 | WER=100.000000 | S=6 D=2 I=0
2026-01-28 14:38:03,545 | INFO | Chunk: 17 | WER=0.000000 | S=1 D=0 I=2
2026-01-28 14:38:03,546 | INFO | Chunk: 18 | WER=200.000000 | S=4 D=0 I=4
2026-01-28 14:38:03,546 | INFO | Chunk: 19 | WER=100.000000 | S=7 D=2 I=0
2026-01-28 14:38:03,546 | INFO | Chunk: 20 | WER=128.571429 | S=7 D=0 I=2
2026-01-28 14:38:03,547 | INFO | Chunk: 21 | WER=94.117647 | S=1 D=8 I=7
2026-01-28 14:38:03,547 | INFO | Chunk: 22 | WER=137.500000 | S=7 D=0 I=4
2026-01-28 14:38:03,548 | INFO | Chunk: 23 | WER=170.000000 | S=10 D=0 I=7
2026-01-28 14:38:03,548 | INFO | Chunk: 24 | WER=100.000000 | S=7 D=4 I=0
2026-01-28 14:38:03,549 | INFO | Chunk: 25 | WER=162.500000 | S=8 D=0 I=5
2026-01-28 14:38:03,549 | INFO | Chunk: 26 | WER=112.500000 | S=8 D=0 I=1
2026-01-28 14:38:03,549 | INFO | Chunk: 27 | WER=125.000000 | S=7 D=0 I=3
2026-01-28 14:38:03,550 | INFO | Chunk: 28 | WER=63.636364 | S=1 D=7 I=6
2026-01-28 14:38:03,551 | INFO | Chunk: 29 | WER=150.000000 | S=5 D=0 I=4
2026-01-28 14:38:03,551 | INFO | Chunk: 30 | WER=100.000000 | S=1 D=6 I=0
2026-01-28 14:38:03,551 | INFO | Chunk: 31 | WER=225.000000 | S=0 D=2 I=7
2026-01-28 14:38:03,551 | INFO | Chunk: 32 | WER=100.000000 | S=0 D=2 I=1
2026-01-28 14:38:03,552 | INFO | Chunk: 33 | WER=66.666667 | S=1 D=1 I=0
2026-01-28 14:38:03,552 | INFO | Chunk: 34 | WER=100.000000 | S=2 D=0 I=0
2026-01-28 14:38:03,552 | INFO | Chunk: 35 | WER=90.909091 | S=0 D=1 I=9
2026-01-28 14:38:03,553 | INFO | Chunk: 36 | WER=171.428571 | S=6 D=0 I=6
2026-01-28 14:38:03,553 | INFO | Chunk: 37 | WER=66.666667 | S=0 D=2 I=2
2026-01-28 14:38:03,553 | INFO | Chunk: 38 | WER=200.000000 | S=2 D=0 I=2
2026-01-28 14:38:03,553 | INFO | Chunk: 39 | WER=100.000000 | S=2 D=2 I=0
2026-01-28 14:38:03,554 | INFO | Chunk: 40 | WER=300.000000 | S=2 D=0 I=4
2026-01-28 14:38:03,554 | INFO | Chunk: 41 | WER=150.000000 | S=10 D=0 I=5
2026-01-28 14:38:03,555 | INFO | Chunk: 42 | WER=100.000000 | S=8 D=2 I=0
2026-01-28 14:38:03,555 | INFO | Chunk: 43 | WER=63.636364 | S=2 D=8 I=4
2026-01-28 14:38:03,556 | INFO | Chunk: 44 | WER=100.000000 | S=1 D=2 I=0
2026-01-28 14:38:03,556 | INFO | Chunk: 45 | WER=93.333333 | S=0 D=3 I=11
2026-01-28 14:38:03,557 | INFO | Chunk: 46 | WER=100.000000 | S=16 D=4 I=0
2026-01-28 14:38:03,557 | INFO | Chunk: 47 | WER=128.571429 | S=7 D=0 I=2
2026-01-28 14:38:03,558 | INFO | Chunk: 48 | WER=100.000000 | S=1 D=1 I=0
2026-01-28 14:38:03,558 | INFO | Chunk: 49 | WER=300.000000 | S=1 D=0 I=2
2026-01-28 14:38:03,558 | INFO | Chunk: 50 | WER=100.000000 | S=2 D=1 I=1
2026-01-28 14:38:03,558 | INFO | Chunk: 51 | WER=200.000000 | S=1 D=0 I=1
2026-01-28 14:38:03,559 | INFO | Chunk: 52 | WER=117.647059 | S=15 D=0 I=5
2026-01-28 14:38:03,559 | INFO | Chunk: 53 | WER=100.000000 | S=1 D=0 I=1
2026-01-28 14:38:03,560 | INFO | Chunk: 54 | WER=7.407407 | S=0 D=1 I=1
2026-01-28 14:38:03,561 | INFO | Chunk: 55 | WER=114.285714 | S=0 D=6 I=10
2026-01-28 14:38:03,563 | INFO | Chunk: 56 | WER=50.000000 | S=1 D=25 I=0
2026-01-28 14:38:03,563 | INFO | Chunk: 57 | WER=120.000000 | S=0 D=5 I=13
2026-01-28 14:38:03,564 | INFO | Chunk: 58 | WER=91.666667 | S=6 D=2 I=3
2026-01-28 14:38:03,565 | INFO | Chunk: 59 | WER=88.000000 | S=0 D=12 I=10
2026-01-28 14:38:03,566 | INFO | Chunk: 60 | WER=85.714286 | S=13 D=2 I=3
2026-01-28 14:38:03,566 | INFO | Chunk: 61 | WER=142.857143 | S=7 D=0 I=3
2026-01-28 14:38:03,567 | INFO | Chunk: 62 | WER=75.000000 | S=9 D=3 I=0
2026-01-28 14:38:03,567 | INFO | Chunk: 63 | WER=100.000000 | S=8 D=1 I=0
2026-01-28 14:38:03,568 | INFO | Chunk: 64 | WER=100.000000 | S=1 D=3 I=1
2026-01-28 14:38:03,568 | INFO | Chunk: 65 | WER=150.000000 | S=2 D=0 I=1
2026-01-28 14:38:03,568 | INFO | Chunk: 66 | WER=150.000000 | S=4 D=0 I=2
2026-01-28 14:38:03,569 | INFO | Chunk: 67 | WER=111.764706 | S=0 D=6 I=13
2026-01-28 14:38:03,570 | INFO | Chunk: 68 | WER=125.000000 | S=4 D=0 I=1
2026-01-28 14:38:03,572 | INFO | Chunk: 69 | WER=55.263158 | S=0 D=10 I=11
2026-01-28 14:38:03,572 | INFO | Chunk: 70 | WER=100.000000 | S=6 D=0 I=0
2026-01-28 14:38:03,572 | INFO | Chunk: 71 | WER=116.666667 | S=6 D=0 I=1
2026-01-28 14:38:03,573 | INFO | Chunk: 72 | WER=120.000000 | S=5 D=0 I=1
2026-01-28 14:38:03,576 | INFO | Chunk: 73 | WER=41.538462 | S=5 D=22 I=0
2026-01-28 14:38:03,576 | INFO | Chunk: 74 | WER=100.000000 | S=9 D=2 I=0
2026-01-28 14:38:04,012 | INFO | File: Rhap-D1001.wav | WER=28.911139 | S=67 D=64 I=100
2026-01-28 14:38:04,012 | INFO | ------------------------------
2026-01-28 14:38:04,012 | INFO | whisper med Done!
2026-01-28 14:39:29,954 | INFO | Chunk: 0 | WER=100.000000 | S=0 D=0 I=4
2026-01-28 14:39:29,955 | INFO | Chunk: 1 | WER=200.000000 | S=4 D=0 I=6
2026-01-28 14:39:29,955 | INFO | Chunk: 2 | WER=116.666667 | S=1 D=0 I=6
2026-01-28 14:39:29,955 | INFO | Chunk: 3 | WER=90.000000 | S=9 D=0 I=0
2026-01-28 14:39:29,956 | INFO | Chunk: 4 | WER=100.000000 | S=2 D=2 I=3
2026-01-28 14:39:29,956 | INFO | Chunk: 5 | WER=83.333333 | S=4 D=1 I=0
2026-01-28 14:39:29,956 | INFO | Chunk: 6 | WER=150.000000 | S=6 D=0 I=3
2026-01-28 14:39:29,956 | INFO | Chunk: 7 | WER=142.857143 | S=7 D=0 I=3
2026-01-28 14:39:29,956 | INFO | Chunk: 8 | WER=100.000000 | S=8 D=0 I=2
2026-01-28 14:39:29,957 | INFO | Chunk: 9 | WER=92.307692 | S=3 D=12 I=9
2026-01-28 14:39:29,957 | INFO | Chunk: 10 | WER=100.000000 | S=4 D=4 I=1
2026-01-28 14:39:29,958 | INFO | Chunk: 11 | WER=100.000000 | S=4 D=2 I=0
2026-01-28 14:39:29,958 | INFO | Chunk: 12 | WER=75.000000 | S=3 D=0 I=0
2026-01-28 14:39:29,958 | INFO | Chunk: 13 | WER=100.000000 | S=2 D=0 I=0
2026-01-28 14:39:29,959 | INFO | Chunk: 14 | WER=42.857143 | S=2 D=5 I=5
2026-01-28 14:39:29,959 | INFO | Chunk: 15 | WER=109.090909 | S=9 D=0 I=3
2026-01-28 14:39:29,959 | INFO | Chunk: 16 | WER=100.000000 | S=5 D=3 I=0
2026-01-28 14:39:29,959 | INFO | Chunk: 17 | WER=0.000000 | S=1 D=0 I=2
2026-01-28 14:39:29,959 | INFO | Chunk: 18 | WER=200.000000 | S=4 D=0 I=4
2026-01-28 14:39:29,960 | INFO | Chunk: 19 | WER=100.000000 | S=7 D=2 I=0
2026-01-28 14:39:29,960 | INFO | Chunk: 20 | WER=128.571429 | S=7 D=0 I=2
2026-01-28 14:39:29,960 | INFO | Chunk: 21 | WER=94.117647 | S=1 D=8 I=7
2026-01-28 14:39:29,961 | INFO | Chunk: 22 | WER=137.500000 | S=7 D=0 I=4
2026-01-28 14:39:29,961 | INFO | Chunk: 23 | WER=170.000000 | S=10 D=0 I=7
2026-01-28 14:39:29,961 | INFO | Chunk: 24 | WER=100.000000 | S=7 D=4 I=0
2026-01-28 14:39:29,962 | INFO | Chunk: 25 | WER=162.500000 | S=8 D=0 I=5
2026-01-28 14:39:29,962 | INFO | Chunk: 26 | WER=112.500000 | S=7 D=0 I=2
2026-01-28 14:39:29,962 | INFO | Chunk: 27 | WER=125.000000 | S=7 D=0 I=3
2026-01-28 14:39:29,963 | INFO | Chunk: 28 | WER=63.636364 | S=1 D=7 I=6
2026-01-28 14:39:29,963 | INFO | Chunk: 29 | WER=150.000000 | S=5 D=0 I=4
2026-01-28 14:39:29,963 | INFO | Chunk: 30 | WER=100.000000 | S=2 D=5 I=0
2026-01-28 14:39:29,963 | INFO | Chunk: 31 | WER=225.000000 | S=4 D=0 I=5
2026-01-28 14:39:29,963 | INFO | Chunk: 32 | WER=100.000000 | S=1 D=1 I=1
2026-01-28 14:39:29,964 | INFO | Chunk: 33 | WER=66.666667 | S=1 D=1 I=0
2026-01-28 14:39:29,964 | INFO | Chunk: 34 | WER=100.000000 | S=2 D=0 I=0
2026-01-28 14:39:29,964 | INFO | Chunk: 35 | WER=90.909091 | S=0 D=1 I=9
2026-01-28 14:39:29,964 | INFO | Chunk: 36 | WER=171.428571 | S=6 D=0 I=6
2026-01-28 14:39:29,965 | INFO | Chunk: 37 | WER=66.666667 | S=0 D=2 I=2
2026-01-28 14:39:29,965 | INFO | Chunk: 38 | WER=150.000000 | S=2 D=0 I=1
2026-01-28 14:39:29,965 | INFO | Chunk: 39 | WER=100.000000 | S=2 D=2 I=0
2026-01-28 14:39:29,965 | INFO | Chunk: 40 | WER=300.000000 | S=2 D=0 I=4
2026-01-28 14:39:29,965 | INFO | Chunk: 41 | WER=160.000000 | S=10 D=0 I=6
2026-01-28 14:39:29,966 | INFO | Chunk: 42 | WER=100.000000 | S=10 D=0 I=0
2026-01-28 14:39:29,966 | INFO | Chunk: 43 | WER=54.545455 | S=0 D=8 I=4
2026-01-28 14:39:29,966 | INFO | Chunk: 44 | WER=100.000000 | S=3 D=0 I=0
2026-01-28 14:39:29,967 | INFO | Chunk: 45 | WER=86.666667 | S=0 D=2 I=11
2026-01-28 14:39:29,967 | INFO | Chunk: 46 | WER=100.000000 | S=17 D=3 I=0
2026-01-28 14:39:29,968 | INFO | Chunk: 47 | WER=142.857143 | S=7 D=0 I=3
2026-01-28 14:39:29,968 | INFO | Chunk: 48 | WER=100.000000 | S=2 D=0 I=0
2026-01-28 14:39:29,968 | INFO | Chunk: 49 | WER=300.000000 | S=1 D=0 I=2
2026-01-28 14:39:29,968 | INFO | Chunk: 50 | WER=125.000000 | S=4 D=0 I=1
2026-01-28 14:39:29,968 | INFO | Chunk: 51 | WER=400.000000 | S=1 D=0 I=3
2026-01-28 14:39:29,969 | INFO | Chunk: 52 | WER=117.647059 | S=15 D=0 I=5
2026-01-28 14:39:29,969 | INFO | Chunk: 53 | WER=150.000000 | S=2 D=0 I=1
2026-01-28 14:39:29,970 | INFO | Chunk: 54 | WER=40.740741 | S=0 D=1 I=10
2026-01-28 14:39:29,970 | INFO | Chunk: 55 | WER=114.285714 | S=0 D=6 I=10
2026-01-28 14:39:29,972 | INFO | Chunk: 56 | WER=63.461538 | S=2 D=17 I=14
2026-01-28 14:39:29,972 | INFO | Chunk: 57 | WER=120.000000 | S=0 D=5 I=13
2026-01-28 14:39:29,972 | INFO | Chunk: 58 | WER=91.666667 | S=6 D=2 I=3
2026-01-28 14:39:29,973 | INFO | Chunk: 59 | WER=92.000000 | S=0 D=13 I=10
2026-01-28 14:39:29,974 | INFO | Chunk: 60 | WER=85.714286 | S=13 D=2 I=3
2026-01-28 14:39:29,974 | INFO | Chunk: 61 | WER=142.857143 | S=7 D=0 I=3
2026-01-28 14:39:29,974 | INFO | Chunk: 62 | WER=81.250000 | S=9 D=3 I=1
2026-01-28 14:39:29,975 | INFO | Chunk: 63 | WER=100.000000 | S=8 D=1 I=0
2026-01-28 14:39:29,975 | INFO | Chunk: 64 | WER=100.000000 | S=1 D=3 I=1
2026-01-28 14:39:29,975 | INFO | Chunk: 65 | WER=250.000000 | S=2 D=0 I=3
2026-01-28 14:39:29,975 | INFO | Chunk: 66 | WER=150.000000 | S=4 D=0 I=2
2026-01-28 14:39:29,976 | INFO | Chunk: 67 | WER=111.764706 | S=0 D=6 I=13
2026-01-28 14:39:29,976 | INFO | Chunk: 68 | WER=150.000000 | S=4 D=0 I=2
2026-01-28 14:39:29,977 | INFO | Chunk: 69 | WER=57.894737 | S=0 D=10 I=12
2026-01-28 14:39:29,977 | INFO | Chunk: 70 | WER=100.000000 | S=6 D=0 I=0
2026-01-28 14:39:29,977 | INFO | Chunk: 71 | WER=133.333333 | S=6 D=0 I=2
2026-01-28 14:39:29,978 | INFO | Chunk: 72 | WER=120.000000 | S=5 D=0 I=1
2026-01-28 14:39:29,980 | INFO | Chunk: 73 | WER=38.461538 | S=2 D=22 I=1
2026-01-28 14:39:29,980 | INFO | Chunk: 74 | WER=118.181818 | S=11 D=0 I=2
2026-01-28 14:39:30,326 | INFO | File: Rhap-D1001.wav | WER=28.785982 | S=64 D=35 I=131
2026-01-28 14:39:30,326 | INFO | ------------------------------
2026-01-28 14:39:30,326 | INFO | whisper large Done!
2026-01-28 14:39:30,503 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 14:39:30,538 | INFO | Vocabulary size: 350
2026-01-28 14:39:31,147 | INFO | Gradient checkpoint layers: []
2026-01-28 14:39:32,052 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 14:39:32,056 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 14:39:32,056 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 14:39:32,057 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 14:39:32,057 | INFO | speech length: 94400
2026-01-28 14:39:32,099 | INFO | decoder input length: 147
2026-01-28 14:39:32,099 | INFO | max output length: 147
2026-01-28 14:39:32,099 | INFO | min output length: 14
2026-01-28 14:39:34,597 | INFO | end detected at 26
2026-01-28 14:39:34,598 | INFO |  -1.63 * 0.5 =  -0.81 for decoder
2026-01-28 14:39:34,598 | INFO |  -0.03 * 0.5 =  -0.02 for ctc
2026-01-28 14:39:34,598 | INFO | total log probability: -0.83
2026-01-28 14:39:34,598 | INFO | normalized log probability: -0.04
2026-01-28 14:39:34,598 | INFO | total number of ended hypotheses: 142
2026-01-28 14:39:34,598 | INFO | best hypo: ▁et▁chaque▁région▁géographique▁a▁aussi▁son▁comportement

2026-01-28 14:39:34,602 | INFO | speech length: 38720
2026-01-28 14:39:34,638 | INFO | decoder input length: 60
2026-01-28 14:39:34,638 | INFO | max output length: 60
2026-01-28 14:39:34,638 | INFO | min output length: 6
2026-01-28 14:39:36,649 | INFO | end detected at 22
2026-01-28 14:39:36,651 | INFO |  -2.09 * 0.5 =  -1.05 for decoder
2026-01-28 14:39:36,651 | INFO | -10.90 * 0.5 =  -5.45 for ctc
2026-01-28 14:39:36,651 | INFO | total log probability: -6.50
2026-01-28 14:39:36,651 | INFO | normalized log probability: -0.41
2026-01-28 14:39:36,651 | INFO | total number of ended hypotheses: 167
2026-01-28 14:39:36,651 | INFO | best hypo: ▁vis▁à▁vis▁de▁la▁foi▁isabelle

2026-01-28 14:39:36,653 | INFO | speech length: 32320
2026-01-28 14:39:36,683 | INFO | decoder input length: 50
2026-01-28 14:39:36,683 | INFO | max output length: 50
2026-01-28 14:39:36,683 | INFO | min output length: 5
2026-01-28 14:39:39,152 | INFO | end detected at 28
2026-01-28 14:39:39,153 | INFO |  -2.36 * 0.5 =  -1.18 for decoder
2026-01-28 14:39:39,153 | INFO |  -4.08 * 0.5 =  -2.04 for ctc
2026-01-28 14:39:39,153 | INFO | total log probability: -3.22
2026-01-28 14:39:39,154 | INFO | normalized log probability: -0.15
2026-01-28 14:39:39,154 | INFO | total number of ended hypotheses: 174
2026-01-28 14:39:39,154 | INFO | best hypo: ▁l'église▁vis▁à▁vis▁de▁la▁vie▁chrétienne

2026-01-28 14:39:39,156 | INFO | speech length: 67520
2026-01-28 14:39:39,187 | INFO | decoder input length: 105
2026-01-28 14:39:39,187 | INFO | max output length: 105
2026-01-28 14:39:39,187 | INFO | min output length: 10
2026-01-28 14:39:41,636 | INFO | end detected at 23
2026-01-28 14:39:41,637 | INFO |  -1.47 * 0.5 =  -0.74 for decoder
2026-01-28 14:39:41,637 | INFO |  -0.28 * 0.5 =  -0.14 for ctc
2026-01-28 14:39:41,637 | INFO | total log probability: -0.88
2026-01-28 14:39:41,637 | INFO | normalized log probability: -0.05
2026-01-28 14:39:41,637 | INFO | total number of ended hypotheses: 139
2026-01-28 14:39:41,637 | INFO | best hypo: ▁on▁dit▁que▁il▁y▁a▁la▁région▁de▁beauce

2026-01-28 14:39:41,639 | INFO | speech length: 36320
2026-01-28 14:39:41,671 | INFO | decoder input length: 56
2026-01-28 14:39:41,671 | INFO | max output length: 56
2026-01-28 14:39:41,671 | INFO | min output length: 5
2026-01-28 14:39:43,937 | INFO | end detected at 23
2026-01-28 14:39:43,939 | INFO |  -1.43 * 0.5 =  -0.72 for decoder
2026-01-28 14:39:43,939 | INFO |  -3.35 * 0.5 =  -1.68 for ctc
2026-01-28 14:39:43,939 | INFO | total log probability: -2.39
2026-01-28 14:39:43,939 | INFO | normalized log probability: -0.13
2026-01-28 14:39:43,939 | INFO | total number of ended hypotheses: 181
2026-01-28 14:39:43,940 | INFO | best hypo: ▁dans▁la▁région▁de▁beauce▁on▁dit▁qu'

2026-01-28 14:39:43,942 | INFO | speech length: 36480
2026-01-28 14:39:43,973 | INFO | decoder input length: 56
2026-01-28 14:39:43,973 | INFO | max output length: 56
2026-01-28 14:39:43,973 | INFO | min output length: 5
2026-01-28 14:39:45,402 | INFO | end detected at 15
2026-01-28 14:39:45,404 | INFO |  -0.88 * 0.5 =  -0.44 for decoder
2026-01-28 14:39:45,404 | INFO |  -0.11 * 0.5 =  -0.06 for ctc
2026-01-28 14:39:45,404 | INFO | total log probability: -0.50
2026-01-28 14:39:45,404 | INFO | normalized log probability: -0.05
2026-01-28 14:39:45,404 | INFO | total number of ended hypotheses: 140
2026-01-28 14:39:45,404 | INFO | best hypo: ▁la▁culture▁est▁une▁culture

2026-01-28 14:39:45,406 | INFO | speech length: 60640
2026-01-28 14:39:45,439 | INFO | decoder input length: 94
2026-01-28 14:39:45,439 | INFO | max output length: 94
2026-01-28 14:39:45,439 | INFO | min output length: 9
2026-01-28 14:39:48,254 | INFO | end detected at 29
2026-01-28 14:39:48,255 | INFO |  -2.20 * 0.5 =  -1.10 for decoder
2026-01-28 14:39:48,255 | INFO |  -2.59 * 0.5 =  -1.30 for ctc
2026-01-28 14:39:48,255 | INFO | total log probability: -2.39
2026-01-28 14:39:48,255 | INFO | normalized log probability: -0.10
2026-01-28 14:39:48,255 | INFO | total number of ended hypotheses: 180
2026-01-28 14:39:48,256 | INFO | best hypo: ▁j'y▁rends▁bien▁mais▁qui▁demande▁beaucoup▁de▁travail

2026-01-28 14:39:48,258 | INFO | speech length: 36480
2026-01-28 14:39:48,289 | INFO | decoder input length: 56
2026-01-28 14:39:48,289 | INFO | max output length: 56
2026-01-28 14:39:48,289 | INFO | min output length: 5
2026-01-28 14:39:50,730 | INFO | end detected at 26
2026-01-28 14:39:50,731 | INFO |  -1.62 * 0.5 =  -0.81 for decoder
2026-01-28 14:39:50,731 | INFO |  -0.09 * 0.5 =  -0.04 for ctc
2026-01-28 14:39:50,731 | INFO | total log probability: -0.85
2026-01-28 14:39:50,731 | INFO | normalized log probability: -0.04
2026-01-28 14:39:50,731 | INFO | total number of ended hypotheses: 148
2026-01-28 14:39:50,732 | INFO | best hypo: ▁c'est▁une▁terre▁solide▁c'est▁une▁terre▁forte

2026-01-28 14:39:50,733 | INFO | speech length: 56960
2026-01-28 14:39:50,773 | INFO | decoder input length: 88
2026-01-28 14:39:50,773 | INFO | max output length: 88
2026-01-28 14:39:50,773 | INFO | min output length: 8
2026-01-28 14:39:53,771 | INFO | end detected at 32
2026-01-28 14:39:53,773 | INFO |  -8.62 * 0.5 =  -4.31 for decoder
2026-01-28 14:39:53,773 | INFO |  -2.32 * 0.5 =  -1.16 for ctc
2026-01-28 14:39:53,773 | INFO | total log probability: -5.47
2026-01-28 14:39:53,773 | INFO | normalized log probability: -0.21
2026-01-28 14:39:53,773 | INFO | total number of ended hypotheses: 169
2026-01-28 14:39:53,774 | INFO | best hypo: ▁c'est▁l▁moi▁mûrir▁et▁a▁poussé▁mais▁c'est▁solide

2026-01-28 14:39:53,776 | INFO | speech length: 161760
2026-01-28 14:39:53,821 | INFO | decoder input length: 252
2026-01-28 14:39:53,821 | INFO | max output length: 252
2026-01-28 14:39:53,821 | INFO | min output length: 25
2026-01-28 14:40:00,698 | INFO | end detected at 60
2026-01-28 14:40:00,699 | INFO | -17.48 * 0.5 =  -8.74 for decoder
2026-01-28 14:40:00,699 | INFO |  -6.01 * 0.5 =  -3.00 for ctc
2026-01-28 14:40:00,699 | INFO | total log probability: -11.75
2026-01-28 14:40:00,699 | INFO | normalized log probability: -0.21
2026-01-28 14:40:00,699 | INFO | total number of ended hypotheses: 161
2026-01-28 14:40:00,700 | INFO | best hypo: ▁alors▁on▁dit▁que▁le▁tempérament▁du▁beauceron▁est▁un▁tempérament▁volontaire▁travailleur▁mais▁fidèle▁et▁quand▁il▁a▁donné▁ça▁sa▁foi

2026-01-28 14:40:00,705 | INFO | speech length: 43680
2026-01-28 14:40:00,742 | INFO | decoder input length: 67
2026-01-28 14:40:00,742 | INFO | max output length: 67
2026-01-28 14:40:00,742 | INFO | min output length: 6
2026-01-28 14:40:02,530 | INFO | end detected at 18
2026-01-28 14:40:02,531 | INFO |  -2.61 * 0.5 =  -1.31 for decoder
2026-01-28 14:40:02,531 | INFO |  -1.92 * 0.5 =  -0.96 for ctc
2026-01-28 14:40:02,531 | INFO | total log probability: -2.27
2026-01-28 14:40:02,531 | INFO | normalized log probability: -0.16
2026-01-28 14:40:02,531 | INFO | total number of ended hypotheses: 155
2026-01-28 14:40:02,531 | INFO | best hypo: ▁à▁quelqu'un▁c'est▁durable

2026-01-28 14:40:02,533 | INFO | speech length: 48640
2026-01-28 14:40:02,574 | INFO | decoder input length: 75
2026-01-28 14:40:02,574 | INFO | max output length: 75
2026-01-28 14:40:02,574 | INFO | min output length: 7
2026-01-28 14:40:04,163 | INFO | end detected at 16
2026-01-28 14:40:04,164 | INFO |  -0.78 * 0.5 =  -0.39 for decoder
2026-01-28 14:40:04,165 | INFO |  -0.28 * 0.5 =  -0.14 for ctc
2026-01-28 14:40:04,165 | INFO | total log probability: -0.53
2026-01-28 14:40:04,165 | INFO | normalized log probability: -0.04
2026-01-28 14:40:04,165 | INFO | total number of ended hypotheses: 146
2026-01-28 14:40:04,165 | INFO | best hypo: ▁nous▁le▁ressentons▁certainement

2026-01-28 14:40:04,167 | INFO | speech length: 44960
2026-01-28 14:40:04,205 | INFO | decoder input length: 69
2026-01-28 14:40:04,205 | INFO | max output length: 69
2026-01-28 14:40:04,205 | INFO | min output length: 6
2026-01-28 14:40:05,843 | INFO | end detected at 17
2026-01-28 14:40:05,844 | INFO |  -0.96 * 0.5 =  -0.48 for decoder
2026-01-28 14:40:05,844 | INFO |  -0.30 * 0.5 =  -0.15 for ctc
2026-01-28 14:40:05,844 | INFO | total log probability: -0.63
2026-01-28 14:40:05,844 | INFO | normalized log probability: -0.05
2026-01-28 14:40:05,844 | INFO | total number of ended hypotheses: 145
2026-01-28 14:40:05,844 | INFO | best hypo: ▁dans▁le▁comportement▁religieux

2026-01-28 14:40:05,846 | INFO | speech length: 16800
2026-01-28 14:40:05,887 | INFO | decoder input length: 25
2026-01-28 14:40:05,887 | INFO | max output length: 25
2026-01-28 14:40:05,887 | INFO | min output length: 2
2026-01-28 14:40:06,744 | INFO | end detected at 9
2026-01-28 14:40:06,745 | INFO |  -8.43 * 0.5 =  -4.21 for decoder
2026-01-28 14:40:06,745 | INFO |  -5.85 * 0.5 =  -2.93 for ctc
2026-01-28 14:40:06,745 | INFO | total log probability: -7.14
2026-01-28 14:40:06,745 | INFO | normalized log probability: -1.78
2026-01-28 14:40:06,745 | INFO | total number of ended hypotheses: 160
2026-01-28 14:40:06,745 | INFO | best hypo: ▁quatrep

2026-01-28 14:40:06,747 | INFO | speech length: 199840
2026-01-28 14:40:06,812 | INFO | decoder input length: 311
2026-01-28 14:40:06,812 | INFO | max output length: 311
2026-01-28 14:40:06,812 | INFO | min output length: 31
2026-01-28 14:40:15,200 | INFO | end detected at 66
2026-01-28 14:40:15,202 | INFO | -15.59 * 0.5 =  -7.80 for decoder
2026-01-28 14:40:15,202 | INFO |  -1.89 * 0.5 =  -0.94 for ctc
2026-01-28 14:40:15,202 | INFO | total log probability: -8.74
2026-01-28 14:40:15,202 | INFO | normalized log probability: -0.14
2026-01-28 14:40:15,202 | INFO | total number of ended hypotheses: 173
2026-01-28 14:40:15,203 | INFO | best hypo: ▁traditions▁deux▁familles▁qui▁se▁transmettent▁dans▁le▁domaine▁religieux▁comme▁dans▁tout▁le▁domaine▁du▁travail▁du▁sérieux▁de▁l'économie▁de▁la▁résistance▁culture

2026-01-28 14:40:15,205 | INFO | speech length: 83840
2026-01-28 14:40:15,254 | INFO | decoder input length: 130
2026-01-28 14:40:15,254 | INFO | max output length: 130
2026-01-28 14:40:15,254 | INFO | min output length: 13
2026-01-28 14:40:19,375 | INFO | end detected at 38
2026-01-28 14:40:19,376 | INFO |  -3.35 * 0.5 =  -1.68 for decoder
2026-01-28 14:40:19,376 | INFO |  -2.87 * 0.5 =  -1.44 for ctc
2026-01-28 14:40:19,376 | INFO | total log probability: -3.11
2026-01-28 14:40:19,376 | INFO | normalized log probability: -0.10
2026-01-28 14:40:19,376 | INFO | total number of ended hypotheses: 187
2026-01-28 14:40:19,377 | INFO | best hypo: ▁il▁y▁a▁la▁tradition▁familiale▁nous▁sentons▁très▁bien▁que▁lorsque▁la▁foi

2026-01-28 14:40:19,380 | INFO | speech length: 37600
2026-01-28 14:40:19,412 | INFO | decoder input length: 58
2026-01-28 14:40:19,412 | INFO | max output length: 58
2026-01-28 14:40:19,412 | INFO | min output length: 5
2026-01-28 14:40:21,250 | INFO | end detected at 19
2026-01-28 14:40:21,251 | INFO |  -1.44 * 0.5 =  -0.72 for decoder
2026-01-28 14:40:21,251 | INFO |  -0.89 * 0.5 =  -0.44 for ctc
2026-01-28 14:40:21,251 | INFO | total log probability: -1.16
2026-01-28 14:40:21,251 | INFO | normalized log probability: -0.08
2026-01-28 14:40:21,251 | INFO | total number of ended hypotheses: 149
2026-01-28 14:40:21,251 | INFO | best hypo: ▁elle▁est▁implantée▁dans▁une▁famille

2026-01-28 14:40:21,253 | INFO | speech length: 9600
2026-01-28 14:40:21,289 | INFO | decoder input length: 14
2026-01-28 14:40:21,289 | INFO | max output length: 14
2026-01-28 14:40:21,289 | INFO | min output length: 1
2026-01-28 14:40:22,478 | INFO | adding <eos> in the last position in the loop
2026-01-28 14:40:22,489 | INFO | end detected at 13
2026-01-28 14:40:22,490 | INFO |  -2.80 * 0.5 =  -1.40 for decoder
2026-01-28 14:40:22,491 | INFO |  -3.51 * 0.5 =  -1.75 for ctc
2026-01-28 14:40:22,491 | INFO | total log probability: -3.16
2026-01-28 14:40:22,491 | INFO | normalized log probability: -0.45
2026-01-28 14:40:22,491 | INFO | total number of ended hypotheses: 198
2026-01-28 14:40:22,491 | INFO | best hypo: ▁et▁solide

2026-01-28 14:40:22,493 | INFO | speech length: 23840
2026-01-28 14:40:22,532 | INFO | decoder input length: 36
2026-01-28 14:40:22,532 | INFO | max output length: 36
2026-01-28 14:40:22,532 | INFO | min output length: 3
2026-01-28 14:40:24,426 | INFO | end detected at 21
2026-01-28 14:40:24,427 | INFO |  -1.30 * 0.5 =  -0.65 for decoder
2026-01-28 14:40:24,428 | INFO |  -0.21 * 0.5 =  -0.11 for ctc
2026-01-28 14:40:24,428 | INFO | total log probability: -0.75
2026-01-28 14:40:24,428 | INFO | normalized log probability: -0.04
2026-01-28 14:40:24,428 | INFO | total number of ended hypotheses: 146
2026-01-28 14:40:24,428 | INFO | best hypo: ▁on▁n'a▁pas▁peur▁de▁l'effort

2026-01-28 14:40:24,430 | INFO | speech length: 49760
2026-01-28 14:40:24,462 | INFO | decoder input length: 77
2026-01-28 14:40:24,462 | INFO | max output length: 77
2026-01-28 14:40:24,462 | INFO | min output length: 7
2026-01-28 14:40:26,325 | INFO | end detected at 20
2026-01-28 14:40:26,327 | INFO |  -1.42 * 0.5 =  -0.71 for decoder
2026-01-28 14:40:26,327 | INFO |  -1.63 * 0.5 =  -0.82 for ctc
2026-01-28 14:40:26,327 | INFO | total log probability: -1.53
2026-01-28 14:40:26,327 | INFO | normalized log probability: -0.10
2026-01-28 14:40:26,327 | INFO | total number of ended hypotheses: 154
2026-01-28 14:40:26,327 | INFO | best hypo: ▁et▁les▁curés▁qui▁sont▁en▁bourse

2026-01-28 14:40:26,330 | INFO | speech length: 50880
2026-01-28 14:40:26,373 | INFO | decoder input length: 79
2026-01-28 14:40:26,373 | INFO | max output length: 79
2026-01-28 14:40:26,373 | INFO | min output length: 7
2026-01-28 14:40:28,955 | INFO | end detected at 27
2026-01-28 14:40:28,957 | INFO |  -1.66 * 0.5 =  -0.83 for decoder
2026-01-28 14:40:28,957 | INFO |  -2.26 * 0.5 =  -1.13 for ctc
2026-01-28 14:40:28,957 | INFO | total log probability: -1.96
2026-01-28 14:40:28,957 | INFO | normalized log probability: -0.09
2026-01-28 14:40:28,957 | INFO | total number of ended hypotheses: 167
2026-01-28 14:40:28,958 | INFO | best hypo: ▁centre▁cet▁attachement▁de▁l'art▁de▁la▁population

2026-01-28 14:40:28,960 | INFO | speech length: 114560
2026-01-28 14:40:29,006 | INFO | decoder input length: 178
2026-01-28 14:40:29,006 | INFO | max output length: 178
2026-01-28 14:40:29,006 | INFO | min output length: 17
2026-01-28 14:40:33,070 | INFO | end detected at 38
2026-01-28 14:40:33,071 | INFO |  -4.26 * 0.5 =  -2.13 for decoder
2026-01-28 14:40:33,071 | INFO |  -1.90 * 0.5 =  -0.95 for ctc
2026-01-28 14:40:33,071 | INFO | total log probability: -3.08
2026-01-28 14:40:33,071 | INFO | normalized log probability: -0.09
2026-01-28 14:40:33,071 | INFO | total number of ended hypotheses: 148
2026-01-28 14:40:33,072 | INFO | best hypo: ▁mais▁on▁sent▁aussi▁que▁le▁moseron▁n'accorde▁sa▁fidélité▁que▁après▁avoir▁bien▁connu

2026-01-28 14:40:33,074 | INFO | speech length: 79520
2026-01-28 14:40:33,123 | INFO | decoder input length: 123
2026-01-28 14:40:33,123 | INFO | max output length: 123
2026-01-28 14:40:33,123 | INFO | min output length: 12
2026-01-28 14:40:36,750 | INFO | end detected at 34
2026-01-28 14:40:36,752 | INFO |  -4.17 * 0.5 =  -2.08 for decoder
2026-01-28 14:40:36,752 | INFO |  -0.77 * 0.5 =  -0.38 for ctc
2026-01-28 14:40:36,752 | INFO | total log probability: -2.47
2026-01-28 14:40:36,752 | INFO | normalized log probability: -0.09
2026-01-28 14:40:36,752 | INFO | total number of ended hypotheses: 164
2026-01-28 14:40:36,752 | INFO | best hypo: ▁il▁faut▁avoir▁fait▁ces▁preuves▁soi▁même▁d'attachement▁au▁pays

2026-01-28 14:40:36,755 | INFO | speech length: 88000
2026-01-28 14:40:36,797 | INFO | decoder input length: 137
2026-01-28 14:40:36,797 | INFO | max output length: 137
2026-01-28 14:40:36,797 | INFO | min output length: 13
2026-01-28 14:40:42,295 | INFO | end detected at 54
2026-01-28 14:40:42,297 | INFO | -12.42 * 0.5 =  -6.21 for decoder
2026-01-28 14:40:42,297 | INFO |  -3.66 * 0.5 =  -1.83 for ctc
2026-01-28 14:40:42,297 | INFO | total log probability: -8.04
2026-01-28 14:40:42,297 | INFO | normalized log probability: -0.17
2026-01-28 14:40:42,297 | INFO | total number of ended hypotheses: 166
2026-01-28 14:40:42,298 | INFO | best hypo: ▁le▁beauson▁s'est▁observé▁longtemps▁mais▁aux▁prêtres▁de▁beauce▁qui▁sont▁les▁curés▁de▁campagne

2026-01-28 14:40:42,300 | INFO | speech length: 66240
2026-01-28 14:40:42,343 | INFO | decoder input length: 103
2026-01-28 14:40:42,343 | INFO | max output length: 103
2026-01-28 14:40:42,343 | INFO | min output length: 10
2026-01-28 14:40:44,850 | INFO | end detected at 25
2026-01-28 14:40:44,853 | INFO |  -2.83 * 0.5 =  -1.42 for decoder
2026-01-28 14:40:44,853 | INFO |  -6.52 * 0.5 =  -3.26 for ctc
2026-01-28 14:40:44,853 | INFO | total log probability: -4.67
2026-01-28 14:40:44,853 | INFO | normalized log probability: -0.31
2026-01-28 14:40:44,853 | INFO | total number of ended hypotheses: 218
2026-01-28 14:40:44,853 | INFO | best hypo: ▁ont▁en▁général▁avec▁leur▁population▁sept

2026-01-28 14:40:44,856 | INFO | speech length: 74560
2026-01-28 14:40:44,893 | INFO | decoder input length: 116
2026-01-28 14:40:44,893 | INFO | max output length: 116
2026-01-28 14:40:44,894 | INFO | min output length: 11
2026-01-28 14:40:48,308 | INFO | end detected at 33
2026-01-28 14:40:48,310 | INFO |  -3.68 * 0.5 =  -1.84 for decoder
2026-01-28 14:40:48,310 | INFO |  -6.68 * 0.5 =  -3.34 for ctc
2026-01-28 14:40:48,310 | INFO | total log probability: -5.18
2026-01-28 14:40:48,310 | INFO | normalized log probability: -0.19
2026-01-28 14:40:48,310 | INFO | total number of ended hypotheses: 167
2026-01-28 14:40:48,311 | INFO | best hypo: ▁ce▁lien▁qui▁voulait▁qui▁n'est▁pas▁superficiel▁mais▁en▁profondeur

2026-01-28 14:40:48,312 | INFO | speech length: 45600
2026-01-28 14:40:48,345 | INFO | decoder input length: 70
2026-01-28 14:40:48,345 | INFO | max output length: 70
2026-01-28 14:40:48,345 | INFO | min output length: 7
2026-01-28 14:40:51,035 | INFO | end detected at 29
2026-01-28 14:40:51,036 | INFO |  -2.77 * 0.5 =  -1.38 for decoder
2026-01-28 14:40:51,037 | INFO |  -0.49 * 0.5 =  -0.24 for ctc
2026-01-28 14:40:51,037 | INFO | total log probability: -1.63
2026-01-28 14:40:51,037 | INFO | normalized log probability: -0.07
2026-01-28 14:40:51,037 | INFO | total number of ended hypotheses: 151
2026-01-28 14:40:51,037 | INFO | best hypo: ▁il▁correspond▁bien▁à▁la▁fidélité▁d'une▁d'une▁race

2026-01-28 14:40:51,039 | INFO | speech length: 64800
2026-01-28 14:40:51,078 | INFO | decoder input length: 100
2026-01-28 14:40:51,078 | INFO | max output length: 100
2026-01-28 14:40:51,078 | INFO | min output length: 10
2026-01-28 14:40:53,665 | INFO | end detected at 29
2026-01-28 14:40:53,666 | INFO |  -2.00 * 0.5 =  -1.00 for decoder
2026-01-28 14:40:53,666 | INFO |  -0.46 * 0.5 =  -0.23 for ctc
2026-01-28 14:40:53,666 | INFO | total log probability: -1.23
2026-01-28 14:40:53,666 | INFO | normalized log probability: -0.05
2026-01-28 14:40:53,666 | INFO | total number of ended hypotheses: 146
2026-01-28 14:40:53,666 | INFO | best hypo: ▁et▁puis▁nous▁avons▁une▁autre▁région▁qui▁est▁le▁gâtinais

2026-01-28 14:40:53,668 | INFO | speech length: 161760
2026-01-28 14:40:53,699 | INFO | decoder input length: 252
2026-01-28 14:40:53,699 | INFO | max output length: 252
2026-01-28 14:40:53,699 | INFO | min output length: 25
2026-01-28 14:41:00,272 | INFO | end detected at 57
2026-01-28 14:41:00,274 | INFO |  -7.55 * 0.5 =  -3.77 for decoder
2026-01-28 14:41:00,274 | INFO | -10.80 * 0.5 =  -5.40 for ctc
2026-01-28 14:41:00,274 | INFO | total log probability: -9.18
2026-01-28 14:41:00,274 | INFO | normalized log probability: -0.18
2026-01-28 14:41:00,274 | INFO | total number of ended hypotheses: 171
2026-01-28 14:41:00,275 | INFO | best hypo: ▁le▁catinel▁luis▁est▁une▁région▁beaucoup▁plus▁boisée▁d'une▁part▁et▁puis▁beaucoup▁plus▁se▁vouer▁à▁l'élevage

2026-01-28 14:41:00,277 | INFO | speech length: 37280
2026-01-28 14:41:00,309 | INFO | decoder input length: 57
2026-01-28 14:41:00,310 | INFO | max output length: 57
2026-01-28 14:41:00,310 | INFO | min output length: 5
2026-01-28 14:41:02,232 | INFO | end detected at 21
2026-01-28 14:41:02,233 | INFO |  -1.50 * 0.5 =  -0.75 for decoder
2026-01-28 14:41:02,233 | INFO |  -0.49 * 0.5 =  -0.25 for ctc
2026-01-28 14:41:02,233 | INFO | total log probability: -1.00
2026-01-28 14:41:02,233 | INFO | normalized log probability: -0.06
2026-01-28 14:41:02,233 | INFO | total number of ended hypotheses: 167
2026-01-28 14:41:02,234 | INFO | best hypo: ▁et▁il▁y▁a▁aussi▁de▁la▁très▁bonne▁culture

2026-01-28 14:41:02,236 | INFO | speech length: 22400
2026-01-28 14:41:02,271 | INFO | decoder input length: 34
2026-01-28 14:41:02,271 | INFO | max output length: 34
2026-01-28 14:41:02,271 | INFO | min output length: 3
2026-01-28 14:41:03,288 | INFO | end detected at 11
2026-01-28 14:41:03,292 | INFO |  -4.86 * 0.5 =  -2.43 for decoder
2026-01-28 14:41:03,292 | INFO |  -8.80 * 0.5 =  -4.40 for ctc
2026-01-28 14:41:03,292 | INFO | total log probability: -6.83
2026-01-28 14:41:03,292 | INFO | normalized log probability: -1.37
2026-01-28 14:41:03,292 | INFO | total number of ended hypotheses: 179
2026-01-28 14:41:03,292 | INFO | best hypo: ▁mais▁eux

2026-01-28 14:41:03,295 | INFO | speech length: 81600
2026-01-28 14:41:03,333 | INFO | decoder input length: 127
2026-01-28 14:41:03,333 | INFO | max output length: 127
2026-01-28 14:41:03,333 | INFO | min output length: 12
2026-01-28 14:41:06,227 | INFO | end detected at 28
2026-01-28 14:41:06,228 | INFO |  -2.65 * 0.5 =  -1.32 for decoder
2026-01-28 14:41:06,228 | INFO |  -2.19 * 0.5 =  -1.09 for ctc
2026-01-28 14:41:06,228 | INFO | total log probability: -2.42
2026-01-28 14:41:06,228 | INFO | normalized log probability: -0.10
2026-01-28 14:41:06,228 | INFO | total number of ended hypotheses: 156
2026-01-28 14:41:06,228 | INFO | best hypo: ▁le▁gâtinet▁est▁très▁accueillant▁il▁est▁très▁bienveillant

2026-01-28 14:41:06,230 | INFO | speech length: 16960
2026-01-28 14:41:06,263 | INFO | decoder input length: 26
2026-01-28 14:41:06,263 | INFO | max output length: 26
2026-01-28 14:41:06,263 | INFO | min output length: 2
2026-01-28 14:41:07,085 | INFO | end detected at 9
2026-01-28 14:41:07,085 | INFO |  -1.04 * 0.5 =  -0.52 for decoder
2026-01-28 14:41:07,086 | INFO |  -2.17 * 0.5 =  -1.09 for ctc
2026-01-28 14:41:07,086 | INFO | total log probability: -1.61
2026-01-28 14:41:07,086 | INFO | normalized log probability: -0.32
2026-01-28 14:41:07,086 | INFO | total number of ended hypotheses: 150
2026-01-28 14:41:07,086 | INFO | best hypo: ▁il▁lit

2026-01-28 14:41:07,087 | INFO | speech length: 16960
2026-01-28 14:41:07,124 | INFO | decoder input length: 26
2026-01-28 14:41:07,124 | INFO | max output length: 26
2026-01-28 14:41:07,124 | INFO | min output length: 2
2026-01-28 14:41:08,080 | INFO | end detected at 11
2026-01-28 14:41:08,081 | INFO |  -0.43 * 0.5 =  -0.21 for decoder
2026-01-28 14:41:08,081 | INFO |  -0.03 * 0.5 =  -0.02 for ctc
2026-01-28 14:41:08,081 | INFO | total log probability: -0.23
2026-01-28 14:41:08,081 | INFO | normalized log probability: -0.03
2026-01-28 14:41:08,081 | INFO | total number of ended hypotheses: 135
2026-01-28 14:41:08,081 | INFO | best hypo: ▁très▁aimable

2026-01-28 14:41:08,083 | INFO | speech length: 21280
2026-01-28 14:41:08,117 | INFO | decoder input length: 32
2026-01-28 14:41:08,117 | INFO | max output length: 32
2026-01-28 14:41:08,117 | INFO | min output length: 3
2026-01-28 14:41:09,270 | INFO | end detected at 13
2026-01-28 14:41:09,273 | INFO |  -3.94 * 0.5 =  -1.97 for decoder
2026-01-28 14:41:09,273 | INFO |  -5.23 * 0.5 =  -2.61 for ctc
2026-01-28 14:41:09,273 | INFO | total log probability: -4.58
2026-01-28 14:41:09,273 | INFO | normalized log probability: -0.92
2026-01-28 14:41:09,273 | INFO | total number of ended hypotheses: 204
2026-01-28 14:41:09,273 | INFO | best hypo: ▁mais▁or

2026-01-28 14:41:09,275 | INFO | speech length: 118720
2026-01-28 14:41:09,311 | INFO | decoder input length: 185
2026-01-28 14:41:09,312 | INFO | max output length: 185
2026-01-28 14:41:09,312 | INFO | min output length: 18
2026-01-28 14:41:14,235 | INFO | end detected at 47
2026-01-28 14:41:14,236 | INFO |  -3.55 * 0.5 =  -1.78 for decoder
2026-01-28 14:41:14,236 | INFO |  -0.74 * 0.5 =  -0.37 for ctc
2026-01-28 14:41:14,236 | INFO | total log probability: -2.15
2026-01-28 14:41:14,236 | INFO | normalized log probability: -0.05
2026-01-28 14:41:14,236 | INFO | total number of ended hypotheses: 151
2026-01-28 14:41:14,236 | INFO | best hypo: ▁on▁ne▁trouve▁pas▁si▁vous▁voulez▁la▁même▁profondeur▁d'attachement▁à▁la▁foi▁chrétienne▁dans▁le▁gâtinais

2026-01-28 14:41:14,238 | INFO | speech length: 39040
2026-01-28 14:41:14,273 | INFO | decoder input length: 60
2026-01-28 14:41:14,273 | INFO | max output length: 60
2026-01-28 14:41:14,273 | INFO | min output length: 6
2026-01-28 14:41:17,022 | INFO | end detected at 30
2026-01-28 14:41:17,023 | INFO |  -2.19 * 0.5 =  -1.10 for decoder
2026-01-28 14:41:17,023 | INFO |  -1.68 * 0.5 =  -0.84 for ctc
2026-01-28 14:41:17,023 | INFO | total log probability: -1.94
2026-01-28 14:41:17,023 | INFO | normalized log probability: -0.08
2026-01-28 14:41:17,023 | INFO | total number of ended hypotheses: 157
2026-01-28 14:41:17,024 | INFO | best hypo: ▁et▁plus▁c'est▁un▁pays▁dans▁lequel▁on▁aime▁la▁bonne▁vie

2026-01-28 14:41:17,026 | INFO | speech length: 23520
2026-01-28 14:41:17,062 | INFO | decoder input length: 36
2026-01-28 14:41:17,062 | INFO | max output length: 36
2026-01-28 14:41:17,062 | INFO | min output length: 3
2026-01-28 14:41:18,677 | INFO | end detected at 18
2026-01-28 14:41:18,677 | INFO |  -0.87 * 0.5 =  -0.44 for decoder
2026-01-28 14:41:18,677 | INFO |  -0.03 * 0.5 =  -0.01 for ctc
2026-01-28 14:41:18,678 | INFO | total log probability: -0.45
2026-01-28 14:41:18,678 | INFO | normalized log probability: -0.03
2026-01-28 14:41:18,678 | INFO | total number of ended hypotheses: 138
2026-01-28 14:41:18,678 | INFO | best hypo: ▁c'est▁un▁pays▁très▁joyeux

2026-01-28 14:41:18,679 | INFO | speech length: 19040
2026-01-28 14:41:18,717 | INFO | decoder input length: 29
2026-01-28 14:41:18,717 | INFO | max output length: 29
2026-01-28 14:41:18,717 | INFO | min output length: 2
2026-01-28 14:41:20,104 | INFO | end detected at 15
2026-01-28 14:41:20,107 | INFO |  -1.48 * 0.5 =  -0.74 for decoder
2026-01-28 14:41:20,107 | INFO |  -2.84 * 0.5 =  -1.42 for ctc
2026-01-28 14:41:20,107 | INFO | total log probability: -2.16
2026-01-28 14:41:20,107 | INFO | normalized log probability: -0.24
2026-01-28 14:41:20,107 | INFO | total number of ended hypotheses: 182
2026-01-28 14:41:20,107 | INFO | best hypo: ▁les▁fêtes▁qui▁sont

2026-01-28 14:41:20,109 | INFO | speech length: 12800
2026-01-28 14:41:20,143 | INFO | decoder input length: 19
2026-01-28 14:41:20,143 | INFO | max output length: 19
2026-01-28 14:41:20,143 | INFO | min output length: 1
2026-01-28 14:41:21,208 | INFO | end detected at 12
2026-01-28 14:41:21,210 | INFO |  -0.95 * 0.5 =  -0.47 for decoder
2026-01-28 14:41:21,210 | INFO |  -1.82 * 0.5 =  -0.91 for ctc
2026-01-28 14:41:21,210 | INFO | total log probability: -1.38
2026-01-28 14:41:21,210 | INFO | normalized log probability: -0.17
2026-01-28 14:41:21,210 | INFO | total number of ended hypotheses: 137
2026-01-28 14:41:21,211 | INFO | best hypo: ▁une▁nombreuse

2026-01-28 14:41:21,212 | INFO | speech length: 33920
2026-01-28 14:41:21,251 | INFO | decoder input length: 52
2026-01-28 14:41:21,251 | INFO | max output length: 52
2026-01-28 14:41:21,251 | INFO | min output length: 5
2026-01-28 14:41:22,683 | INFO | end detected at 16
2026-01-28 14:41:22,684 | INFO |  -1.07 * 0.5 =  -0.53 for decoder
2026-01-28 14:41:22,684 | INFO |  -0.24 * 0.5 =  -0.12 for ctc
2026-01-28 14:41:22,684 | INFO | total log probability: -0.66
2026-01-28 14:41:22,684 | INFO | normalized log probability: -0.05
2026-01-28 14:41:22,684 | INFO | total number of ended hypotheses: 142
2026-01-28 14:41:22,684 | INFO | best hypo: ▁je▁pourrais▁vous▁dire▁que▁certainement

2026-01-28 14:41:22,686 | INFO | speech length: 91520
2026-01-28 14:41:22,722 | INFO | decoder input length: 142
2026-01-28 14:41:22,723 | INFO | max output length: 142
2026-01-28 14:41:22,723 | INFO | min output length: 14
2026-01-28 14:41:26,767 | INFO | end detected at 42
2026-01-28 14:41:26,768 | INFO |  -9.61 * 0.5 =  -4.81 for decoder
2026-01-28 14:41:26,768 | INFO |  -7.43 * 0.5 =  -3.71 for ctc
2026-01-28 14:41:26,768 | INFO | total log probability: -8.52
2026-01-28 14:41:26,768 | INFO | normalized log probability: -0.22
2026-01-28 14:41:26,768 | INFO | total number of ended hypotheses: 162
2026-01-28 14:41:26,769 | INFO | best hypo: ▁la▁morale▁chrétienne▁ne▁paraît▁plus▁difficile▁à▁nos▁gens▁du▁gâtinec▁que▁dans▁la▁beauche

2026-01-28 14:41:26,771 | INFO | speech length: 50560
2026-01-28 14:41:26,809 | INFO | decoder input length: 78
2026-01-28 14:41:26,809 | INFO | max output length: 78
2026-01-28 14:41:26,809 | INFO | min output length: 7
2026-01-28 14:41:28,790 | INFO | end detected at 21
2026-01-28 14:41:28,792 | INFO |  -1.25 * 0.5 =  -0.62 for decoder
2026-01-28 14:41:28,792 | INFO |  -0.23 * 0.5 =  -0.12 for ctc
2026-01-28 14:41:28,792 | INFO | total log probability: -0.74
2026-01-28 14:41:28,792 | INFO | normalized log probability: -0.04
2026-01-28 14:41:28,792 | INFO | total number of ended hypotheses: 148
2026-01-28 14:41:28,793 | INFO | best hypo: ▁c'est▁aussi▁une▁des▁raisons▁pour▁lesquelles

2026-01-28 14:41:28,795 | INFO | speech length: 147520
2026-01-28 14:41:28,832 | INFO | decoder input length: 230
2026-01-28 14:41:28,832 | INFO | max output length: 230
2026-01-28 14:41:28,832 | INFO | min output length: 23
2026-01-28 14:41:35,060 | INFO | end detected at 54
2026-01-28 14:41:35,062 | INFO |  -4.53 * 0.5 =  -2.27 for decoder
2026-01-28 14:41:35,062 | INFO |  -7.94 * 0.5 =  -3.97 for ctc
2026-01-28 14:41:35,062 | INFO | total log probability: -6.24
2026-01-28 14:41:35,062 | INFO | normalized log probability: -0.13
2026-01-28 14:41:35,063 | INFO | total number of ended hypotheses: 187
2026-01-28 14:41:35,063 | INFO | best hypo: ▁l'attachement▁est▁peut▁être▁moins▁en▁profondeur▁mais▁il▁gagne▁tellement▁en▁spontanéité▁en▁amabilité▁en▁gentillesse

2026-01-28 14:41:35,065 | INFO | speech length: 22560
2026-01-28 14:41:35,098 | INFO | decoder input length: 34
2026-01-28 14:41:35,098 | INFO | max output length: 34
2026-01-28 14:41:35,098 | INFO | min output length: 3
2026-01-28 14:41:36,226 | INFO | end detected at 13
2026-01-28 14:41:36,228 | INFO |  -0.62 * 0.5 =  -0.31 for decoder
2026-01-28 14:41:36,228 | INFO |  -1.03 * 0.5 =  -0.52 for ctc
2026-01-28 14:41:36,228 | INFO | total log probability: -0.83
2026-01-28 14:41:36,228 | INFO | normalized log probability: -0.10
2026-01-28 14:41:36,228 | INFO | total number of ended hypotheses: 168
2026-01-28 14:41:36,229 | INFO | best hypo: ▁qu'il▁veut

2026-01-28 14:41:36,231 | INFO | speech length: 132320
2026-01-28 14:41:36,282 | INFO | decoder input length: 206
2026-01-28 14:41:36,282 | INFO | max output length: 206
2026-01-28 14:41:36,283 | INFO | min output length: 20
2026-01-28 14:41:43,466 | INFO | end detected at 66
2026-01-28 14:41:43,467 | INFO |  -5.44 * 0.5 =  -2.72 for decoder
2026-01-28 14:41:43,467 | INFO |  -4.01 * 0.5 =  -2.01 for ctc
2026-01-28 14:41:43,467 | INFO | total log probability: -4.73
2026-01-28 14:41:43,468 | INFO | normalized log probability: -0.08
2026-01-28 14:41:43,468 | INFO | total number of ended hypotheses: 178
2026-01-28 14:41:43,468 | INFO | best hypo: ▁ils▁ont▁peut▁être▁davantage▁si▁vous▁voulez▁de▁l'évangile▁là▁la▁caractéristique▁de▁la▁sympathie▁de▁l'accueil▁et▁de▁la▁charité

2026-01-28 14:41:43,471 | INFO | speech length: 124640
2026-01-28 14:41:43,541 | INFO | decoder input length: 194
2026-01-28 14:41:43,541 | INFO | max output length: 194
2026-01-28 14:41:43,541 | INFO | min output length: 19
2026-01-28 14:41:48,516 | INFO | end detected at 46
2026-01-28 14:41:48,517 | INFO | -10.77 * 0.5 =  -5.39 for decoder
2026-01-28 14:41:48,517 | INFO |  -7.38 * 0.5 =  -3.69 for ctc
2026-01-28 14:41:48,517 | INFO | total log probability: -9.08
2026-01-28 14:41:48,517 | INFO | normalized log probability: -0.22
2026-01-28 14:41:48,517 | INFO | total number of ended hypotheses: 168
2026-01-28 14:41:48,518 | INFO | best hypo: ▁mais▁que▁il▁faudrait▁peut▁être▁moi▁moins▁chercher▁du▁côté▁du▁renoncement▁de▁de▁l'austérité

2026-01-28 14:41:48,520 | INFO | speech length: 47200
2026-01-28 14:41:48,592 | INFO | decoder input length: 73
2026-01-28 14:41:48,592 | INFO | max output length: 73
2026-01-28 14:41:48,593 | INFO | min output length: 7
2026-01-28 14:41:50,692 | INFO | end detected at 23
2026-01-28 14:41:50,694 | INFO |  -2.17 * 0.5 =  -1.08 for decoder
2026-01-28 14:41:50,694 | INFO |  -2.03 * 0.5 =  -1.01 for ctc
2026-01-28 14:41:50,694 | INFO | total log probability: -2.10
2026-01-28 14:41:50,694 | INFO | normalized log probability: -0.12
2026-01-28 14:41:50,694 | INFO | total number of ended hypotheses: 177
2026-01-28 14:41:50,694 | INFO | best hypo: ▁est▁ce▁qu'il▁y▁a▁une▁différence▁nette

2026-01-28 14:41:50,696 | INFO | speech length: 9760
2026-01-28 14:41:50,728 | INFO | decoder input length: 14
2026-01-28 14:41:50,728 | INFO | max output length: 14
2026-01-28 14:41:50,728 | INFO | min output length: 1
2026-01-28 14:41:51,497 | INFO | end detected at 9
2026-01-28 14:41:51,498 | INFO |  -0.54 * 0.5 =  -0.27 for decoder
2026-01-28 14:41:51,498 | INFO |  -1.23 * 0.5 =  -0.61 for ctc
2026-01-28 14:41:51,498 | INFO | total log probability: -0.88
2026-01-28 14:41:51,498 | INFO | normalized log probability: -0.18
2026-01-28 14:41:51,498 | INFO | total number of ended hypotheses: 158
2026-01-28 14:41:51,498 | INFO | best hypo: ▁parmir

2026-01-28 14:41:51,500 | INFO | speech length: 25440
2026-01-28 14:41:51,546 | INFO | decoder input length: 39
2026-01-28 14:41:51,546 | INFO | max output length: 39
2026-01-28 14:41:51,546 | INFO | min output length: 3
2026-01-28 14:41:53,162 | INFO | end detected at 18
2026-01-28 14:41:53,164 | INFO |  -4.66 * 0.5 =  -2.33 for decoder
2026-01-28 14:41:53,164 | INFO |  -6.91 * 0.5 =  -3.45 for ctc
2026-01-28 14:41:53,164 | INFO | total log probability: -5.78
2026-01-28 14:41:53,165 | INFO | normalized log probability: -0.48
2026-01-28 14:41:53,165 | INFO | total number of ended hypotheses: 189
2026-01-28 14:41:53,165 | INFO | best hypo: ▁il▁est▁en▁couche▁sociale

2026-01-28 14:41:53,167 | INFO | speech length: 18400
2026-01-28 14:41:53,207 | INFO | decoder input length: 28
2026-01-28 14:41:53,208 | INFO | max output length: 28
2026-01-28 14:41:53,208 | INFO | min output length: 2
2026-01-28 14:41:54,390 | INFO | end detected at 13
2026-01-28 14:41:54,392 | INFO |  -0.88 * 0.5 =  -0.44 for decoder
2026-01-28 14:41:54,392 | INFO |  -2.48 * 0.5 =  -1.24 for ctc
2026-01-28 14:41:54,392 | INFO | total log probability: -1.68
2026-01-28 14:41:54,392 | INFO | normalized log probability: -0.24
2026-01-28 14:41:54,392 | INFO | total number of ended hypotheses: 157
2026-01-28 14:41:54,392 | INFO | best hypo: ▁vous▁arrivez

2026-01-28 14:41:54,394 | INFO | speech length: 83840
2026-01-28 14:41:54,440 | INFO | decoder input length: 130
2026-01-28 14:41:54,440 | INFO | max output length: 130
2026-01-28 14:41:54,440 | INFO | min output length: 13
2026-01-28 14:41:59,393 | INFO | end detected at 50
2026-01-28 14:41:59,395 | INFO |  -7.43 * 0.5 =  -3.71 for decoder
2026-01-28 14:41:59,395 | INFO |  -8.18 * 0.5 =  -4.09 for ctc
2026-01-28 14:41:59,395 | INFO | total log probability: -7.81
2026-01-28 14:41:59,395 | INFO | normalized log probability: -0.19
2026-01-28 14:41:59,395 | INFO | total number of ended hypotheses: 188
2026-01-28 14:41:59,396 | INFO | best hypo: ▁il▁y▁a▁des▁couches▁sociales▁qui▁plus▁que▁d'autres▁disons▁dans▁orléans▁même▁ou▁évidemment▁là

2026-01-28 14:41:59,399 | INFO | speech length: 141600
2026-01-28 14:41:59,448 | INFO | decoder input length: 220
2026-01-28 14:41:59,448 | INFO | max output length: 220
2026-01-28 14:41:59,448 | INFO | min output length: 22
2026-01-28 14:42:04,899 | INFO | end detected at 88
2026-01-28 14:42:04,901 | INFO | -22.10 * 0.5 = -11.05 for decoder
2026-01-28 14:42:04,901 | INFO | -11.63 * 0.5 =  -5.81 for ctc
2026-01-28 14:42:04,901 | INFO | total log probability: -16.86
2026-01-28 14:42:04,901 | INFO | normalized log probability: -0.21
2026-01-28 14:42:04,901 | INFO | total number of ended hypotheses: 183
2026-01-28 14:42:04,902 | INFO | best hypo: ▁oui▁alors▁là▁on▁aborde▁l'autre▁point▁de▁vue▁qui▁n'est▁plus▁le▁point▁de▁vue▁géographique▁bec▁et▁le▁point▁de▁vue▁sociologique▁n'est▁pas▁les▁différentes▁couches▁de▁la▁société

2026-01-28 14:42:04,905 | INFO | speech length: 94880
2026-01-28 14:42:04,969 | INFO | decoder input length: 147
2026-01-28 14:42:04,969 | INFO | max output length: 147
2026-01-28 14:42:04,969 | INFO | min output length: 14
2026-01-28 14:42:07,088 | INFO | end detected at 38
2026-01-28 14:42:07,089 | INFO |  -2.64 * 0.5 =  -1.32 for decoder
2026-01-28 14:42:07,089 | INFO |  -3.24 * 0.5 =  -1.62 for ctc
2026-01-28 14:42:07,089 | INFO | total log probability: -2.94
2026-01-28 14:42:07,089 | INFO | normalized log probability: -0.09
2026-01-28 14:42:07,089 | INFO | total number of ended hypotheses: 144
2026-01-28 14:42:07,090 | INFO | best hypo: ▁il▁est▁bien▁vrai▁que▁dans▁les▁campagnes▁le▁prêtre▁est▁à▁même▁de▁côtoyer▁tout▁le▁monde

2026-01-28 14:42:07,092 | INFO | speech length: 284480
2026-01-28 14:42:07,141 | INFO | decoder input length: 444
2026-01-28 14:42:07,142 | INFO | max output length: 444
2026-01-28 14:42:07,142 | INFO | min output length: 44
2026-01-28 14:42:17,235 | INFO | end detected at 109
2026-01-28 14:42:17,236 | INFO | -175.00 * 0.5 = -87.50 for decoder
2026-01-28 14:42:17,236 | INFO | -40.86 * 0.5 = -20.43 for ctc
2026-01-28 14:42:17,236 | INFO | total log probability: -107.93
2026-01-28 14:42:17,236 | INFO | normalized log probability: -1.03
2026-01-28 14:42:17,236 | INFO | total number of ended hypotheses: 135
2026-01-28 14:42:17,238 | INFO | best hypo: ▁il▁y▁a▁beaucoup▁moins▁de▁dices▁de▁distinctions▁entre▁les▁contacts▁que▁peut▁avoir▁le▁prêtre▁et▁puis▁les▁classes▁dirigeants▁et▁le▁prêtre▁et▁le▁monde▁ouvrier▁lui▁prêtre▁connaîtrait▁bien▁tout▁le▁monde▁à▁il▁est▁contact▁avec▁tout▁le▁monde▁dans▁un▁petit▁pays

2026-01-28 14:42:17,240 | INFO | speech length: 104320
2026-01-28 14:42:17,314 | INFO | decoder input length: 162
2026-01-28 14:42:17,314 | INFO | max output length: 162
2026-01-28 14:42:17,314 | INFO | min output length: 16
2026-01-28 14:42:20,180 | INFO | end detected at 51
2026-01-28 14:42:20,181 | INFO |  -5.25 * 0.5 =  -2.63 for decoder
2026-01-28 14:42:20,181 | INFO |  -4.70 * 0.5 =  -2.35 for ctc
2026-01-28 14:42:20,181 | INFO | total log probability: -4.97
2026-01-28 14:42:20,181 | INFO | normalized log probability: -0.11
2026-01-28 14:42:20,181 | INFO | total number of ended hypotheses: 154
2026-01-28 14:42:20,182 | INFO | best hypo: ▁lorsqu'il▁s'agit▁d'une▁grande▁ville▁coborléans▁le▁prêtre▁est▁bien▁plus▁en▁contact▁avec▁ceux▁qui▁viennent▁le▁voir

2026-01-28 14:42:20,184 | INFO | speech length: 38240
2026-01-28 14:42:20,231 | INFO | decoder input length: 59
2026-01-28 14:42:20,231 | INFO | max output length: 59
2026-01-28 14:42:20,231 | INFO | min output length: 5
2026-01-28 14:42:21,365 | INFO | end detected at 26
2026-01-28 14:42:21,366 | INFO |  -1.51 * 0.5 =  -0.75 for decoder
2026-01-28 14:42:21,366 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-28 14:42:21,366 | INFO | total log probability: -0.76
2026-01-28 14:42:21,366 | INFO | normalized log probability: -0.03
2026-01-28 14:42:21,366 | INFO | total number of ended hypotheses: 143
2026-01-28 14:42:21,366 | INFO | best hypo: ▁parce▁qu'il▁ne▁peut▁pas▁être▁en▁contact▁avec▁tout▁le▁monde

2026-01-28 14:42:21,368 | INFO | speech length: 152160
2026-01-28 14:42:21,429 | INFO | decoder input length: 237
2026-01-28 14:42:21,429 | INFO | max output length: 237
2026-01-28 14:42:21,429 | INFO | min output length: 23
2026-01-28 14:42:25,217 | INFO | end detected at 51
2026-01-28 14:42:25,219 | INFO |  -3.68 * 0.5 =  -1.84 for decoder
2026-01-28 14:42:25,220 | INFO |  -5.21 * 0.5 =  -2.60 for ctc
2026-01-28 14:42:25,220 | INFO | total log probability: -4.44
2026-01-28 14:42:25,220 | INFO | normalized log probability: -0.10
2026-01-28 14:42:25,220 | INFO | total number of ended hypotheses: 174
2026-01-28 14:42:25,220 | INFO | best hypo: ▁alors▁ceux▁qui▁viennent▁le▁voir▁ce▁sont▁les▁hommes▁qui▁ont▁une▁formation▁chrétienne▁qui▁font▁partie▁de▁la▁paroisse

2026-01-28 14:42:25,223 | INFO | speech length: 120640
2026-01-28 14:42:25,280 | INFO | decoder input length: 188
2026-01-28 14:42:25,280 | INFO | max output length: 188
2026-01-28 14:42:25,280 | INFO | min output length: 18
2026-01-28 14:42:29,475 | INFO | end detected at 71
2026-01-28 14:42:29,476 | INFO |  -8.48 * 0.5 =  -4.24 for decoder
2026-01-28 14:42:29,477 | INFO |  -1.32 * 0.5 =  -0.66 for ctc
2026-01-28 14:42:29,477 | INFO | total log probability: -4.90
2026-01-28 14:42:29,477 | INFO | normalized log probability: -0.07
2026-01-28 14:42:29,477 | INFO | total number of ended hypotheses: 134
2026-01-28 14:42:29,478 | INFO | best hypo: ▁les▁qui▁viennent▁à▁l'occasion▁de▁l'éducation▁des▁enfants▁à▁l'occasion▁des▁offices▁à▁l'occasion▁des▁cérémonies▁religieuses

2026-01-28 14:42:29,480 | INFO | speech length: 48480
2026-01-28 14:42:29,523 | INFO | decoder input length: 75
2026-01-28 14:42:29,523 | INFO | max output length: 75
2026-01-28 14:42:29,523 | INFO | min output length: 7
2026-01-28 14:42:30,893 | INFO | end detected at 31
2026-01-28 14:42:30,895 | INFO |  -2.19 * 0.5 =  -1.10 for decoder
2026-01-28 14:42:30,895 | INFO |  -0.39 * 0.5 =  -0.19 for ctc
2026-01-28 14:42:30,895 | INFO | total log probability: -1.29
2026-01-28 14:42:30,895 | INFO | normalized log probability: -0.05
2026-01-28 14:42:30,895 | INFO | total number of ended hypotheses: 152
2026-01-28 14:42:30,896 | INFO | best hypo: ▁ils▁viennent▁pour▁les▁baptêmes▁pour▁les▁mariages▁les▁enterrements

2026-01-28 14:42:30,898 | INFO | speech length: 85920
2026-01-28 14:42:30,943 | INFO | decoder input length: 133
2026-01-28 14:42:30,944 | INFO | max output length: 133
2026-01-28 14:42:30,944 | INFO | min output length: 13
2026-01-28 14:42:32,913 | INFO | end detected at 37
2026-01-28 14:42:32,915 | INFO |  -2.73 * 0.5 =  -1.37 for decoder
2026-01-28 14:42:32,915 | INFO |  -3.27 * 0.5 =  -1.64 for ctc
2026-01-28 14:42:32,915 | INFO | total log probability: -3.00
2026-01-28 14:42:32,915 | INFO | normalized log probability: -0.10
2026-01-28 14:42:32,915 | INFO | total number of ended hypotheses: 172
2026-01-28 14:42:32,916 | INFO | best hypo: ▁qui▁viennent▁toujours▁pour▁un▁but▁précis▁qui▁a▁trait▁à▁la▁vie▁chrétienne

2026-01-28 14:42:32,918 | INFO | speech length: 75680
2026-01-28 14:42:32,957 | INFO | decoder input length: 117
2026-01-28 14:42:32,958 | INFO | max output length: 117
2026-01-28 14:42:32,958 | INFO | min output length: 11
2026-01-28 14:42:34,312 | INFO | end detected at 25
2026-01-28 14:42:34,314 | INFO |  -1.51 * 0.5 =  -0.76 for decoder
2026-01-28 14:42:34,315 | INFO |  -0.18 * 0.5 =  -0.09 for ctc
2026-01-28 14:42:34,315 | INFO | total log probability: -0.85
2026-01-28 14:42:34,315 | INFO | normalized log probability: -0.04
2026-01-28 14:42:34,315 | INFO | total number of ended hypotheses: 165
2026-01-28 14:42:34,315 | INFO | best hypo: ▁mais▁les▁contacts▁du▁prêtre▁avec▁les▁couches

2026-01-28 14:42:34,317 | INFO | speech length: 23360
2026-01-28 14:42:34,368 | INFO | decoder input length: 36
2026-01-28 14:42:34,368 | INFO | max output length: 36
2026-01-28 14:42:34,368 | INFO | min output length: 3
2026-01-28 14:42:34,993 | INFO | end detected at 14
2026-01-28 14:42:34,996 | INFO |  -0.87 * 0.5 =  -0.44 for decoder
2026-01-28 14:42:34,996 | INFO |  -0.22 * 0.5 =  -0.11 for ctc
2026-01-28 14:42:34,996 | INFO | total log probability: -0.55
2026-01-28 14:42:34,996 | INFO | normalized log probability: -0.05
2026-01-28 14:42:34,996 | INFO | total number of ended hypotheses: 156
2026-01-28 14:42:34,996 | INFO | best hypo: ▁avec▁les▁problèmes

2026-01-28 14:42:34,998 | INFO | speech length: 25440
2026-01-28 14:42:35,041 | INFO | decoder input length: 39
2026-01-28 14:42:35,041 | INFO | max output length: 39
2026-01-28 14:42:35,041 | INFO | min output length: 3
2026-01-28 14:42:35,926 | INFO | end detected at 20
2026-01-28 14:42:35,927 | INFO |  -3.82 * 0.5 =  -1.91 for decoder
2026-01-28 14:42:35,928 | INFO |  -1.97 * 0.5 =  -0.99 for ctc
2026-01-28 14:42:35,928 | INFO | total log probability: -2.90
2026-01-28 14:42:35,928 | INFO | normalized log probability: -0.21
2026-01-28 14:42:35,928 | INFO | total number of ended hypotheses: 165
2026-01-28 14:42:35,928 | INFO | best hypo: ▁direct▁des▁hommes▁qui▁sont

2026-01-28 14:42:35,931 | INFO | speech length: 36800
2026-01-28 14:42:35,975 | INFO | decoder input length: 57
2026-01-28 14:42:35,975 | INFO | max output length: 57
2026-01-28 14:42:35,975 | INFO | min output length: 5
2026-01-28 14:42:36,938 | INFO | end detected at 22
2026-01-28 14:42:36,940 | INFO |  -1.48 * 0.5 =  -0.74 for decoder
2026-01-28 14:42:36,940 | INFO |  -0.11 * 0.5 =  -0.06 for ctc
2026-01-28 14:42:36,940 | INFO | total log probability: -0.79
2026-01-28 14:42:36,940 | INFO | normalized log probability: -0.04
2026-01-28 14:42:36,940 | INFO | total number of ended hypotheses: 145
2026-01-28 14:42:36,941 | INFO | best hypo: ▁le▁travail▁sous▁tous▁ces▁aspects

2026-01-28 14:42:36,943 | INFO | speech length: 137440
2026-01-28 14:42:36,977 | INFO | decoder input length: 214
2026-01-28 14:42:36,977 | INFO | max output length: 214
2026-01-28 14:42:36,978 | INFO | min output length: 21
2026-01-28 14:42:40,497 | INFO | end detected at 56
2026-01-28 14:42:40,498 | INFO |  -3.86 * 0.5 =  -1.93 for decoder
2026-01-28 14:42:40,499 | INFO |  -0.71 * 0.5 =  -0.36 for ctc
2026-01-28 14:42:40,499 | INFO | total log probability: -2.28
2026-01-28 14:42:40,499 | INFO | normalized log probability: -0.04
2026-01-28 14:42:40,499 | INFO | total number of ended hypotheses: 170
2026-01-28 14:42:40,499 | INFO | best hypo: ▁les▁problèmes▁là▁sont▁certainement▁plus▁difficiles▁à▁aborder▁parce▁que▁le▁nombre▁des▁prêtres▁en▁ville▁est▁quand▁même▁très▁petit▁par▁rapport

2026-01-28 14:42:40,502 | INFO | speech length: 16160
2026-01-28 14:42:40,559 | INFO | decoder input length: 24
2026-01-28 14:42:40,560 | INFO | max output length: 24
2026-01-28 14:42:40,560 | INFO | min output length: 2
2026-01-28 14:42:41,263 | INFO | end detected at 15
2026-01-28 14:42:41,265 | INFO |  -1.55 * 0.5 =  -0.77 for decoder
2026-01-28 14:42:41,265 | INFO |  -2.07 * 0.5 =  -1.04 for ctc
2026-01-28 14:42:41,265 | INFO | total log probability: -1.81
2026-01-28 14:42:41,265 | INFO | normalized log probability: -0.20
2026-01-28 14:42:41,265 | INFO | total number of ended hypotheses: 174
2026-01-28 14:42:41,265 | INFO | best hypo: ▁la▁proportion▁de▁lac

2026-01-28 14:42:41,267 | INFO | speech length: 265280
2026-01-28 14:42:41,344 | INFO | decoder input length: 414
2026-01-28 14:42:41,345 | INFO | max output length: 414
2026-01-28 14:42:41,345 | INFO | min output length: 41
2026-01-28 14:42:50,209 | INFO | end detected at 106
2026-01-28 14:42:50,211 | INFO | -125.72 * 0.5 = -62.86 for decoder
2026-01-28 14:42:50,212 | INFO | -48.16 * 0.5 = -24.08 for ctc
2026-01-28 14:42:50,212 | INFO | total log probability: -86.94
2026-01-28 14:42:50,212 | INFO | normalized log probability: -0.85
2026-01-28 14:42:50,212 | INFO | total number of ended hypotheses: 145
2026-01-28 14:42:50,213 | INFO | best hypo: ▁alors▁vous▁abordez▁là▁tout▁tout▁cet▁aspect▁de▁l'effort▁de▁l'église▁actuelle▁pour▁n'être▁pas▁simplement▁en▁contact▁avecve▁ceux▁qui▁traditionnellement▁sont▁chrétiens▁n'est▁le▁problème▁de▁l'églis▁en▁contact▁avec▁tous▁les▁hommes

2026-01-28 14:42:50,216 | INFO | speech length: 34400
2026-01-28 14:42:50,272 | INFO | decoder input length: 53
2026-01-28 14:42:50,272 | INFO | max output length: 53
2026-01-28 14:42:50,272 | INFO | min output length: 5
2026-01-28 14:42:51,073 | INFO | end detected at 19
2026-01-28 14:42:51,075 | INFO |  -1.73 * 0.5 =  -0.87 for decoder
2026-01-28 14:42:51,075 | INFO |  -2.28 * 0.5 =  -1.14 for ctc
2026-01-28 14:42:51,075 | INFO | total log probability: -2.01
2026-01-28 14:42:51,075 | INFO | normalized log probability: -0.14
2026-01-28 14:42:51,075 | INFO | total number of ended hypotheses: 173
2026-01-28 14:42:51,075 | INFO | best hypo: ▁il▁est▁en▁particulier▁avec▁ceux▁ci

2026-01-28 14:42:51,077 | INFO | speech length: 44000
2026-01-28 14:42:51,128 | INFO | decoder input length: 68
2026-01-28 14:42:51,128 | INFO | max output length: 68
2026-01-28 14:42:51,128 | INFO | min output length: 6
2026-01-28 14:42:52,204 | INFO | end detected at 24
2026-01-28 14:42:52,205 | INFO |  -1.47 * 0.5 =  -0.74 for decoder
2026-01-28 14:42:52,206 | INFO |  -7.83 * 0.5 =  -3.92 for ctc
2026-01-28 14:42:52,206 | INFO | total log probability: -4.65
2026-01-28 14:42:52,206 | INFO | normalized log probability: -0.26
2026-01-28 14:42:52,206 | INFO | total number of ended hypotheses: 181
2026-01-28 14:42:52,206 | INFO | best hypo: ▁ils▁sont▁plus▁éprouvés▁quoi▁dans▁le▁travail

2026-01-28 14:42:52,208 | INFO | speech length: 31680
2026-01-28 14:42:52,260 | INFO | decoder input length: 49
2026-01-28 14:42:52,260 | INFO | max output length: 49
2026-01-28 14:42:52,260 | INFO | min output length: 4
2026-01-28 14:42:53,278 | INFO | end detected at 24
2026-01-28 14:42:53,280 | INFO |  -1.40 * 0.5 =  -0.70 for decoder
2026-01-28 14:42:53,280 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-28 14:42:53,280 | INFO | total log probability: -0.71
2026-01-28 14:42:53,280 | INFO | normalized log probability: -0.04
2026-01-28 14:42:53,280 | INFO | total number of ended hypotheses: 127
2026-01-28 14:42:53,281 | INFO | best hypo: ▁vous▁connaissez▁bien▁les▁problèmes▁actuels

2026-01-28 14:42:53,283 | INFO | speech length: 314240
2026-01-28 14:42:53,333 | INFO | decoder input length: 490
2026-01-28 14:42:53,333 | INFO | max output length: 490
2026-01-28 14:42:53,334 | INFO | min output length: 49
2026-01-28 14:43:10,293 | INFO | end detected at 179
2026-01-28 14:43:10,294 | INFO | -429.16 * 0.5 = -214.58 for decoder
2026-01-28 14:43:10,294 | INFO | -161.34 * 0.5 = -80.67 for ctc
2026-01-28 14:43:10,294 | INFO | total log probability: -295.25
2026-01-28 14:43:10,294 | INFO | normalized log probability: -1.71
2026-01-28 14:43:10,295 | INFO | total number of ended hypotheses: 160
2026-01-28 14:43:10,297 | INFO | best hypo: ▁je▁n'ai▁pas▁l'attention▁en▁ce▁moment▁de▁pouvoir▁vous▁exposer▁quelles▁sont▁les▁difficultés▁de▁contact▁qu'il▁peut▁y▁avoir▁entre▁les▁églises▁que▁ce▁soit▁l'église▁catholique▁d'église▁protestante▁église▁orthodoxe▁ou▁de▁toutes▁les▁églises▁ou▁le▁monde▁des▁travailleursme▁nous▁connaissmes▁dous▁'es▁parf▁dans▁le▁monde▁des▁travailleurspuis▁sa▁siècle▁'▁travaillées▁par▁les▁théme▁théorie▁du▁marxisme

2026-01-28 14:43:10,299 | INFO | speech length: 47680
2026-01-28 14:43:10,353 | INFO | decoder input length: 74
2026-01-28 14:43:10,353 | INFO | max output length: 74
2026-01-28 14:43:10,353 | INFO | min output length: 7
2026-01-28 14:43:12,196 | INFO | end detected at 40
2026-01-28 14:43:12,197 | INFO |  -2.79 * 0.5 =  -1.40 for decoder
2026-01-28 14:43:12,197 | INFO |  -0.14 * 0.5 =  -0.07 for ctc
2026-01-28 14:43:12,197 | INFO | total log probability: -1.47
2026-01-28 14:43:12,197 | INFO | normalized log probability: -0.04
2026-01-28 14:43:12,197 | INFO | total number of ended hypotheses: 147
2026-01-28 14:43:12,198 | INFO | best hypo: ▁nous▁savons▁très▁bien▁quelle▁que▁soit▁l'église▁à▁laquelle▁nous▁appartenons

2026-01-28 14:43:12,209 | INFO | Chunk: 0 | WER=100.000000 | S=0 D=0 I=4
2026-01-28 14:43:12,209 | INFO | Chunk: 1 | WER=120.000000 | S=4 D=0 I=2
2026-01-28 14:43:12,210 | INFO | Chunk: 2 | WER=66.666667 | S=1 D=0 I=3
2026-01-28 14:43:12,210 | INFO | Chunk: 3 | WER=90.000000 | S=9 D=0 I=0
2026-01-28 14:43:12,210 | INFO | Chunk: 4 | WER=85.714286 | S=1 D=2 I=3
2026-01-28 14:43:12,210 | INFO | Chunk: 5 | WER=83.333333 | S=4 D=1 I=0
2026-01-28 14:43:12,211 | INFO | Chunk: 6 | WER=166.666667 | S=6 D=0 I=4
2026-01-28 14:43:12,211 | INFO | Chunk: 7 | WER=142.857143 | S=7 D=0 I=3
2026-01-28 14:43:12,211 | INFO | Chunk: 8 | WER=100.000000 | S=8 D=0 I=2
2026-01-28 14:43:12,212 | INFO | Chunk: 9 | WER=92.307692 | S=3 D=12 I=9
2026-01-28 14:43:12,212 | INFO | Chunk: 10 | WER=100.000000 | S=4 D=4 I=1
2026-01-28 14:43:12,212 | INFO | Chunk: 11 | WER=100.000000 | S=4 D=2 I=0
2026-01-28 14:43:12,213 | INFO | Chunk: 12 | WER=75.000000 | S=3 D=0 I=0
2026-01-28 14:43:12,213 | INFO | Chunk: 13 | WER=100.000000 | S=1 D=1 I=0
2026-01-28 14:43:12,214 | INFO | Chunk: 14 | WER=42.857143 | S=2 D=6 I=4
2026-01-28 14:43:12,214 | INFO | Chunk: 15 | WER=109.090909 | S=9 D=0 I=3
2026-01-28 14:43:12,214 | INFO | Chunk: 16 | WER=100.000000 | S=6 D=2 I=0
2026-01-28 14:43:12,214 | INFO | Chunk: 17 | WER=0.000000 | S=1 D=0 I=1
2026-01-28 14:43:12,215 | INFO | Chunk: 18 | WER=200.000000 | S=4 D=0 I=4
2026-01-28 14:43:12,215 | INFO | Chunk: 19 | WER=100.000000 | S=7 D=2 I=0
2026-01-28 14:43:12,215 | INFO | Chunk: 20 | WER=128.571429 | S=7 D=0 I=2
2026-01-28 14:43:12,215 | INFO | Chunk: 21 | WER=94.117647 | S=1 D=8 I=7
2026-01-28 14:43:12,216 | INFO | Chunk: 22 | WER=137.500000 | S=7 D=0 I=4
2026-01-28 14:43:12,216 | INFO | Chunk: 23 | WER=170.000000 | S=10 D=0 I=7
2026-01-28 14:43:12,216 | INFO | Chunk: 24 | WER=100.000000 | S=7 D=4 I=0
2026-01-28 14:43:12,217 | INFO | Chunk: 25 | WER=150.000000 | S=8 D=0 I=4
2026-01-28 14:43:12,217 | INFO | Chunk: 26 | WER=137.500000 | S=8 D=0 I=3
2026-01-28 14:43:12,217 | INFO | Chunk: 27 | WER=125.000000 | S=7 D=0 I=3
2026-01-28 14:43:12,218 | INFO | Chunk: 28 | WER=77.272727 | S=2 D=8 I=7
2026-01-28 14:43:12,218 | INFO | Chunk: 29 | WER=150.000000 | S=5 D=0 I=4
2026-01-28 14:43:12,218 | INFO | Chunk: 30 | WER=100.000000 | S=2 D=5 I=0
2026-01-28 14:43:12,218 | INFO | Chunk: 31 | WER=225.000000 | S=0 D=2 I=7
2026-01-28 14:43:12,219 | INFO | Chunk: 32 | WER=100.000000 | S=2 D=1 I=0
2026-01-28 14:43:12,219 | INFO | Chunk: 33 | WER=66.666667 | S=1 D=1 I=0
2026-01-28 14:43:12,219 | INFO | Chunk: 34 | WER=100.000000 | S=2 D=0 I=0
2026-01-28 14:43:12,219 | INFO | Chunk: 35 | WER=90.909091 | S=0 D=1 I=9
2026-01-28 14:43:12,220 | INFO | Chunk: 36 | WER=171.428571 | S=6 D=0 I=6
2026-01-28 14:43:12,220 | INFO | Chunk: 37 | WER=66.666667 | S=0 D=2 I=2
2026-01-28 14:43:12,220 | INFO | Chunk: 38 | WER=200.000000 | S=2 D=0 I=2
2026-01-28 14:43:12,220 | INFO | Chunk: 39 | WER=100.000000 | S=2 D=2 I=0
2026-01-28 14:43:12,220 | INFO | Chunk: 40 | WER=300.000000 | S=2 D=0 I=4
2026-01-28 14:43:12,221 | INFO | Chunk: 41 | WER=160.000000 | S=10 D=0 I=6
2026-01-28 14:43:12,221 | INFO | Chunk: 42 | WER=100.000000 | S=8 D=2 I=0
2026-01-28 14:43:12,222 | INFO | Chunk: 43 | WER=54.545455 | S=0 D=8 I=4
2026-01-28 14:43:12,222 | INFO | Chunk: 44 | WER=100.000000 | S=3 D=0 I=0
2026-01-28 14:43:12,222 | INFO | Chunk: 45 | WER=100.000000 | S=0 D=3 I=12
2026-01-28 14:43:12,223 | INFO | Chunk: 46 | WER=100.000000 | S=1 D=11 I=8
2026-01-28 14:43:12,223 | INFO | Chunk: 47 | WER=128.571429 | S=7 D=0 I=2
2026-01-28 14:43:12,223 | INFO | Chunk: 48 | WER=100.000000 | S=1 D=1 I=0
2026-01-28 14:43:12,223 | INFO | Chunk: 49 | WER=500.000000 | S=1 D=0 I=4
2026-01-28 14:43:12,223 | INFO | Chunk: 50 | WER=100.000000 | S=2 D=2 I=0
2026-01-28 14:43:12,224 | INFO | Chunk: 51 | WER=100.000000 | S=16 D=0 I=1
2026-01-28 14:43:12,225 | INFO | Chunk: 52 | WER=51.851852 | S=2 D=2 I=10
2026-01-28 14:43:12,225 | INFO | Chunk: 53 | WER=114.285714 | S=0 D=6 I=10
2026-01-28 14:43:12,227 | INFO | Chunk: 54 | WER=63.461538 | S=5 D=16 I=12
2026-01-28 14:43:12,228 | INFO | Chunk: 55 | WER=126.666667 | S=2 D=5 I=12
2026-01-28 14:43:12,228 | INFO | Chunk: 56 | WER=91.666667 | S=6 D=2 I=3
2026-01-28 14:43:12,229 | INFO | Chunk: 57 | WER=88.000000 | S=0 D=13 I=9
2026-01-28 14:43:12,231 | INFO | Chunk: 58 | WER=85.714286 | S=13 D=2 I=3
2026-01-28 14:43:12,231 | INFO | Chunk: 59 | WER=142.857143 | S=7 D=0 I=3
2026-01-28 14:43:12,232 | INFO | Chunk: 60 | WER=81.250000 | S=9 D=3 I=1
2026-01-28 14:43:12,232 | INFO | Chunk: 61 | WER=100.000000 | S=8 D=1 I=0
2026-01-28 14:43:12,232 | INFO | Chunk: 62 | WER=100.000000 | S=1 D=3 I=1
2026-01-28 14:43:12,233 | INFO | Chunk: 63 | WER=250.000000 | S=2 D=0 I=3
2026-01-28 14:43:12,233 | INFO | Chunk: 64 | WER=150.000000 | S=4 D=0 I=2
2026-01-28 14:43:12,234 | INFO | Chunk: 65 | WER=117.647059 | S=1 D=6 I=13
2026-01-28 14:43:12,234 | INFO | Chunk: 66 | WER=100.000000 | S=4 D=0 I=0
2026-01-28 14:43:12,235 | INFO | Chunk: 67 | WER=60.526316 | S=2 D=9 I=12
2026-01-28 14:43:12,236 | INFO | Chunk: 68 | WER=116.666667 | S=6 D=0 I=1
2026-01-28 14:43:12,236 | INFO | Chunk: 69 | WER=133.333333 | S=6 D=0 I=2
2026-01-28 14:43:12,236 | INFO | Chunk: 70 | WER=120.000000 | S=5 D=0 I=1
2026-01-28 14:43:12,239 | INFO | Chunk: 71 | WER=47.692308 | S=9 D=10 I=12
2026-01-28 14:43:12,239 | INFO | Chunk: 72 | WER=118.181818 | S=11 D=0 I=2
2026-01-28 14:43:12,669 | INFO | File: Rhap-D1001.wav | WER=31.030151 | S=92 D=31 I=124
2026-01-28 14:43:12,669 | INFO | ------------------------------
2026-01-28 14:43:12,670 | INFO | Conf cv Done!
2026-01-28 14:43:12,853 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 14:43:12,876 | INFO | Vocabulary size: 47
2026-01-28 14:43:13,872 | INFO | Gradient checkpoint layers: []
2026-01-28 14:43:14,550 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 14:43:14,571 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 14:43:14,572 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 14:43:14,573 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 14:43:14,576 | INFO | speech length: 94400
2026-01-28 14:43:14,635 | INFO | decoder input length: 147
2026-01-28 14:43:14,636 | INFO | max output length: 147
2026-01-28 14:43:14,636 | INFO | min output length: 14
2026-01-28 14:43:17,890 | INFO | end detected at 65
2026-01-28 14:43:17,892 | INFO |  -4.71 * 0.5 =  -2.35 for decoder
2026-01-28 14:43:17,892 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-28 14:43:17,892 | INFO | total log probability: -2.36
2026-01-28 14:43:17,892 | INFO | normalized log probability: -0.04
2026-01-28 14:43:17,892 | INFO | total number of ended hypotheses: 165
2026-01-28 14:43:17,893 | INFO | best hypo: et<space>chaque<space>région<space>géographique<space>a<space>aussi<space>son<space>comportement<space>euh

2026-01-28 14:43:17,895 | INFO | speech length: 38720
2026-01-28 14:43:17,939 | INFO | decoder input length: 60
2026-01-28 14:43:17,939 | INFO | max output length: 60
2026-01-28 14:43:17,939 | INFO | min output length: 6
2026-01-28 14:43:19,477 | INFO | end detected at 39
2026-01-28 14:43:19,479 | INFO |  -3.27 * 0.5 =  -1.63 for decoder
2026-01-28 14:43:19,479 | INFO |  -8.02 * 0.5 =  -4.01 for ctc
2026-01-28 14:43:19,479 | INFO | total log probability: -5.65
2026-01-28 14:43:19,479 | INFO | normalized log probability: -0.17
2026-01-28 14:43:19,479 | INFO | total number of ended hypotheses: 179
2026-01-28 14:43:19,480 | INFO | best hypo: vis<space>à<space>vis<space>de<space>la<space>foi<space>vis<space>à<space>vis<space>de

2026-01-28 14:43:19,482 | INFO | speech length: 32320
2026-01-28 14:43:19,527 | INFO | decoder input length: 50
2026-01-28 14:43:19,527 | INFO | max output length: 50
2026-01-28 14:43:19,527 | INFO | min output length: 5
2026-01-28 14:43:21,282 | INFO | end detected at 48
2026-01-28 14:43:21,283 | INFO |  -3.49 * 0.5 =  -1.74 for decoder
2026-01-28 14:43:21,283 | INFO |  -0.03 * 0.5 =  -0.01 for ctc
2026-01-28 14:43:21,283 | INFO | total log probability: -1.76
2026-01-28 14:43:21,283 | INFO | normalized log probability: -0.04
2026-01-28 14:43:21,283 | INFO | total number of ended hypotheses: 153
2026-01-28 14:43:21,284 | INFO | best hypo: de<space>l'église<space>vis<space>à<space>vis<space>de<space>la<space>vie<space>chrétienne

2026-01-28 14:43:21,286 | INFO | speech length: 67520
2026-01-28 14:43:21,331 | INFO | decoder input length: 105
2026-01-28 14:43:21,331 | INFO | max output length: 105
2026-01-28 14:43:21,331 | INFO | min output length: 10
2026-01-28 14:43:23,633 | INFO | end detected at 47
2026-01-28 14:43:23,636 | INFO |  -3.46 * 0.5 =  -1.73 for decoder
2026-01-28 14:43:23,636 | INFO |  -2.08 * 0.5 =  -1.04 for ctc
2026-01-28 14:43:23,637 | INFO | total log probability: -2.77
2026-01-28 14:43:23,637 | INFO | normalized log probability: -0.07
2026-01-28 14:43:23,637 | INFO | total number of ended hypotheses: 179
2026-01-28 14:43:23,638 | INFO | best hypo: on<space>dit<space>que<space>il<space>y<space>a<space>la<space>région<space>de<space>bourse

2026-01-28 14:43:23,641 | INFO | speech length: 36320
2026-01-28 14:43:23,739 | INFO | decoder input length: 56
2026-01-28 14:43:23,739 | INFO | max output length: 56
2026-01-28 14:43:23,739 | INFO | min output length: 5
2026-01-28 14:43:25,361 | INFO | end detected at 41
2026-01-28 14:43:25,363 | INFO |  -3.75 * 0.5 =  -1.87 for decoder
2026-01-28 14:43:25,363 | INFO |  -4.76 * 0.5 =  -2.38 for ctc
2026-01-28 14:43:25,363 | INFO | total log probability: -4.26
2026-01-28 14:43:25,363 | INFO | normalized log probability: -0.12
2026-01-28 14:43:25,363 | INFO | total number of ended hypotheses: 178
2026-01-28 14:43:25,364 | INFO | best hypo: dans<space>la<space>région<space>de<space>boss<space>on<space>dit<space>que

2026-01-28 14:43:25,366 | INFO | speech length: 36480
2026-01-28 14:43:25,418 | INFO | decoder input length: 56
2026-01-28 14:43:25,418 | INFO | max output length: 56
2026-01-28 14:43:25,418 | INFO | min output length: 5
2026-01-28 14:43:26,890 | INFO | end detected at 37
2026-01-28 14:43:26,893 | INFO |  -3.22 * 0.5 =  -1.61 for decoder
2026-01-28 14:43:26,893 | INFO |  -1.61 * 0.5 =  -0.81 for ctc
2026-01-28 14:43:26,893 | INFO | total log probability: -2.42
2026-01-28 14:43:26,893 | INFO | normalized log probability: -0.08
2026-01-28 14:43:26,893 | INFO | total number of ended hypotheses: 185
2026-01-28 14:43:26,893 | INFO | best hypo: la<space>culture<space>est<space>une<space>culture<space>euh

2026-01-28 14:43:26,896 | INFO | speech length: 60640
2026-01-28 14:43:26,942 | INFO | decoder input length: 94
2026-01-28 14:43:26,942 | INFO | max output length: 94
2026-01-28 14:43:26,942 | INFO | min output length: 9
2026-01-28 14:43:29,517 | INFO | end detected at 58
2026-01-28 14:43:29,518 | INFO |  -4.23 * 0.5 =  -2.11 for decoder
2026-01-28 14:43:29,518 | INFO |  -0.44 * 0.5 =  -0.22 for ctc
2026-01-28 14:43:29,518 | INFO | total log probability: -2.33
2026-01-28 14:43:29,518 | INFO | normalized log probability: -0.04
2026-01-28 14:43:29,518 | INFO | total number of ended hypotheses: 174
2026-01-28 14:43:29,519 | INFO | best hypo: qui<space>rend<space>bien<space>mais<space>qui<space>demande<space>beaucoup<space>de<space>travail

2026-01-28 14:43:29,522 | INFO | speech length: 36480
2026-01-28 14:43:29,585 | INFO | decoder input length: 56
2026-01-28 14:43:29,585 | INFO | max output length: 56
2026-01-28 14:43:29,585 | INFO | min output length: 5
2026-01-28 14:43:31,529 | INFO | end detected at 50
2026-01-28 14:43:31,530 | INFO |  -3.66 * 0.5 =  -1.83 for decoder
2026-01-28 14:43:31,530 | INFO |  -0.14 * 0.5 =  -0.07 for ctc
2026-01-28 14:43:31,530 | INFO | total log probability: -1.90
2026-01-28 14:43:31,530 | INFO | normalized log probability: -0.04
2026-01-28 14:43:31,530 | INFO | total number of ended hypotheses: 151
2026-01-28 14:43:31,531 | INFO | best hypo: c'est<space>une<space>terre<space>solide<space>c'est<space>une<space>terre<space>forte

2026-01-28 14:43:31,533 | INFO | speech length: 56960
2026-01-28 14:43:31,582 | INFO | decoder input length: 88
2026-01-28 14:43:31,582 | INFO | max output length: 88
2026-01-28 14:43:31,582 | INFO | min output length: 8
2026-01-28 14:43:34,080 | INFO | end detected at 56
2026-01-28 14:43:34,081 | INFO |  -9.73 * 0.5 =  -4.86 for decoder
2026-01-28 14:43:34,081 | INFO |  -6.02 * 0.5 =  -3.01 for ctc
2026-01-28 14:43:34,081 | INFO | total log probability: -7.87
2026-01-28 14:43:34,081 | INFO | normalized log probability: -0.15
2026-01-28 14:43:34,082 | INFO | total number of ended hypotheses: 175
2026-01-28 14:43:34,082 | INFO | best hypo: c'est<space>loin<space>à<space>burir<space>et<space>à<space>pousser<space>mais<space>c'est<space>solide

2026-01-28 14:43:34,085 | INFO | speech length: 161760
2026-01-28 14:43:34,123 | INFO | decoder input length: 252
2026-01-28 14:43:34,124 | INFO | max output length: 252
2026-01-28 14:43:34,124 | INFO | min output length: 25
2026-01-28 14:43:42,548 | INFO | end detected at 139
2026-01-28 14:43:42,550 | INFO | -17.53 * 0.5 =  -8.77 for decoder
2026-01-28 14:43:42,551 | INFO |  -7.22 * 0.5 =  -3.61 for ctc
2026-01-28 14:43:42,551 | INFO | total log probability: -12.37
2026-01-28 14:43:42,551 | INFO | normalized log probability: -0.09
2026-01-28 14:43:42,551 | INFO | total number of ended hypotheses: 207
2026-01-28 14:43:42,553 | INFO | best hypo: alors<space>on<space>dit<space>que<space>le<space>temps<space>pérament<space>du<space>beau<space>seron<space>est<space>intempérament<space>volontaire<space>travailleur<space>mais<space>fidèle<space>et<space>quand<space>il<space>a<space>donné<space>sa<space>sa<space>foi

2026-01-28 14:43:42,556 | INFO | speech length: 43680
2026-01-28 14:43:42,592 | INFO | decoder input length: 67
2026-01-28 14:43:42,592 | INFO | max output length: 67
2026-01-28 14:43:42,592 | INFO | min output length: 6
2026-01-28 14:43:43,999 | INFO | end detected at 34
2026-01-28 14:43:44,002 | INFO |  -2.14 * 0.5 =  -1.07 for decoder
2026-01-28 14:43:44,002 | INFO |  -2.73 * 0.5 =  -1.37 for ctc
2026-01-28 14:43:44,002 | INFO | total log probability: -2.44
2026-01-28 14:43:44,002 | INFO | normalized log probability: -0.09
2026-01-28 14:43:44,002 | INFO | total number of ended hypotheses: 205
2026-01-28 14:43:44,002 | INFO | best hypo: à<space>quelqu'un<space>c'est<space>durable

2026-01-28 14:43:44,005 | INFO | speech length: 48640
2026-01-28 14:43:44,054 | INFO | decoder input length: 75
2026-01-28 14:43:44,054 | INFO | max output length: 75
2026-01-28 14:43:44,054 | INFO | min output length: 7
2026-01-28 14:43:45,839 | INFO | end detected at 42
2026-01-28 14:43:45,841 | INFO |  -3.04 * 0.5 =  -1.52 for decoder
2026-01-28 14:43:45,841 | INFO |  -1.14 * 0.5 =  -0.57 for ctc
2026-01-28 14:43:45,841 | INFO | total log probability: -2.09
2026-01-28 14:43:45,841 | INFO | normalized log probability: -0.06
2026-01-28 14:43:45,841 | INFO | total number of ended hypotheses: 176
2026-01-28 14:43:45,841 | INFO | best hypo: euh<space>nous<space>le<space>ressentons<space>certainement

2026-01-28 14:43:45,844 | INFO | speech length: 44960
2026-01-28 14:43:45,880 | INFO | decoder input length: 69
2026-01-28 14:43:45,880 | INFO | max output length: 69
2026-01-28 14:43:45,880 | INFO | min output length: 6
2026-01-28 14:43:47,385 | INFO | end detected at 37
2026-01-28 14:43:47,387 | INFO |  -2.55 * 0.5 =  -1.28 for decoder
2026-01-28 14:43:47,387 | INFO |  -0.25 * 0.5 =  -0.12 for ctc
2026-01-28 14:43:47,387 | INFO | total log probability: -1.40
2026-01-28 14:43:47,387 | INFO | normalized log probability: -0.04
2026-01-28 14:43:47,387 | INFO | total number of ended hypotheses: 162
2026-01-28 14:43:47,387 | INFO | best hypo: dans<space>le<space>comportement<space>religieux

2026-01-28 14:43:47,390 | INFO | speech length: 16800
2026-01-28 14:43:47,427 | INFO | decoder input length: 25
2026-01-28 14:43:47,427 | INFO | max output length: 25
2026-01-28 14:43:47,427 | INFO | min output length: 2
2026-01-28 14:43:48,071 | INFO | end detected at 17
2026-01-28 14:43:48,073 | INFO |  -0.75 * 0.5 =  -0.38 for decoder
2026-01-28 14:43:48,073 | INFO |  -6.79 * 0.5 =  -3.40 for ctc
2026-01-28 14:43:48,073 | INFO | total log probability: -3.77
2026-01-28 14:43:48,074 | INFO | normalized log probability: -0.47
2026-01-28 14:43:48,074 | INFO | total number of ended hypotheses: 200
2026-01-28 14:43:48,074 | INFO | best hypo: il<space>y<space>a

2026-01-28 14:43:48,076 | INFO | speech length: 199840
2026-01-28 14:43:48,126 | INFO | decoder input length: 311
2026-01-28 14:43:48,127 | INFO | max output length: 311
2026-01-28 14:43:48,127 | INFO | min output length: 31
2026-01-28 14:44:00,531 | INFO | end detected at 188
2026-01-28 14:44:00,533 | INFO | -20.31 * 0.5 = -10.16 for decoder
2026-01-28 14:44:00,533 | INFO | -23.49 * 0.5 = -11.75 for ctc
2026-01-28 14:44:00,533 | INFO | total log probability: -21.90
2026-01-28 14:44:00,533 | INFO | normalized log probability: -0.12
2026-01-28 14:44:00,533 | INFO | total number of ended hypotheses: 190
2026-01-28 14:44:00,535 | INFO | best hypo: tractions<space>de<space>familles<space>qui<space>se<space>transmettent<space>dans<space>le<space>domaine<space>religieux<space>comme<space>dans<space>tout<space>le<space>domaine<space>du<space>travail<space>du<space>sérieux<space>de<space>l'économie<space>de<space>la<space>résistance<space>des<space>cultures<space>pour<space>cent<space>hesitation

2026-01-28 14:44:00,538 | INFO | speech length: 83840
2026-01-28 14:44:00,577 | INFO | decoder input length: 130
2026-01-28 14:44:00,577 | INFO | max output length: 130
2026-01-28 14:44:00,577 | INFO | min output length: 13
2026-01-28 14:44:04,341 | INFO | end detected at 80
2026-01-28 14:44:04,344 | INFO |  -5.94 * 0.5 =  -2.97 for decoder
2026-01-28 14:44:04,345 | INFO |  -2.03 * 0.5 =  -1.02 for ctc
2026-01-28 14:44:04,345 | INFO | total log probability: -3.99
2026-01-28 14:44:04,345 | INFO | normalized log probability: -0.05
2026-01-28 14:44:04,345 | INFO | total number of ended hypotheses: 203
2026-01-28 14:44:04,346 | INFO | best hypo: il<space>y<space>a<space>la<space>tradition<space>familiale<space>nous<space>sentons<space>très<space>bien<space>que<space>lorsque<space>la<space>foi

2026-01-28 14:44:04,350 | INFO | speech length: 37600
2026-01-28 14:44:04,395 | INFO | decoder input length: 58
2026-01-28 14:44:04,395 | INFO | max output length: 58
2026-01-28 14:44:04,395 | INFO | min output length: 5
2026-01-28 14:44:05,864 | INFO | end detected at 36
2026-01-28 14:44:05,865 | INFO |  -2.71 * 0.5 =  -1.36 for decoder
2026-01-28 14:44:05,865 | INFO |  -0.77 * 0.5 =  -0.39 for ctc
2026-01-28 14:44:05,865 | INFO | total log probability: -1.74
2026-01-28 14:44:05,865 | INFO | normalized log probability: -0.06
2026-01-28 14:44:05,865 | INFO | total number of ended hypotheses: 157
2026-01-28 14:44:05,866 | INFO | best hypo: est<space>implanté<space>dans<space>une<space>famille

2026-01-28 14:44:05,868 | INFO | speech length: 9600
2026-01-28 14:44:05,897 | INFO | decoder input length: 14
2026-01-28 14:44:05,897 | INFO | max output length: 14
2026-01-28 14:44:05,897 | INFO | min output length: 1
2026-01-28 14:44:06,352 | INFO | adding <eos> in the last position in the loop
2026-01-28 14:44:06,359 | INFO | no hypothesis. Finish decoding.
2026-01-28 14:44:06,360 | INFO |  -1.06 * 0.5 =  -0.53 for decoder
2026-01-28 14:44:06,360 | INFO |  -0.03 * 0.5 =  -0.01 for ctc
2026-01-28 14:44:06,360 | INFO | total log probability: -0.55
2026-01-28 14:44:06,360 | INFO | normalized log probability: -0.04
2026-01-28 14:44:06,360 | INFO | total number of ended hypotheses: 70
2026-01-28 14:44:06,360 | INFO | best hypo: c'est<space>solide

2026-01-28 14:44:06,362 | INFO | speech length: 23840
2026-01-28 14:44:06,391 | INFO | decoder input length: 36
2026-01-28 14:44:06,392 | INFO | max output length: 36
2026-01-28 14:44:06,392 | INFO | min output length: 3
2026-01-28 14:44:07,598 | INFO | end detected at 33
2026-01-28 14:44:07,599 | INFO |  -2.24 * 0.5 =  -1.12 for decoder
2026-01-28 14:44:07,599 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-28 14:44:07,599 | INFO | total log probability: -1.13
2026-01-28 14:44:07,600 | INFO | normalized log probability: -0.04
2026-01-28 14:44:07,600 | INFO | total number of ended hypotheses: 145
2026-01-28 14:44:07,600 | INFO | best hypo: on<space>n'a<space>pas<space>peur<space>de<space>l'effort

2026-01-28 14:44:07,602 | INFO | speech length: 49760
2026-01-28 14:44:07,650 | INFO | decoder input length: 77
2026-01-28 14:44:07,650 | INFO | max output length: 77
2026-01-28 14:44:07,650 | INFO | min output length: 7
2026-01-28 14:44:09,394 | INFO | end detected at 40
2026-01-28 14:44:09,395 | INFO |  -5.42 * 0.5 =  -2.71 for decoder
2026-01-28 14:44:09,396 | INFO |  -2.52 * 0.5 =  -1.26 for ctc
2026-01-28 14:44:09,396 | INFO | total log probability: -3.97
2026-01-28 14:44:09,396 | INFO | normalized log probability: -0.12
2026-01-28 14:44:09,396 | INFO | total number of ended hypotheses: 174
2026-01-28 14:44:09,396 | INFO | best hypo: et<space>euh<space>les<space>curés<space>sont<space>en<space>bourse

2026-01-28 14:44:09,399 | INFO | speech length: 50880
2026-01-28 14:44:09,436 | INFO | decoder input length: 79
2026-01-28 14:44:09,436 | INFO | max output length: 79
2026-01-28 14:44:09,436 | INFO | min output length: 7
2026-01-28 14:44:11,491 | INFO | end detected at 50
2026-01-28 14:44:11,492 | INFO |  -3.56 * 0.5 =  -1.78 for decoder
2026-01-28 14:44:11,493 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-28 14:44:11,493 | INFO | total log probability: -1.79
2026-01-28 14:44:11,493 | INFO | normalized log probability: -0.04
2026-01-28 14:44:11,493 | INFO | total number of ended hypotheses: 165
2026-01-28 14:44:11,494 | INFO | best hypo: sans<space>cet<space>attachement<space>de<space>la<space>de<space>la<space>population

2026-01-28 14:44:11,496 | INFO | speech length: 114560
2026-01-28 14:44:11,532 | INFO | decoder input length: 178
2026-01-28 14:44:11,532 | INFO | max output length: 178
2026-01-28 14:44:11,532 | INFO | min output length: 17
2026-01-28 14:44:16,454 | INFO | end detected at 93
2026-01-28 14:44:16,456 | INFO |  -8.99 * 0.5 =  -4.50 for decoder
2026-01-28 14:44:16,456 | INFO |  -4.40 * 0.5 =  -2.20 for ctc
2026-01-28 14:44:16,456 | INFO | total log probability: -6.70
2026-01-28 14:44:16,456 | INFO | normalized log probability: -0.08
2026-01-28 14:44:16,456 | INFO | total number of ended hypotheses: 204
2026-01-28 14:44:16,458 | INFO | best hypo: mais<space>on<space>sent<space>aussi<space>que<space>le<space>mostron<space>n'accorde<space>euh<space>sa<space>fidélité<space>que<space>après<space>avoir<space>bien<space>connu

2026-01-28 14:44:16,461 | INFO | speech length: 79520
2026-01-28 14:44:16,510 | INFO | decoder input length: 123
2026-01-28 14:44:16,511 | INFO | max output length: 123
2026-01-28 14:44:16,511 | INFO | min output length: 12
2026-01-28 14:44:19,645 | INFO | end detected at 68
2026-01-28 14:44:19,646 | INFO |  -4.99 * 0.5 =  -2.50 for decoder
2026-01-28 14:44:19,646 | INFO |  -0.16 * 0.5 =  -0.08 for ctc
2026-01-28 14:44:19,646 | INFO | total log probability: -2.57
2026-01-28 14:44:19,646 | INFO | normalized log probability: -0.04
2026-01-28 14:44:19,646 | INFO | total number of ended hypotheses: 164
2026-01-28 14:44:19,647 | INFO | best hypo: il<space>faut<space>avoir<space>fait<space>ses<space>preuves<space>soi<space>même<space>d'attachement<space>au<space>pays

2026-01-28 14:44:19,650 | INFO | speech length: 88000
2026-01-28 14:44:19,691 | INFO | decoder input length: 137
2026-01-28 14:44:19,692 | INFO | max output length: 137
2026-01-28 14:44:19,692 | INFO | min output length: 13
2026-01-28 14:44:24,336 | INFO | end detected at 101
2026-01-28 14:44:24,339 | INFO |  -8.68 * 0.5 =  -4.34 for decoder
2026-01-28 14:44:24,339 | INFO | -14.02 * 0.5 =  -7.01 for ctc
2026-01-28 14:44:24,339 | INFO | total log probability: -11.35
2026-01-28 14:44:24,339 | INFO | normalized log probability: -0.12
2026-01-28 14:44:24,339 | INFO | total number of ended hypotheses: 212
2026-01-28 14:44:24,340 | INFO | best hypo: le<space>bosson<space>s'est<space>observé<space>longtemps<space>mais<space>nos<space>prêtres<space>de<space>bosse<space>qui<space>sont<space>les<space>curés<space>de<space>campagne

2026-01-28 14:44:24,343 | INFO | speech length: 66240
2026-01-28 14:44:24,373 | INFO | decoder input length: 103
2026-01-28 14:44:24,373 | INFO | max output length: 103
2026-01-28 14:44:24,373 | INFO | min output length: 10
2026-01-28 14:44:26,705 | INFO | end detected at 53
2026-01-28 14:44:26,708 | INFO |  -4.24 * 0.5 =  -2.12 for decoder
2026-01-28 14:44:26,708 | INFO |  -0.75 * 0.5 =  -0.38 for ctc
2026-01-28 14:44:26,708 | INFO | total log probability: -2.50
2026-01-28 14:44:26,708 | INFO | normalized log probability: -0.05
2026-01-28 14:44:26,708 | INFO | total number of ended hypotheses: 181
2026-01-28 14:44:26,709 | INFO | best hypo: ont<space>en<space>général<space>avec<space>leur<space>population<space>cette<space>euh

2026-01-28 14:44:26,711 | INFO | speech length: 74560
2026-01-28 14:44:26,746 | INFO | decoder input length: 116
2026-01-28 14:44:26,746 | INFO | max output length: 116
2026-01-28 14:44:26,747 | INFO | min output length: 11
2026-01-28 14:44:30,116 | INFO | end detected at 77
2026-01-28 14:44:30,117 | INFO |  -6.27 * 0.5 =  -3.14 for decoder
2026-01-28 14:44:30,117 | INFO |  -2.90 * 0.5 =  -1.45 for ctc
2026-01-28 14:44:30,117 | INFO | total log probability: -4.59
2026-01-28 14:44:30,117 | INFO | normalized log probability: -0.06
2026-01-28 14:44:30,118 | INFO | total number of ended hypotheses: 176
2026-01-28 14:44:30,119 | INFO | best hypo: ce<space>lien<space>si<space>vous<space>voulez<space>et<space>qui<space>n'est<space>pas<space>superficiel<space>mais<space>en<space>profondeur

2026-01-28 14:44:30,121 | INFO | speech length: 45600
2026-01-28 14:44:30,156 | INFO | decoder input length: 70
2026-01-28 14:44:30,156 | INFO | max output length: 70
2026-01-28 14:44:30,156 | INFO | min output length: 7
2026-01-28 14:44:32,347 | INFO | end detected at 56
2026-01-28 14:44:32,348 | INFO |  -4.13 * 0.5 =  -2.07 for decoder
2026-01-28 14:44:32,348 | INFO |  -0.28 * 0.5 =  -0.14 for ctc
2026-01-28 14:44:32,348 | INFO | total log probability: -2.21
2026-01-28 14:44:32,348 | INFO | normalized log probability: -0.04
2026-01-28 14:44:32,348 | INFO | total number of ended hypotheses: 150
2026-01-28 14:44:32,349 | INFO | best hypo: qui<space>correspond<space>bien<space>à<space>la<space>fidélité<space>d'une<space>d'une<space>race

2026-01-28 14:44:32,351 | INFO | speech length: 64800
2026-01-28 14:44:32,387 | INFO | decoder input length: 100
2026-01-28 14:44:32,387 | INFO | max output length: 100
2026-01-28 14:44:32,387 | INFO | min output length: 10
2026-01-28 14:44:35,235 | INFO | end detected at 62
2026-01-28 14:44:35,236 | INFO |  -5.38 * 0.5 =  -2.69 for decoder
2026-01-28 14:44:35,236 | INFO |  -1.72 * 0.5 =  -0.86 for ctc
2026-01-28 14:44:35,236 | INFO | total log probability: -3.55
2026-01-28 14:44:35,236 | INFO | normalized log probability: -0.06
2026-01-28 14:44:35,237 | INFO | total number of ended hypotheses: 186
2026-01-28 14:44:35,237 | INFO | best hypo: et<space>puis<space>nous<space>avons<space>une<space>autre<space>région<space>qui<space>est<space>le<space>catinet

2026-01-28 14:44:35,240 | INFO | speech length: 161760
2026-01-28 14:44:35,283 | INFO | decoder input length: 252
2026-01-28 14:44:35,284 | INFO | max output length: 252
2026-01-28 14:44:35,284 | INFO | min output length: 25
2026-01-28 14:44:42,446 | INFO | end detected at 120
2026-01-28 14:44:42,447 | INFO | -13.27 * 0.5 =  -6.64 for decoder
2026-01-28 14:44:42,447 | INFO |  -3.10 * 0.5 =  -1.55 for ctc
2026-01-28 14:44:42,447 | INFO | total log probability: -8.18
2026-01-28 14:44:42,447 | INFO | normalized log probability: -0.07
2026-01-28 14:44:42,447 | INFO | total number of ended hypotheses: 175
2026-01-28 14:44:42,449 | INFO | best hypo: le<space>câtinel<space>lui<space>c'est<space>une<space>région<space>beaucoup<space>plus<space>euh<space>boisée<space>d'une<space>part<space>et<space>puis<space>beaucoup<space>plus<space>euh<space>vouée<space>à<space>à<space>l'élevard

2026-01-28 14:44:42,452 | INFO | speech length: 37280
2026-01-28 14:44:42,511 | INFO | decoder input length: 57
2026-01-28 14:44:42,512 | INFO | max output length: 57
2026-01-28 14:44:42,512 | INFO | min output length: 5
2026-01-28 14:44:44,302 | INFO | end detected at 46
2026-01-28 14:44:44,303 | INFO |  -3.28 * 0.5 =  -1.64 for decoder
2026-01-28 14:44:44,303 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-28 14:44:44,304 | INFO | total log probability: -1.64
2026-01-28 14:44:44,304 | INFO | normalized log probability: -0.04
2026-01-28 14:44:44,304 | INFO | total number of ended hypotheses: 153
2026-01-28 14:44:44,304 | INFO | best hypo: et<space>il<space>y<space>a<space>aussi<space>de<space>la<space>très<space>bonne<space>culture

2026-01-28 14:44:44,307 | INFO | speech length: 22400
2026-01-28 14:44:44,358 | INFO | decoder input length: 34
2026-01-28 14:44:44,358 | INFO | max output length: 34
2026-01-28 14:44:44,358 | INFO | min output length: 3
2026-01-28 14:44:45,092 | INFO | end detected at 18
2026-01-28 14:44:45,094 | INFO |  -0.74 * 0.5 =  -0.37 for decoder
2026-01-28 14:44:45,094 | INFO |  -2.95 * 0.5 =  -1.47 for ctc
2026-01-28 14:44:45,094 | INFO | total log probability: -1.84
2026-01-28 14:44:45,094 | INFO | normalized log probability: -0.18
2026-01-28 14:44:45,094 | INFO | total number of ended hypotheses: 163
2026-01-28 14:44:45,095 | INFO | best hypo: mais<space>euh

2026-01-28 14:44:45,097 | INFO | speech length: 81600
2026-01-28 14:44:45,135 | INFO | decoder input length: 127
2026-01-28 14:44:45,135 | INFO | max output length: 127
2026-01-28 14:44:45,135 | INFO | min output length: 12
2026-01-28 14:44:48,157 | INFO | end detected at 63
2026-01-28 14:44:48,158 | INFO |  -7.90 * 0.5 =  -3.95 for decoder
2026-01-28 14:44:48,159 | INFO |  -7.56 * 0.5 =  -3.78 for ctc
2026-01-28 14:44:48,159 | INFO | total log probability: -7.73
2026-01-28 14:44:48,159 | INFO | normalized log probability: -0.13
2026-01-28 14:44:48,159 | INFO | total number of ended hypotheses: 166
2026-01-28 14:44:48,160 | INFO | best hypo: le<space>cabinet<space>est<space>très<space>accueillant<space>il<space>est<space>très<space>bien<space>veillant

2026-01-28 14:44:48,162 | INFO | speech length: 16960
2026-01-28 14:44:48,203 | INFO | decoder input length: 26
2026-01-28 14:44:48,203 | INFO | max output length: 26
2026-01-28 14:44:48,203 | INFO | min output length: 2
2026-01-28 14:44:48,701 | INFO | end detected at 13
2026-01-28 14:44:48,703 | INFO |  -0.64 * 0.5 =  -0.32 for decoder
2026-01-28 14:44:48,703 | INFO |  -0.14 * 0.5 =  -0.07 for ctc
2026-01-28 14:44:48,703 | INFO | total log probability: -0.39
2026-01-28 14:44:48,703 | INFO | normalized log probability: -0.05
2026-01-28 14:44:48,703 | INFO | total number of ended hypotheses: 163
2026-01-28 14:44:48,703 | INFO | best hypo: il<space>est

2026-01-28 14:44:48,705 | INFO | speech length: 16960
2026-01-28 14:44:48,767 | INFO | decoder input length: 26
2026-01-28 14:44:48,767 | INFO | max output length: 26
2026-01-28 14:44:48,767 | INFO | min output length: 2
2026-01-28 14:44:49,579 | INFO | end detected at 20
2026-01-28 14:44:49,581 | INFO |  -1.90 * 0.5 =  -0.95 for decoder
2026-01-28 14:44:49,582 | INFO |  -5.44 * 0.5 =  -2.72 for ctc
2026-01-28 14:44:49,582 | INFO | total log probability: -3.67
2026-01-28 14:44:49,582 | INFO | normalized log probability: -0.28
2026-01-28 14:44:49,582 | INFO | total number of ended hypotheses: 198
2026-01-28 14:44:49,582 | INFO | best hypo: très<space>émarre

2026-01-28 14:44:49,584 | INFO | speech length: 21280
2026-01-28 14:44:49,621 | INFO | decoder input length: 32
2026-01-28 14:44:49,621 | INFO | max output length: 32
2026-01-28 14:44:49,621 | INFO | min output length: 3
2026-01-28 14:44:50,446 | INFO | end detected at 22
2026-01-28 14:44:50,448 | INFO |  -2.42 * 0.5 =  -1.21 for decoder
2026-01-28 14:44:50,448 | INFO |  -6.01 * 0.5 =  -3.00 for ctc
2026-01-28 14:44:50,448 | INFO | total log probability: -4.22
2026-01-28 14:44:50,448 | INFO | normalized log probability: -0.38
2026-01-28 14:44:50,448 | INFO | total number of ended hypotheses: 146
2026-01-28 14:44:50,448 | INFO | best hypo: mais<space>pour

2026-01-28 14:44:50,450 | INFO | speech length: 118720
2026-01-28 14:44:50,485 | INFO | decoder input length: 185
2026-01-28 14:44:50,485 | INFO | max output length: 185
2026-01-28 14:44:50,485 | INFO | min output length: 18
2026-01-28 14:44:56,147 | INFO | end detected at 111
2026-01-28 14:44:56,148 | INFO | -11.13 * 0.5 =  -5.56 for decoder
2026-01-28 14:44:56,148 | INFO |  -1.79 * 0.5 =  -0.89 for ctc
2026-01-28 14:44:56,148 | INFO | total log probability: -6.46
2026-01-28 14:44:56,148 | INFO | normalized log probability: -0.06
2026-01-28 14:44:56,148 | INFO | total number of ended hypotheses: 163
2026-01-28 14:44:56,150 | INFO | best hypo: on<space>ne<space>trouve<space>pas<space>si<space>vous<space>voulez<space>euh<space>la<space>même<space>profondeur<space>d'attachement<space>à<space>la<space>foi<space>chrétienne<space>dans<space>le<space>gatinet

2026-01-28 14:44:56,152 | INFO | speech length: 39040
2026-01-28 14:44:56,187 | INFO | decoder input length: 60
2026-01-28 14:44:56,188 | INFO | max output length: 60
2026-01-28 14:44:56,188 | INFO | min output length: 6
2026-01-28 14:44:58,337 | INFO | adding <eos> in the last position in the loop
2026-01-28 14:44:58,346 | INFO | no hypothesis. Finish decoding.
2026-01-28 14:44:58,347 | INFO |  -4.41 * 0.5 =  -2.20 for decoder
2026-01-28 14:44:58,347 | INFO |  -0.04 * 0.5 =  -0.02 for ctc
2026-01-28 14:44:58,347 | INFO | total log probability: -2.23
2026-01-28 14:44:58,347 | INFO | normalized log probability: -0.04
2026-01-28 14:44:58,347 | INFO | total number of ended hypotheses: 157
2026-01-28 14:44:58,348 | INFO | best hypo: et<space>puis<space>c'est<space>un<space>pays<space>dans<space>lequel<space>on<space>aime<space>la<space>bonne<space>vie

2026-01-28 14:44:58,350 | INFO | speech length: 23520
2026-01-28 14:44:58,380 | INFO | decoder input length: 36
2026-01-28 14:44:58,380 | INFO | max output length: 36
2026-01-28 14:44:58,380 | INFO | min output length: 3
2026-01-28 14:44:59,527 | INFO | end detected at 32
2026-01-28 14:44:59,528 | INFO |  -2.28 * 0.5 =  -1.14 for decoder
2026-01-28 14:44:59,528 | INFO |  -0.40 * 0.5 =  -0.20 for ctc
2026-01-28 14:44:59,528 | INFO | total log probability: -1.34
2026-01-28 14:44:59,528 | INFO | normalized log probability: -0.05
2026-01-28 14:44:59,528 | INFO | total number of ended hypotheses: 165
2026-01-28 14:44:59,529 | INFO | best hypo: c'est<space>un<space>pays<space>très<space>joyeux

2026-01-28 14:44:59,531 | INFO | speech length: 19040
2026-01-28 14:44:59,571 | INFO | decoder input length: 29
2026-01-28 14:44:59,571 | INFO | max output length: 29
2026-01-28 14:44:59,571 | INFO | min output length: 2
2026-01-28 14:45:00,483 | INFO | end detected at 25
2026-01-28 14:45:00,484 | INFO |  -1.62 * 0.5 =  -0.81 for decoder
2026-01-28 14:45:00,484 | INFO |  -0.39 * 0.5 =  -0.20 for ctc
2026-01-28 14:45:00,484 | INFO | total log probability: -1.00
2026-01-28 14:45:00,484 | INFO | normalized log probability: -0.05
2026-01-28 14:45:00,484 | INFO | total number of ended hypotheses: 172
2026-01-28 14:45:00,485 | INFO | best hypo: les<space>faits<space>qui<space>sont

2026-01-28 14:45:00,487 | INFO | speech length: 12800
2026-01-28 14:45:00,521 | INFO | decoder input length: 19
2026-01-28 14:45:00,521 | INFO | max output length: 19
2026-01-28 14:45:00,522 | INFO | min output length: 1
2026-01-28 14:45:01,150 | INFO | adding <eos> in the last position in the loop
2026-01-28 14:45:01,160 | INFO | no hypothesis. Finish decoding.
2026-01-28 14:45:01,161 | INFO |  -3.27 * 0.5 =  -1.63 for decoder
2026-01-28 14:45:01,161 | INFO |  -0.85 * 0.5 =  -0.43 for ctc
2026-01-28 14:45:01,161 | INFO | total log probability: -2.06
2026-01-28 14:45:01,161 | INFO | normalized log probability: -0.13
2026-01-28 14:45:01,161 | INFO | total number of ended hypotheses: 134
2026-01-28 14:45:01,161 | INFO | best hypo: plus<space>nombreuse

2026-01-28 14:45:01,163 | INFO | speech length: 33920
2026-01-28 14:45:01,193 | INFO | decoder input length: 52
2026-01-28 14:45:01,193 | INFO | max output length: 52
2026-01-28 14:45:01,193 | INFO | min output length: 5
2026-01-28 14:45:03,049 | INFO | end detected at 44
2026-01-28 14:45:03,050 | INFO |  -3.16 * 0.5 =  -1.58 for decoder
2026-01-28 14:45:03,050 | INFO |  -0.09 * 0.5 =  -0.05 for ctc
2026-01-28 14:45:03,050 | INFO | total log probability: -1.62
2026-01-28 14:45:03,050 | INFO | normalized log probability: -0.04
2026-01-28 14:45:03,050 | INFO | total number of ended hypotheses: 152
2026-01-28 14:45:03,051 | INFO | best hypo: je<space>pourrais<space>vous<space>dire<space>que<space>certainement

2026-01-28 14:45:03,053 | INFO | speech length: 91520
2026-01-28 14:45:03,083 | INFO | decoder input length: 142
2026-01-28 14:45:03,083 | INFO | max output length: 142
2026-01-28 14:45:03,083 | INFO | min output length: 14
2026-01-28 14:45:07,585 | INFO | end detected at 95
2026-01-28 14:45:07,587 | INFO |  -9.20 * 0.5 =  -4.60 for decoder
2026-01-28 14:45:07,587 | INFO |  -6.32 * 0.5 =  -3.16 for ctc
2026-01-28 14:45:07,587 | INFO | total log probability: -7.76
2026-01-28 14:45:07,587 | INFO | normalized log probability: -0.09
2026-01-28 14:45:07,587 | INFO | total number of ended hypotheses: 200
2026-01-28 14:45:07,588 | INFO | best hypo: la<space>morale<space>chrétienne<space>me<space>paraît<space>plus<space>difficile<space>euh<space>à<space>nos<space>gens<space>du<space>gâtinec<space>dans<space>la<space>bourse

2026-01-28 14:45:07,591 | INFO | speech length: 50560
2026-01-28 14:45:07,621 | INFO | decoder input length: 78
2026-01-28 14:45:07,622 | INFO | max output length: 78
2026-01-28 14:45:07,622 | INFO | min output length: 7
2026-01-28 14:45:09,981 | INFO | end detected at 57
2026-01-28 14:45:09,982 | INFO |  -4.15 * 0.5 =  -2.08 for decoder
2026-01-28 14:45:09,983 | INFO |  -0.15 * 0.5 =  -0.08 for ctc
2026-01-28 14:45:09,983 | INFO | total log probability: -2.15
2026-01-28 14:45:09,983 | INFO | normalized log probability: -0.04
2026-01-28 14:45:09,983 | INFO | total number of ended hypotheses: 151
2026-01-28 14:45:09,984 | INFO | best hypo: euh<space>c'est<space>aussi<space>une<space>des<space>raisons<space>pour<space>lesquelles<space>euh

2026-01-28 14:45:09,986 | INFO | speech length: 147520
2026-01-28 14:45:10,016 | INFO | decoder input length: 230
2026-01-28 14:45:10,017 | INFO | max output length: 230
2026-01-28 14:45:10,017 | INFO | min output length: 23
2026-01-28 14:45:17,500 | INFO | end detected at 126
2026-01-28 14:45:17,502 | INFO | -10.78 * 0.5 =  -5.39 for decoder
2026-01-28 14:45:17,502 | INFO |  -8.33 * 0.5 =  -4.16 for ctc
2026-01-28 14:45:17,503 | INFO | total log probability: -9.56
2026-01-28 14:45:17,503 | INFO | normalized log probability: -0.08
2026-01-28 14:45:17,503 | INFO | total number of ended hypotheses: 198
2026-01-28 14:45:17,504 | INFO | best hypo: l'attachement<space>est<space>peut<space>être<space>moins<space>en<space>profondeur<space>ben<space>il<space>gagne<space>tellement<space>en<space>spontanéité<space>en<space>en<space>amabilité<space>aux<space>gentillesses

2026-01-28 14:45:17,507 | INFO | speech length: 22560
2026-01-28 14:45:17,557 | INFO | decoder input length: 34
2026-01-28 14:45:17,557 | INFO | max output length: 34
2026-01-28 14:45:17,557 | INFO | min output length: 3
2026-01-28 14:45:18,341 | INFO | end detected at 15
2026-01-28 14:45:18,344 | INFO |  -1.56 * 0.5 =  -0.78 for decoder
2026-01-28 14:45:18,344 | INFO |  -1.50 * 0.5 =  -0.75 for ctc
2026-01-28 14:45:18,344 | INFO | total log probability: -1.53
2026-01-28 14:45:18,344 | INFO | normalized log probability: -0.22
2026-01-28 14:45:18,344 | INFO | total number of ended hypotheses: 188
2026-01-28 14:45:18,345 | INFO | best hypo: qu'il

2026-01-28 14:45:18,347 | INFO | speech length: 132320
2026-01-28 14:45:18,397 | INFO | decoder input length: 206
2026-01-28 14:45:18,397 | INFO | max output length: 206
2026-01-28 14:45:18,397 | INFO | min output length: 20
2026-01-28 14:45:26,032 | INFO | end detected at 134
2026-01-28 14:45:26,033 | INFO | -10.30 * 0.5 =  -5.15 for decoder
2026-01-28 14:45:26,033 | INFO |  -0.24 * 0.5 =  -0.12 for ctc
2026-01-28 14:45:26,034 | INFO | total log probability: -5.27
2026-01-28 14:45:26,034 | INFO | normalized log probability: -0.04
2026-01-28 14:45:26,034 | INFO | total number of ended hypotheses: 170
2026-01-28 14:45:26,035 | INFO | best hypo: qu'ils<space>ont<space>peut<space>être<space>davantage<space>si<space>vous<space>voulez<space>de<space>l'évangile<space>la<space>la<space>caractéristique<space>de<space>la<space>sympathie<space>de<space>l'accueil<space>et<space>de<space>la<space>charité

2026-01-28 14:45:26,038 | INFO | speech length: 124640
2026-01-28 14:45:26,074 | INFO | decoder input length: 194
2026-01-28 14:45:26,074 | INFO | max output length: 194
2026-01-28 14:45:26,074 | INFO | min output length: 19
2026-01-28 14:45:31,589 | INFO | end detected at 101
2026-01-28 14:45:31,590 | INFO |  -7.56 * 0.5 =  -3.78 for decoder
2026-01-28 14:45:31,591 | INFO |  -0.66 * 0.5 =  -0.33 for ctc
2026-01-28 14:45:31,591 | INFO | total log probability: -4.11
2026-01-28 14:45:31,591 | INFO | normalized log probability: -0.04
2026-01-28 14:45:31,591 | INFO | total number of ended hypotheses: 190
2026-01-28 14:45:31,592 | INFO | best hypo: mais<space>que<space>il<space>faudrait<space>peut<space>être<space>moins<space>moins<space>chercher<space>du<space>côté<space>du<space>renoncement<space>de<space>de<space>l'austérité

2026-01-28 14:45:31,595 | INFO | speech length: 47200
2026-01-28 14:45:31,631 | INFO | decoder input length: 73
2026-01-28 14:45:31,631 | INFO | max output length: 73
2026-01-28 14:45:31,631 | INFO | min output length: 7
2026-01-28 14:45:33,481 | INFO | end detected at 46
2026-01-28 14:45:33,484 | INFO |  -3.99 * 0.5 =  -2.00 for decoder
2026-01-28 14:45:33,484 | INFO |  -2.64 * 0.5 =  -1.32 for ctc
2026-01-28 14:45:33,484 | INFO | total log probability: -3.31
2026-01-28 14:45:33,484 | INFO | normalized log probability: -0.08
2026-01-28 14:45:33,484 | INFO | total number of ended hypotheses: 189
2026-01-28 14:45:33,485 | INFO | best hypo: est<space>ce<space>qu'il<space>y<space>a<space>une<space>différence<space>dette

2026-01-28 14:45:33,487 | INFO | speech length: 9760
2026-01-28 14:45:33,516 | INFO | decoder input length: 14
2026-01-28 14:45:33,516 | INFO | max output length: 14
2026-01-28 14:45:33,516 | INFO | min output length: 1
2026-01-28 14:45:33,921 | INFO | end detected at 11
2026-01-28 14:45:33,922 | INFO |  -0.49 * 0.5 =  -0.24 for decoder
2026-01-28 14:45:33,922 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 14:45:33,922 | INFO | total log probability: -0.25
2026-01-28 14:45:33,922 | INFO | normalized log probability: -0.04
2026-01-28 14:45:33,922 | INFO | total number of ended hypotheses: 134
2026-01-28 14:45:33,922 | INFO | best hypo: parmi

2026-01-28 14:45:33,924 | INFO | speech length: 25440
2026-01-28 14:45:33,970 | INFO | decoder input length: 39
2026-01-28 14:45:33,971 | INFO | max output length: 39
2026-01-28 14:45:33,971 | INFO | min output length: 3
2026-01-28 14:45:35,380 | INFO | end detected at 31
2026-01-28 14:45:35,382 | INFO |  -5.59 * 0.5 =  -2.79 for decoder
2026-01-28 14:45:35,382 | INFO |  -8.90 * 0.5 =  -4.45 for ctc
2026-01-28 14:45:35,382 | INFO | total log probability: -7.24
2026-01-28 14:45:35,382 | INFO | normalized log probability: -0.30
2026-01-28 14:45:35,383 | INFO | total number of ended hypotheses: 197
2026-01-28 14:45:35,383 | INFO | best hypo: violent<space>couche<space>sociale

2026-01-28 14:45:35,386 | INFO | speech length: 18400
2026-01-28 14:45:35,419 | INFO | decoder input length: 28
2026-01-28 14:45:35,419 | INFO | max output length: 28
2026-01-28 14:45:35,419 | INFO | min output length: 2
2026-01-28 14:45:36,215 | INFO | end detected at 22
2026-01-28 14:45:36,217 | INFO |  -1.31 * 0.5 =  -0.66 for decoder
2026-01-28 14:45:36,217 | INFO |  -0.54 * 0.5 =  -0.27 for ctc
2026-01-28 14:45:36,217 | INFO | total log probability: -0.93
2026-01-28 14:45:36,217 | INFO | normalized log probability: -0.06
2026-01-28 14:45:36,217 | INFO | total number of ended hypotheses: 176
2026-01-28 14:45:36,218 | INFO | best hypo: vous<space>arrivez<space>à

2026-01-28 14:45:36,220 | INFO | speech length: 83840
2026-01-28 14:45:36,258 | INFO | decoder input length: 130
2026-01-28 14:45:36,258 | INFO | max output length: 130
2026-01-28 14:45:36,259 | INFO | min output length: 13
2026-01-28 14:45:41,084 | INFO | end detected at 109
2026-01-28 14:45:41,087 | INFO | -12.92 * 0.5 =  -6.46 for decoder
2026-01-28 14:45:41,087 | INFO | -12.06 * 0.5 =  -6.03 for ctc
2026-01-28 14:45:41,087 | INFO | total log probability: -12.49
2026-01-28 14:45:41,087 | INFO | normalized log probability: -0.12
2026-01-28 14:45:41,087 | INFO | total number of ended hypotheses: 221
2026-01-28 14:45:41,088 | INFO | best hypo: parce<space>qu'il<space>y<space>a<space>des<space>couches<space>sociales<space>qui<space>plus<space>que<space>d'autres<space>disons<space>dans<space>ordéon<space>même<space>où<space>évidemment<space>la

2026-01-28 14:45:41,091 | INFO | speech length: 141600
2026-01-28 14:45:41,129 | INFO | decoder input length: 220
2026-01-28 14:45:41,129 | INFO | max output length: 220
2026-01-28 14:45:41,129 | INFO | min output length: 22
2026-01-28 14:45:51,246 | INFO | end detected at 189
2026-01-28 14:45:51,248 | INFO | -21.34 * 0.5 = -10.67 for decoder
2026-01-28 14:45:51,248 | INFO | -10.29 * 0.5 =  -5.14 for ctc
2026-01-28 14:45:51,249 | INFO | total log probability: -15.81
2026-01-28 14:45:51,249 | INFO | normalized log probability: -0.09
2026-01-28 14:45:51,249 | INFO | total number of ended hypotheses: 198
2026-01-28 14:45:51,251 | INFO | best hypo: oui<space>alors<space>là<space>on<space>aborde<space>l'autre<space>point<space>de<space>vue<space>qui<space>n'est<space>plus<space>le<space>point<space>de<space>vue<space>géographique<space>mais<space>qui<space>est<space>le<space>point<space>de<space>vue<space>sociologique<space>où<space>il<space>est<space>pas<space>le<space>différentes<space>couches<space>de<space>la<space>société

2026-01-28 14:45:51,254 | INFO | speech length: 94880
2026-01-28 14:45:51,292 | INFO | decoder input length: 147
2026-01-28 14:45:51,292 | INFO | max output length: 147
2026-01-28 14:45:51,292 | INFO | min output length: 14
2026-01-28 14:45:56,349 | INFO | end detected at 103
2026-01-28 14:45:56,351 | INFO |  -8.77 * 0.5 =  -4.39 for decoder
2026-01-28 14:45:56,351 | INFO |  -7.38 * 0.5 =  -3.69 for ctc
2026-01-28 14:45:56,351 | INFO | total log probability: -8.08
2026-01-28 14:45:56,351 | INFO | normalized log probability: -0.09
2026-01-28 14:45:56,351 | INFO | total number of ended hypotheses: 214
2026-01-28 14:45:56,353 | INFO | best hypo: euh<space>il<space>est<space>bien<space>vrai<space>que<space>dans<space>les<space>campagnes<space>le<space>prêtre<space>est<space>quand<space>même<space>de<space>côtoyer<space>tout<space>le<space>monde

2026-01-28 14:45:56,356 | INFO | speech length: 284480
2026-01-28 14:45:56,395 | INFO | decoder input length: 444
2026-01-28 14:45:56,395 | INFO | max output length: 444
2026-01-28 14:45:56,395 | INFO | min output length: 44
2026-01-28 14:46:18,137 | INFO | end detected at 267
2026-01-28 14:46:18,139 | INFO | -38.89 * 0.5 = -19.45 for decoder
2026-01-28 14:46:18,139 | INFO | -12.93 * 0.5 =  -6.46 for ctc
2026-01-28 14:46:18,139 | INFO | total log probability: -25.91
2026-01-28 14:46:18,139 | INFO | normalized log probability: -0.10
2026-01-28 14:46:18,140 | INFO | total number of ended hypotheses: 177
2026-01-28 14:46:18,143 | INFO | best hypo: il<space>y<space>a<space>beaucoup<space>moins<space>de<space>dis<space>de<space>distinction<space>entre<space>les<space>contacts<space>que<space>peut<space>avoir<space>le<space>prêtre<space>et<space>puis<space>les<space>classes<space>dirigeantes<space>et<space>le<space>prêtre<space>et<space>le<space>monde<space>ouvrier<space>euh<space>le<space>prêtre<space>connaît<space>très<space>bien<space>tout<space>le<space>monde<space>et<space>il<space>est<space>en<space>contact<space>avec<space>tout<space>le<space>monde<space>dans<space>un<space>petit<space>pays

2026-01-28 14:46:18,146 | INFO | speech length: 104320
2026-01-28 14:46:18,180 | INFO | decoder input length: 162
2026-01-28 14:46:18,180 | INFO | max output length: 162
2026-01-28 14:46:18,180 | INFO | min output length: 16
2026-01-28 14:46:24,856 | INFO | end detected at 123
2026-01-28 14:46:24,859 | INFO |  -9.65 * 0.5 =  -4.83 for decoder
2026-01-28 14:46:24,859 | INFO |  -2.45 * 0.5 =  -1.22 for ctc
2026-01-28 14:46:24,859 | INFO | total log probability: -6.05
2026-01-28 14:46:24,859 | INFO | normalized log probability: -0.05
2026-01-28 14:46:24,859 | INFO | total number of ended hypotheses: 193
2026-01-28 14:46:24,861 | INFO | best hypo: lorsqu'il<space>s'agit<space>d'une<space>grande<space>ville<space>comme<space>orléans<space>le<space>prêtre<space>est<space>bien<space>plus<space>en<space>contact<space>avec<space>ceux<space>qui<space>viennent<space>le<space>voir

2026-01-28 14:46:24,863 | INFO | speech length: 38240
2026-01-28 14:46:24,901 | INFO | decoder input length: 59
2026-01-28 14:46:24,901 | INFO | max output length: 59
2026-01-28 14:46:24,901 | INFO | min output length: 5
2026-01-28 14:46:27,122 | INFO | adding <eos> in the last position in the loop
2026-01-28 14:46:27,133 | INFO | no hypothesis. Finish decoding.
2026-01-28 14:46:27,134 | INFO |  -4.79 * 0.5 =  -2.39 for decoder
2026-01-28 14:46:27,134 | INFO |  -0.34 * 0.5 =  -0.17 for ctc
2026-01-28 14:46:27,134 | INFO | total log probability: -2.57
2026-01-28 14:46:27,134 | INFO | normalized log probability: -0.04
2026-01-28 14:46:27,134 | INFO | total number of ended hypotheses: 71
2026-01-28 14:46:27,136 | INFO | best hypo: parce<space>qu'il<space>ne<space>peut<space>pas<space>être<space>en<space>contact<space>avec<space>tout<space>le<space>monde<sos/eos>

2026-01-28 14:46:27,136 | WARNING | best hypo length: 59 == max output length: 59
2026-01-28 14:46:27,136 | WARNING | decoding may be stopped by the max output length limitation, please consider to increase the maxlenratio.
2026-01-28 14:46:27,138 | INFO | speech length: 152160
2026-01-28 14:46:27,183 | INFO | decoder input length: 237
2026-01-28 14:46:27,183 | INFO | max output length: 237
2026-01-28 14:46:27,183 | INFO | min output length: 23
2026-01-28 14:46:34,835 | INFO | end detected at 130
2026-01-28 14:46:34,839 | INFO | -10.56 * 0.5 =  -5.28 for decoder
2026-01-28 14:46:34,839 | INFO |  -4.16 * 0.5 =  -2.08 for ctc
2026-01-28 14:46:34,839 | INFO | total log probability: -7.36
2026-01-28 14:46:34,839 | INFO | normalized log probability: -0.06
2026-01-28 14:46:34,839 | INFO | total number of ended hypotheses: 207
2026-01-28 14:46:34,841 | INFO | best hypo: alors<space>ceux<space>qui<space>viennent<space>le<space>voir<space>ce<space>sont<space>les<space>les<space>hommes<space>qui<space>ont<space>une<space>formation<space>chrétienne<space>et<space>qui<space>font<space>partie<space>de<space>la<space>paroire

2026-01-28 14:46:34,846 | INFO | speech length: 120640
2026-01-28 14:46:34,883 | INFO | decoder input length: 188
2026-01-28 14:46:34,884 | INFO | max output length: 188
2026-01-28 14:46:34,884 | INFO | min output length: 18
2026-01-28 14:46:42,882 | INFO | end detected at 127
2026-01-28 14:46:42,883 | INFO |  -9.80 * 0.5 =  -4.90 for decoder
2026-01-28 14:46:42,883 | INFO |  -0.38 * 0.5 =  -0.19 for ctc
2026-01-28 14:46:42,883 | INFO | total log probability: -5.09
2026-01-28 14:46:42,884 | INFO | normalized log probability: -0.04
2026-01-28 14:46:42,884 | INFO | total number of ended hypotheses: 181
2026-01-28 14:46:42,885 | INFO | best hypo: et<space>qui<space>viennent<space>à<space>l'occasion<space>de<space>l'éducation<space>des<space>enfants<space>à<space>l'occasion<space>des<space>offices<space>à<space>l'occasion<space>des<space>cérémonies<space>religieuses

2026-01-28 14:46:42,888 | INFO | speech length: 48480
2026-01-28 14:46:42,923 | INFO | decoder input length: 75
2026-01-28 14:46:42,924 | INFO | max output length: 75
2026-01-28 14:46:42,924 | INFO | min output length: 7
2026-01-28 14:46:46,010 | INFO | end detected at 71
2026-01-28 14:46:46,011 | INFO |  -9.36 * 0.5 =  -4.68 for decoder
2026-01-28 14:46:46,011 | INFO |  -2.92 * 0.5 =  -1.46 for ctc
2026-01-28 14:46:46,011 | INFO | total log probability: -6.14
2026-01-28 14:46:46,011 | INFO | normalized log probability: -0.09
2026-01-28 14:46:46,011 | INFO | total number of ended hypotheses: 161
2026-01-28 14:46:46,012 | INFO | best hypo: qui<space>viennent<space>pour<space>les<space>bataines<space>pour<space>les<space>mariages<space>les<space>enterrements

2026-01-28 14:46:46,015 | INFO | speech length: 85920
2026-01-28 14:46:46,052 | INFO | decoder input length: 133
2026-01-28 14:46:46,052 | INFO | max output length: 133
2026-01-28 14:46:46,052 | INFO | min output length: 13
2026-01-28 14:46:50,429 | INFO | end detected at 90
2026-01-28 14:46:50,432 | INFO |  -6.70 * 0.5 =  -3.35 for decoder
2026-01-28 14:46:50,433 | INFO |  -0.92 * 0.5 =  -0.46 for ctc
2026-01-28 14:46:50,433 | INFO | total log probability: -3.81
2026-01-28 14:46:50,433 | INFO | normalized log probability: -0.05
2026-01-28 14:46:50,433 | INFO | total number of ended hypotheses: 198
2026-01-28 14:46:50,435 | INFO | best hypo: qui<space>viennent<space>toujours<space>pour<space>un<space>but<space>précis<space>qui<space>a<space>trait<space>à<space>la<space>à<space>la<space>vie<space>euh<space>chrétienne

2026-01-28 14:46:50,439 | INFO | speech length: 75680
2026-01-28 14:46:50,481 | INFO | decoder input length: 117
2026-01-28 14:46:50,482 | INFO | max output length: 117
2026-01-28 14:46:50,482 | INFO | min output length: 11
2026-01-28 14:46:53,405 | INFO | end detected at 63
2026-01-28 14:46:53,407 | INFO |  -4.38 * 0.5 =  -2.19 for decoder
2026-01-28 14:46:53,408 | INFO |  -6.63 * 0.5 =  -3.31 for ctc
2026-01-28 14:46:53,408 | INFO | total log probability: -5.50
2026-01-28 14:46:53,408 | INFO | normalized log probability: -0.11
2026-01-28 14:46:53,408 | INFO | total number of ended hypotheses: 234
2026-01-28 14:46:53,408 | INFO | best hypo: mais<space>euh<space>les<space>contacts<space>du<space>prêtre<space>avec<space>les<space>gouches

2026-01-28 14:46:53,411 | INFO | speech length: 23360
2026-01-28 14:46:53,442 | INFO | decoder input length: 36
2026-01-28 14:46:53,442 | INFO | max output length: 36
2026-01-28 14:46:53,442 | INFO | min output length: 3
2026-01-28 14:46:54,438 | INFO | end detected at 23
2026-01-28 14:46:54,440 | INFO |  -1.41 * 0.5 =  -0.71 for decoder
2026-01-28 14:46:54,440 | INFO |  -0.11 * 0.5 =  -0.06 for ctc
2026-01-28 14:46:54,440 | INFO | total log probability: -0.76
2026-01-28 14:46:54,440 | INFO | normalized log probability: -0.04
2026-01-28 14:46:54,440 | INFO | total number of ended hypotheses: 165
2026-01-28 14:46:54,441 | INFO | best hypo: avec<space>le<space>problème

2026-01-28 14:46:54,443 | INFO | speech length: 25440
2026-01-28 14:46:54,472 | INFO | decoder input length: 39
2026-01-28 14:46:54,472 | INFO | max output length: 39
2026-01-28 14:46:54,472 | INFO | min output length: 3
2026-01-28 14:46:55,746 | INFO | end detected at 29
2026-01-28 14:46:55,748 | INFO |  -2.03 * 0.5 =  -1.01 for decoder
2026-01-28 14:46:55,748 | INFO |  -0.56 * 0.5 =  -0.28 for ctc
2026-01-28 14:46:55,748 | INFO | total log probability: -1.29
2026-01-28 14:46:55,748 | INFO | normalized log probability: -0.05
2026-01-28 14:46:55,748 | INFO | total number of ended hypotheses: 170
2026-01-28 14:46:55,749 | INFO | best hypo: directe<space>des<space>hommes<space>qui

2026-01-28 14:46:55,752 | INFO | speech length: 36800
2026-01-28 14:46:55,831 | INFO | decoder input length: 57
2026-01-28 14:46:55,831 | INFO | max output length: 57
2026-01-28 14:46:55,831 | INFO | min output length: 5
2026-01-28 14:46:57,511 | INFO | end detected at 39
2026-01-28 14:46:57,513 | INFO |  -3.30 * 0.5 =  -1.65 for decoder
2026-01-28 14:46:57,513 | INFO |  -0.51 * 0.5 =  -0.25 for ctc
2026-01-28 14:46:57,513 | INFO | total log probability: -1.90
2026-01-28 14:46:57,513 | INFO | normalized log probability: -0.06
2026-01-28 14:46:57,513 | INFO | total number of ended hypotheses: 174
2026-01-28 14:46:57,514 | INFO | best hypo: le<space>travail<space>sous<space>tous<space>ces<space>aspects

2026-01-28 14:46:57,516 | INFO | speech length: 137440
2026-01-28 14:46:57,554 | INFO | decoder input length: 214
2026-01-28 14:46:57,555 | INFO | max output length: 214
2026-01-28 14:46:57,555 | INFO | min output length: 21
2026-01-28 14:47:06,191 | INFO | end detected at 145
2026-01-28 14:47:06,192 | INFO | -11.31 * 0.5 =  -5.65 for decoder
2026-01-28 14:47:06,192 | INFO |  -0.31 * 0.5 =  -0.16 for ctc
2026-01-28 14:47:06,193 | INFO | total log probability: -5.81
2026-01-28 14:47:06,193 | INFO | normalized log probability: -0.04
2026-01-28 14:47:06,193 | INFO | total number of ended hypotheses: 150
2026-01-28 14:47:06,195 | INFO | best hypo: ces<space>problèmes<space>là<space>sont<space>certainement<space>plus<space>difficiles<space>à<space>aborder<space>parce<space>que<space>le<space>nombre<space>des<space>prêtres<space>en<space>ville<space>est<space>quand<space>même<space>très<space>petit<space>par<space>rapport

2026-01-28 14:47:06,197 | INFO | speech length: 16160
2026-01-28 14:47:06,238 | INFO | decoder input length: 24
2026-01-28 14:47:06,238 | INFO | max output length: 24
2026-01-28 14:47:06,238 | INFO | min output length: 2
2026-01-28 14:47:07,050 | INFO | adding <eos> in the last position in the loop
2026-01-28 14:47:07,059 | INFO | no hypothesis. Finish decoding.
2026-01-28 14:47:07,061 | INFO |  -1.62 * 0.5 =  -0.81 for decoder
2026-01-28 14:47:07,061 | INFO |  -0.03 * 0.5 =  -0.01 for ctc
2026-01-28 14:47:07,061 | INFO | total log probability: -0.82
2026-01-28 14:47:07,061 | INFO | normalized log probability: -0.04
2026-01-28 14:47:07,061 | INFO | total number of ended hypotheses: 129
2026-01-28 14:47:07,061 | INFO | best hypo: la<space>proportion<space>de<space>la

2026-01-28 14:47:07,063 | INFO | speech length: 265280
2026-01-28 14:47:07,101 | INFO | decoder input length: 414
2026-01-28 14:47:07,101 | INFO | max output length: 414
2026-01-28 14:47:07,101 | INFO | min output length: 41
2026-01-28 14:47:26,649 | INFO | end detected at 233
2026-01-28 14:47:26,652 | INFO | -19.53 * 0.5 =  -9.77 for decoder
2026-01-28 14:47:26,652 | INFO |  -8.01 * 0.5 =  -4.01 for ctc
2026-01-28 14:47:26,652 | INFO | total log probability: -13.77
2026-01-28 14:47:26,652 | INFO | normalized log probability: -0.06
2026-01-28 14:47:26,652 | INFO | total number of ended hypotheses: 176
2026-01-28 14:47:26,656 | INFO | best hypo: alors<space>vous<space>abordez<space>là<space>tout<space>tout<space>cet<space>aspect<space>de<space>l'effort<space>de<space>l'église<space>actuelle<space>pour<space>n'être<space>pas<space>simplement<space>en<space>contact<space>avec<space>ceux<space>qui<space>traditionnellement<space>sont<space>chrétiens<space>mais<space>le<space>problème<space>euh<space>de<space>l'église<space>du<space>contact<space>avec<space>tous<space>les<space>hommes

2026-01-28 14:47:26,660 | INFO | speech length: 34400
2026-01-28 14:47:26,704 | INFO | decoder input length: 53
2026-01-28 14:47:26,704 | INFO | max output length: 53
2026-01-28 14:47:26,704 | INFO | min output length: 5
2026-01-28 14:47:28,193 | INFO | end detected at 38
2026-01-28 14:47:28,194 | INFO |  -2.60 * 0.5 =  -1.30 for decoder
2026-01-28 14:47:28,195 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 14:47:28,195 | INFO | total log probability: -1.30
2026-01-28 14:47:28,195 | INFO | normalized log probability: -0.04
2026-01-28 14:47:28,195 | INFO | total number of ended hypotheses: 177
2026-01-28 14:47:28,195 | INFO | best hypo: et<space>en<space>particulier<space>avec<space>ceux<space>qui

2026-01-28 14:47:28,198 | INFO | speech length: 44000
2026-01-28 14:47:28,250 | INFO | decoder input length: 68
2026-01-28 14:47:28,250 | INFO | max output length: 68
2026-01-28 14:47:28,250 | INFO | min output length: 6
2026-01-28 14:47:30,906 | INFO | end detected at 55
2026-01-28 14:47:30,907 | INFO |  -4.47 * 0.5 =  -2.24 for decoder
2026-01-28 14:47:30,907 | INFO |  -0.31 * 0.5 =  -0.16 for ctc
2026-01-28 14:47:30,907 | INFO | total log probability: -2.39
2026-01-28 14:47:30,907 | INFO | normalized log probability: -0.05
2026-01-28 14:47:30,907 | INFO | total number of ended hypotheses: 170
2026-01-28 14:47:30,908 | INFO | best hypo: qui<space>sont<space>plus<space>plus<space>éprouvés<space>quoi<space>dans<space>le<space>travail

2026-01-28 14:47:30,911 | INFO | speech length: 31680
2026-01-28 14:47:30,951 | INFO | decoder input length: 49
2026-01-28 14:47:30,951 | INFO | max output length: 49
2026-01-28 14:47:30,952 | INFO | min output length: 4
2026-01-28 14:47:32,860 | INFO | adding <eos> in the last position in the loop
2026-01-28 14:47:32,870 | INFO | end detected at 48
2026-01-28 14:47:32,871 | INFO |  -3.47 * 0.5 =  -1.74 for decoder
2026-01-28 14:47:32,871 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-28 14:47:32,871 | INFO | total log probability: -1.74
2026-01-28 14:47:32,871 | INFO | normalized log probability: -0.04
2026-01-28 14:47:32,871 | INFO | total number of ended hypotheses: 181
2026-01-28 14:47:32,872 | INFO | best hypo: vous<space>connaissez<space>bien<space>les<space>problèmes<space>actuels

2026-01-28 14:47:32,874 | INFO | speech length: 314240
2026-01-28 14:47:32,918 | INFO | decoder input length: 490
2026-01-28 14:47:32,918 | INFO | max output length: 490
2026-01-28 14:47:32,918 | INFO | min output length: 49
2026-01-28 14:48:05,484 | INFO | end detected at 394
2026-01-28 14:48:05,486 | INFO | -62.86 * 0.5 = -31.43 for decoder
2026-01-28 14:48:05,486 | INFO | -21.80 * 0.5 = -10.90 for ctc
2026-01-28 14:48:05,487 | INFO | total log probability: -42.33
2026-01-28 14:48:05,487 | INFO | normalized log probability: -0.11
2026-01-28 14:48:05,487 | INFO | total number of ended hypotheses: 191
2026-01-28 14:48:05,492 | INFO | best hypo: n'ai<space>pas<space>l'intention<space>en<space>ce<space>moment<space>de<space>pouvoir<space>nous<space>exposer<space>quelles<space>sont<space>les<space>difficultés<space>les<space>contacts<space>qu'il<space>peut<space>y<space>avoir<space>entre<space>les<space>églises<space>que<space>ce<space>soit<space>l'église<space>catholique<space>l'église<space>protestante<space>église<space>orthodoxe<space>et<space>toutes<space>les<space>églises<space>le<space>monde<space>des<space>travailleurs<space>nous<space>connaissons<space>tous<space>parfois<space>dans<space>le<space>monde<space>des<space>travailleurs<space>depuis<space>un<space>siècle<space>a<space>été<space>travaillé<space>par<space>le<space>théo<space>des<space>théories<space>du<space>marxisme

2026-01-28 14:48:05,495 | INFO | speech length: 47680
2026-01-28 14:48:05,532 | INFO | decoder input length: 74
2026-01-28 14:48:05,532 | INFO | max output length: 74
2026-01-28 14:48:05,532 | INFO | min output length: 7
2026-01-28 14:48:08,344 | INFO | adding <eos> in the last position in the loop
2026-01-28 14:48:08,354 | INFO | no hypothesis. Finish decoding.
2026-01-28 14:48:08,354 | INFO | -22.98 * 0.5 = -11.49 for decoder
2026-01-28 14:48:08,355 | INFO | -28.38 * 0.5 = -14.19 for ctc
2026-01-28 14:48:08,355 | INFO | total log probability: -25.68
2026-01-28 14:48:08,355 | INFO | normalized log probability: -0.36
2026-01-28 14:48:08,355 | INFO | total number of ended hypotheses: 100
2026-01-28 14:48:08,356 | INFO | best hypo: nous<space>savons<space>trèsbien<space>quel<space>que<space>soit<space>l'église<space>àlaquelle<space>nous<space>appartenons

2026-01-28 14:48:08,366 | INFO | Chunk: 0 | WER=125.000000 | S=0 D=0 I=5
2026-01-28 14:48:08,367 | INFO | Chunk: 1 | WER=180.000000 | S=4 D=0 I=5
2026-01-28 14:48:08,367 | INFO | Chunk: 2 | WER=83.333333 | S=1 D=0 I=4
2026-01-28 14:48:08,367 | INFO | Chunk: 3 | WER=90.000000 | S=9 D=0 I=0
2026-01-28 14:48:08,367 | INFO | Chunk: 4 | WER=100.000000 | S=2 D=2 I=3
2026-01-28 14:48:08,368 | INFO | Chunk: 5 | WER=100.000000 | S=4 D=1 I=1
2026-01-28 14:48:08,368 | INFO | Chunk: 6 | WER=150.000000 | S=6 D=0 I=3
2026-01-28 14:48:08,368 | INFO | Chunk: 7 | WER=142.857143 | S=7 D=0 I=3
2026-01-28 14:48:08,368 | INFO | Chunk: 8 | WER=100.000000 | S=8 D=0 I=2
2026-01-28 14:48:08,369 | INFO | Chunk: 9 | WER=96.153846 | S=23 D=2 I=0
2026-01-28 14:48:08,369 | INFO | Chunk: 10 | WER=100.000000 | S=4 D=4 I=1
2026-01-28 14:48:08,370 | INFO | Chunk: 11 | WER=100.000000 | S=5 D=1 I=0
2026-01-28 14:48:08,370 | INFO | Chunk: 12 | WER=75.000000 | S=3 D=0 I=0
2026-01-28 14:48:08,370 | INFO | Chunk: 13 | WER=150.000000 | S=2 D=0 I=1
2026-01-28 14:48:08,371 | INFO | Chunk: 14 | WER=57.142857 | S=2 D=6 I=8
2026-01-28 14:48:08,371 | INFO | Chunk: 15 | WER=109.090909 | S=9 D=0 I=3
2026-01-28 14:48:08,371 | INFO | Chunk: 16 | WER=100.000000 | S=5 D=3 I=0
2026-01-28 14:48:08,372 | INFO | Chunk: 17 | WER=0.000000 | S=1 D=0 I=2
2026-01-28 14:48:08,372 | INFO | Chunk: 18 | WER=200.000000 | S=4 D=0 I=4
2026-01-28 14:48:08,372 | INFO | Chunk: 19 | WER=100.000000 | S=7 D=2 I=0
2026-01-28 14:48:08,372 | INFO | Chunk: 20 | WER=114.285714 | S=7 D=0 I=1
2026-01-28 14:48:08,373 | INFO | Chunk: 21 | WER=94.117647 | S=16 D=0 I=0
2026-01-28 14:48:08,373 | INFO | Chunk: 22 | WER=137.500000 | S=7 D=0 I=4
2026-01-28 14:48:08,373 | INFO | Chunk: 23 | WER=170.000000 | S=10 D=0 I=7
2026-01-28 14:48:08,374 | INFO | Chunk: 24 | WER=100.000000 | S=8 D=3 I=0
2026-01-28 14:48:08,374 | INFO | Chunk: 25 | WER=175.000000 | S=8 D=0 I=6
2026-01-28 14:48:08,374 | INFO | Chunk: 26 | WER=125.000000 | S=7 D=0 I=3
2026-01-28 14:48:08,375 | INFO | Chunk: 27 | WER=125.000000 | S=7 D=0 I=3
2026-01-28 14:48:08,375 | INFO | Chunk: 28 | WER=77.272727 | S=1 D=7 I=9
2026-01-28 14:48:08,375 | INFO | Chunk: 29 | WER=150.000000 | S=5 D=0 I=4
2026-01-28 14:48:08,376 | INFO | Chunk: 30 | WER=100.000000 | S=2 D=5 I=0
2026-01-28 14:48:08,376 | INFO | Chunk: 31 | WER=250.000000 | S=4 D=0 I=6
2026-01-28 14:48:08,376 | INFO | Chunk: 32 | WER=100.000000 | S=0 D=2 I=1
2026-01-28 14:48:08,376 | INFO | Chunk: 33 | WER=66.666667 | S=1 D=1 I=0
2026-01-28 14:48:08,376 | INFO | Chunk: 34 | WER=100.000000 | S=2 D=0 I=0
2026-01-28 14:48:08,377 | INFO | Chunk: 35 | WER=100.000000 | S=0 D=1 I=10
2026-01-28 14:48:08,377 | INFO | Chunk: 36 | WER=171.428571 | S=6 D=0 I=6
2026-01-28 14:48:08,377 | INFO | Chunk: 37 | WER=66.666667 | S=0 D=2 I=2
2026-01-28 14:48:08,377 | INFO | Chunk: 38 | WER=200.000000 | S=2 D=0 I=2
2026-01-28 14:48:08,378 | INFO | Chunk: 39 | WER=100.000000 | S=2 D=2 I=0
2026-01-28 14:48:08,378 | INFO | Chunk: 40 | WER=300.000000 | S=2 D=0 I=4
2026-01-28 14:48:08,378 | INFO | Chunk: 41 | WER=160.000000 | S=10 D=0 I=6
2026-01-28 14:48:08,378 | INFO | Chunk: 42 | WER=100.000000 | S=10 D=0 I=0
2026-01-28 14:48:08,379 | INFO | Chunk: 43 | WER=63.636364 | S=1 D=8 I=5
2026-01-28 14:48:08,379 | INFO | Chunk: 44 | WER=100.000000 | S=2 D=1 I=0
2026-01-28 14:48:08,380 | INFO | Chunk: 45 | WER=93.333333 | S=0 D=2 I=12
2026-01-28 14:48:08,380 | INFO | Chunk: 46 | WER=95.000000 | S=0 D=11 I=8
2026-01-28 14:48:08,380 | INFO | Chunk: 47 | WER=128.571429 | S=7 D=0 I=2
2026-01-28 14:48:08,381 | INFO | Chunk: 48 | WER=100.000000 | S=1 D=1 I=0
2026-01-28 14:48:08,381 | INFO | Chunk: 49 | WER=300.000000 | S=1 D=0 I=2
2026-01-28 14:48:08,381 | INFO | Chunk: 50 | WER=100.000000 | S=3 D=1 I=0
2026-01-28 14:48:08,381 | INFO | Chunk: 51 | WER=111.764706 | S=14 D=1 I=4
2026-01-28 14:48:08,382 | INFO | Chunk: 52 | WER=44.444444 | S=0 D=1 I=11
2026-01-28 14:48:08,383 | INFO | Chunk: 53 | WER=114.285714 | S=1 D=5 I=10
2026-01-28 14:48:08,385 | INFO | Chunk: 54 | WER=55.769231 | S=0 D=15 I=14
2026-01-28 14:48:08,385 | INFO | Chunk: 55 | WER=120.000000 | S=0 D=5 I=13
2026-01-28 14:48:08,386 | INFO | Chunk: 56 | WER=100.000000 | S=9 D=0 I=3
2026-01-28 14:48:08,386 | INFO | Chunk: 57 | WER=88.000000 | S=0 D=12 I=10
2026-01-28 14:48:08,387 | INFO | Chunk: 58 | WER=85.714286 | S=11 D=3 I=4
2026-01-28 14:48:08,387 | INFO | Chunk: 59 | WER=142.857143 | S=7 D=0 I=3
2026-01-28 14:48:08,388 | INFO | Chunk: 60 | WER=87.500000 | S=13 D=0 I=1
2026-01-28 14:48:08,388 | INFO | Chunk: 61 | WER=100.000000 | S=9 D=0 I=0
2026-01-28 14:48:08,388 | INFO | Chunk: 62 | WER=100.000000 | S=3 D=2 I=0
2026-01-28 14:48:08,388 | INFO | Chunk: 63 | WER=200.000000 | S=2 D=0 I=2
2026-01-28 14:48:08,389 | INFO | Chunk: 64 | WER=150.000000 | S=4 D=0 I=2
2026-01-28 14:48:08,389 | INFO | Chunk: 65 | WER=111.764706 | S=0 D=6 I=13
2026-01-28 14:48:08,389 | INFO | Chunk: 66 | WER=100.000000 | S=4 D=0 I=0
2026-01-28 14:48:08,391 | INFO | Chunk: 67 | WER=55.263158 | S=0 D=9 I=12
2026-01-28 14:48:08,391 | INFO | Chunk: 68 | WER=100.000000 | S=6 D=0 I=0
2026-01-28 14:48:08,391 | INFO | Chunk: 69 | WER=150.000000 | S=6 D=0 I=3
2026-01-28 14:48:08,391 | INFO | Chunk: 70 | WER=120.000000 | S=5 D=0 I=1
2026-01-28 14:48:08,394 | INFO | Chunk: 71 | WER=46.153846 | S=10 D=9 I=11
2026-01-28 14:48:08,395 | INFO | Chunk: 72 | WER=100.000000 | S=11 D=0 I=0
2026-01-28 14:48:08,819 | INFO | File: Rhap-D1001.wav | WER=30.527638 | S=73 D=20 I=150
2026-01-28 14:48:08,819 | INFO | ------------------------------
2026-01-28 14:48:08,819 | INFO | Conf ester Done!
2026-01-28 14:55:27,471 | INFO | Chunk: 0 | WER=100.000000 | S=0 D=0 I=4
2026-01-28 14:55:27,487 | INFO | Chunk: 1 | WER=120.000000 | S=4 D=0 I=2
2026-01-28 14:55:27,488 | INFO | Chunk: 2 | WER=66.666667 | S=1 D=0 I=3
2026-01-28 14:55:27,488 | INFO | Chunk: 3 | WER=90.000000 | S=9 D=0 I=0
2026-01-28 14:55:27,489 | INFO | Chunk: 4 | WER=100.000000 | S=2 D=2 I=3
2026-01-28 14:55:27,489 | INFO | Chunk: 5 | WER=83.333333 | S=4 D=1 I=0
2026-01-28 14:55:27,489 | INFO | Chunk: 6 | WER=150.000000 | S=6 D=0 I=3
2026-01-28 14:55:27,489 | INFO | Chunk: 7 | WER=157.142857 | S=7 D=0 I=4
2026-01-28 14:55:27,490 | INFO | Chunk: 8 | WER=110.000000 | S=10 D=0 I=1
2026-01-28 14:55:27,491 | INFO | Chunk: 9 | WER=84.615385 | S=4 D=12 I=6
2026-01-28 14:55:27,491 | INFO | Chunk: 10 | WER=100.000000 | S=4 D=4 I=1
2026-01-28 14:55:27,492 | INFO | Chunk: 11 | WER=100.000000 | S=4 D=2 I=0
2026-01-28 14:55:27,492 | INFO | Chunk: 12 | WER=75.000000 | S=3 D=0 I=0
2026-01-28 14:55:27,492 | INFO | Chunk: 13 | WER=150.000000 | S=2 D=0 I=1
2026-01-28 14:55:27,493 | INFO | Chunk: 14 | WER=50.000000 | S=4 D=6 I=4
2026-01-28 14:55:27,494 | INFO | Chunk: 15 | WER=118.181818 | S=9 D=0 I=4
2026-01-28 14:55:27,494 | INFO | Chunk: 16 | WER=100.000000 | S=5 D=3 I=0
2026-01-28 14:55:27,494 | INFO | Chunk: 17 | WER=0.000000 | S=1 D=0 I=1
2026-01-28 14:55:27,494 | INFO | Chunk: 18 | WER=200.000000 | S=4 D=0 I=4
2026-01-28 14:55:27,495 | INFO | Chunk: 19 | WER=100.000000 | S=6 D=3 I=0
2026-01-28 14:55:27,495 | INFO | Chunk: 20 | WER=114.285714 | S=7 D=0 I=1
2026-01-28 14:55:27,496 | INFO | Chunk: 21 | WER=94.117647 | S=15 D=1 I=0
2026-01-28 14:55:27,496 | INFO | Chunk: 22 | WER=137.500000 | S=7 D=0 I=4
2026-01-28 14:55:27,497 | INFO | Chunk: 23 | WER=170.000000 | S=10 D=0 I=7
2026-01-28 14:55:27,497 | INFO | Chunk: 24 | WER=100.000000 | S=7 D=4 I=0
2026-01-28 14:55:27,497 | INFO | Chunk: 25 | WER=137.500000 | S=8 D=0 I=3
2026-01-28 14:55:27,498 | INFO | Chunk: 26 | WER=125.000000 | S=8 D=0 I=2
2026-01-28 14:55:27,498 | INFO | Chunk: 27 | WER=100.000000 | S=7 D=0 I=1
2026-01-28 14:55:27,499 | INFO | Chunk: 28 | WER=86.363636 | S=16 D=3 I=0
2026-01-28 14:55:27,499 | INFO | Chunk: 29 | WER=150.000000 | S=5 D=0 I=4
2026-01-28 14:55:27,499 | INFO | Chunk: 30 | WER=100.000000 | S=1 D=6 I=0
2026-01-28 14:55:27,500 | INFO | Chunk: 31 | WER=225.000000 | S=4 D=0 I=5
2026-01-28 14:55:27,500 | INFO | Chunk: 32 | WER=100.000000 | S=0 D=2 I=1
2026-01-28 14:55:27,500 | INFO | Chunk: 33 | WER=100.000000 | S=2 D=1 I=0
2026-01-28 14:55:27,500 | INFO | Chunk: 34 | WER=100.000000 | S=2 D=0 I=0
2026-01-28 14:55:27,501 | INFO | Chunk: 35 | WER=118.181818 | S=1 D=4 I=8
2026-01-28 14:55:27,501 | INFO | Chunk: 36 | WER=171.428571 | S=6 D=0 I=6
2026-01-28 14:55:27,501 | INFO | Chunk: 37 | WER=66.666667 | S=0 D=2 I=2
2026-01-28 14:55:27,501 | INFO | Chunk: 38 | WER=150.000000 | S=2 D=0 I=1
2026-01-28 14:55:27,502 | INFO | Chunk: 39 | WER=100.000000 | S=2 D=2 I=0
2026-01-28 14:55:27,502 | INFO | Chunk: 40 | WER=300.000000 | S=2 D=0 I=4
2026-01-28 14:55:27,502 | INFO | Chunk: 41 | WER=130.000000 | S=10 D=0 I=3
2026-01-28 14:55:27,503 | INFO | Chunk: 42 | WER=100.000000 | S=8 D=2 I=0
2026-01-28 14:55:27,503 | INFO | Chunk: 43 | WER=54.545455 | S=1 D=8 I=3
2026-01-28 14:55:27,504 | INFO | Chunk: 44 | WER=100.000000 | S=2 D=1 I=0
2026-01-28 14:55:27,504 | INFO | Chunk: 45 | WER=86.666667 | S=0 D=3 I=10
2026-01-28 14:55:27,505 | INFO | Chunk: 46 | WER=90.000000 | S=0 D=11 I=7
2026-01-28 14:55:27,505 | INFO | Chunk: 47 | WER=128.571429 | S=7 D=0 I=2
2026-01-28 14:55:27,506 | INFO | Chunk: 48 | WER=100.000000 | S=1 D=1 I=0
2026-01-28 14:55:27,506 | INFO | Chunk: 49 | WER=500.000000 | S=1 D=0 I=4
2026-01-28 14:55:27,506 | INFO | Chunk: 50 | WER=100.000000 | S=4 D=0 I=0
2026-01-28 14:55:27,506 | INFO | Chunk: 51 | WER=300.000000 | S=1 D=0 I=2
2026-01-28 14:55:27,507 | INFO | Chunk: 52 | WER=100.000000 | S=13 D=3 I=1
2026-01-28 14:55:27,507 | INFO | Chunk: 53 | WER=100.000000 | S=1 D=1 I=0
2026-01-28 14:55:27,508 | INFO | Chunk: 54 | WER=29.629630 | S=0 D=1 I=7
2026-01-28 14:55:27,509 | INFO | Chunk: 55 | WER=128.571429 | S=1 D=6 I=11
2026-01-28 14:55:27,511 | INFO | Chunk: 56 | WER=57.692308 | S=3 D=14 I=13
2026-01-28 14:55:27,512 | INFO | Chunk: 57 | WER=120.000000 | S=0 D=5 I=13
2026-01-28 14:55:27,513 | INFO | Chunk: 58 | WER=91.666667 | S=6 D=2 I=3
2026-01-28 14:55:27,514 | INFO | Chunk: 59 | WER=84.000000 | S=0 D=12 I=9
2026-01-28 14:55:27,515 | INFO | Chunk: 60 | WER=85.714286 | S=13 D=2 I=3
2026-01-28 14:55:27,516 | INFO | Chunk: 61 | WER=128.571429 | S=7 D=0 I=2
2026-01-28 14:55:27,516 | INFO | Chunk: 62 | WER=81.250000 | S=9 D=3 I=1
2026-01-28 14:55:27,516 | INFO | Chunk: 63 | WER=100.000000 | S=9 D=0 I=0
2026-01-28 14:55:27,517 | INFO | Chunk: 64 | WER=100.000000 | S=3 D=2 I=0
2026-01-28 14:55:27,517 | INFO | Chunk: 65 | WER=150.000000 | S=2 D=0 I=1
2026-01-28 14:55:27,517 | INFO | Chunk: 66 | WER=150.000000 | S=4 D=0 I=2
2026-01-28 14:55:27,519 | INFO | Chunk: 67 | WER=135.294118 | S=3 D=7 I=13
2026-01-28 14:55:27,519 | INFO | Chunk: 68 | WER=125.000000 | S=4 D=0 I=1
2026-01-28 14:55:27,521 | INFO | Chunk: 69 | WER=57.894737 | S=1 D=10 I=11
2026-01-28 14:55:27,521 | INFO | Chunk: 70 | WER=116.666667 | S=6 D=0 I=1
2026-01-28 14:55:27,521 | INFO | Chunk: 71 | WER=166.666667 | S=6 D=0 I=4
2026-01-28 14:55:27,522 | INFO | Chunk: 72 | WER=120.000000 | S=5 D=0 I=1
2026-01-28 14:55:27,526 | INFO | Chunk: 73 | WER=47.692308 | S=7 D=12 I=12
2026-01-28 14:55:27,526 | INFO | Chunk: 74 | WER=118.181818 | S=11 D=0 I=2
2026-01-28 14:55:27,974 | INFO | File: Rhap-D1001.wav | WER=32.040050 | S=117 D=35 I=104
2026-01-28 14:55:27,975 | INFO | ------------------------------
2026-01-28 14:55:27,975 | INFO | hmm_tdnn Done!
2026-01-28 14:55:28,231 | INFO | ==================================Rhap-D1002.wav=========================================
2026-01-28 14:55:28,477 | INFO | Using rVAD model
2026-01-28 14:55:37,812 | INFO | Chunk: 0 | WER=43.750000 | S=4 D=1 I=2
2026-01-28 14:55:37,816 | INFO | Chunk: 1 | WER=32.530120 | S=5 D=21 I=1
2026-01-28 14:55:37,816 | INFO | Chunk: 2 | WER=75.000000 | S=2 D=4 I=0
2026-01-28 14:55:37,816 | INFO | Chunk: 3 | WER=0.000000 | S=1 D=0 I=0
2026-01-28 14:55:37,817 | INFO | Chunk: 4 | WER=23.333333 | S=1 D=6 I=0
2026-01-28 14:55:37,817 | INFO | Chunk: 5 | WER=100.000000 | S=1 D=0 I=0
2026-01-28 14:55:37,818 | INFO | Chunk: 6 | WER=64.285714 | S=7 D=20 I=0
2026-01-28 14:55:37,819 | INFO | Chunk: 7 | WER=23.076923 | S=4 D=2 I=0
2026-01-28 14:55:37,819 | INFO | Chunk: 8 | WER=7.142857 | S=0 D=1 I=0
2026-01-28 14:55:37,824 | INFO | Chunk: 9 | WER=14.444444 | S=7 D=6 I=0
2026-01-28 14:55:37,826 | INFO | Chunk: 10 | WER=33.823529 | S=11 D=12 I=0
2026-01-28 14:55:37,827 | INFO | Chunk: 11 | WER=100.000000 | S=1 D=1 I=0
2026-01-28 14:55:37,829 | INFO | Chunk: 12 | WER=16.666667 | S=5 D=5 I=0
2026-01-28 14:55:37,829 | INFO | Chunk: 13 | WER=27.777778 | S=2 D=3 I=0
2026-01-28 14:55:37,829 | INFO | Chunk: 14 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 14:55:37,830 | INFO | Chunk: 15 | WER=18.181818 | S=1 D=1 I=0
2026-01-28 14:55:37,923 | INFO | File: Rhap-D1002.wav | WER=28.813559 | S=53 D=81 I=2
2026-01-28 14:55:37,923 | INFO | ------------------------------
2026-01-28 14:55:37,923 | INFO | w2vec vad chunk Done!
2026-01-28 14:55:58,356 | INFO | Chunk: 0 | WER=31.250000 | S=3 D=1 I=1
2026-01-28 14:55:58,358 | INFO | Chunk: 1 | WER=77.108434 | S=1 D=63 I=0
2026-01-28 14:55:58,358 | INFO | Chunk: 2 | WER=25.000000 | S=1 D=1 I=0
2026-01-28 14:55:58,358 | INFO | Chunk: 3 | WER=0.000000 | S=1 D=0 I=0
2026-01-28 14:55:58,359 | INFO | Chunk: 4 | WER=20.000000 | S=1 D=5 I=0
2026-01-28 14:55:58,359 | INFO | Chunk: 5 | WER=200.000000 | S=1 D=0 I=1
2026-01-28 14:55:58,361 | INFO | Chunk: 6 | WER=35.714286 | S=9 D=6 I=0
2026-01-28 14:55:58,362 | INFO | Chunk: 7 | WER=3.846154 | S=0 D=1 I=0
2026-01-28 14:55:58,362 | INFO | Chunk: 8 | WER=7.142857 | S=0 D=1 I=0
2026-01-28 14:55:58,365 | INFO | Chunk: 9 | WER=57.777778 | S=12 D=40 I=0
2026-01-28 14:55:58,367 | INFO | Chunk: 10 | WER=63.235294 | S=2 D=40 I=1
2026-01-28 14:55:58,367 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 14:55:58,369 | INFO | Chunk: 12 | WER=30.000000 | S=7 D=10 I=1
2026-01-28 14:55:58,370 | INFO | Chunk: 13 | WER=61.111111 | S=7 D=4 I=0
2026-01-28 14:55:58,370 | INFO | Chunk: 14 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 14:55:58,371 | INFO | Chunk: 15 | WER=18.181818 | S=1 D=1 I=0
2026-01-28 14:55:58,447 | INFO | File: Rhap-D1002.wav | WER=46.822034 | S=45 D=172 I=4
2026-01-28 14:55:58,447 | INFO | ------------------------------
2026-01-28 14:55:58,447 | INFO | whisper med Done!
2026-01-28 14:56:25,510 | INFO | Chunk: 0 | WER=18.750000 | S=1 D=2 I=0
2026-01-28 14:56:25,512 | INFO | Chunk: 1 | WER=67.469880 | S=4 D=52 I=0
2026-01-28 14:56:25,513 | INFO | Chunk: 2 | WER=12.500000 | S=0 D=1 I=0
2026-01-28 14:56:25,513 | INFO | Chunk: 3 | WER=0.000000 | S=1 D=0 I=3
2026-01-28 14:56:25,513 | INFO | Chunk: 4 | WER=16.666667 | S=0 D=5 I=0
2026-01-28 14:56:25,514 | INFO | Chunk: 5 | WER=100.000000 | S=1 D=0 I=0
2026-01-28 14:56:25,515 | INFO | Chunk: 6 | WER=35.714286 | S=1 D=14 I=0
2026-01-28 14:56:25,515 | INFO | Chunk: 7 | WER=11.538462 | S=1 D=2 I=0
2026-01-28 14:56:25,516 | INFO | Chunk: 8 | WER=28.571429 | S=1 D=2 I=1
2026-01-28 14:56:25,519 | INFO | Chunk: 9 | WER=50.000000 | S=7 D=37 I=1
2026-01-28 14:56:25,522 | INFO | Chunk: 10 | WER=45.588235 | S=8 D=20 I=3
2026-01-28 14:56:25,522 | INFO | Chunk: 11 | WER=100.000000 | S=1 D=1 I=0
2026-01-28 14:56:25,524 | INFO | Chunk: 12 | WER=40.000000 | S=3 D=20 I=1
2026-01-28 14:56:25,524 | INFO | Chunk: 13 | WER=50.000000 | S=5 D=4 I=0
2026-01-28 14:56:25,524 | INFO | Chunk: 14 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 14:56:25,525 | INFO | Chunk: 15 | WER=18.181818 | S=1 D=1 I=0
2026-01-28 14:56:25,613 | INFO | File: Rhap-D1002.wav | WER=43.220339 | S=35 D=160 I=9
2026-01-28 14:56:25,614 | INFO | ------------------------------
2026-01-28 14:56:25,614 | INFO | whisper large Done!
2026-01-28 14:56:25,796 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 14:56:25,834 | INFO | Vocabulary size: 350
2026-01-28 14:56:26,502 | INFO | Gradient checkpoint layers: []
2026-01-28 14:56:27,345 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 14:56:27,350 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 14:56:27,350 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 14:56:27,350 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 14:56:27,351 | INFO | speech length: 47040
2026-01-28 14:56:27,389 | INFO | decoder input length: 73
2026-01-28 14:56:27,389 | INFO | max output length: 73
2026-01-28 14:56:27,390 | INFO | min output length: 7
2026-01-28 14:56:31,533 | INFO | end detected at 45
2026-01-28 14:56:31,535 | INFO |  -7.26 * 0.5 =  -3.63 for decoder
2026-01-28 14:56:31,535 | INFO |  -9.21 * 0.5 =  -4.60 for ctc
2026-01-28 14:56:31,535 | INFO | total log probability: -8.24
2026-01-28 14:56:31,535 | INFO | normalized log probability: -0.22
2026-01-28 14:56:31,535 | INFO | total number of ended hypotheses: 190
2026-01-28 14:56:31,536 | INFO | best hypo: ▁pourquoi▁il▁réponsa▁déjà▁est▁ce▁que▁c'est▁vraiment▁tout▁à▁fait▁sérieux▁que▁pour▁répondre

2026-01-28 14:56:31,539 | INFO | speech length: 332000
2026-01-28 14:56:31,582 | INFO | decoder input length: 518
2026-01-28 14:56:31,582 | INFO | max output length: 518
2026-01-28 14:56:31,582 | INFO | min output length: 51
2026-01-28 14:56:56,717 | INFO | end detected at 170
2026-01-28 14:56:56,718 | INFO | -347.44 * 0.5 = -173.72 for decoder
2026-01-28 14:56:56,718 | INFO | -59.93 * 0.5 = -29.97 for ctc
2026-01-28 14:56:56,718 | INFO | total log probability: -203.68
2026-01-28 14:56:56,718 | INFO | normalized log probability: -1.23
2026-01-28 14:56:56,718 | INFO | total number of ended hypotheses: 161
2026-01-28 14:56:56,720 | INFO | best hypo: ▁c'est▁il▁a▁pas▁trop▁envie▁de▁répondre▁voyez▁non▁le▁il▁n'a▁pas▁de▁définition▁forcément▁de▁scélar▁à▁l'époque▁c'est▁pas▁ça▁fait▁pas▁partie▁de▁ses▁artistes▁très▁théoriciens▁et▁il▁a▁il▁y▁a▁une▁approche▁directe▁de▁ce▁qu'il▁fait▁mais▁il▁n'a▁pas▁beaucoup▁thépisé▁ce▁qu'il▁a▁fait▁par▁contre▁quand▁même▁s'il▁ne▁c'est▁une▁référence▁c'est▁une▁petite▁référence▁philosophie

2026-01-28 14:56:56,723 | INFO | speech length: 34720
2026-01-28 14:56:56,761 | INFO | decoder input length: 53
2026-01-28 14:56:56,761 | INFO | max output length: 53
2026-01-28 14:56:56,761 | INFO | min output length: 5
2026-01-28 14:56:58,844 | INFO | end detected at 23
2026-01-28 14:56:58,846 | INFO |  -5.15 * 0.5 =  -2.57 for decoder
2026-01-28 14:56:58,846 | INFO |  -4.75 * 0.5 =  -2.38 for ctc
2026-01-28 14:56:58,846 | INFO | total log probability: -4.95
2026-01-28 14:56:58,846 | INFO | normalized log probability: -0.29
2026-01-28 14:56:58,846 | INFO | total number of ended hypotheses: 188
2026-01-28 14:56:58,847 | INFO | best hypo: ▁et▁un▁philosophe▁qu'a▁dit

2026-01-28 14:56:58,849 | INFO | speech length: 99360
2026-01-28 14:56:58,886 | INFO | decoder input length: 154
2026-01-28 14:56:58,886 | INFO | max output length: 154
2026-01-28 14:56:58,887 | INFO | min output length: 15
2026-01-28 14:57:04,907 | INFO | end detected at 59
2026-01-28 14:57:04,909 | INFO | -10.12 * 0.5 =  -5.06 for decoder
2026-01-28 14:57:04,909 | INFO | -18.42 * 0.5 =  -9.21 for ctc
2026-01-28 14:57:04,909 | INFO | total log probability: -14.27
2026-01-28 14:57:04,909 | INFO | normalized log probability: -0.28
2026-01-28 14:57:04,909 | INFO | total number of ended hypotheses: 150
2026-01-28 14:57:04,910 | INFO | best hypo: ▁qui▁a▁fait▁l'éloge▁de▁la▁grande▁santé▁et▁qui▁a▁dit▁notamment▁l'art▁saluait▁du▁côté▁de▁la▁grande▁santé▁se▁dit▁le▁tourisme

2026-01-28 14:57:04,912 | INFO | speech length: 214080
2026-01-28 14:57:04,958 | INFO | decoder input length: 334
2026-01-28 14:57:04,958 | INFO | max output length: 334
2026-01-28 14:57:04,958 | INFO | min output length: 33
2026-01-28 14:57:16,459 | INFO | end detected at 92
2026-01-28 14:57:16,461 | INFO | -69.76 * 0.5 = -34.88 for decoder
2026-01-28 14:57:16,461 | INFO | -27.74 * 0.5 = -13.87 for ctc
2026-01-28 14:57:16,461 | INFO | total log probability: -48.75
2026-01-28 14:57:16,461 | INFO | normalized log probability: -0.58
2026-01-28 14:57:16,461 | INFO | total number of ended hypotheses: 187
2026-01-28 14:57:16,462 | INFO | best hypo: ▁nietzsche▁le▁philosophe▁allemand▁parle▁de▁à▁une▁définition▁un▁peu▁uniquement▁de▁l'art▁mais▁notamment▁de▁l'art▁comme▁quelque▁chose▁qui▁serait▁du▁côté▁leur▁versité▁il▁dise▁sur▁toute▁la▁grande▁sens

2026-01-28 14:57:16,465 | INFO | speech length: 185920
2026-01-28 14:57:16,507 | INFO | decoder input length: 290
2026-01-28 14:57:16,507 | INFO | max output length: 290
2026-01-28 14:57:16,507 | INFO | min output length: 29
2026-01-28 14:57:24,839 | INFO | end detected at 69
2026-01-28 14:57:24,841 | INFO | -17.49 * 0.5 =  -8.74 for decoder
2026-01-28 14:57:24,841 | INFO | -13.32 * 0.5 =  -6.66 for ctc
2026-01-28 14:57:24,841 | INFO | total log probability: -15.40
2026-01-28 14:57:24,841 | INFO | normalized log probability: -0.24
2026-01-28 14:57:24,841 | INFO | total number of ended hypotheses: 168
2026-01-28 14:57:24,842 | INFO | best hypo: ▁pour▁dire▁quoi▁pour▁principalement▁pour▁résumer▁assez▁vite▁s'opposer▁à▁cette▁idée▁de▁l'art▁cette▁idée▁qu'on▁a▁déjà▁vu▁un▁ensemble

2026-01-28 14:57:24,844 | INFO | speech length: 66400
2026-01-28 14:57:24,879 | INFO | decoder input length: 103
2026-01-28 14:57:24,879 | INFO | max output length: 103
2026-01-28 14:57:24,879 | INFO | min output length: 10
2026-01-28 14:57:28,623 | INFO | end detected at 41
2026-01-28 14:57:28,626 | INFO |  -5.15 * 0.5 =  -2.57 for decoder
2026-01-28 14:57:28,626 | INFO |  -5.09 * 0.5 =  -2.55 for ctc
2026-01-28 14:57:28,626 | INFO | total log probability: -5.12
2026-01-28 14:57:28,626 | INFO | normalized log probability: -0.14
2026-01-28 14:57:28,626 | INFO | total number of ended hypotheses: 182
2026-01-28 14:57:28,626 | INFO | best hypo: ▁de▁cette▁idée▁qui▁s'est▁largement▁développée▁au▁dix▁neuvième▁siècle▁rome▁antique

2026-01-28 14:57:28,629 | INFO | speech length: 429440
2026-01-28 14:57:28,666 | INFO | decoder input length: 670
2026-01-28 14:57:28,666 | INFO | max output length: 670
2026-01-28 14:57:28,666 | INFO | min output length: 67
2026-01-28 14:57:59,761 | INFO | end detected at 187
2026-01-28 14:57:59,763 | INFO | -383.47 * 0.5 = -191.73 for decoder
2026-01-28 14:57:59,763 | INFO | -73.96 * 0.5 = -36.98 for ctc
2026-01-28 14:57:59,763 | INFO | total log probability: -228.71
2026-01-28 14:57:59,763 | INFO | normalized log probability: -1.27
2026-01-28 14:57:59,763 | INFO | total number of ended hypotheses: 156
2026-01-28 14:57:59,766 | INFO | best hypo: ▁il▁a▁continué▁par▁la▁suite▁cette▁idée▁clare▁et▁du▁côté▁de▁la▁souffrance▁du▁malheur▁des▁tourments▁intérieurs▁de▁l'artiste▁qui▁doivent▁exploser▁comme▁ça▁dans▁la▁société▁quitte▁de▁toute▁fa▁son▁et▁contre▁niche▁dit▁tous▁ça▁si▁une▁conception▁maladive▁c'est▁le▁c'est▁la▁conception▁maladive▁de▁la▁création▁ça▁il▁oppose▁un▁art▁qui▁serait▁du▁côté▁de▁la▁grande▁santé▁qui▁serait▁du▁côté▁que▁de▁l'a▁affirm▁et▁pas▁de▁la▁négation

2026-01-28 14:57:59,769 | INFO | speech length: 273760
2026-01-28 14:57:59,819 | INFO | decoder input length: 427
2026-01-28 14:57:59,819 | INFO | max output length: 427
2026-01-28 14:57:59,819 | INFO | min output length: 42
2026-01-28 14:58:17,334 | INFO | end detected at 125
2026-01-28 14:58:17,336 | INFO | -267.04 * 0.5 = -133.52 for decoder
2026-01-28 14:58:17,336 | INFO | -87.57 * 0.5 = -43.78 for ctc
2026-01-28 14:58:17,336 | INFO | total log probability: -177.30
2026-01-28 14:58:17,336 | INFO | normalized log probability: -1.50
2026-01-28 14:58:17,336 | INFO | total number of ended hypotheses: 152
2026-01-28 14:58:17,338 | INFO | best hypo: ▁c'est▁ça▁qui▁veut▁dire▁une▁plaince▁qui▁veut▁dire▁c'est▁que▁y'a▁rien▁de▁morbide▁là▁dedans▁et▁donc▁y▁compris▁ce▁qui▁faudrait▁dire▁c'est▁toutes▁ses▁interprétations'▁sur▁la▁mortte▁à▁la▁fin'un▁qui▁dise▁sire▁cette▁trace▁ce▁les▁traces'un▁corps▁mort▁qui▁va▁mourir

2026-01-28 14:58:17,340 | INFO | speech length: 8800
2026-01-28 14:58:17,373 | INFO | decoder input length: 13
2026-01-28 14:58:17,373 | INFO | max output length: 13
2026-01-28 14:58:17,374 | INFO | min output length: 1
2026-01-28 14:58:18,105 | INFO | end detected at 9
2026-01-28 14:58:18,106 | INFO |  -0.36 * 0.5 =  -0.18 for decoder
2026-01-28 14:58:18,106 | INFO |  -1.05 * 0.5 =  -0.53 for ctc
2026-01-28 14:58:18,106 | INFO | total log probability: -0.71
2026-01-28 14:58:18,107 | INFO | normalized log probability: -0.14
2026-01-28 14:58:18,107 | INFO | total number of ended hypotheses: 155
2026-01-28 14:58:18,107 | INFO | best hypo: ▁c'est

2026-01-28 14:58:18,109 | INFO | speech length: 240320
2026-01-28 14:58:18,149 | INFO | decoder input length: 375
2026-01-28 14:58:18,149 | INFO | max output length: 375
2026-01-28 14:58:18,149 | INFO | min output length: 37
2026-01-28 14:58:33,996 | INFO | end detected at 120
2026-01-28 14:58:33,998 | INFO | -135.85 * 0.5 = -67.92 for decoder
2026-01-28 14:58:33,998 | INFO | -36.52 * 0.5 = -18.26 for ctc
2026-01-28 14:58:33,998 | INFO | total log probability: -86.19
2026-01-28 14:58:33,998 | INFO | normalized log probability: -0.75
2026-01-28 14:58:33,998 | INFO | total number of ended hypotheses: 177
2026-01-28 14:58:34,000 | INFO | best hypo: ▁un▁ty▁peu▁du▁contre▁sens▁par▁rapport▁à▁ce▁qu'heure▁yves▁clin▁n'a▁pas▁toujours▁été▁très▁clair▁avec▁ça▁mais▁en▁tout▁cas▁pour▁lui▁c'est▁plutôt▁un▁jeu▁ainsi▁du▁côté▁de▁l'affirmation▁ou▁jeu▁de▁la▁performance▁et▁il▁est▁pas▁tellement▁dans▁une▁réflexion▁sur▁la▁trace▁du▁corps▁en▁mort

2026-01-28 14:58:34,003 | INFO | speech length: 80960
2026-01-28 14:58:34,044 | INFO | decoder input length: 126
2026-01-28 14:58:34,045 | INFO | max output length: 126
2026-01-28 14:58:34,045 | INFO | min output length: 12
2026-01-28 14:58:39,342 | INFO | end detected at 50
2026-01-28 14:58:39,343 | INFO |  -5.98 * 0.5 =  -2.99 for decoder
2026-01-28 14:58:39,343 | INFO | -13.97 * 0.5 =  -6.99 for ctc
2026-01-28 14:58:39,343 | INFO | total log probability: -9.98
2026-01-28 14:58:39,343 | INFO | normalized log probability: -0.25
2026-01-28 14:58:39,343 | INFO | total number of ended hypotheses: 165
2026-01-28 14:58:39,344 | INFO | best hypo: ▁donc▁quand▁il▁réponsa▁on▁peut▁comprendre▁on▁peut▁comprendre▁l'affirmation▁de▁l'artiste

2026-01-28 14:58:39,346 | INFO | speech length: 24960
2026-01-28 14:58:39,377 | INFO | decoder input length: 38
2026-01-28 14:58:39,377 | INFO | max output length: 38
2026-01-28 14:58:39,377 | INFO | min output length: 3
2026-01-28 14:58:40,711 | INFO | end detected at 14
2026-01-28 14:58:40,712 | INFO |  -1.18 * 0.5 =  -0.59 for decoder
2026-01-28 14:58:40,713 | INFO |  -0.08 * 0.5 =  -0.04 for ctc
2026-01-28 14:58:40,713 | INFO | total log probability: -0.63
2026-01-28 14:58:40,717 | INFO | normalized log probability: -0.06
2026-01-28 14:58:40,717 | INFO | total number of ended hypotheses: 142
2026-01-28 14:58:40,717 | INFO | best hypo: ▁joyeux▁qui▁affirme

2026-01-28 14:58:40,723 | INFO | speech length: 36640
2026-01-28 14:58:40,819 | INFO | decoder input length: 56
2026-01-28 14:58:40,820 | INFO | max output length: 56
2026-01-28 14:58:40,820 | INFO | min output length: 5
2026-01-28 14:58:43,009 | INFO | end detected at 24
2026-01-28 14:58:43,010 | INFO |  -2.35 * 0.5 =  -1.18 for decoder
2026-01-28 14:58:43,010 | INFO |  -0.66 * 0.5 =  -0.33 for ctc
2026-01-28 14:58:43,010 | INFO | total log probability: -1.51
2026-01-28 14:58:43,011 | INFO | normalized log probability: -0.08
2026-01-28 14:58:43,011 | INFO | total number of ended hypotheses: 154
2026-01-28 14:58:43,011 | INFO | best hypo: ▁il▁n'est▁pas▁du▁côté▁de▁la▁maladie▁romantique

2026-01-28 14:58:43,018 | INFO | Chunk: 0 | WER=43.750000 | S=4 D=1 I=2
2026-01-28 14:58:43,022 | INFO | Chunk: 1 | WER=25.301205 | S=11 D=7 I=3
2026-01-28 14:58:43,022 | INFO | Chunk: 2 | WER=50.000000 | S=2 D=2 I=0
2026-01-28 14:58:43,023 | INFO | Chunk: 3 | WER=23.333333 | S=4 D=3 I=0
2026-01-28 14:58:43,024 | INFO | Chunk: 4 | WER=35.714286 | S=7 D=7 I=1
2026-01-28 14:58:43,025 | INFO | Chunk: 5 | WER=11.538462 | S=1 D=1 I=1
2026-01-28 14:58:43,026 | INFO | Chunk: 6 | WER=21.428571 | S=1 D=1 I=1
2026-01-28 14:58:43,030 | INFO | Chunk: 7 | WER=23.333333 | S=11 D=8 I=2
2026-01-28 14:58:43,033 | INFO | Chunk: 8 | WER=38.235294 | S=13 D=13 I=0
2026-01-28 14:58:43,033 | INFO | Chunk: 9 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 14:58:43,036 | INFO | Chunk: 10 | WER=18.333333 | S=7 D=2 I=2
2026-01-28 14:58:43,036 | INFO | Chunk: 11 | WER=33.333333 | S=3 D=3 I=0
2026-01-28 14:58:43,036 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 14:58:43,037 | INFO | Chunk: 13 | WER=18.181818 | S=1 D=1 I=0
2026-01-28 14:58:43,146 | INFO | File: Rhap-D1002.wav | WER=26.539278 | S=66 D=48 I=11
2026-01-28 14:58:43,146 | INFO | ------------------------------
2026-01-28 14:58:43,146 | INFO | Conf cv Done!
2026-01-28 14:58:43,338 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 14:58:43,361 | INFO | Vocabulary size: 47
2026-01-28 14:58:44,101 | INFO | Gradient checkpoint layers: []
2026-01-28 14:58:44,897 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 14:58:44,901 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 14:58:44,902 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 14:58:44,902 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 14:58:44,905 | INFO | speech length: 47040
2026-01-28 14:58:44,966 | INFO | decoder input length: 73
2026-01-28 14:58:44,967 | INFO | max output length: 73
2026-01-28 14:58:44,967 | INFO | min output length: 7
2026-01-28 14:58:51,446 | INFO | adding <eos> in the last position in the loop
2026-01-28 14:58:51,456 | INFO | no hypothesis. Finish decoding.
2026-01-28 14:58:51,456 | INFO | -27.74 * 0.5 = -13.87 for decoder
2026-01-28 14:58:51,456 | INFO | -102.95 * 0.5 = -51.47 for ctc
2026-01-28 14:58:51,456 | INFO | total log probability: -65.35
2026-01-28 14:58:51,456 | INFO | normalized log probability: -0.87
2026-01-28 14:58:51,456 | INFO | total number of ended hypotheses: 57
2026-01-28 14:58:51,457 | INFO | best hypo: pourquoi<space>y<space>répond<space>ça<space>est<space>ce<space>c'est<space>vraiment<space>ce<space>fait<space>sérieux<space>pour<space>répondre<sos/eos>

2026-01-28 14:58:51,458 | WARNING | best hypo length: 73 == max output length: 73
2026-01-28 14:58:51,458 | WARNING | decoding may be stopped by the max output length limitation, please consider to increase the maxlenratio.
2026-01-28 14:58:51,459 | INFO | speech length: 332000
2026-01-28 14:58:51,496 | INFO | decoder input length: 518
2026-01-28 14:58:51,496 | INFO | max output length: 518
2026-01-28 14:58:51,496 | INFO | min output length: 51
2026-01-28 14:59:44,649 | INFO | end detected at 403
2026-01-28 14:59:44,651 | INFO | -85.49 * 0.5 = -42.75 for decoder
2026-01-28 14:59:44,651 | INFO | -42.49 * 0.5 = -21.24 for ctc
2026-01-28 14:59:44,651 | INFO | total log probability: -63.99
2026-01-28 14:59:44,651 | INFO | normalized log probability: -0.16
2026-01-28 14:59:44,651 | INFO | total number of ended hypotheses: 216
2026-01-28 14:59:44,656 | INFO | best hypo: c'est<space>euh<space>il<space>a<space>pas<space>trop<space>envie<space>de<space>répondre<space>vous<space>voyez<space>bon<space>le<space>il<space>n'a<space>pas<space>de<space>définition<space>forcément<space>de<space>ce<space>qu'est<space>l'art<space>euh<space>c'est<space>pas<space>ou<space>c'est<space>pas<space>ça<space>fait<space>pas<space>partie<space>de<space>ces<space>artistes<space>très<space>théoriciens<space>rive<space>qu'un<space>il<space>a<space>il<space>a<space>une<space>approche<space>directe<space>de<space>ce<space>qu'il<space>fait<space>mais<space>il<space>a<space>pas<space>beaucoup<space>théorisé<space>ce<space>qu'il<space>a<space>fait<space>par<space>contre<space>quand<space>même<space>c'est<space>une<space>c'est<space>une<space>référence<space>à<space>c'est<space>une<space>petite<space>référence<space>philosophie

2026-01-28 14:59:44,659 | INFO | speech length: 34720
2026-01-28 14:59:44,702 | INFO | decoder input length: 53
2026-01-28 14:59:44,702 | INFO | max output length: 53
2026-01-28 14:59:44,702 | INFO | min output length: 5
2026-01-28 14:59:48,216 | INFO | end detected at 40
2026-01-28 14:59:48,218 | INFO |  -3.15 * 0.5 =  -1.57 for decoder
2026-01-28 14:59:48,218 | INFO |  -4.23 * 0.5 =  -2.11 for ctc
2026-01-28 14:59:48,218 | INFO | total log probability: -3.69
2026-01-28 14:59:48,219 | INFO | normalized log probability: -0.12
2026-01-28 14:59:48,219 | INFO | total number of ended hypotheses: 193
2026-01-28 14:59:48,219 | INFO | best hypo: et<space>un<space>philosophe<space>qui<space>a<space>dit<space>euh

2026-01-28 14:59:48,222 | INFO | speech length: 99360
2026-01-28 14:59:48,262 | INFO | decoder input length: 154
2026-01-28 14:59:48,262 | INFO | max output length: 154
2026-01-28 14:59:48,262 | INFO | min output length: 15
2026-01-28 15:00:00,960 | INFO | end detected at 136
2026-01-28 15:00:00,962 | INFO | -15.60 * 0.5 =  -7.80 for decoder
2026-01-28 15:00:00,962 | INFO | -19.18 * 0.5 =  -9.59 for ctc
2026-01-28 15:00:00,962 | INFO | total log probability: -17.39
2026-01-28 15:00:00,962 | INFO | normalized log probability: -0.14
2026-01-28 15:00:00,962 | INFO | total number of ended hypotheses: 186
2026-01-28 15:00:00,964 | INFO | best hypo: qui<space>a<space>fait<space>l'éloge<space>de<space>la<space>grande<space>santé<space>et<space>qui<space>a<space>dit<space>notamment<space>l'art<space>ça<space>doit<space>être<space>du<space>côté<space>de<space>la<space>grande<space>santé<space>c'est<space>à<space>dire

2026-01-28 15:00:00,967 | INFO | speech length: 214080
2026-01-28 15:00:01,003 | INFO | decoder input length: 334
2026-01-28 15:00:01,003 | INFO | max output length: 334
2026-01-28 15:00:01,003 | INFO | min output length: 33
2026-01-28 15:00:27,313 | INFO | end detected at 226
2026-01-28 15:00:27,315 | INFO | -36.98 * 0.5 = -18.49 for decoder
2026-01-28 15:00:27,315 | INFO | -38.06 * 0.5 = -19.03 for ctc
2026-01-28 15:00:27,315 | INFO | total log probability: -37.52
2026-01-28 15:00:27,315 | INFO | normalized log probability: -0.18
2026-01-28 15:00:27,315 | INFO | total number of ended hypotheses: 234
2026-01-28 15:00:27,318 | INFO | best hypo: euh<space>nitch<space>le<space>philosophe<space>allemand<space>parle<space>de<space>à<space>une<space>définition<space>d'une<space>euh<space>qui<space>alors<space>pas<space>uniquement<space>de<space>là<space>mais<space>notamment<space>de<space>l'art<space>comme<space>quelque<space>chose<space>qui<space>serait<space>du<space>côté<space>alors<space>qu'il<space>y<space>a<space>santé<space>sur<space>toute<space>la<space>grande<space>sante

2026-01-28 15:00:27,321 | INFO | speech length: 185920
2026-01-28 15:00:27,359 | INFO | decoder input length: 290
2026-01-28 15:00:27,359 | INFO | max output length: 290
2026-01-28 15:00:27,359 | INFO | min output length: 29
2026-01-28 15:00:46,056 | INFO | end detected at 155
2026-01-28 15:00:46,059 | INFO | -16.13 * 0.5 =  -8.07 for decoder
2026-01-28 15:00:46,060 | INFO | -14.89 * 0.5 =  -7.44 for ctc
2026-01-28 15:00:46,060 | INFO | total log probability: -15.51
2026-01-28 15:00:46,060 | INFO | normalized log probability: -0.11
2026-01-28 15:00:46,060 | INFO | total number of ended hypotheses: 217
2026-01-28 15:00:46,062 | INFO | best hypo: euh<space>pour<space>dire<space>quoi<space>sont<space>principalement<space>pour<space>résumer<space>assez<space>vite<space>leur<space>s'opposer<space>à<space>cette<space>idée<space>de<space>l'art<space>cette<space>idée<space>qu'on<space>a<space>déjà<space>vue<space>hein<space>ensemble<space>euh

2026-01-28 15:00:46,064 | INFO | speech length: 66400
2026-01-28 15:00:46,101 | INFO | decoder input length: 103
2026-01-28 15:00:46,101 | INFO | max output length: 103
2026-01-28 15:00:46,102 | INFO | min output length: 10
2026-01-28 15:00:54,384 | INFO | end detected at 87
2026-01-28 15:00:54,386 | INFO |  -7.62 * 0.5 =  -3.81 for decoder
2026-01-28 15:00:54,386 | INFO |  -3.79 * 0.5 =  -1.89 for ctc
2026-01-28 15:00:54,386 | INFO | total log probability: -5.70
2026-01-28 15:00:54,386 | INFO | normalized log probability: -0.07
2026-01-28 15:00:54,386 | INFO | total number of ended hypotheses: 199
2026-01-28 15:00:54,387 | INFO | best hypo: de<space>cette<space>idée<space>qui<space>s'est<space>largement<space>développée<space>au<space>dix<space>neuvième<space>siècle<space>en<space>antique

2026-01-28 15:00:54,390 | INFO | speech length: 429440
2026-01-28 15:00:54,428 | INFO | decoder input length: 670
2026-01-28 15:00:54,428 | INFO | max output length: 670
2026-01-28 15:00:54,428 | INFO | min output length: 67
2026-01-28 15:01:59,017 | INFO | end detected at 441
2026-01-28 15:01:59,019 | INFO | -192.48 * 0.5 = -96.24 for decoder
2026-01-28 15:01:59,019 | INFO | -14.27 * 0.5 =  -7.13 for ctc
2026-01-28 15:01:59,019 | INFO | total log probability: -103.37
2026-01-28 15:01:59,019 | INFO | normalized log probability: -0.24
2026-01-28 15:01:59,019 | INFO | total number of ended hypotheses: 172
2026-01-28 15:01:59,025 | INFO | best hypo: qui<space>a<space>continué<space>par<space>la<space>suite<space>cette<space>idée<space>que<space>là<space>est<space>du<space>côté<space>de<space>la<space>souffrance<space>du<space>malheur<space>de<space>des<space>tournants<space>intérieurs<space>de<space>l'artiste<space>qui<space>doivent<space>exploser<space>comme<space>ça<space>dans<space>la<space>société<space>qui<space>de<space>toute<space>façon<space>est<space>contre<space>euh<space>mixique<space>tout<space>ça<space>c'est<space>une<space>conception<space>maladive<space>c'est<space>le<space>c'est<space>la<space>conception<space>maladive<space>de<space>la<space>création<space>à<space>ça<space>quil<space>oppose<space>euh<space>un<space>art<space>qui<space>serait<space>du<space>côté<space>de<space>la<space>grande<space>santé<space>qui<space>serait<space>du<space>côté<space>que<space>de<space>l'affirmation<space>et<space>pas<space>de<space>la<space>négation

2026-01-28 15:01:59,028 | INFO | speech length: 273760
2026-01-28 15:01:59,063 | INFO | decoder input length: 427
2026-01-28 15:01:59,064 | INFO | max output length: 427
2026-01-28 15:01:59,064 | INFO | min output length: 42
2026-01-28 15:02:35,905 | INFO | end detected at 300
2026-01-28 15:02:35,908 | INFO | -32.74 * 0.5 = -16.37 for decoder
2026-01-28 15:02:35,908 | INFO | -22.42 * 0.5 = -11.21 for ctc
2026-01-28 15:02:35,908 | INFO | total log probability: -27.58
2026-01-28 15:02:35,908 | INFO | normalized log probability: -0.10
2026-01-28 15:02:35,908 | INFO | total number of ended hypotheses: 237
2026-01-28 15:02:35,912 | INFO | best hypo: euh<space>c'est<space>ça<space>hein<space>qui<space>veut<space>dire<space>les<space>flins<space>ce<space>qui<space>veut<space>dire<space>c'est<space>que<space>y<space>a<space>rien<space>de<space>morbide<space>là<space>dedans<space>et<space>donc<space>y<space>compris<space>euh<space>le<space>ce<space>qu'il<space>faudrait<space>dire<space>c'est<space>que<space>toutes<space>ces<space>interprétations<space>sur<space>la<space>mort<space>à<space>la<space>fin<space>qui<space>disent<space>oui<space>euh<space>euh<space>cette<space>trace<space>et<space>les<space>traces<space>d'un<space>corps<space>mort<space>ou<space>qui<space>va<space>mourir

2026-01-28 15:02:35,916 | INFO | speech length: 8800
2026-01-28 15:02:35,948 | INFO | decoder input length: 13
2026-01-28 15:02:35,949 | INFO | max output length: 13
2026-01-28 15:02:35,949 | INFO | min output length: 1
2026-01-28 15:02:36,813 | INFO | end detected at 11
2026-01-28 15:02:36,814 | INFO |  -0.49 * 0.5 =  -0.24 for decoder
2026-01-28 15:02:36,814 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 15:02:36,814 | INFO | total log probability: -0.25
2026-01-28 15:02:36,815 | INFO | normalized log probability: -0.04
2026-01-28 15:02:36,815 | INFO | total number of ended hypotheses: 134
2026-01-28 15:02:36,815 | INFO | best hypo: c'est

2026-01-28 15:02:36,834 | INFO | speech length: 240320
2026-01-28 15:02:36,876 | INFO | decoder input length: 375
2026-01-28 15:02:36,876 | INFO | max output length: 375
2026-01-28 15:02:36,876 | INFO | min output length: 37
2026-01-28 15:03:09,759 | INFO | end detected at 291
2026-01-28 15:03:09,761 | INFO | -28.50 * 0.5 = -14.25 for decoder
2026-01-28 15:03:09,761 | INFO |  -4.06 * 0.5 =  -2.03 for ctc
2026-01-28 15:03:09,761 | INFO | total log probability: -16.28
2026-01-28 15:03:09,761 | INFO | normalized log probability: -0.06
2026-01-28 15:03:09,761 | INFO | total number of ended hypotheses: 209
2026-01-28 15:03:09,765 | INFO | best hypo: un<space>petit<space>peu<space>du<space>contresens<space>par<space>rapport<space>à<space>ce<space>que<space>alors<space>yves<space>cle<space>n'a<space>pas<space>toujours<space>été<space>très<space>clair<space>avec<space>ça<space>mais<space>en<space>tout<space>cas<space>pour<space>lui<space>c'est<space>plutôt<space>un<space>jeu<space>hein<space>c'est<space>du<space>côté<space>de<space>l'affirmation<space>du<space>jeu<space>de<space>la<space>performance<space>et<space>il<space>est<space>pas<space>tellement<space>dans<space>une<space>réflexion<space>sur<space>la<space>trace<space>du<space>corps<space>mort

2026-01-28 15:03:09,768 | INFO | speech length: 80960
2026-01-28 15:03:09,811 | INFO | decoder input length: 126
2026-01-28 15:03:09,811 | INFO | max output length: 126
2026-01-28 15:03:09,811 | INFO | min output length: 12
2026-01-28 15:03:20,634 | INFO | end detected at 115
2026-01-28 15:03:20,635 | INFO | -16.28 * 0.5 =  -8.14 for decoder
2026-01-28 15:03:20,635 | INFO | -14.47 * 0.5 =  -7.23 for ctc
2026-01-28 15:03:20,635 | INFO | total log probability: -15.37
2026-01-28 15:03:20,635 | INFO | normalized log probability: -0.14
2026-01-28 15:03:20,635 | INFO | total number of ended hypotheses: 170
2026-01-28 15:03:20,637 | INFO | best hypo: euh<space>donc<space>quand<space>les<space>réponds<space>ça<space>on<space>peut<space>comprendre<space>euh<space>on<space>se<space>comprend<space>mitch<space>c'est<space>le<space>l'affirmation<space>de<space>l'artiste

2026-01-28 15:03:20,640 | INFO | speech length: 24960
2026-01-28 15:03:20,695 | INFO | decoder input length: 38
2026-01-28 15:03:20,696 | INFO | max output length: 38
2026-01-28 15:03:20,696 | INFO | min output length: 3
2026-01-28 15:03:23,015 | INFO | end detected at 27
2026-01-28 15:03:23,017 | INFO |  -1.99 * 0.5 =  -0.99 for decoder
2026-01-28 15:03:23,017 | INFO |  -0.69 * 0.5 =  -0.35 for ctc
2026-01-28 15:03:23,017 | INFO | total log probability: -1.34
2026-01-28 15:03:23,017 | INFO | normalized log probability: -0.07
2026-01-28 15:03:23,017 | INFO | total number of ended hypotheses: 193
2026-01-28 15:03:23,018 | INFO | best hypo: joyeux<space>qui<space>affirme

2026-01-28 15:03:23,020 | INFO | speech length: 36640
2026-01-28 15:03:23,058 | INFO | decoder input length: 56
2026-01-28 15:03:23,058 | INFO | max output length: 56
2026-01-28 15:03:23,058 | INFO | min output length: 5
2026-01-28 15:03:27,679 | INFO | adding <eos> in the last position in the loop
2026-01-28 15:03:27,689 | INFO | end detected at 55
2026-01-28 15:03:27,691 | INFO |  -4.86 * 0.5 =  -2.43 for decoder
2026-01-28 15:03:27,691 | INFO |  -3.15 * 0.5 =  -1.57 for ctc
2026-01-28 15:03:27,691 | INFO | total log probability: -4.01
2026-01-28 15:03:27,691 | INFO | normalized log probability: -0.08
2026-01-28 15:03:27,691 | INFO | total number of ended hypotheses: 193
2026-01-28 15:03:27,692 | INFO | best hypo: il<space>n'est<space>pas<space>du<space>côté<space>de<space>la<space>maladie<space>euh<space>romantique

2026-01-28 15:03:27,699 | INFO | Chunk: 0 | WER=56.250000 | S=5 D=2 I=2
2026-01-28 15:03:27,703 | INFO | Chunk: 1 | WER=12.048193 | S=5 D=0 I=5
2026-01-28 15:03:27,704 | INFO | Chunk: 2 | WER=50.000000 | S=1 D=2 I=1
2026-01-28 15:03:27,704 | INFO | Chunk: 3 | WER=16.666667 | S=4 D=1 I=0
2026-01-28 15:03:27,706 | INFO | Chunk: 4 | WER=42.857143 | S=8 D=5 I=5
2026-01-28 15:03:27,707 | INFO | Chunk: 5 | WER=19.230769 | S=2 D=0 I=3
2026-01-28 15:03:27,707 | INFO | Chunk: 6 | WER=21.428571 | S=1 D=1 I=1
2026-01-28 15:03:27,712 | INFO | Chunk: 7 | WER=13.333333 | S=7 D=4 I=1
2026-01-28 15:03:27,715 | INFO | Chunk: 8 | WER=26.470588 | S=9 D=6 I=3
2026-01-28 15:03:27,715 | INFO | Chunk: 9 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:03:27,717 | INFO | Chunk: 10 | WER=5.000000 | S=1 D=1 I=1
2026-01-28 15:03:27,718 | INFO | Chunk: 11 | WER=61.111111 | S=7 D=0 I=4
2026-01-28 15:03:27,718 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:03:27,718 | INFO | Chunk: 13 | WER=27.272727 | S=1 D=1 I=1
2026-01-28 15:03:27,852 | INFO | File: Rhap-D1002.wav | WER=21.019108 | S=53 D=21 I=25
2026-01-28 15:03:27,852 | INFO | ------------------------------
2026-01-28 15:03:27,852 | INFO | Conf ester Done!
2026-01-28 15:06:15,606 | INFO | Chunk: 0 | WER=50.000000 | S=5 D=3 I=0
2026-01-28 15:06:15,612 | INFO | Chunk: 1 | WER=25.301205 | S=6 D=10 I=5
2026-01-28 15:06:15,612 | INFO | Chunk: 2 | WER=75.000000 | S=2 D=4 I=0
2026-01-28 15:06:15,612 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:06:15,614 | INFO | Chunk: 4 | WER=26.666667 | S=1 D=7 I=0
2026-01-28 15:06:15,614 | INFO | Chunk: 5 | WER=200.000000 | S=1 D=0 I=1
2026-01-28 15:06:15,615 | INFO | Chunk: 6 | WER=59.523810 | S=12 D=12 I=1
2026-01-28 15:06:15,617 | INFO | Chunk: 7 | WER=26.923077 | S=3 D=1 I=3
2026-01-28 15:06:15,617 | INFO | Chunk: 8 | WER=21.428571 | S=2 D=1 I=0
2026-01-28 15:06:15,624 | INFO | Chunk: 9 | WER=14.444444 | S=4 D=9 I=0
2026-01-28 15:06:15,628 | INFO | Chunk: 10 | WER=27.941176 | S=8 D=9 I=2
2026-01-28 15:06:15,628 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:06:15,631 | INFO | Chunk: 12 | WER=21.666667 | S=8 D=3 I=2
2026-01-28 15:06:15,632 | INFO | Chunk: 13 | WER=61.111111 | S=8 D=3 I=0
2026-01-28 15:06:15,632 | INFO | Chunk: 14 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:06:15,633 | INFO | Chunk: 15 | WER=36.363636 | S=1 D=3 I=0
2026-01-28 15:06:15,781 | INFO | File: Rhap-D1002.wav | WER=29.449153 | S=62 D=64 I=13
2026-01-28 15:06:15,781 | INFO | ------------------------------
2026-01-28 15:06:15,781 | INFO | hmm_tdnn Done!
2026-01-28 15:06:16,054 | INFO | ==================================Rhap-D1003.wav=========================================
2026-01-28 15:06:16,055 | WARNING | The file /vol/corpora/Rhapsodie/wav16k_corrected/Rhap-D1003.wav does not exist
2026-01-28 15:06:16,055 | INFO | ==================================Rhap-D2001.wav=========================================
2026-01-28 15:06:16,396 | INFO | Using rVAD model
2026-01-28 15:06:50,163 | INFO | Chunk: 0 | WER=4.761905 | S=1 D=0 I=0
2026-01-28 15:06:50,163 | INFO | Chunk: 1 | WER=7.692308 | S=0 D=0 I=1
2026-01-28 15:06:50,164 | INFO | Chunk: 2 | WER=13.333333 | S=1 D=1 I=0
2026-01-28 15:06:50,166 | INFO | Chunk: 3 | WER=4.000000 | S=1 D=1 I=0
2026-01-28 15:06:50,166 | INFO | Chunk: 4 | WER=6.666667 | S=1 D=1 I=0
2026-01-28 15:06:50,167 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:06:50,168 | INFO | Chunk: 6 | WER=25.000000 | S=2 D=1 I=0
2026-01-28 15:06:50,168 | INFO | Chunk: 7 | WER=5.263158 | S=0 D=1 I=0
2026-01-28 15:06:50,172 | INFO | Chunk: 8 | WER=9.876543 | S=3 D=5 I=0
2026-01-28 15:06:50,174 | INFO | Chunk: 9 | WER=13.461538 | S=6 D=1 I=0
2026-01-28 15:06:50,176 | INFO | Chunk: 10 | WER=4.000000 | S=1 D=1 I=0
2026-01-28 15:06:50,176 | INFO | Chunk: 11 | WER=11.764706 | S=2 D=0 I=0
2026-01-28 15:06:50,179 | INFO | Chunk: 12 | WER=31.884058 | S=15 D=7 I=0
2026-01-28 15:06:50,180 | INFO | Chunk: 13 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:06:50,181 | INFO | Chunk: 14 | WER=46.153846 | S=6 D=6 I=0
2026-01-28 15:06:50,181 | INFO | Chunk: 15 | WER=18.181818 | S=1 D=1 I=0
2026-01-28 15:06:50,181 | INFO | Chunk: 16 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:06:50,184 | INFO | Chunk: 17 | WER=15.714286 | S=3 D=8 I=0
2026-01-28 15:06:50,184 | INFO | Chunk: 18 | WER=42.857143 | S=1 D=0 I=2
2026-01-28 15:06:50,185 | INFO | Chunk: 19 | WER=18.750000 | S=2 D=1 I=0
2026-01-28 15:06:50,185 | INFO | Chunk: 20 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:06:50,185 | INFO | Chunk: 21 | WER=20.000000 | S=0 D=1 I=0
2026-01-28 15:06:50,187 | INFO | Chunk: 22 | WER=5.882353 | S=1 D=1 I=0
2026-01-28 15:06:50,187 | INFO | Chunk: 23 | WER=28.571429 | S=8 D=0 I=0
2026-01-28 15:06:50,194 | INFO | Chunk: 24 | WER=13.636364 | S=5 D=7 I=3
2026-01-28 15:06:50,203 | INFO | Chunk: 25 | WER=22.689076 | S=16 D=11 I=0
2026-01-28 15:06:50,210 | INFO | Chunk: 26 | WER=12.068966 | S=5 D=7 I=2
2026-01-28 15:06:50,214 | INFO | Chunk: 27 | WER=5.128205 | S=3 D=1 I=0
2026-01-28 15:06:50,216 | INFO | Chunk: 28 | WER=10.526316 | S=2 D=3 I=1
2026-01-28 15:06:50,217 | INFO | Chunk: 29 | WER=18.181818 | S=1 D=1 I=0
2026-01-28 15:06:50,218 | INFO | Chunk: 30 | WER=11.111111 | S=3 D=0 I=0
2026-01-28 15:06:50,226 | INFO | Chunk: 31 | WER=20.312500 | S=12 D=13 I=1
2026-01-28 15:06:50,227 | INFO | Chunk: 32 | WER=28.000000 | S=3 D=4 I=0
2026-01-28 15:06:50,232 | INFO | Chunk: 33 | WER=40.566038 | S=19 D=24 I=0
2026-01-28 15:06:50,239 | INFO | Chunk: 34 | WER=15.929204 | S=7 D=11 I=0
2026-01-28 15:06:50,239 | INFO | Chunk: 35 | WER=22.222222 | S=2 D=1 I=1
2026-01-28 15:06:50,241 | INFO | Chunk: 36 | WER=21.276596 | S=4 D=5 I=1
2026-01-28 15:06:50,242 | INFO | Chunk: 37 | WER=12.000000 | S=1 D=2 I=0
2026-01-28 15:06:50,242 | INFO | Chunk: 38 | WER=100.000000 | S=1 D=0 I=0
2026-01-28 15:06:50,249 | INFO | Chunk: 39 | WER=15.929204 | S=11 D=7 I=0
2026-01-28 15:06:50,250 | INFO | Chunk: 40 | WER=8.333333 | S=1 D=3 I=0
2026-01-28 15:06:50,253 | INFO | Chunk: 41 | WER=26.229508 | S=2 D=14 I=0
2026-01-28 15:06:50,253 | INFO | Chunk: 42 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:06:50,254 | INFO | Chunk: 43 | WER=3.703704 | S=1 D=0 I=0
2026-01-28 15:06:52,180 | INFO | File: Rhap-D2001.wav | WER=16.416365 | S=154 D=151 I=12
2026-01-28 15:06:52,181 | INFO | ------------------------------
2026-01-28 15:06:52,181 | INFO | w2vec vad chunk Done!
2026-01-28 15:08:00,875 | INFO | Chunk: 0 | WER=4.761905 | S=1 D=0 I=0
2026-01-28 15:08:00,876 | INFO | Chunk: 1 | WER=7.692308 | S=1 D=0 I=0
2026-01-28 15:08:00,877 | INFO | Chunk: 2 | WER=20.000000 | S=2 D=0 I=1
2026-01-28 15:08:00,878 | INFO | Chunk: 3 | WER=12.000000 | S=1 D=5 I=0
2026-01-28 15:08:00,879 | INFO | Chunk: 4 | WER=6.666667 | S=2 D=0 I=0
2026-01-28 15:08:00,880 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:08:00,880 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:08:00,881 | INFO | Chunk: 7 | WER=5.263158 | S=0 D=1 I=0
2026-01-28 15:08:00,883 | INFO | Chunk: 8 | WER=74.074074 | S=0 D=60 I=0
2026-01-28 15:08:00,884 | INFO | Chunk: 9 | WER=46.153846 | S=4 D=20 I=0
2026-01-28 15:08:00,886 | INFO | Chunk: 10 | WER=10.000000 | S=3 D=2 I=0
2026-01-28 15:08:00,886 | INFO | Chunk: 11 | WER=17.647059 | S=1 D=1 I=1
2026-01-28 15:08:00,888 | INFO | Chunk: 12 | WER=76.811594 | S=0 D=53 I=0
2026-01-28 15:08:00,888 | INFO | Chunk: 13 | WER=13.333333 | S=2 D=0 I=0
2026-01-28 15:08:00,889 | INFO | Chunk: 14 | WER=42.307692 | S=5 D=6 I=0
2026-01-28 15:08:00,889 | INFO | Chunk: 15 | WER=45.454545 | S=1 D=4 I=0
2026-01-28 15:08:00,889 | INFO | Chunk: 16 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:08:00,891 | INFO | Chunk: 17 | WER=51.428571 | S=0 D=36 I=0
2026-01-28 15:08:00,891 | INFO | Chunk: 18 | WER=28.571429 | S=1 D=1 I=0
2026-01-28 15:08:00,892 | INFO | Chunk: 19 | WER=6.250000 | S=1 D=0 I=0
2026-01-28 15:08:00,892 | INFO | Chunk: 20 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:08:00,892 | INFO | Chunk: 21 | WER=40.000000 | S=1 D=1 I=0
2026-01-28 15:08:00,893 | INFO | Chunk: 22 | WER=14.705882 | S=3 D=2 I=0
2026-01-28 15:08:00,894 | INFO | Chunk: 23 | WER=28.571429 | S=8 D=0 I=0
2026-01-28 15:08:00,898 | INFO | Chunk: 24 | WER=55.454545 | S=4 D=56 I=1
2026-01-28 15:08:00,902 | INFO | Chunk: 25 | WER=65.546218 | S=2 D=76 I=0
2026-01-28 15:08:00,905 | INFO | Chunk: 26 | WER=75.000000 | S=3 D=83 I=1
2026-01-28 15:08:00,906 | INFO | Chunk: 27 | WER=85.897436 | S=3 D=64 I=0
2026-01-28 15:08:00,908 | INFO | Chunk: 28 | WER=31.578947 | S=5 D=12 I=1
2026-01-28 15:08:00,908 | INFO | Chunk: 29 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:08:00,909 | INFO | Chunk: 30 | WER=22.222222 | S=4 D=2 I=0
2026-01-28 15:08:00,912 | INFO | Chunk: 31 | WER=78.125000 | S=2 D=97 I=1
2026-01-28 15:08:00,913 | INFO | Chunk: 32 | WER=28.000000 | S=2 D=5 I=0
2026-01-28 15:08:00,916 | INFO | Chunk: 33 | WER=66.981132 | S=6 D=65 I=0
2026-01-28 15:08:00,919 | INFO | Chunk: 34 | WER=63.716814 | S=4 D=68 I=0
2026-01-28 15:08:00,920 | INFO | Chunk: 35 | WER=22.222222 | S=1 D=2 I=1
2026-01-28 15:08:00,921 | INFO | Chunk: 36 | WER=57.446809 | S=4 D=23 I=0
2026-01-28 15:08:00,921 | INFO | Chunk: 37 | WER=16.000000 | S=2 D=2 I=0
2026-01-28 15:08:00,922 | INFO | Chunk: 38 | WER=100.000000 | S=0 D=0 I=1
2026-01-28 15:08:00,923 | INFO | Chunk: 39 | WER=90.265487 | S=3 D=99 I=0
2026-01-28 15:08:00,924 | INFO | Chunk: 40 | WER=72.916667 | S=3 D=31 I=1
2026-01-28 15:08:00,926 | INFO | Chunk: 41 | WER=65.573770 | S=1 D=39 I=0
2026-01-28 15:08:00,926 | INFO | Chunk: 42 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:08:00,927 | INFO | Chunk: 43 | WER=18.518519 | S=2 D=3 I=0
2026-01-28 15:08:02,036 | INFO | File: Rhap-D2001.wav | WER=52.615225 | S=88 D=919 I=9
2026-01-28 15:08:02,036 | INFO | ------------------------------
2026-01-28 15:08:02,036 | INFO | whisper med Done!
2026-01-28 15:09:29,684 | INFO | Chunk: 0 | WER=4.761905 | S=1 D=0 I=0
2026-01-28 15:09:29,685 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:09:29,685 | INFO | Chunk: 2 | WER=13.333333 | S=2 D=0 I=0
2026-01-28 15:09:29,687 | INFO | Chunk: 3 | WER=10.000000 | S=5 D=0 I=0
2026-01-28 15:09:29,688 | INFO | Chunk: 4 | WER=3.333333 | S=1 D=0 I=0
2026-01-28 15:09:29,689 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:09:29,689 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:09:29,690 | INFO | Chunk: 7 | WER=5.263158 | S=0 D=0 I=1
2026-01-28 15:09:29,691 | INFO | Chunk: 8 | WER=80.246914 | S=0 D=65 I=0
2026-01-28 15:09:29,692 | INFO | Chunk: 9 | WER=50.000000 | S=4 D=22 I=0
2026-01-28 15:09:29,694 | INFO | Chunk: 10 | WER=8.000000 | S=2 D=2 I=0
2026-01-28 15:09:29,695 | INFO | Chunk: 11 | WER=11.764706 | S=1 D=1 I=0
2026-01-28 15:09:29,696 | INFO | Chunk: 12 | WER=73.913043 | S=1 D=50 I=0
2026-01-28 15:09:29,696 | INFO | Chunk: 13 | WER=6.666667 | S=1 D=0 I=0
2026-01-28 15:09:29,697 | INFO | Chunk: 14 | WER=23.076923 | S=3 D=3 I=0
2026-01-28 15:09:29,697 | INFO | Chunk: 15 | WER=45.454545 | S=1 D=4 I=0
2026-01-28 15:09:29,698 | INFO | Chunk: 16 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:09:29,700 | INFO | Chunk: 17 | WER=57.142857 | S=0 D=40 I=0
2026-01-28 15:09:29,700 | INFO | Chunk: 18 | WER=42.857143 | S=0 D=2 I=1
2026-01-28 15:09:29,700 | INFO | Chunk: 19 | WER=6.250000 | S=1 D=0 I=0
2026-01-28 15:09:29,701 | INFO | Chunk: 20 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:09:29,701 | INFO | Chunk: 21 | WER=40.000000 | S=1 D=1 I=0
2026-01-28 15:09:29,702 | INFO | Chunk: 22 | WER=2.941176 | S=0 D=1 I=0
2026-01-28 15:09:29,703 | INFO | Chunk: 23 | WER=32.142857 | S=9 D=0 I=0
2026-01-28 15:09:29,708 | INFO | Chunk: 24 | WER=54.545455 | S=18 D=42 I=0
2026-01-28 15:09:29,713 | INFO | Chunk: 25 | WER=63.865546 | S=13 D=62 I=1
2026-01-28 15:09:29,714 | INFO | Chunk: 26 | WER=93.103448 | S=1 D=107 I=0
2026-01-28 15:09:29,715 | INFO | Chunk: 27 | WER=82.051282 | S=0 D=64 I=0
2026-01-28 15:09:29,717 | INFO | Chunk: 28 | WER=21.052632 | S=2 D=10 I=0
2026-01-28 15:09:29,718 | INFO | Chunk: 29 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:09:29,718 | INFO | Chunk: 30 | WER=18.518519 | S=4 D=0 I=1
2026-01-28 15:09:29,720 | INFO | Chunk: 31 | WER=91.406250 | S=2 D=115 I=0
2026-01-28 15:09:29,721 | INFO | Chunk: 32 | WER=28.000000 | S=2 D=4 I=1
2026-01-28 15:09:29,726 | INFO | Chunk: 33 | WER=58.490566 | S=27 D=35 I=0
2026-01-28 15:09:29,729 | INFO | Chunk: 34 | WER=61.061947 | S=3 D=66 I=0
2026-01-28 15:09:29,730 | INFO | Chunk: 35 | WER=11.111111 | S=0 D=1 I=1
2026-01-28 15:09:29,731 | INFO | Chunk: 36 | WER=46.808511 | S=2 D=20 I=0
2026-01-28 15:09:29,732 | INFO | Chunk: 37 | WER=20.000000 | S=1 D=4 I=0
2026-01-28 15:09:29,732 | INFO | Chunk: 38 | WER=300.000000 | S=1 D=0 I=2
2026-01-28 15:09:29,734 | INFO | Chunk: 39 | WER=86.725664 | S=2 D=96 I=0
2026-01-28 15:09:29,735 | INFO | Chunk: 40 | WER=10.416667 | S=2 D=3 I=0
2026-01-28 15:09:29,736 | INFO | Chunk: 41 | WER=72.131148 | S=1 D=43 I=0
2026-01-28 15:09:29,737 | INFO | Chunk: 42 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:09:29,737 | INFO | Chunk: 43 | WER=14.814815 | S=2 D=2 I=0
2026-01-28 15:09:30,875 | INFO | File: Rhap-D2001.wav | WER=51.216986 | S=116 D=865 I=8
2026-01-28 15:09:30,875 | INFO | ------------------------------
2026-01-28 15:09:30,876 | INFO | whisper large Done!
2026-01-28 15:09:31,063 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 15:09:31,102 | INFO | Vocabulary size: 350
2026-01-28 15:09:31,898 | INFO | Gradient checkpoint layers: []
2026-01-28 15:09:32,650 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 15:09:32,654 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 15:09:32,654 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 15:09:32,655 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 15:09:32,655 | INFO | speech length: 148800
2026-01-28 15:09:32,696 | INFO | decoder input length: 232
2026-01-28 15:09:32,697 | INFO | max output length: 232
2026-01-28 15:09:32,697 | INFO | min output length: 23
2026-01-28 15:09:37,277 | INFO | end detected at 71
2026-01-28 15:09:37,279 | INFO | -18.24 * 0.5 =  -9.12 for decoder
2026-01-28 15:09:37,279 | INFO |  -8.12 * 0.5 =  -4.06 for ctc
2026-01-28 15:09:37,279 | INFO | total log probability: -13.18
2026-01-28 15:09:37,279 | INFO | normalized log probability: -0.20
2026-01-28 15:09:37,279 | INFO | total number of ended hypotheses: 160
2026-01-28 15:09:37,280 | INFO | best hypo: ▁vous▁étiez▁stélodectilo▁vous▁êtes▁directrice▁de▁l'express▁de▁deux▁petites▁phrases▁deux▁vraies▁options▁qui▁dessinent▁votre▁route▁une▁route

2026-01-28 15:09:37,284 | INFO | speech length: 50720
2026-01-28 15:09:37,333 | INFO | decoder input length: 78
2026-01-28 15:09:37,333 | INFO | max output length: 78
2026-01-28 15:09:37,333 | INFO | min output length: 7
2026-01-28 15:09:38,770 | INFO | end detected at 32
2026-01-28 15:09:38,770 | INFO |  -2.19 * 0.5 =  -1.09 for decoder
2026-01-28 15:09:38,770 | INFO |  -0.10 * 0.5 =  -0.05 for ctc
2026-01-28 15:09:38,771 | INFO | total log probability: -1.14
2026-01-28 15:09:38,771 | INFO | normalized log probability: -0.04
2026-01-28 15:09:38,771 | INFO | total number of ended hypotheses: 135
2026-01-28 15:09:38,771 | INFO | best hypo: ▁qui▁témoigne▁d'une▁certaine▁d'une▁bonne▁d'une▁très▁bonne▁conduite

2026-01-28 15:09:38,773 | INFO | speech length: 92800
2026-01-28 15:09:38,820 | INFO | decoder input length: 144
2026-01-28 15:09:38,820 | INFO | max output length: 144
2026-01-28 15:09:38,820 | INFO | min output length: 14
2026-01-28 15:09:41,643 | INFO | end detected at 53
2026-01-28 15:09:41,644 | INFO |  -5.16 * 0.5 =  -2.58 for decoder
2026-01-28 15:09:41,644 | INFO |  -2.96 * 0.5 =  -1.48 for ctc
2026-01-28 15:09:41,645 | INFO | total log probability: -4.06
2026-01-28 15:09:41,645 | INFO | normalized log probability: -0.09
2026-01-28 15:09:41,645 | INFO | total number of ended hypotheses: 184
2026-01-28 15:09:41,645 | INFO | best hypo: ▁françois▁giroud▁vous▁occupez▁un▁poste▁de▁l'observation▁que▁les▁gens▁haut▁placé▁vous▁envie

2026-01-28 15:09:41,648 | INFO | speech length: 235200
2026-01-28 15:09:41,693 | INFO | decoder input length: 367
2026-01-28 15:09:41,693 | INFO | max output length: 367
2026-01-28 15:09:41,693 | INFO | min output length: 36
2026-01-28 15:09:50,800 | INFO | end detected at 111
2026-01-28 15:09:50,802 | INFO | -99.78 * 0.5 = -49.89 for decoder
2026-01-28 15:09:50,802 | INFO | -12.88 * 0.5 =  -6.44 for ctc
2026-01-28 15:09:50,802 | INFO | total log probability: -56.33
2026-01-28 15:09:50,802 | INFO | normalized log probability: -0.53
2026-01-28 15:09:50,802 | INFO | total number of ended hypotheses: 191
2026-01-28 15:09:50,803 | INFO | best hypo: ▁si▁je▁ne▁craignais▁pas▁d'entrer▁dans▁le▁jeu▁de▁certains▁hommes▁qui▁abusent▁de▁leurs▁conditions▁je▁dirais▁que▁vous▁avez▁donné▁quelque▁chose▁de▁plus▁à▁la▁femme▁des▁armes▁de▁persuasions▁use▁vous▁à▁ce▁niveau▁du▁charme▁de▁la▁femme▁pour▁réussir▁tout▁à▁elle

2026-01-28 15:09:50,807 | INFO | speech length: 95840
2026-01-28 15:09:50,845 | INFO | decoder input length: 149
2026-01-28 15:09:50,845 | INFO | max output length: 149
2026-01-28 15:09:50,845 | INFO | min output length: 14
2026-01-28 15:09:54,076 | INFO | end detected at 61
2026-01-28 15:09:54,077 | INFO |  -4.42 * 0.5 =  -2.21 for decoder
2026-01-28 15:09:54,077 | INFO |  -3.74 * 0.5 =  -1.87 for ctc
2026-01-28 15:09:54,077 | INFO | total log probability: -4.08
2026-01-28 15:09:54,078 | INFO | normalized log probability: -0.07
2026-01-28 15:09:54,078 | INFO | total number of ended hypotheses: 164
2026-01-28 15:09:54,079 | INFO | best hypo: ▁je▁suis▁incapable▁de▁vous▁répondre▁sur▁ce▁point▁avec▁les▁hommes▁qui▁travaillent▁avec▁moi▁ou▁qui▁ont▁travaillé▁avec▁moi▁qui▁pourrais▁vous▁répondre

2026-01-28 15:09:54,081 | INFO | speech length: 146560
2026-01-28 15:09:54,132 | INFO | decoder input length: 228
2026-01-28 15:09:54,132 | INFO | max output length: 228
2026-01-28 15:09:54,132 | INFO | min output length: 22
2026-01-28 15:09:58,616 | INFO | end detected at 70
2026-01-28 15:09:58,617 | INFO | -10.95 * 0.5 =  -5.47 for decoder
2026-01-28 15:09:58,618 | INFO |  -0.54 * 0.5 =  -0.27 for ctc
2026-01-28 15:09:58,618 | INFO | total log probability: -5.74
2026-01-28 15:09:58,618 | INFO | normalized log probability: -0.09
2026-01-28 15:09:58,618 | INFO | total number of ended hypotheses: 162
2026-01-28 15:09:58,619 | INFO | best hypo: ▁je▁voudrais▁simplement▁reprendre▁un▁mot▁dans▁ce▁que▁vous▁avez▁dit▁vous▁avez▁dit▁que▁disons▁ma▁carrière▁pour▁simplifier▁témoigner▁de▁ma▁bonne▁conduite

2026-01-28 15:09:58,622 | INFO | speech length: 43680
2026-01-28 15:09:58,663 | INFO | decoder input length: 67
2026-01-28 15:09:58,663 | INFO | max output length: 67
2026-01-28 15:09:58,663 | INFO | min output length: 6
2026-01-28 15:10:00,042 | INFO | end detected at 31
2026-01-28 15:10:00,045 | INFO |  -4.88 * 0.5 =  -2.44 for decoder
2026-01-28 15:10:00,045 | INFO |  -3.61 * 0.5 =  -1.80 for ctc
2026-01-28 15:10:00,045 | INFO | total log probability: -4.24
2026-01-28 15:10:00,045 | INFO | normalized log probability: -0.17
2026-01-28 15:10:00,045 | INFO | total number of ended hypotheses: 185
2026-01-28 15:10:00,045 | INFO | best hypo: ▁il▁me▁semble▁que▁c'est▁le▁contraire▁m'expliquait▁elle

2026-01-28 15:10:00,048 | INFO | speech length: 68480
2026-01-28 15:10:00,096 | INFO | decoder input length: 106
2026-01-28 15:10:00,097 | INFO | max output length: 106
2026-01-28 15:10:00,097 | INFO | min output length: 10
2026-01-28 15:10:02,165 | INFO | end detected at 41
2026-01-28 15:10:02,167 | INFO |  -2.92 * 0.5 =  -1.46 for decoder
2026-01-28 15:10:02,167 | INFO |  -4.33 * 0.5 =  -2.16 for ctc
2026-01-28 15:10:02,167 | INFO | total log probability: -3.62
2026-01-28 15:10:02,167 | INFO | normalized log probability: -0.10
2026-01-28 15:10:02,167 | INFO | total number of ended hypotheses: 169
2026-01-28 15:10:02,168 | INFO | best hypo: ▁je▁crois▁que▁je▁ne▁me▁suis▁pas▁conduit▁d'une▁façon▁conforme▁à▁ce▁qu'on▁attend

2026-01-28 15:10:02,170 | INFO | speech length: 393920
2026-01-28 15:10:02,213 | INFO | decoder input length: 615
2026-01-28 15:10:02,214 | INFO | max output length: 615
2026-01-28 15:10:02,214 | INFO | min output length: 61
2026-01-28 15:10:24,551 | INFO | end detected at 196
2026-01-28 15:10:24,552 | INFO | -490.80 * 0.5 = -245.40 for decoder
2026-01-28 15:10:24,552 | INFO | -141.73 * 0.5 = -70.87 for ctc
2026-01-28 15:10:24,552 | INFO | total log probability: -316.27
2026-01-28 15:10:24,552 | INFO | normalized log probability: -1.66
2026-01-28 15:10:24,552 | INFO | total number of ended hypotheses: 155
2026-01-28 15:10:24,555 | INFO | best hypo: ▁c'est▁d'une▁jeune▁fille▁d'abord▁et▁d'une▁femme▁ensuite▁une▁jeune▁bourgeoise▁disons▁d'une▁jeune▁bourgeoise▁voilà▁vous▁le▁regrettez▁aujourd'hui▁à▁nous▁pas▁du▁tout▁je▁voudrais▁le▁reprendrez▁à▁une▁phrases▁de▁dos▁et▁d'chat▁et▁vous▁me▁direz▁si▁vous▁êtes▁d'accord▁et▁marc▁char▁écrivait▁elle▁est▁très▁julie▁et▁elle▁et▁même▁belles▁elle▁est▁élégante▁on▁s'en▁m'aperçoit▁tout▁de▁suite▁mais▁on▁l'oublie▁dès▁onna▁regarder▁ses▁yeux

2026-01-28 15:10:24,557 | INFO | speech length: 245920
2026-01-28 15:10:24,599 | INFO | decoder input length: 383
2026-01-28 15:10:24,599 | INFO | max output length: 383
2026-01-28 15:10:24,599 | INFO | min output length: 38
2026-01-28 15:10:35,015 | INFO | end detected at 123
2026-01-28 15:10:35,016 | INFO | -179.77 * 0.5 = -89.89 for decoder
2026-01-28 15:10:35,016 | INFO | -48.72 * 0.5 = -24.36 for ctc
2026-01-28 15:10:35,016 | INFO | total log probability: -114.24
2026-01-28 15:10:35,016 | INFO | normalized log probability: -0.97
2026-01-28 15:10:35,016 | INFO | total number of ended hypotheses: 157
2026-01-28 15:10:35,018 | INFO | best hypo: ▁des▁yeux▁qui▁sont▁en▁même▁temps▁et▁c'est▁difficile▁profond▁et▁persan▁plein▁de▁curiosité▁mystère▁des▁yeux▁qui▁enregistrent▁chaque▁détail▁avec▁l'implacable▁précision▁d'une▁caméra▁et▁je▁l'impressension▁que▁c'est▁avec▁ses▁yeux▁que▁vous▁avez▁des▁sinée▁votre▁au▁route▁françois▁géant

2026-01-28 15:10:35,020 | INFO | speech length: 163360
2026-01-28 15:10:35,089 | INFO | decoder input length: 254
2026-01-28 15:10:35,089 | INFO | max output length: 254
2026-01-28 15:10:35,089 | INFO | min output length: 25
2026-01-28 15:10:43,226 | INFO | end detected at 119
2026-01-28 15:10:43,227 | INFO | -79.85 * 0.5 = -39.93 for decoder
2026-01-28 15:10:43,227 | INFO | -24.91 * 0.5 = -12.45 for ctc
2026-01-28 15:10:43,227 | INFO | total log probability: -52.38
2026-01-28 15:10:43,227 | INFO | normalized log probability: -0.46
2026-01-28 15:10:43,227 | INFO | total number of ended hypotheses: 159
2026-01-28 15:10:43,229 | INFO | best hypo: ▁coûtez▁vous▁m'avez▁demandé▁si▁je▁suis▁d'accord▁avec▁marcel▁achar▁je▁suppose▁que▁vous▁connaissez▁marcel▁achard▁c'est▁un▁homme▁incapable▁dire▁quoi▁que▁ce▁soit▁de▁désagréable▁sur▁qui▁que▁ce▁soit▁je▁ne▁bois▁pas▁pourquo▁j'aurais▁fait▁exception▁de▁cette▁règle

2026-01-28 15:10:43,231 | INFO | speech length: 72960
2026-01-28 15:10:43,285 | INFO | decoder input length: 113
2026-01-28 15:10:43,285 | INFO | max output length: 113
2026-01-28 15:10:43,285 | INFO | min output length: 11
2026-01-28 15:10:45,062 | INFO | end detected at 34
2026-01-28 15:10:45,063 | INFO |  -7.25 * 0.5 =  -3.62 for decoder
2026-01-28 15:10:45,063 | INFO |  -5.48 * 0.5 =  -2.74 for ctc
2026-01-28 15:10:45,063 | INFO | total log probability: -6.36
2026-01-28 15:10:45,063 | INFO | normalized log probability: -0.21
2026-01-28 15:10:45,063 | INFO | total number of ended hypotheses: 160
2026-01-28 15:10:45,064 | INFO | best hypo: ▁et▁est▁ce▁que▁c'est▁avec▁ses▁yeux▁si▁vous▁voulez▁dire▁que▁je▁suis▁regardé

2026-01-28 15:10:45,066 | INFO | speech length: 325760
2026-01-28 15:10:45,114 | INFO | decoder input length: 508
2026-01-28 15:10:45,114 | INFO | max output length: 508
2026-01-28 15:10:45,114 | INFO | min output length: 50
2026-01-28 15:11:02,423 | INFO | end detected at 174
2026-01-28 15:11:02,425 | INFO | -247.21 * 0.5 = -123.61 for decoder
2026-01-28 15:11:02,425 | INFO | -74.45 * 0.5 = -37.23 for ctc
2026-01-28 15:11:02,425 | INFO | total log probability: -160.83
2026-01-28 15:11:02,425 | INFO | normalized log probability: -0.96
2026-01-28 15:11:02,425 | INFO | total number of ended hypotheses: 182
2026-01-28 15:11:02,428 | INFO | best hypo: ▁je▁crois▁que▁je▁sais▁regardé▁mais▁je▁crois▁que▁j'ai▁toujours▁oui▁eu▁eu▁cette▁disons▁cette▁faculté▁ce▁qu'il▁a▁toujours▁intéressés▁c'étaient▁chez▁les▁gens▁le▁mécanismes▁de▁la▁carrière▁de▁mécanisme▁tout▁simplement▁pas▁spécialement▁de▁la▁carrière▁et▁la▁personnalité▁et▁j'aime▁bien▁comprre▁comment▁ça▁chez▁les▁autres▁ça▁ces▁chez▁les▁toujours▁ches

2026-01-28 15:11:02,431 | INFO | speech length: 46880
2026-01-28 15:11:02,477 | INFO | decoder input length: 72
2026-01-28 15:11:02,477 | INFO | max output length: 72
2026-01-28 15:11:02,477 | INFO | min output length: 7
2026-01-28 15:11:04,175 | INFO | end detected at 38
2026-01-28 15:11:04,176 | INFO |  -3.50 * 0.5 =  -1.75 for decoder
2026-01-28 15:11:04,176 | INFO |  -8.58 * 0.5 =  -4.29 for ctc
2026-01-28 15:11:04,176 | INFO | total log probability: -6.04
2026-01-28 15:11:04,176 | INFO | normalized log probability: -0.18
2026-01-28 15:11:04,176 | INFO | total number of ended hypotheses: 173
2026-01-28 15:11:04,177 | INFO | best hypo: ▁par▁conséquent▁j'avais▁envie▁de▁savoir▁un▁petit▁peu▁mieux▁comment▁ça▁marcher

2026-01-28 15:11:04,179 | INFO | speech length: 96480
2026-01-28 15:11:04,224 | INFO | decoder input length: 150
2026-01-28 15:11:04,224 | INFO | max output length: 150
2026-01-28 15:11:04,224 | INFO | min output length: 15
2026-01-28 15:11:08,227 | INFO | end detected at 71
2026-01-28 15:11:08,228 | INFO | -20.36 * 0.5 = -10.18 for decoder
2026-01-28 15:11:08,229 | INFO | -22.09 * 0.5 = -11.04 for ctc
2026-01-28 15:11:08,229 | INFO | total log probability: -21.22
2026-01-28 15:11:08,229 | INFO | normalized log probability: -0.34
2026-01-28 15:11:08,229 | INFO | total number of ended hypotheses: 192
2026-01-28 15:11:08,230 | INFO | best hypo: ▁et▁puis▁je▁suis▁toujours▁étonné▁maintenant▁plus▁je▁précise▁d'ailleurs▁que▁qu'embarsait▁la▁charriecristex▁qu'il▁est▁déjà▁présent

2026-01-28 15:11:08,233 | INFO | speech length: 101120
2026-01-28 15:11:08,280 | INFO | decoder input length: 157
2026-01-28 15:11:08,280 | INFO | max output length: 157
2026-01-28 15:11:08,280 | INFO | min output length: 15
2026-01-28 15:11:10,455 | INFO | end detected at 31
2026-01-28 15:11:10,457 | INFO |  -3.77 * 0.5 =  -1.89 for decoder
2026-01-28 15:11:10,457 | INFO |  -1.63 * 0.5 =  -0.81 for ctc
2026-01-28 15:11:10,457 | INFO | total log probability: -2.70
2026-01-28 15:11:10,457 | INFO | normalized log probability: -0.11
2026-01-28 15:11:10,457 | INFO | total number of ended hypotheses: 199
2026-01-28 15:11:10,458 | INFO | best hypo: ▁dix▁sept▁ans▁mais▁ensuite▁j'ai▁toujours▁été▁étonné

2026-01-28 15:11:10,460 | INFO | speech length: 53440
2026-01-28 15:11:10,504 | INFO | decoder input length: 83
2026-01-28 15:11:10,505 | INFO | max output length: 83
2026-01-28 15:11:10,505 | INFO | min output length: 8
2026-01-28 15:11:12,084 | INFO | end detected at 34
2026-01-28 15:11:12,086 | INFO |  -2.36 * 0.5 =  -1.18 for decoder
2026-01-28 15:11:12,086 | INFO |  -0.19 * 0.5 =  -0.09 for ctc
2026-01-28 15:11:12,086 | INFO | total log probability: -1.27
2026-01-28 15:11:12,086 | INFO | normalized log probability: -0.04
2026-01-28 15:11:12,086 | INFO | total number of ended hypotheses: 145
2026-01-28 15:11:12,086 | INFO | best hypo: ▁de▁voir▁que▁les▁gens▁ne▁voyaient▁pas▁des▁choses▁qui▁me▁paraissaient▁évidentes

2026-01-28 15:11:12,088 | INFO | speech length: 270240
2026-01-28 15:11:12,138 | INFO | decoder input length: 421
2026-01-28 15:11:12,138 | INFO | max output length: 421
2026-01-28 15:11:12,138 | INFO | min output length: 42
2026-01-28 15:11:25,972 | INFO | end detected at 151
2026-01-28 15:11:25,973 | INFO | -207.35 * 0.5 = -103.67 for decoder
2026-01-28 15:11:25,973 | INFO | -78.07 * 0.5 = -39.04 for ctc
2026-01-28 15:11:25,973 | INFO | total log probability: -142.71
2026-01-28 15:11:25,973 | INFO | normalized log probability: -0.98
2026-01-28 15:11:25,973 | INFO | total number of ended hypotheses: 164
2026-01-28 15:11:25,975 | INFO | best hypo: ▁dans▁le▁comportement▁des▁autres▁à▁dire▁ce▁su'est▁toujours▁l'impression▁que▁c'est▁cet▁énorme▁que▁c'est▁là▁en▁noir▁sur▁blanc▁et▁les▁autres▁qui▁ne▁voient▁pas▁sa▁méthode▁puisque▁vous▁avez▁eu▁un▁exces▁d'un▁destin▁exceptionnel▁où▁est▁ce▁simplement▁la▁réussite▁qui▁devait▁venir▁ou▁s'est▁il▁passait▁tout▁à▁fait▁d'autreschose▁dans▁retreville

2026-01-28 15:11:25,977 | INFO | speech length: 31680
2026-01-28 15:11:26,023 | INFO | decoder input length: 49
2026-01-28 15:11:26,023 | INFO | max output length: 49
2026-01-28 15:11:26,023 | INFO | min output length: 4
2026-01-28 15:11:27,154 | INFO | end detected at 27
2026-01-28 15:11:27,156 | INFO |  -3.35 * 0.5 =  -1.68 for decoder
2026-01-28 15:11:27,156 | INFO | -10.40 * 0.5 =  -5.20 for ctc
2026-01-28 15:11:27,156 | INFO | total log probability: -6.88
2026-01-28 15:11:27,156 | INFO | normalized log probability: -0.36
2026-01-28 15:11:27,156 | INFO | total number of ended hypotheses: 191
2026-01-28 15:11:27,156 | INFO | best hypo: ▁vous▁écoutais▁j'ai▁eu▁un▁destin

2026-01-28 15:11:27,159 | INFO | speech length: 59680
2026-01-28 15:11:27,201 | INFO | decoder input length: 92
2026-01-28 15:11:27,201 | INFO | max output length: 92
2026-01-28 15:11:27,201 | INFO | min output length: 9
2026-01-28 15:11:29,482 | INFO | end detected at 44
2026-01-28 15:11:29,484 | INFO |  -3.99 * 0.5 =  -2.00 for decoder
2026-01-28 15:11:29,484 | INFO |  -6.45 * 0.5 =  -3.22 for ctc
2026-01-28 15:11:29,484 | INFO | total log probability: -5.22
2026-01-28 15:11:29,484 | INFO | normalized log probability: -0.13
2026-01-28 15:11:29,484 | INFO | total number of ended hypotheses: 176
2026-01-28 15:11:29,485 | INFO | best hypo: ▁l'exceptionnel▁me▁paraît▁un▁grand▁mot▁disant▁que▁j'ai▁un▁destin▁un▁peu▁différent

2026-01-28 15:11:29,489 | INFO | speech length: 72480
2026-01-28 15:11:29,535 | INFO | decoder input length: 112
2026-01-28 15:11:29,536 | INFO | max output length: 112
2026-01-28 15:11:29,536 | INFO | min output length: 11
2026-01-28 15:11:31,675 | INFO | end detected at 43
2026-01-28 15:11:31,677 | INFO |  -2.91 * 0.5 =  -1.45 for decoder
2026-01-28 15:11:31,677 | INFO |  -4.55 * 0.5 =  -2.28 for ctc
2026-01-28 15:11:31,677 | INFO | total log probability: -3.73
2026-01-28 15:11:31,677 | INFO | normalized log probability: -0.10
2026-01-28 15:11:31,677 | INFO | total number of ended hypotheses: 154
2026-01-28 15:11:31,678 | INFO | best hypo: ▁parce▁que▁les▁conditions▁objectives▁de▁ma▁vie▁ont▁été▁en▁effet▁différentes

2026-01-28 15:11:31,680 | INFO | speech length: 35680
2026-01-28 15:11:31,733 | INFO | decoder input length: 55
2026-01-28 15:11:31,733 | INFO | max output length: 55
2026-01-28 15:11:31,733 | INFO | min output length: 5
2026-01-28 15:11:32,485 | INFO | end detected at 16
2026-01-28 15:11:32,487 | INFO |  -0.87 * 0.5 =  -0.44 for decoder
2026-01-28 15:11:32,487 | INFO |  -5.84 * 0.5 =  -2.92 for ctc
2026-01-28 15:11:32,487 | INFO | total log probability: -3.36
2026-01-28 15:11:32,487 | INFO | normalized log probability: -0.28
2026-01-28 15:11:32,487 | INFO | total number of ended hypotheses: 179
2026-01-28 15:11:32,487 | INFO | best hypo: ▁par▁exemple▁j'étais

2026-01-28 15:11:32,490 | INFO | speech length: 156960
2026-01-28 15:11:32,531 | INFO | decoder input length: 244
2026-01-28 15:11:32,531 | INFO | max output length: 244
2026-01-28 15:11:32,531 | INFO | min output length: 24
2026-01-28 15:11:38,149 | INFO | end detected at 87
2026-01-28 15:11:38,151 | INFO | -11.39 * 0.5 =  -5.70 for decoder
2026-01-28 15:11:38,151 | INFO |  -6.22 * 0.5 =  -3.11 for ctc
2026-01-28 15:11:38,151 | INFO | total log probability: -8.80
2026-01-28 15:11:38,151 | INFO | normalized log probability: -0.11
2026-01-28 15:11:38,151 | INFO | total number of ended hypotheses: 163
2026-01-28 15:11:38,152 | INFO | best hypo: ▁j'ai▁été▁obligé▁de▁travailler▁très▁tôt▁à▁une▁époque▁où▁les▁jeunes▁filles▁de▁ma▁génération▁et▁appartenant▁au▁monde▁bourgeois▁ne▁travaillaient▁pas▁maintenant▁c'est▁tout▁à▁fait▁normal

2026-01-28 15:11:38,155 | INFO | speech length: 122080
2026-01-28 15:11:38,200 | INFO | decoder input length: 190
2026-01-28 15:11:38,200 | INFO | max output length: 190
2026-01-28 15:11:38,200 | INFO | min output length: 19
2026-01-28 15:11:43,044 | INFO | end detected at 76
2026-01-28 15:11:43,045 | INFO |  -9.43 * 0.5 =  -4.71 for decoder
2026-01-28 15:11:43,046 | INFO |  -4.64 * 0.5 =  -2.32 for ctc
2026-01-28 15:11:43,046 | INFO | total log probability: -7.03
2026-01-28 15:11:43,046 | INFO | normalized log probability: -0.10
2026-01-28 15:11:43,046 | INFO | total number of ended hypotheses: 178
2026-01-28 15:11:43,047 | INFO | best hypo: ▁disons▁que▁si▁mon▁père▁était▁mort▁en▁facile▁les▁circonstances▁disons▁qui▁m'ont▁conduit▁à▁être▁obligé▁de▁gagner▁ma▁vie▁très▁tôt▁s'était▁produite

2026-01-28 15:11:43,049 | INFO | speech length: 455040
2026-01-28 15:11:43,118 | INFO | decoder input length: 710
2026-01-28 15:11:43,118 | INFO | max output length: 710
2026-01-28 15:11:43,118 | INFO | min output length: 71
2026-01-28 15:12:14,318 | INFO | end detected at 244
2026-01-28 15:12:14,320 | INFO | -562.41 * 0.5 = -281.20 for decoder
2026-01-28 15:12:14,320 | INFO | -232.23 * 0.5 = -116.12 for ctc
2026-01-28 15:12:14,320 | INFO | total log probability: -397.32
2026-01-28 15:12:14,320 | INFO | normalized log probability: -1.67
2026-01-28 15:12:14,320 | INFO | total number of ended hypotheses: 170
2026-01-28 15:12:14,323 | INFO | best hypo: ▁sensiblement▁plus▁tard▁je▁ne▁sais▁pas▁moi▁quand▁j'avais▁vingt▁ans▁ou▁vingt▁deux▁ans▁et▁bien▁peut▁être▁que▁je▁me▁serais▁marié▁et▁j'aurais▁trouvé▁plus▁commode▁d'épouser▁à▁monsieur▁avec▁delargence▁que▁je▁sais▁moi▁mais▁j'avais▁quatorze▁ans▁et▁j'aus▁choisie▁de▁travailler▁et▁elle▁y▁avais▁tout▁de▁me▁les▁cinconstances▁oibjectives▁et▁pourquoi▁j'ai▁fait▁de▁journalisque▁parce▁que▁et▁çailsait▁tout▁de▁suite▁après▁la▁guerre▁et▁je▁n'avais▁aucelle▁raisine▁d'alleraire▁faire▁ce▁métier▁je▁gagnais▁beaucoup▁d'argent▁dans▁le▁cinéma▁j'avais▁un▁bon▁nom

2026-01-28 15:12:14,326 | INFO | speech length: 454080
2026-01-28 15:12:14,373 | INFO | decoder input length: 709
2026-01-28 15:12:14,374 | INFO | max output length: 709
2026-01-28 15:12:14,374 | INFO | min output length: 70
2026-01-28 15:12:44,192 | INFO | end detected at 244
2026-01-28 15:12:44,194 | INFO | -612.02 * 0.5 = -306.01 for decoder
2026-01-28 15:12:44,194 | INFO | -123.46 * 0.5 = -61.73 for ctc
2026-01-28 15:12:44,194 | INFO | total log probability: -367.74
2026-01-28 15:12:44,194 | INFO | normalized log probability: -1.55
2026-01-28 15:12:44,194 | INFO | total number of ended hypotheses: 176
2026-01-28 15:12:44,197 | INFO | best hypo: ▁je▁ne▁ce▁n'aurais▁certainement▁pas▁débuté▁à▁ce▁moment▁là▁comme▁jeune▁journaliste▁pourquoi▁ystrouf▁keilagher▁ristreuve▁que▁tout▁a▁été▁bouleversé▁qu'il▁n'y▁avait▁plus▁un▁journaliste▁ap▁paris▁puisqu'elle▁il▁avait▁tous▁plus▁ou▁moins▁collaboré▁et▁que▁au▁moment▁de▁faire▁un▁journal▁on▁m'appose▁d'y▁entrer▁comme▁directrice▁ce▁qui▁est▁incroyable▁comme▁une▁directrice▁de▁la▁rédaction▁touss▁à▁des▁conditions▁objectives▁droyant▁de▁revenir▁un▁peu▁au▁début▁il▁y▁a▁eu▁votre▁père▁votre▁père▁ét▁riche▁riche▁c'est▁un▁grand▁moment▁disant▁qu'il▁appartenait▁à▁ce

2026-01-28 15:12:44,200 | INFO | speech length: 447040
2026-01-28 15:12:44,252 | INFO | decoder input length: 698
2026-01-28 15:12:44,253 | INFO | max output length: 698
2026-01-28 15:12:44,253 | INFO | min output length: 69
2026-01-28 15:13:14,157 | INFO | end detected at 236
2026-01-28 15:13:14,159 | INFO | -452.75 * 0.5 = -226.37 for decoder
2026-01-28 15:13:14,159 | INFO | -94.97 * 0.5 = -47.49 for ctc
2026-01-28 15:13:14,159 | INFO | total log probability: -273.86
2026-01-28 15:13:14,159 | INFO | normalized log probability: -1.20
2026-01-28 15:13:14,159 | INFO | total number of ended hypotheses: 172
2026-01-28 15:13:14,162 | INFO | best hypo: ▁bourgeoisie▁qui▁n'a▁pas▁de▁problèmes▁d'argent▁lui▁disons▁les▁mots▁comme▁ils▁onù▁vous▁avez▁été▁ruinés▁il▁a▁fallu▁travailler▁et▁vous▁savez▁ce▁que▁c'est▁que▁d'être▁pauvre▁je▁sais▁ce▁que▁c'est▁que▁d'être▁pauvre▁je▁sais▁surtout▁ce▁qui▁me▁paraîait▁plus▁utiîle▁ce▁que▁c'est▁que▁de▁travailler▁dans▁les▁allois▁subalternes▁avec▁le▁sentiment▁qu'on▁n'en▁sortira▁jamais▁mais▁c'est▁expérience▁ça▁que▁je▁n'a▁jamais▁oublié▁ce▁qui▁est▁dre▁ce▁n'est▁pas▁surtout▁candonné▁très▁jeune▁ce▁n'est▁vraiment▁pas▁d'être▁pauvre▁ce▁n'est▁pas▁grave▁quand▁on▁est

2026-01-28 15:13:14,165 | INFO | speech length: 301120
2026-01-28 15:13:14,216 | INFO | decoder input length: 470
2026-01-28 15:13:14,216 | INFO | max output length: 470
2026-01-28 15:13:14,216 | INFO | min output length: 47
2026-01-28 15:13:30,805 | INFO | end detected at 164
2026-01-28 15:13:30,806 | INFO | -299.78 * 0.5 = -149.89 for decoder
2026-01-28 15:13:30,806 | INFO | -157.94 * 0.5 = -78.97 for ctc
2026-01-28 15:13:30,806 | INFO | total log probability: -228.86
2026-01-28 15:13:30,806 | INFO | normalized log probability: -1.45
2026-01-28 15:13:30,806 | INFO | total number of ended hypotheses: 169
2026-01-28 15:13:30,809 | INFO | best hypo: ▁ce▁qui▁est▁torri▁ou▁essaie▁de▁se▁dire▁je▁n'en▁sortirai▁jamais▁mais▁ce▁que▁vous▁aviez▁prévu▁tous▁cela▁car▁vous▁aviez▁pris▁des▁cours▁de▁sténodactylos▁à▁non▁bas▁du▁tout▁non▁je▁n'ai▁pris▁des▁cours▁de▁ce▁d'actilo▁la▁vérité▁que▁et▁au▁moment▁où▁il▁avallus▁vraiment▁que▁je▁de▁gagne▁ma▁vie▁et▁je▁vousu▁dirai▁et▁c'ai▁de▁seule▁is▁de▁ma▁vie▁don▁je▁sois▁fière

2026-01-28 15:13:30,813 | INFO | speech length: 229440
2026-01-28 15:13:30,858 | INFO | decoder input length: 358
2026-01-28 15:13:30,858 | INFO | max output length: 358
2026-01-28 15:13:30,858 | INFO | min output length: 35
2026-01-28 15:13:40,694 | INFO | end detected at 112
2026-01-28 15:13:40,695 | INFO | -92.49 * 0.5 = -46.25 for decoder
2026-01-28 15:13:40,695 | INFO | -34.30 * 0.5 = -17.15 for ctc
2026-01-28 15:13:40,695 | INFO | total log probability: -63.39
2026-01-28 15:13:40,695 | INFO | normalized log probability: -0.60
2026-01-28 15:13:40,695 | INFO | total number of ended hypotheses: 152
2026-01-28 15:13:40,696 | INFO | best hypo: ▁et▁au▁lieu▁d'accepter▁de▁prendre▁tout▁de▁suite▁un▁emploi▁du▁type▁vendeuse▁je▁me▁suis▁dit▁ce▁qui▁faussait▁avoir▁un▁métier▁même▁s'il▁est▁très▁humble▁ça▁aucun▁avoir▁principaut▁que▁je▁sache▁faire▁quelque▁chose▁et▁permment▁je▁ne▁sais▁rien▁faire

2026-01-28 15:13:40,699 | INFO | speech length: 30880
2026-01-28 15:13:40,736 | INFO | decoder input length: 47
2026-01-28 15:13:40,736 | INFO | max output length: 47
2026-01-28 15:13:40,736 | INFO | min output length: 4
2026-01-28 15:13:42,018 | INFO | end detected at 31
2026-01-28 15:13:42,020 | INFO |  -2.88 * 0.5 =  -1.44 for decoder
2026-01-28 15:13:42,020 | INFO |  -0.80 * 0.5 =  -0.40 for ctc
2026-01-28 15:13:42,020 | INFO | total log probability: -1.84
2026-01-28 15:13:42,020 | INFO | normalized log probability: -0.07
2026-01-28 15:13:42,020 | INFO | total number of ended hypotheses: 146
2026-01-28 15:13:42,021 | INFO | best hypo: ▁je▁sais▁ce▁qu'on▁apprend▁aux▁jeunes▁filles▁bien▁élevées

2026-01-28 15:13:42,023 | INFO | speech length: 78880
2026-01-28 15:13:42,068 | INFO | decoder input length: 122
2026-01-28 15:13:42,068 | INFO | max output length: 122
2026-01-28 15:13:42,068 | INFO | min output length: 12
2026-01-28 15:13:45,423 | INFO | end detected at 67
2026-01-28 15:13:45,425 | INFO |  -9.66 * 0.5 =  -4.83 for decoder
2026-01-28 15:13:45,425 | INFO | -17.42 * 0.5 =  -8.71 for ctc
2026-01-28 15:13:45,425 | INFO | total log probability: -13.54
2026-01-28 15:13:45,425 | INFO | normalized log probability: -0.22
2026-01-28 15:13:45,425 | INFO | total number of ended hypotheses: 193
2026-01-28 15:13:45,426 | INFO | best hypo: ▁donc▁qu'est▁ce▁que▁je▁peux▁apprendre▁je▁me▁suggé▁à▁prendre▁d'asténodactylo▁avec▁sa▁quoiqu'il▁arrive▁j'aurai▁un▁métier

2026-01-28 15:13:45,428 | INFO | speech length: 452640
2026-01-28 15:13:45,484 | INFO | decoder input length: 706
2026-01-28 15:13:45,484 | INFO | max output length: 706
2026-01-28 15:13:45,484 | INFO | min output length: 70
2026-01-28 15:14:19,445 | INFO | end detected at 279
2026-01-28 15:14:19,446 | INFO | -612.79 * 0.5 = -306.39 for decoder
2026-01-28 15:14:19,446 | INFO | -155.93 * 0.5 = -77.96 for ctc
2026-01-28 15:14:19,446 | INFO | total log probability: -384.36
2026-01-28 15:14:19,446 | INFO | normalized log probability: -1.41
2026-01-28 15:14:19,446 | INFO | total number of ended hypotheses: 157
2026-01-28 15:14:19,450 | INFO | best hypo: ▁j'ai▁donc▁été▁à▁l'école▁remington▁apprendre▁le▁sténo▁d'actylo▁c'était▁le▁vrai▁départ▁eh▁oui▁en▁fassez▁comme▁ça▁que▁j'ai▁commencé▁à▁gannibaler▁insudier▁une▁librairie▁c'est▁dans▁cette▁librairie▁que▁je▁crois▁que▁allégrez▁vous▁a▁trouvée▁c'était▁je▁connais▁à▁ligret▁très▁bien▁j'ai▁en▁jusion▁rentré▁dans▁cette▁librairie▁et▁j'étais▁à▁la▁fois▁la▁secrétaire▁du▁libraire'▁est▁vendeuse▁et▁puis▁je▁ne▁saavait▁pas▁de▁tourage▁que▁jamais▁et▁m'a▁laissée▁toute▁sa▁librairie▁qu'▁n'était▁pas▁très▁sérieux▁'avais▁j'avais▁l'air▁d'avoir▁dix▁huit▁ou▁dix▁neuf▁ans▁d'in

2026-01-28 15:14:19,453 | INFO | speech length: 62400
2026-01-28 15:14:19,495 | INFO | decoder input length: 97
2026-01-28 15:14:19,496 | INFO | max output length: 97
2026-01-28 15:14:19,496 | INFO | min output length: 9
2026-01-28 15:14:21,535 | INFO | end detected at 45
2026-01-28 15:14:21,537 | INFO |  -5.42 * 0.5 =  -2.71 for decoder
2026-01-28 15:14:21,537 | INFO | -12.11 * 0.5 =  -6.05 for ctc
2026-01-28 15:14:21,537 | INFO | total log probability: -8.76
2026-01-28 15:14:21,537 | INFO | normalized log probability: -0.22
2026-01-28 15:14:21,537 | INFO | total number of ended hypotheses: 175
2026-01-28 15:14:21,538 | INFO | best hypo: ▁qui▁quand▁même▁n'était▁pas▁très▁sérieux▁mais▁ne▁se▁doutait▁quand▁même▁pas▁qu'il▁n'en▁avait▁pas▁case

2026-01-28 15:14:21,540 | INFO | speech length: 319680
2026-01-28 15:14:21,587 | INFO | decoder input length: 499
2026-01-28 15:14:21,587 | INFO | max output length: 499
2026-01-28 15:14:21,587 | INFO | min output length: 49
2026-01-28 15:14:42,321 | INFO | end detected at 222
2026-01-28 15:14:42,322 | INFO | -353.11 * 0.5 = -176.56 for decoder
2026-01-28 15:14:42,322 | INFO | -174.13 * 0.5 = -87.07 for ctc
2026-01-28 15:14:42,323 | INFO | total log probability: -263.62
2026-01-28 15:14:42,323 | INFO | normalized log probability: -1.21
2026-01-28 15:14:42,323 | INFO | total number of ended hypotheses: 135
2026-01-28 15:14:42,326 | INFO | best hypo: ▁et▁je▁connaissais▁très▁bien▁marc▁allégret▁depuis▁très▁longtemps▁enfin▁ma▁famille▁le▁connaissait▁il▁est▁entré▁un▁jour▁dans▁cet▁hybride▁madame▁j'est▁ce▁que▁tu▁fais▁très▁surpris▁chez▁dis▁bal▁tu▁vois▁j'en▁des▁livres▁il▁m'adis▁qu'est▁ce▁que▁tu▁gagnes▁et▁j'ai▁dis▁je▁gagne▁trois▁cents▁enfants▁par▁moi▁et▁m'ai▁dit▁mais▁tu▁pertantins▁j'ai▁bien▁oui▁mais▁en▁attendant▁j'ai▁paye▁mon▁loye▁enf▁mon▁louis▁j'apitais▁avec▁moi▁mère▁m'ai▁je▁gagné▁un▁peu▁d'argent

2026-01-28 15:14:42,328 | INFO | speech length: 465120
2026-01-28 15:14:42,379 | INFO | decoder input length: 726
2026-01-28 15:14:42,379 | INFO | max output length: 726
2026-01-28 15:14:42,379 | INFO | min output length: 72
2026-01-28 15:15:26,171 | INFO | end detected at 263
2026-01-28 15:15:26,172 | INFO | -625.73 * 0.5 = -312.87 for decoder
2026-01-28 15:15:26,173 | INFO | -244.36 * 0.5 = -122.18 for ctc
2026-01-28 15:15:26,173 | INFO | total log probability: -435.05
2026-01-28 15:15:26,173 | INFO | normalized log probability: -1.69
2026-01-28 15:15:26,173 | INFO | total number of ended hypotheses: 160
2026-01-28 15:15:26,176 | INFO | best hypo: ▁et▁il▁m'a▁dit▁que▁tout▁ça▁n'est▁pas▁sérieux▁et▁il▁faut▁que▁tu▁deviennes▁script▁girl▁par▁exemple▁bien▁travailler▁avec▁moi▁c'était▁une▁autre▁époque▁on▁pouvait▁faire▁ça▁aujourd'hui▁je▁ne▁crois▁pas▁que▁ce▁serait▁posible▁mais▁à▁l'apoque▁et▁person▁ne▁savait▁que▁je▁n'avais▁jamais▁me▁fait▁ce▁métierais▁sauf▁et▁lui▁tout▁était▁beussible▁là▁on▁d'isible▁oui▁le▁cin'émat▁était▁beaucoup▁ou▁moins▁n'était▁pas▁une▁organisation▁rigide▁comme▁elle▁n'est▁aujourd'hui▁c'est▁bien▁être▁à▁ce▁moment▁là▁que▁vous▁avez▁raté▁une▁autre▁charrière▁cars▁on▁vous▁avez▁reposé▁de▁onni▁dans▁un▁film▁oui

2026-01-28 15:15:26,178 | INFO | speech length: 42400
2026-01-28 15:15:26,239 | INFO | decoder input length: 65
2026-01-28 15:15:26,239 | INFO | max output length: 65
2026-01-28 15:15:26,239 | INFO | min output length: 6
2026-01-28 15:15:31,584 | INFO | end detected at 46
2026-01-28 15:15:31,586 | INFO |  -5.94 * 0.5 =  -2.97 for decoder
2026-01-28 15:15:31,586 | INFO | -10.43 * 0.5 =  -5.21 for ctc
2026-01-28 15:15:31,586 | INFO | total log probability: -8.18
2026-01-28 15:15:31,586 | INFO | normalized log probability: -0.20
2026-01-28 15:15:31,586 | INFO | total number of ended hypotheses: 176
2026-01-28 15:15:31,587 | INFO | best hypo: ▁ne▁crois▁pas▁que▁j'ai▁raté▁une▁carrière▁je▁crois▁vraiment▁que▁j'étais▁pas▁faite▁pour▁cela

2026-01-28 15:15:31,589 | INFO | speech length: 173920
2026-01-28 15:15:31,648 | INFO | decoder input length: 271
2026-01-28 15:15:31,648 | INFO | max output length: 271
2026-01-28 15:15:31,648 | INFO | min output length: 27
2026-01-28 15:15:48,084 | INFO | end detected at 111
2026-01-28 15:15:48,086 | INFO | -82.64 * 0.5 = -41.32 for decoder
2026-01-28 15:15:48,086 | INFO | -37.73 * 0.5 = -18.87 for ctc
2026-01-28 15:15:48,086 | INFO | total log probability: -60.19
2026-01-28 15:15:48,086 | INFO | normalized log probability: -0.57
2026-01-28 15:15:48,086 | INFO | total number of ended hypotheses: 170
2026-01-28 15:15:48,088 | INFO | best hypo: ▁non▁mes▁mettons▁vous▁ayez▁tourné▁dans▁ce▁film▁mais▁que▁le▁film▁est▁eu▁du▁succès▁et▁soyez▁devenu▁une▁vedette▁vous▁étiez▁normalement▁entraînée▁je▁ne▁serai▁pas▁devenu▁vedette▁pour▁crois▁véritablement▁que▁je▁n'étais▁pas▁doué▁pour▁ce

2026-01-28 15:15:48,091 | INFO | speech length: 119040
2026-01-28 15:15:48,141 | INFO | decoder input length: 185
2026-01-28 15:15:48,141 | INFO | max output length: 185
2026-01-28 15:15:48,141 | INFO | min output length: 18
2026-01-28 15:15:52,518 | INFO | end detected at 50
2026-01-28 15:15:52,520 | INFO |  -7.02 * 0.5 =  -3.51 for decoder
2026-01-28 15:15:52,520 | INFO |  -8.24 * 0.5 =  -4.12 for ctc
2026-01-28 15:15:52,520 | INFO | total log probability: -7.63
2026-01-28 15:15:52,521 | INFO | normalized log probability: -0.17
2026-01-28 15:15:52,521 | INFO | total number of ended hypotheses: 179
2026-01-28 15:15:52,521 | INFO | best hypo: ▁pas▁du▁tout▁je▁ne▁suis▁pas▁du▁tout▁quelqu'un▁d'extraverti▁et▁je▁ne▁l'étais▁surtout▁pas▁à▁cet▁as▁là

2026-01-28 15:15:52,524 | INFO | speech length: 10880
2026-01-28 15:15:52,569 | INFO | decoder input length: 16
2026-01-28 15:15:52,569 | INFO | max output length: 16
2026-01-28 15:15:52,569 | INFO | min output length: 1
2026-01-28 15:15:53,343 | INFO | end detected at 9
2026-01-28 15:15:53,345 | INFO |  -1.64 * 0.5 =  -0.82 for decoder
2026-01-28 15:15:53,346 | INFO |  -3.47 * 0.5 =  -1.73 for ctc
2026-01-28 15:15:53,346 | INFO | total log probability: -2.56
2026-01-28 15:15:53,346 | INFO | normalized log probability: -0.85
2026-01-28 15:15:53,346 | INFO | total number of ended hypotheses: 186
2026-01-28 15:15:53,346 | INFO | best hypo: ▁six

2026-01-28 15:15:53,348 | INFO | speech length: 465600
2026-01-28 15:15:53,397 | INFO | decoder input length: 727
2026-01-28 15:15:53,397 | INFO | max output length: 727
2026-01-28 15:15:53,397 | INFO | min output length: 72
2026-01-28 15:16:39,131 | INFO | end detected at 260
2026-01-28 15:16:39,133 | INFO | -577.69 * 0.5 = -288.84 for decoder
2026-01-28 15:16:39,133 | INFO | -261.58 * 0.5 = -130.79 for ctc
2026-01-28 15:16:39,133 | INFO | total log probability: -419.63
2026-01-28 15:16:39,133 | INFO | normalized log probability: -1.65
2026-01-28 15:16:39,133 | INFO | total number of ended hypotheses: 163
2026-01-28 15:16:39,136 | INFO | best hypo: ▁je▁crois▁que▁j'avais▁aucune▁des▁qualités▁ni▁des▁défauts▁qui▁font▁une▁bonne▁comédienne▁il▁y▁avant▁même▁dans▁la▁vie▁des▁enchaînements▁normaux▁car▁vous▁avez▁tourné▁avec▁jean▁renoir▁je▁veux▁dire▁que▁oui▁vous▁aviez▁travaillé▁avec▁jean▁renoir▁ma▁grande▁illusion▁il▁y▁avait▁jacques▁becker▁avec▁jacques▁becker▁ensuite▁c'étaient▁autre▁enchos▁car▁vous▁avez▁trahi▁de▁travaillé▁directement▁avec▁lui▁oui▁à▁ce▁dire▁nous▁avons▁étéau▁tous▁et▁nous▁avon▁tous▁les▁docu▁l'abats▁avec▁renoir▁et▁nous▁nous▁avons▁et▁eu▁beaucute▁d'amitiés▁d'un▁pour▁l'autre▁pendant▁la▁grande▁et▁illusités▁pour▁la▁des▁raiss▁que▁j'ai

2026-01-28 15:16:39,139 | INFO | speech length: 240160
2026-01-28 15:16:39,188 | INFO | decoder input length: 374
2026-01-28 15:16:39,189 | INFO | max output length: 374
2026-01-28 15:16:39,189 | INFO | min output length: 37
2026-01-28 15:16:56,159 | INFO | end detected at 106
2026-01-28 15:16:56,161 | INFO | -87.02 * 0.5 = -43.51 for decoder
2026-01-28 15:16:56,161 | INFO | -26.92 * 0.5 = -13.46 for ctc
2026-01-28 15:16:56,161 | INFO | total log probability: -56.97
2026-01-28 15:16:56,161 | INFO | normalized log probability: -0.57
2026-01-28 15:16:56,161 | INFO | total number of ended hypotheses: 170
2026-01-28 15:16:56,163 | INFO | best hypo: ▁déjà▁dites▁une▁fois▁je▁m'excuse▁de▁les▁répéter▁si▁vous▁les▁connaissez▁et▁c'est▁que▁il▁y▁avait▁beaucoup▁de▁choses▁communes▁entre▁nous▁nous▁étions▁tous▁les▁deux▁d'origine▁bourgeoise▁élevé▁un▁peu▁de▁la▁la▁même▁manière▁c'est▁à▁dire

2026-01-28 15:16:56,166 | INFO | speech length: 251838
2026-01-28 15:16:56,224 | INFO | decoder input length: 392
2026-01-28 15:16:56,224 | INFO | max output length: 392
2026-01-28 15:16:56,224 | INFO | min output length: 39
2026-01-28 15:17:18,097 | INFO | end detected at 122
2026-01-28 15:17:18,099 | INFO | -147.76 * 0.5 = -73.88 for decoder
2026-01-28 15:17:18,099 | INFO | -69.33 * 0.5 = -34.67 for ctc
2026-01-28 15:17:18,099 | INFO | total log probability: -108.54
2026-01-28 15:17:18,099 | INFO | normalized log probability: -0.92
2026-01-28 15:17:18,100 | INFO | total number of ended hypotheses: 144
2026-01-28 15:17:18,102 | INFO | best hypo: ▁dix▁ans▁d'une▁façon▁à▁vue▁britannique▁dans▁le▁comportement▁n'est▁ce▁pas▁on▁ne▁pleure▁pas▁on▁se▁tient▁toujours▁wayse▁leder▁nicussen▁qui▁peu▁rigide▁à▁cet▁égard▁nous▁avions▁donc▁seuls▁communs▁nous▁étions▁tous▁les▁deux▁extraordinairements▁pauvres▁à▁ce▁moment▁là

2026-01-28 15:17:18,105 | INFO | speech length: 46560
2026-01-28 15:17:18,152 | INFO | decoder input length: 72
2026-01-28 15:17:18,153 | INFO | max output length: 72
2026-01-28 15:17:18,153 | INFO | min output length: 7
2026-01-28 15:17:20,588 | INFO | end detected at 23
2026-01-28 15:17:20,590 | INFO |  -1.35 * 0.5 =  -0.68 for decoder
2026-01-28 15:17:20,590 | INFO |  -2.63 * 0.5 =  -1.32 for ctc
2026-01-28 15:17:20,590 | INFO | total log probability: -1.99
2026-01-28 15:17:20,590 | INFO | normalized log probability: -0.11
2026-01-28 15:17:20,590 | INFO | total number of ended hypotheses: 160
2026-01-28 15:17:20,590 | INFO | best hypo: ▁en▁contact▁avec▁ce▁milieu▁de▁cinéma

2026-01-28 15:17:20,593 | INFO | speech length: 123360
2026-01-28 15:17:20,649 | INFO | decoder input length: 192
2026-01-28 15:17:20,649 | INFO | max output length: 192
2026-01-28 15:17:20,649 | INFO | min output length: 19
2026-01-28 15:17:28,618 | INFO | end detected at 64
2026-01-28 15:17:28,620 | INFO | -10.43 * 0.5 =  -5.22 for decoder
2026-01-28 15:17:28,620 | INFO |  -6.35 * 0.5 =  -3.17 for ctc
2026-01-28 15:17:28,620 | INFO | total log probability: -8.39
2026-01-28 15:17:28,620 | INFO | normalized log probability: -0.14
2026-01-28 15:17:28,620 | INFO | total number of ended hypotheses: 167
2026-01-28 15:17:28,621 | INFO | best hypo: ▁très▁très▁exhibitionniste▁en▁matière▁d'argent▁et▁nous▁allions▁tous▁les▁deux▁la▁passion▁de▁la▁musique▁et▁nous▁jouions▁tous▁les▁deux▁du▁piano

2026-01-28 15:17:28,635 | INFO | Chunk: 0 | WER=9.523810 | S=1 D=0 I=1
2026-01-28 15:17:28,636 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:17:28,636 | INFO | Chunk: 2 | WER=33.333333 | S=4 D=0 I=1
2026-01-28 15:17:28,638 | INFO | Chunk: 3 | WER=10.000000 | S=5 D=0 I=0
2026-01-28 15:17:28,639 | INFO | Chunk: 4 | WER=20.000000 | S=2 D=4 I=0
2026-01-28 15:17:28,640 | INFO | Chunk: 5 | WER=3.846154 | S=1 D=0 I=0
2026-01-28 15:17:28,640 | INFO | Chunk: 6 | WER=25.000000 | S=2 D=1 I=0
2026-01-28 15:17:28,641 | INFO | Chunk: 7 | WER=5.263158 | S=0 D=1 I=0
2026-01-28 15:17:28,645 | INFO | Chunk: 8 | WER=30.864198 | S=14 D=3 I=8
2026-01-28 15:17:28,647 | INFO | Chunk: 9 | WER=26.923077 | S=9 D=3 I=2
2026-01-28 15:17:28,649 | INFO | Chunk: 10 | WER=12.000000 | S=5 D=1 I=0
2026-01-28 15:17:28,649 | INFO | Chunk: 11 | WER=17.647059 | S=3 D=0 I=0
2026-01-28 15:17:28,652 | INFO | Chunk: 12 | WER=27.536232 | S=14 D=5 I=0
2026-01-28 15:17:28,653 | INFO | Chunk: 13 | WER=20.000000 | S=2 D=1 I=0
2026-01-28 15:17:28,653 | INFO | Chunk: 14 | WER=46.153846 | S=8 D=4 I=0
2026-01-28 15:17:28,654 | INFO | Chunk: 15 | WER=18.181818 | S=1 D=1 I=0
2026-01-28 15:17:28,654 | INFO | Chunk: 16 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:17:28,657 | INFO | Chunk: 17 | WER=31.428571 | S=12 D=7 I=3
2026-01-28 15:17:28,657 | INFO | Chunk: 18 | WER=28.571429 | S=2 D=0 I=0
2026-01-28 15:17:28,658 | INFO | Chunk: 19 | WER=18.750000 | S=1 D=1 I=1
2026-01-28 15:17:28,658 | INFO | Chunk: 20 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:17:28,658 | INFO | Chunk: 21 | WER=40.000000 | S=1 D=1 I=0
2026-01-28 15:17:28,660 | INFO | Chunk: 22 | WER=5.882353 | S=1 D=1 I=0
2026-01-28 15:17:28,660 | INFO | Chunk: 23 | WER=21.428571 | S=6 D=0 I=0
2026-01-28 15:17:28,668 | INFO | Chunk: 24 | WER=28.181818 | S=19 D=9 I=3
2026-01-28 15:17:28,676 | INFO | Chunk: 25 | WER=31.092437 | S=17 D=18 I=2
2026-01-28 15:17:28,683 | INFO | Chunk: 26 | WER=20.689655 | S=11 D=8 I=5
2026-01-28 15:17:28,687 | INFO | Chunk: 27 | WER=32.051282 | S=16 D=4 I=5
2026-01-28 15:17:28,689 | INFO | Chunk: 28 | WER=29.824561 | S=6 D=10 I=1
2026-01-28 15:17:28,689 | INFO | Chunk: 29 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:17:28,690 | INFO | Chunk: 30 | WER=37.037037 | S=7 D=3 I=0
2026-01-28 15:17:28,698 | INFO | Chunk: 31 | WER=38.281250 | S=25 D=20 I=4
2026-01-28 15:17:28,699 | INFO | Chunk: 32 | WER=28.000000 | S=4 D=3 I=0
2026-01-28 15:17:28,705 | INFO | Chunk: 33 | WER=35.849057 | S=26 D=11 I=1
2026-01-28 15:17:28,712 | INFO | Chunk: 34 | WER=23.008850 | S=19 D=1 I=6
2026-01-28 15:17:28,713 | INFO | Chunk: 35 | WER=11.111111 | S=1 D=0 I=1
2026-01-28 15:17:28,715 | INFO | Chunk: 36 | WER=31.914894 | S=9 D=5 I=1
2026-01-28 15:17:28,715 | INFO | Chunk: 37 | WER=8.000000 | S=1 D=1 I=0
2026-01-28 15:17:28,715 | INFO | Chunk: 38 | WER=100.000000 | S=1 D=0 I=0
2026-01-28 15:17:28,723 | INFO | Chunk: 39 | WER=34.513274 | S=21 D=9 I=9
2026-01-28 15:17:28,725 | INFO | Chunk: 40 | WER=12.500000 | S=1 D=3 I=2
2026-01-28 15:17:28,727 | INFO | Chunk: 41 | WER=40.983607 | S=10 D=14 I=1
2026-01-28 15:17:28,727 | INFO | Chunk: 42 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:17:28,728 | INFO | Chunk: 43 | WER=7.407407 | S=1 D=1 I=0
2026-01-28 15:17:30,712 | INFO | File: Rhap-D2001.wav | WER=25.893320 | S=289 D=154 I=57
2026-01-28 15:17:30,712 | INFO | ------------------------------
2026-01-28 15:17:30,713 | INFO | Conf cv Done!
2026-01-28 15:17:30,894 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 15:17:30,917 | INFO | Vocabulary size: 47
2026-01-28 15:17:31,817 | INFO | Gradient checkpoint layers: []
2026-01-28 15:17:32,914 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 15:17:32,922 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 15:17:32,923 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 15:17:32,924 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 15:17:32,928 | INFO | speech length: 148800
2026-01-28 15:17:32,989 | INFO | decoder input length: 232
2026-01-28 15:17:32,990 | INFO | max output length: 232
2026-01-28 15:17:32,990 | INFO | min output length: 23
2026-01-28 15:17:54,975 | INFO | end detected at 144
2026-01-28 15:17:54,979 | INFO | -13.68 * 0.5 =  -6.84 for decoder
2026-01-28 15:17:54,979 | INFO |  -7.81 * 0.5 =  -3.90 for ctc
2026-01-28 15:17:54,979 | INFO | total log probability: -10.74
2026-01-28 15:17:54,979 | INFO | normalized log probability: -0.08
2026-01-28 15:17:54,979 | INFO | total number of ended hypotheses: 176
2026-01-28 15:17:54,982 | INFO | best hypo: vous<space>étiez<space>stélonectilo<space>vous<space>êtes<space>directrice<space>de<space>l'express<space>deux<space>petites<space>phrases<space>deux<space>vrais<space>options<space>qui<space>dessinent<space>votre<space>route<space>une<space>route

2026-01-28 15:17:54,986 | INFO | speech length: 50720
2026-01-28 15:17:55,042 | INFO | decoder input length: 78
2026-01-28 15:17:55,042 | INFO | max output length: 78
2026-01-28 15:17:55,042 | INFO | min output length: 7
2026-01-28 15:18:01,961 | INFO | end detected at 73
2026-01-28 15:18:01,962 | INFO |  -6.19 * 0.5 =  -3.09 for decoder
2026-01-28 15:18:01,962 | INFO |  -1.10 * 0.5 =  -0.55 for ctc
2026-01-28 15:18:01,962 | INFO | total log probability: -3.64
2026-01-28 15:18:01,962 | INFO | normalized log probability: -0.05
2026-01-28 15:18:01,962 | INFO | total number of ended hypotheses: 173
2026-01-28 15:18:01,963 | INFO | best hypo: qui<space>témoignent<space>d'une<space>certaine<space>d'une<space>bonne<space>d'une<space>très<space>bonne<space>conduite

2026-01-28 15:18:01,966 | INFO | speech length: 92800
2026-01-28 15:18:02,014 | INFO | decoder input length: 144
2026-01-28 15:18:02,014 | INFO | max output length: 144
2026-01-28 15:18:02,014 | INFO | min output length: 14
2026-01-28 15:18:11,966 | INFO | end detected at 100
2026-01-28 15:18:11,970 | INFO |  -9.86 * 0.5 =  -4.93 for decoder
2026-01-28 15:18:11,971 | INFO | -10.81 * 0.5 =  -5.40 for ctc
2026-01-28 15:18:11,971 | INFO | total log probability: -10.33
2026-01-28 15:18:11,971 | INFO | normalized log probability: -0.11
2026-01-28 15:18:11,971 | INFO | total number of ended hypotheses: 238
2026-01-28 15:18:11,972 | INFO | best hypo: françois<space>gayrou<space>vous<space>occupez<space>un<space>poste<space>d'observation<space>que<space>les<space>gens<space>haut<space>placé<space>vous<space>envient

2026-01-28 15:18:11,977 | INFO | speech length: 235200
2026-01-28 15:18:12,082 | INFO | decoder input length: 367
2026-01-28 15:18:12,082 | INFO | max output length: 367
2026-01-28 15:18:12,082 | INFO | min output length: 36
2026-01-28 15:18:40,776 | INFO | end detected at 254
2026-01-28 15:18:40,780 | INFO | -21.38 * 0.5 = -10.69 for decoder
2026-01-28 15:18:40,780 | INFO |  -1.20 * 0.5 =  -0.60 for ctc
2026-01-28 15:18:40,780 | INFO | total log probability: -11.29
2026-01-28 15:18:40,780 | INFO | normalized log probability: -0.05
2026-01-28 15:18:40,780 | INFO | total number of ended hypotheses: 204
2026-01-28 15:18:40,784 | INFO | best hypo: si<space>je<space>ne<space>craignais<space>pas<space>d'entrer<space>dans<space>le<space>jeu<space>de<space>certains<space>hommes<space>qui<space>abusent<space>de<space>leurs<space>conditions<space>je<space>dirais<space>que<space>vous<space>avez<space>donné<space>quelque<space>chose<space>de<space>plus<space>à<space>la<space>femme<space>des<space>armes<space>de<space>persuasion<space>usez<space>vous<space>à<space>ce<space>niveau<space>du<space>charme<space>de<space>la<space>femme<space>pour<space>réussir<space>tout<space>à

2026-01-28 15:18:40,788 | INFO | speech length: 95840
2026-01-28 15:18:40,851 | INFO | decoder input length: 149
2026-01-28 15:18:40,851 | INFO | max output length: 149
2026-01-28 15:18:40,852 | INFO | min output length: 14
2026-01-28 15:18:55,934 | INFO | adding <eos> in the last position in the loop
2026-01-28 15:18:55,944 | INFO | no hypothesis. Finish decoding.
2026-01-28 15:18:55,944 | INFO | -36.83 * 0.5 = -18.42 for decoder
2026-01-28 15:18:55,944 | INFO | -101.36 * 0.5 = -50.68 for ctc
2026-01-28 15:18:55,944 | INFO | total log probability: -69.10
2026-01-28 15:18:55,945 | INFO | normalized log probability: -0.48
2026-01-28 15:18:55,945 | INFO | total number of ended hypotheses: 123
2026-01-28 15:18:55,946 | INFO | best hypo: je<space>suis<space>incapable<space>de<space>vous<space>rpondre<space>sur<space>ce<space>point<space>et<space>ni<space>que<space>le<space>homme<space>qui<space>travaille<space>avec<space>moi<space>ou<space>qui<space>ont<space>travail<space>avec<space>moi<space>qui<space>pourrait<space>vous<space>répondre

2026-01-28 15:18:55,948 | INFO | speech length: 146560
2026-01-28 15:18:55,987 | INFO | decoder input length: 228
2026-01-28 15:18:55,987 | INFO | max output length: 228
2026-01-28 15:18:55,987 | INFO | min output length: 22
2026-01-28 15:19:14,358 | INFO | end detected at 160
2026-01-28 15:19:14,360 | INFO | -12.71 * 0.5 =  -6.35 for decoder
2026-01-28 15:19:14,360 | INFO |  -0.62 * 0.5 =  -0.31 for ctc
2026-01-28 15:19:14,360 | INFO | total log probability: -6.67
2026-01-28 15:19:14,360 | INFO | normalized log probability: -0.04
2026-01-28 15:19:14,360 | INFO | total number of ended hypotheses: 165
2026-01-28 15:19:14,362 | INFO | best hypo: je<space>voudrais<space>simplement<space>reprendre<space>un<space>mot<space>dans<space>ce<space>que<space>vous<space>avez<space>dit<space>vous<space>avez<space>dit<space>que<space>euh<space>disons<space>ma<space>carrière<space>pour<space>simplifier<space>témoigner<space>de<space>ma<space>bonne<space>conduite

2026-01-28 15:19:14,365 | INFO | speech length: 43680
2026-01-28 15:19:14,420 | INFO | decoder input length: 67
2026-01-28 15:19:14,420 | INFO | max output length: 67
2026-01-28 15:19:14,420 | INFO | min output length: 6
2026-01-28 15:19:20,411 | INFO | end detected at 64
2026-01-28 15:19:20,413 | INFO |  -5.61 * 0.5 =  -2.80 for decoder
2026-01-28 15:19:20,413 | INFO |  -1.08 * 0.5 =  -0.54 for ctc
2026-01-28 15:19:20,413 | INFO | total log probability: -3.34
2026-01-28 15:19:20,413 | INFO | normalized log probability: -0.06
2026-01-28 15:19:20,413 | INFO | total number of ended hypotheses: 168
2026-01-28 15:19:20,414 | INFO | best hypo: il<space>me<space>semble<space>que<space>c'est<space>le<space>contraire<space>vous<space>m'expliquez<space>alors

2026-01-28 15:19:20,416 | INFO | speech length: 68480
2026-01-28 15:19:20,466 | INFO | decoder input length: 106
2026-01-28 15:19:20,466 | INFO | max output length: 106
2026-01-28 15:19:20,466 | INFO | min output length: 10
2026-01-28 15:19:28,572 | INFO | end detected at 88
2026-01-28 15:19:28,573 | INFO |  -7.28 * 0.5 =  -3.64 for decoder
2026-01-28 15:19:28,573 | INFO |  -2.23 * 0.5 =  -1.11 for ctc
2026-01-28 15:19:28,573 | INFO | total log probability: -4.75
2026-01-28 15:19:28,573 | INFO | normalized log probability: -0.06
2026-01-28 15:19:28,573 | INFO | total number of ended hypotheses: 150
2026-01-28 15:19:28,574 | INFO | best hypo: bien<space>je<space>crois<space>que<space>je<space>ne<space>me<space>suis<space>pas<space>conduit<space>d'une<space>façon<space>conforme<space>à<space>ce<space>qu'on<space>attend

2026-01-28 15:19:28,576 | INFO | speech length: 393920
2026-01-28 15:19:28,617 | INFO | decoder input length: 615
2026-01-28 15:19:28,617 | INFO | max output length: 615
2026-01-28 15:19:28,617 | INFO | min output length: 61
2026-01-28 15:20:30,784 | INFO | end detected at 424
2026-01-28 15:20:30,786 | INFO | -145.67 * 0.5 = -72.84 for decoder
2026-01-28 15:20:30,786 | INFO | -40.93 * 0.5 = -20.46 for ctc
2026-01-28 15:20:30,786 | INFO | total log probability: -93.30
2026-01-28 15:20:30,786 | INFO | normalized log probability: -0.22
2026-01-28 15:20:30,786 | INFO | total number of ended hypotheses: 167
2026-01-28 15:20:30,792 | INFO | best hypo: c'est<space>d'une<space>jeune<space>fille<space>d'abord<space>est<space>une<space>femme<space>ensuite<space>une<space>jeune<space>bourgeoise<space>disons<space>d'une<space>jeune<space>bourgeoise<space>voilà<space>vous<space>le<space>regrettez<space>aujourd'hui<space>ah<space>non<space>pas<space>du<space>tout<space>je<space>voudrais<space>reprendre<space>euh<space>des<space>phrases<space>de<space>marché<space>et<space>la<space>char<space>et<space>vous<space>me<space>direz<space>si<space>vous<space>êtes<space>d'accord<space>marcel<space>achat<space>écrivait<space>elle<space>est<space>très<space>jolie<space>elle<space>est<space>même<space>belle<space>elle<space>est<space>élégante<space>on<space>s'en<space>aperçoit<space>tout<space>de<space>suite<space>mais<space>on<space>l'oublie<space>dès<space>qu'on<space>a<space>regadé<space>ses<space>yeux

2026-01-28 15:20:30,796 | INFO | speech length: 245920
2026-01-28 15:20:30,896 | INFO | decoder input length: 383
2026-01-28 15:20:30,896 | INFO | max output length: 383
2026-01-28 15:20:30,896 | INFO | min output length: 38
2026-01-28 15:21:08,217 | INFO | end detected at 286
2026-01-28 15:21:08,219 | INFO | -24.98 * 0.5 = -12.49 for decoder
2026-01-28 15:21:08,220 | INFO |  -4.14 * 0.5 =  -2.07 for ctc
2026-01-28 15:21:08,220 | INFO | total log probability: -14.56
2026-01-28 15:21:08,220 | INFO | normalized log probability: -0.05
2026-01-28 15:21:08,220 | INFO | total number of ended hypotheses: 188
2026-01-28 15:21:08,224 | INFO | best hypo: des<space>yeux<space>qui<space>sont<space>en<space>même<space>temps<space>et<space>c'est<space>difficile<space>profond<space>et<space>persan<space>plein<space>de<space>curiosités<space>de<space>mystères<space>des<space>yeux<space>qui<space>enregistrent<space>chaque<space>détail<space>avec<space>l'implacable<space>précision<space>d'une<space>caméra<space>et<space>j'ai<space>l'impression<space>que<space>c'est<space>avec<space>ses<space>yeux<space>que<space>vous<space>avez<space>dessiné<space>votre<space>route<space>françois<space>le<space>géron

2026-01-28 15:21:08,228 | INFO | speech length: 163360
2026-01-28 15:21:08,292 | INFO | decoder input length: 254
2026-01-28 15:21:08,292 | INFO | max output length: 254
2026-01-28 15:21:08,292 | INFO | min output length: 25
2026-01-28 15:21:45,381 | INFO | adding <eos> in the last position in the loop
2026-01-28 15:21:45,398 | INFO | no hypothesis. Finish decoding.
2026-01-28 15:21:45,399 | INFO | -50.90 * 0.5 = -25.45 for decoder
2026-01-28 15:21:45,399 | INFO | -225.07 * 0.5 = -112.54 for ctc
2026-01-28 15:21:45,400 | INFO | total log probability: -137.99
2026-01-28 15:21:45,400 | INFO | normalized log probability: -0.56
2026-01-28 15:21:45,400 | INFO | total number of ended hypotheses: 124
2026-01-28 15:21:45,405 | INFO | best hypo: écoutezvous<space>m'avez<space>demand<space>si<space>je<space>suis<space>d'accord<space>avec<space>marcel<space>achar<space>je<space>suppose<space>que<space>vous<space>connaissez<space>marcel<space>achar<space>c'est<space>un<space>homme<space>incapable<space>de<space>dire<space>quoi<space>que<space>ce<space>soit<space>désagréable<space>sur<space>qui<space>que<space>ce<space>soit<space>je<space>ne<space>vx<space>pas<space>pourquoi<space>j'aurai<space>fait<space>exception<space>à<space>cet<space>règle

2026-01-28 15:21:45,409 | INFO | speech length: 72960
2026-01-28 15:21:45,471 | INFO | decoder input length: 113
2026-01-28 15:21:45,471 | INFO | max output length: 113
2026-01-28 15:21:45,471 | INFO | min output length: 11
2026-01-28 15:21:55,882 | INFO | end detected at 81
2026-01-28 15:21:55,886 | INFO | -10.30 * 0.5 =  -5.15 for decoder
2026-01-28 15:21:55,886 | INFO |  -3.57 * 0.5 =  -1.78 for ctc
2026-01-28 15:21:55,887 | INFO | total log probability: -6.93
2026-01-28 15:21:55,887 | INFO | normalized log probability: -0.09
2026-01-28 15:21:55,887 | INFO | total number of ended hypotheses: 193
2026-01-28 15:21:55,888 | INFO | best hypo: et<space>est<space>ce<space>que<space>c'est<space>avec<space>ses<space>yeux<space>si<space>vous<space>voulez<space>dire<space>que<space>je<space>s'ai<space>regardé

2026-01-28 15:21:55,893 | INFO | speech length: 325760
2026-01-28 15:21:55,943 | INFO | decoder input length: 508
2026-01-28 15:21:55,943 | INFO | max output length: 508
2026-01-28 15:21:55,943 | INFO | min output length: 50
2026-01-28 15:22:50,615 | INFO | end detected at 382
2026-01-28 15:22:50,617 | INFO | -60.97 * 0.5 = -30.49 for decoder
2026-01-28 15:22:50,618 | INFO | -17.29 * 0.5 =  -8.65 for ctc
2026-01-28 15:22:50,618 | INFO | total log probability: -39.13
2026-01-28 15:22:50,618 | INFO | normalized log probability: -0.11
2026-01-28 15:22:50,618 | INFO | total number of ended hypotheses: 211
2026-01-28 15:22:50,623 | INFO | best hypo: je<space>crois<space>que<space>je<space>suis<space>regardée<space>mais<space>je<space>crois<space>que<space>j'ai<space>toujours<space>oui<space>eu<space>eu<space>cette<space>cette<space>euh<space>disons<space>cette<space>faculté<space>ce<space>qui<space>vous<space>a<space>toujours<space>intéressé<space>c'était<space>chez<space>les<space>gens<space>le<space>mécanisme<space>de<space>la<space>carrière<space>de<space>mécanisme<space>tout<space>simplement<space>pas<space>spécialement<space>de<space>la<space>carrière<space>et<space>la<space>personnalité<space>aussi<space>fin<space>j'aime<space>bien<space>comprendre<space>comment<space>ça<space>marche<space>les<space>autres<space>ça<space>c'est<space>bon<space>je<space>l'ai<space>toujours<space>eu<space>je

2026-01-28 15:22:50,627 | INFO | speech length: 46880
2026-01-28 15:22:50,671 | INFO | decoder input length: 72
2026-01-28 15:22:50,671 | INFO | max output length: 72
2026-01-28 15:22:50,671 | INFO | min output length: 7
2026-01-28 15:22:57,637 | INFO | adding <eos> in the last position in the loop
2026-01-28 15:22:57,645 | INFO | no hypothesis. Finish decoding.
2026-01-28 15:22:57,646 | INFO | -35.96 * 0.5 = -17.98 for decoder
2026-01-28 15:22:57,646 | INFO | -41.17 * 0.5 = -20.58 for ctc
2026-01-28 15:22:57,646 | INFO | total log probability: -38.57
2026-01-28 15:22:57,646 | INFO | normalized log probability: -0.54
2026-01-28 15:22:57,646 | INFO | total number of ended hypotheses: 70
2026-01-28 15:22:57,647 | INFO | best hypo: par<space>consquent<space>j'ai<space>e<space>envie<space>de<space>savoir<space>un<space>petpeu<space>mieux<space>comme<space>ça<space>marhait

2026-01-28 15:22:57,648 | INFO | speech length: 96480
2026-01-28 15:22:57,688 | INFO | decoder input length: 150
2026-01-28 15:22:57,688 | INFO | max output length: 150
2026-01-28 15:22:57,688 | INFO | min output length: 15
2026-01-28 15:23:11,108 | INFO | end detected at 146
2026-01-28 15:23:11,110 | INFO | -16.99 * 0.5 =  -8.50 for decoder
2026-01-28 15:23:11,110 | INFO | -26.40 * 0.5 = -13.20 for ctc
2026-01-28 15:23:11,110 | INFO | total log probability: -21.70
2026-01-28 15:23:11,110 | INFO | normalized log probability: -0.16
2026-01-28 15:23:11,110 | INFO | total number of ended hypotheses: 207
2026-01-28 15:23:11,112 | INFO | best hypo: et<space>puis<space>je<space>suis<space>toujours<space>étonné<space>maintenant<space>plus<space>et<space>et<space>je<space>précise<space>d'ailleurs<space>que<space>quand<space>marcel<space>achat<space>avec<space>les<space>textes<space>et<space>elle<space>est<space>déjà<space>prête

2026-01-28 15:23:11,115 | INFO | speech length: 101120
2026-01-28 15:23:11,158 | INFO | decoder input length: 157
2026-01-28 15:23:11,158 | INFO | max output length: 157
2026-01-28 15:23:11,158 | INFO | min output length: 15
2026-01-28 15:23:17,231 | INFO | end detected at 63
2026-01-28 15:23:17,233 | INFO |  -4.42 * 0.5 =  -2.21 for decoder
2026-01-28 15:23:17,234 | INFO |  -1.33 * 0.5 =  -0.67 for ctc
2026-01-28 15:23:17,234 | INFO | total log probability: -2.88
2026-01-28 15:23:17,234 | INFO | normalized log probability: -0.05
2026-01-28 15:23:17,234 | INFO | total number of ended hypotheses: 198
2026-01-28 15:23:17,235 | INFO | best hypo: dix<space>sept<space>ans<space>et<space>mais<space>ensuite<space>j'ai<space>toujours<space>été<space>étonné

2026-01-28 15:23:17,238 | INFO | speech length: 53440
2026-01-28 15:23:17,278 | INFO | decoder input length: 83
2026-01-28 15:23:17,278 | INFO | max output length: 83
2026-01-28 15:23:17,278 | INFO | min output length: 8
2026-01-28 15:23:20,551 | INFO | adding <eos> in the last position in the loop
2026-01-28 15:23:20,561 | INFO | no hypothesis. Finish decoding.
2026-01-28 15:23:20,562 | INFO |  -6.22 * 0.5 =  -3.11 for decoder
2026-01-28 15:23:20,562 | INFO |  -0.75 * 0.5 =  -0.38 for ctc
2026-01-28 15:23:20,562 | INFO | total log probability: -3.49
2026-01-28 15:23:20,563 | INFO | normalized log probability: -0.04
2026-01-28 15:23:20,563 | INFO | total number of ended hypotheses: 166
2026-01-28 15:23:20,564 | INFO | best hypo: de<space>voir<space>que<space>les<space>gens<space>ne<space>voyaient<space>pas<space>des<space>choses<space>qui<space>me<space>paraissaient<space>évidentes

2026-01-28 15:23:20,566 | INFO | speech length: 270240
2026-01-28 15:23:20,614 | INFO | decoder input length: 421
2026-01-28 15:23:20,614 | INFO | max output length: 421
2026-01-28 15:23:20,614 | INFO | min output length: 42
2026-01-28 15:23:46,343 | INFO | end detected at 332
2026-01-28 15:23:46,346 | INFO | -33.33 * 0.5 = -16.67 for decoder
2026-01-28 15:23:46,346 | INFO |  -7.49 * 0.5 =  -3.74 for ctc
2026-01-28 15:23:46,346 | INFO | total log probability: -20.41
2026-01-28 15:23:46,346 | INFO | normalized log probability: -0.06
2026-01-28 15:23:46,346 | INFO | total number of ended hypotheses: 206
2026-01-28 15:23:46,351 | INFO | best hypo: dans<space>le<space>comportement<space>des<space>autres<space>je<space>veux<space>dire<space>j'ai<space>toujours<space>l'impression<space>que<space>c'est<space>c'est<space>énorme<space>que<space>c'est<space>là<space>en<space>noir<space>sur<space>blanc<space>et<space>les<space>autres<space>ne<space>voient<space>pas<space>ça<space>m'étonne<space>est<space>ce<space>que<space>vous<space>avez<space>eu<space>un<space>d<space>un<space>dessin<space>exceptionnel<space>ou<space>est<space>ce<space>simplement<space>la<space>réussite<space>qui<space>devait<space>venir<space>ou<space>s'est<space>il<space>passé<space>tout<space>à<space>fait<space>autre<space>chose<space>à<space>votre<space>vie

2026-01-28 15:23:46,355 | INFO | speech length: 31680
2026-01-28 15:23:46,397 | INFO | decoder input length: 49
2026-01-28 15:23:46,397 | INFO | max output length: 49
2026-01-28 15:23:46,397 | INFO | min output length: 4
2026-01-28 15:23:48,068 | INFO | end detected at 39
2026-01-28 15:23:48,069 | INFO |  -4.42 * 0.5 =  -2.21 for decoder
2026-01-28 15:23:48,070 | INFO |  -2.47 * 0.5 =  -1.23 for ctc
2026-01-28 15:23:48,070 | INFO | total log probability: -3.44
2026-01-28 15:23:48,070 | INFO | normalized log probability: -0.10
2026-01-28 15:23:48,070 | INFO | total number of ended hypotheses: 168
2026-01-28 15:23:48,070 | INFO | best hypo: oh<space>écoutez<space>j'ai<space>eu<space>un<space>destin<space>euh

2026-01-28 15:23:48,073 | INFO | speech length: 59680
2026-01-28 15:23:48,114 | INFO | decoder input length: 92
2026-01-28 15:23:48,114 | INFO | max output length: 92
2026-01-28 15:23:48,114 | INFO | min output length: 9
2026-01-28 15:23:51,874 | INFO | end detected at 87
2026-01-28 15:23:51,875 | INFO |  -7.27 * 0.5 =  -3.64 for decoder
2026-01-28 15:23:51,875 | INFO |  -3.26 * 0.5 =  -1.63 for ctc
2026-01-28 15:23:51,875 | INFO | total log probability: -5.27
2026-01-28 15:23:51,875 | INFO | normalized log probability: -0.06
2026-01-28 15:23:51,876 | INFO | total number of ended hypotheses: 162
2026-01-28 15:23:51,877 | INFO | best hypo: exceptionnel<space>me<space>paraît<space>un<space>grand<space>mot<space>disons<space>que<space>j'ai<space>eu<space>un<space>destin<space>un<space>peu<space>différent

2026-01-28 15:23:51,879 | INFO | speech length: 72480
2026-01-28 15:23:51,940 | INFO | decoder input length: 112
2026-01-28 15:23:51,940 | INFO | max output length: 112
2026-01-28 15:23:51,941 | INFO | min output length: 11
2026-01-28 15:23:58,355 | INFO | end detected at 85
2026-01-28 15:23:58,357 | INFO |  -6.37 * 0.5 =  -3.18 for decoder
2026-01-28 15:23:58,357 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 15:23:58,358 | INFO | total log probability: -3.19
2026-01-28 15:23:58,358 | INFO | normalized log probability: -0.04
2026-01-28 15:23:58,358 | INFO | total number of ended hypotheses: 173
2026-01-28 15:23:58,360 | INFO | best hypo: euh<space>parce<space>que<space>les<space>conditions<space>objectives<space>de<space>ma<space>vie<space>ont<space>été<space>en<space>effet<space>différentes

2026-01-28 15:23:58,364 | INFO | speech length: 35680
2026-01-28 15:23:58,412 | INFO | decoder input length: 55
2026-01-28 15:23:58,412 | INFO | max output length: 55
2026-01-28 15:23:58,412 | INFO | min output length: 5
2026-01-28 15:23:59,815 | INFO | end detected at 31
2026-01-28 15:23:59,816 | INFO |  -2.07 * 0.5 =  -1.03 for decoder
2026-01-28 15:23:59,816 | INFO |  -0.13 * 0.5 =  -0.06 for ctc
2026-01-28 15:23:59,816 | INFO | total log probability: -1.10
2026-01-28 15:23:59,816 | INFO | normalized log probability: -0.04
2026-01-28 15:23:59,816 | INFO | total number of ended hypotheses: 171
2026-01-28 15:23:59,817 | INFO | best hypo: euh<space>par<space>exemple<space>j'ai<space>été

2026-01-28 15:23:59,820 | INFO | speech length: 156960
2026-01-28 15:23:59,871 | INFO | decoder input length: 244
2026-01-28 15:23:59,872 | INFO | max output length: 244
2026-01-28 15:23:59,872 | INFO | min output length: 24
2026-01-28 15:24:11,438 | INFO | end detected at 191
2026-01-28 15:24:11,439 | INFO | -15.01 * 0.5 =  -7.51 for decoder
2026-01-28 15:24:11,439 | INFO |  -1.77 * 0.5 =  -0.89 for ctc
2026-01-28 15:24:11,439 | INFO | total log probability: -8.39
2026-01-28 15:24:11,439 | INFO | normalized log probability: -0.05
2026-01-28 15:24:11,439 | INFO | total number of ended hypotheses: 176
2026-01-28 15:24:11,442 | INFO | best hypo: j'ai<space>été<space>obligé<space>de<space>travailler<space>très<space>tôt<space>euh<space>à<space>une<space>époque<space>où<space>les<space>jeunes<space>filles<space>de<space>ma<space>génération<space>y<space>appartenant<space>au<space>monde<space>bourgeois<space>ne<space>travaillaient<space>pas<space>maintenant<space>c'est<space>tout<space>à<space>fait<space>normal

2026-01-28 15:24:11,444 | INFO | speech length: 122080
2026-01-28 15:24:11,506 | INFO | decoder input length: 190
2026-01-28 15:24:11,507 | INFO | max output length: 190
2026-01-28 15:24:11,507 | INFO | min output length: 19
2026-01-28 15:24:21,514 | INFO | end detected at 156
2026-01-28 15:24:21,517 | INFO | -14.33 * 0.5 =  -7.16 for decoder
2026-01-28 15:24:21,517 | INFO |  -1.70 * 0.5 =  -0.85 for ctc
2026-01-28 15:24:21,517 | INFO | total log probability: -8.01
2026-01-28 15:24:21,517 | INFO | normalized log probability: -0.05
2026-01-28 15:24:21,517 | INFO | total number of ended hypotheses: 203
2026-01-28 15:24:21,520 | INFO | best hypo: euh<space>disons<space>que<space>si<space>mon<space>père<space>était<space>mort<space>en<space>facile<space>les<space>circonstances<space>disons<space>qui<space>m'ont<space>conduit<space>à<space>être<space>obligé<space>de<space>gagner<space>ma<space>vie<space>très<space>tôt<space>s'était<space>produite

2026-01-28 15:24:21,524 | INFO | speech length: 455040
2026-01-28 15:24:21,577 | INFO | decoder input length: 710
2026-01-28 15:24:21,577 | INFO | max output length: 710
2026-01-28 15:24:21,577 | INFO | min output length: 71
2026-01-28 15:25:26,646 | INFO | end detected at 560
2026-01-28 15:25:26,648 | INFO | -485.53 * 0.5 = -242.77 for decoder
2026-01-28 15:25:26,648 | INFO | -38.71 * 0.5 = -19.36 for ctc
2026-01-28 15:25:26,649 | INFO | total log probability: -262.12
2026-01-28 15:25:26,649 | INFO | normalized log probability: -0.47
2026-01-28 15:25:26,649 | INFO | total number of ended hypotheses: 184
2026-01-28 15:25:26,660 | INFO | best hypo: sensiblement<space>plus<space>tard<space>je<space>ne<space>sais<space>pas<space>moi<space>quand<space>j'avais<space>vingt<space>ans<space>ou<space>vingt<space>deux<space>ans<space>eh<space>bien<space>peut<space>être<space>que<space>je<space>me<space>serais<space>mariée<space>et<space>que<space>j'aurais<space>trouvé<space>plus<space>que<space>moi<space>de<space>déposer<space>à<space>monsieur<space>avec<space>de<space>l'argent<space>et<space>ce<space>que<space>je<space>sais<space>moi<space>moi<space>j'avais<space>quatorze<space>ans<space>eh<space>ben<space>j'ai<space>choisi<space>de<space>travailler<space>alors<space>y<space>a<space>tout<space>de<space>même<space>des<space>circonstances<space>objectives<space>euh<space>pourquoi<space>j'ai<space>fait<space>du<space>journalisme<space>parce<space>que<space>euh<space>ça<space>se<space>passait<space>tout<space>de<space>suite<space>après<space>la<space>guerre<space>et<space>je<space>n'avais<space>aucune<space>raison<space>d'aller<space>faire<space>ce<space>métier<space>t<space>je<space>gainais<space>beaucoup<space>d<space>d'argent<space>dans<space>le<space>cinéma<space>euh<space>j'avais<space>un<space>bon<space>nom

2026-01-28 15:25:26,665 | INFO | speech length: 454080
2026-01-28 15:25:26,767 | INFO | decoder input length: 709
2026-01-28 15:25:26,768 | INFO | max output length: 709
2026-01-28 15:25:26,768 | INFO | min output length: 70
2026-01-28 15:26:27,677 | INFO | end detected at 612
2026-01-28 15:26:27,678 | INFO | -675.71 * 0.5 = -337.86 for decoder
2026-01-28 15:26:27,678 | INFO | -85.38 * 0.5 = -42.69 for ctc
2026-01-28 15:26:27,678 | INFO | total log probability: -380.54
2026-01-28 15:26:27,678 | INFO | normalized log probability: -0.63
2026-01-28 15:26:27,678 | INFO | total number of ended hypotheses: 186
2026-01-28 15:26:27,687 | INFO | best hypo: je<space>ne<space>s<space>n'aurais<space>certainement<space>pas<space>débuté<space>à<space>ce<space>moment<space>là<space>que<space>une<space>jeune<space>journaliste<space>pourquoi<space>i<space>se<space>trouve<space>qu'<space>y<space>a<space>eu<space>la<space>guerre<space>i<space>se<space>trouve<space>que<space>tout<space>a<space>été<space>bouleversé<space>qu'il<space>n'y<space>avait<space>plus<space>un<space>journaliste<space>à<space>paris<space>puisqu'elles<space>les<space>avaient<space>tous<space>plus<space>ou<space>moins<space>collaborés<space>et<space>que<space>au<space>moment<space>de<space>faire<space>un<space>journal<space>on<space>m'a<space>proposé<space>d'y<space>entrer<space>comme<space>directrice<space>ce<space>qui<space>est<space>incroyable<space>comme<space>directrice<space>de<space>la<space>rédaction<space>euh<space>tout<space>ça<space>ce<space>sont<space>des<space>conditions<space>objectie<space>s<space>il<space>vouran<space>quand<space>même<space>revenir<space>un<space>peu<space>au<space>début<space>il<space>y<space>a<space>eu<space>votre<space>père<space>votre<space>père<space>était<space>riche<space>riche<space>jet<space>t<space>cest<space>un<space>grand<space>mon<space>mait<space>enfin<space>disons<space>qu'il<space>appartenait<space>à<space>cet

2026-01-28 15:26:27,691 | INFO | speech length: 447040
2026-01-28 15:26:27,733 | INFO | decoder input length: 698
2026-01-28 15:26:27,733 | INFO | max output length: 698
2026-01-28 15:26:27,734 | INFO | min output length: 69
2026-01-28 15:27:58,125 | INFO | end detected at 560
2026-01-28 15:27:58,128 | INFO | -562.03 * 0.5 = -281.02 for decoder
2026-01-28 15:27:58,128 | INFO | -41.59 * 0.5 = -20.79 for ctc
2026-01-28 15:27:58,128 | INFO | total log probability: -301.81
2026-01-28 15:27:58,128 | INFO | normalized log probability: -0.55
2026-01-28 15:27:58,128 | INFO | total number of ended hypotheses: 203
2026-01-28 15:27:58,136 | INFO | best hypo: bourgeoisie<space>euh<space>qui<space>n'a<space>pas<space>de<space>problème<space>d'argent<space>ensuite<space>euh<space>disons<space>les<space>mots<space>comme<space>ils<space>ont<space>vous<space>avez<space>été<space>ruinés<space>il<space>a<space>fallu<space>travailler<space>vous<space>savez<space>ce<space>que<space>c'est<space>que<space>d'être<space>pauvre<space>je<space>sais<space>ce<space>que<space>c'est<space>que<space>d'être<space>pauvre<space>je<space>sais<space>surtout<space>ce<space>qui<space>me<space>paraît<space>plus<space>utile<space>euh<space>ce<space>que<space>c'est<space>que<space>de<space>travailler<space>dans<space>des<space>emplois<space>subalternes<space>avec<space>le<space>sentiment<space>qu'on<space>n'en<space>sortira<space>jamais<space>c'est<space>une<space>expérience<space>ça<space>qun<space>je<space>n'ai<space>jamais<space>oublié<space>ce<space>qui<space>est<space>dur<space>ce<space>n'est<space>pas<space>surtout<space>quand<space>on<space>est<space>très<space>jeun<space>je<space>n'est<space>vraiment<space>pas<space>d'être<space>pauvres<space>e<space>e<space>n'est<space>c'est<space>pas<space>grave<space>quand<space>on<space>est

2026-01-28 15:27:58,139 | INFO | speech length: 301120
2026-01-28 15:27:58,188 | INFO | decoder input length: 470
2026-01-28 15:27:58,188 | INFO | max output length: 470
2026-01-28 15:27:58,188 | INFO | min output length: 47
2026-01-28 15:28:49,556 | INFO | end detected at 363
2026-01-28 15:28:49,558 | INFO | -49.48 * 0.5 = -24.74 for decoder
2026-01-28 15:28:49,558 | INFO | -11.51 * 0.5 =  -5.76 for ctc
2026-01-28 15:28:49,558 | INFO | total log probability: -30.49
2026-01-28 15:28:49,558 | INFO | normalized log probability: -0.09
2026-01-28 15:28:49,558 | INFO | total number of ended hypotheses: 192
2026-01-28 15:28:49,562 | INFO | best hypo: ce<space>qui<space>est<space>horrible<space>c'est<space>de<space>se<space>dire<space>je<space>n'en<space>sortirai<space>jamais<space>mais<space>ce<space>que<space>vous<space>aviez<space>prévu<space>tout<space>cela<space>car<space>vous<space>aviez<space>pris<space>des<space>cours<space>de<space>stylo<space>d'actylo<space>ah<space>non<space>pas<space>du<space>tout<space>non<space>je<space>n'ai<space>pas<space>pris<space>des<space>cours<space>de<space>séodactylo<space>la<space>vérité<space>que<space>euh<space>au<space>moment<space>où<space>il<space>a<space>fallu<space>vraiment<space>que<space>je<space>gagne<space>ma<space>vie<space>je<space>vous<space>dirais<space>que<space>c'est<space>la<space>seule<space>décision<space>de<space>ma<space>vie<space>dont<space>je<space>sois<space>fier

2026-01-28 15:28:49,565 | INFO | speech length: 229440
2026-01-28 15:28:49,606 | INFO | decoder input length: 358
2026-01-28 15:28:49,606 | INFO | max output length: 358
2026-01-28 15:28:49,606 | INFO | min output length: 35
2026-01-28 15:29:28,161 | INFO | end detected at 273
2026-01-28 15:29:28,164 | INFO | -23.19 * 0.5 = -11.60 for decoder
2026-01-28 15:29:28,164 | INFO | -13.79 * 0.5 =  -6.89 for ctc
2026-01-28 15:29:28,164 | INFO | total log probability: -18.49
2026-01-28 15:29:28,165 | INFO | normalized log probability: -0.07
2026-01-28 15:29:28,165 | INFO | total number of ended hypotheses: 211
2026-01-28 15:29:28,168 | INFO | best hypo: euh<space>au<space>lieu<space>d'accepter<space>de<space>prendre<space>tout<space>de<space>suite<space>un<space>emploi<space>du<space>type<space>vendeuse<space>du<space>type<space>je<space>me<space>suis<space>dit<space>ce<space>qu'i<space>faut<space>c'est<space>avoir<space>un<space>métier<space>même<space>s'il<space>est<space>très<space>humble<space>ça<space>n'a<space>aucune<space>importance<space>i<space>faut<space>que<space>je<space>sache<space>faire<space>quelque<space>chose<space>et<space>pour<space>le<space>moment<space>je<space>ne<space>sais<space>rien<space>faire

2026-01-28 15:29:28,172 | INFO | speech length: 30880
2026-01-28 15:29:28,241 | INFO | decoder input length: 47
2026-01-28 15:29:28,241 | INFO | max output length: 47
2026-01-28 15:29:28,241 | INFO | min output length: 4
2026-01-28 15:29:33,151 | INFO | adding <eos> in the last position in the loop
2026-01-28 15:29:33,160 | INFO | no hypothesis. Finish decoding.
2026-01-28 15:29:33,160 | INFO | -23.91 * 0.5 = -11.96 for decoder
2026-01-28 15:29:33,160 | INFO | -45.57 * 0.5 = -22.78 for ctc
2026-01-28 15:29:33,160 | INFO | total log probability: -34.74
2026-01-28 15:29:33,160 | INFO | normalized log probability: -0.76
2026-01-28 15:29:33,160 | INFO | total number of ended hypotheses: 62
2026-01-28 15:29:33,161 | INFO | best hypo: je<space>sais<space>ce<space>qu'on<space>prend<space>jeune<space>fille<space>bien<space>éles

2026-01-28 15:29:33,162 | INFO | speech length: 78880
2026-01-28 15:29:33,209 | INFO | decoder input length: 122
2026-01-28 15:29:33,210 | INFO | max output length: 122
2026-01-28 15:29:33,210 | INFO | min output length: 12
2026-01-28 15:29:47,355 | INFO | adding <eos> in the last position in the loop
2026-01-28 15:29:47,364 | INFO | no hypothesis. Finish decoding.
2026-01-28 15:29:47,365 | INFO | -39.35 * 0.5 = -19.67 for decoder
2026-01-28 15:29:47,365 | INFO | -102.16 * 0.5 = -51.08 for ctc
2026-01-28 15:29:47,366 | INFO | total log probability: -70.75
2026-01-28 15:29:47,366 | INFO | normalized log probability: -0.59
2026-01-28 15:29:47,366 | INFO | total number of ended hypotheses: 132
2026-01-28 15:29:47,367 | INFO | best hypo: donc<space>qu'est<space>ce<space>qu<space>je<space>peux<space>aprendre<space>je<space>me<space>s<space>je<space>vais<space>apprendre<space>asténo<space>dactylo<space>avec<space>ça<space>quoiqu'il<space>arrive<space>j'aurai<space>un<space>métier

2026-01-28 15:29:47,370 | INFO | speech length: 452640
2026-01-28 15:29:47,423 | INFO | decoder input length: 706
2026-01-28 15:29:47,423 | INFO | max output length: 706
2026-01-28 15:29:47,423 | INFO | min output length: 70
2026-01-28 15:31:21,602 | INFO | end detected at 604
2026-01-28 15:31:21,603 | INFO | -628.73 * 0.5 = -314.36 for decoder
2026-01-28 15:31:21,603 | INFO | -97.11 * 0.5 = -48.55 for ctc
2026-01-28 15:31:21,603 | INFO | total log probability: -362.92
2026-01-28 15:31:21,603 | INFO | normalized log probability: -0.61
2026-01-28 15:31:21,603 | INFO | total number of ended hypotheses: 154
2026-01-28 15:31:21,611 | INFO | best hypo: et<space>j'ai<space>donc<space>été<space>à<space>l'école<space>ravington<space>à<space>prendre<space>la<space>sténonecture<space>c'était<space>le<space>vrai<space>départ<space>euh<space>oui<space>mais<space>enfin<space>c'est<space>comme<space>ça<space>que<space>j'ai<space>commencé<space>à<space>gagner<space>oui<space>ensuite<space>y<space>a<space>une<space>librairie<space>c'est<space>dans<space>cette<space>librairie<space>je<space>crois<space>que<space>allegre<space>vous<space>a<space>trouvé<space>euh<space>c'était<space>que<space>je<space>connaissais<space>les<space>grés<space>très<space>bien<space>euh<space>j'ai<space>donc<space>je<space>suis<space>donc<space>rentré<space>dans<space>cette<space>librairie<space>et<space>j'étais<space>à<space>la<space>fois<space>la<space>la<space>secrétaire<space>du<space>libraire<space>et<space>une<space>est<space>vendeuse<space>et<space>puis<space>il<space>ne<space>savait<space>pas<space>du<space>tout<space>lâge<space>que<space>j'avmais<space>t<space>il<space>m'a<space>laissé<space>toute<space>sa<space>librairie<space>cr<space>qui<space>n<space>était<space>pas<space>très<space>sérieux<space>mai<space>enfin<space>j'avais<space>l'air<space>d'avoir<space>dix<space>huit<space>ou<space>dix<space>neuf<space>ans<space>donc<space>

2026-01-28 15:31:21,613 | INFO | speech length: 62400
2026-01-28 15:31:21,673 | INFO | decoder input length: 97
2026-01-28 15:31:21,673 | INFO | max output length: 97
2026-01-28 15:31:21,673 | INFO | min output length: 9
2026-01-28 15:31:33,876 | INFO | adding <eos> in the last position in the loop
2026-01-28 15:31:33,891 | INFO | no hypothesis. Finish decoding.
2026-01-28 15:31:33,892 | INFO | -24.80 * 0.5 = -12.40 for decoder
2026-01-28 15:31:33,892 | INFO | -47.63 * 0.5 = -23.81 for ctc
2026-01-28 15:31:33,892 | INFO | total log probability: -36.21
2026-01-28 15:31:33,892 | INFO | normalized log probability: -0.38
2026-01-28 15:31:33,892 | INFO | total number of ended hypotheses: 69
2026-01-28 15:31:33,894 | INFO | best hypo: qui<space>quand<space>même<space>n'était<space>pas<space>trèsérieux<space>mais<space>ne<space>se<space>doutait<space>qand<space>même<space>pas<space>que<space>j'en<space>avais<space>pas<space>case

2026-01-28 15:31:33,896 | INFO | speech length: 319680
2026-01-28 15:31:33,953 | INFO | decoder input length: 499
2026-01-28 15:31:33,953 | INFO | max output length: 499
2026-01-28 15:31:33,953 | INFO | min output length: 49
2026-01-28 15:32:35,739 | INFO | end detected at 476
2026-01-28 15:32:35,740 | INFO | -231.44 * 0.5 = -115.72 for decoder
2026-01-28 15:32:35,741 | INFO | -299.31 * 0.5 = -149.66 for ctc
2026-01-28 15:32:35,741 | INFO | total log probability: -265.37
2026-01-28 15:32:35,741 | INFO | normalized log probability: -0.56
2026-01-28 15:32:35,741 | INFO | total number of ended hypotheses: 163
2026-01-28 15:32:35,747 | INFO | best hypo: et<space>je<space>connaissais<space>très<space>bien<space>marc<space>el<space>egret<space>depuis<space>très<space>longtemps<space>enfin<space>ma<space>famille<space>le<space>connaissait<space>il<space>est<space>entré<space>un<space>jour<space>dans<space>ces<space>tiberi<space>i<space>m'a<space>dit<space>mais<space>qu'est<space>ce<space>que<space>t<space>fais<space>là<space>très<space>surpris<space>j'ai<space>dit<space>ben<space>tu<space>vois<space>je<space>dans<space>des<space>livres<space>i<space>m'a<space>dit<space>qu'est<space>ce<space>que<space>tu<space>gagne<space>et<space>j'ai<space>dit<space>je<space>gagne<space>trois<space>cents<space>francs<space>par<space>mois<space>et<space>i<space>m'a<space>dit<space>mais<space>tu<space>pers<space>ton<space>temps<space>j'ai<space>bah<space>oui<space>mais<space>en<space>attendant<space>j'ai<space>pas<space>y<space>mon<space>loyes<space>enfinfin<space>on<space>je<space>je<space>j'ai<space>gai<space>gai<space>gai<space>gaileus<space>mai<space>fi<space>je<space>gagne<space>un<space>pe<space>l'argent

2026-01-28 15:32:35,749 | INFO | speech length: 465120
2026-01-28 15:32:35,797 | INFO | decoder input length: 726
2026-01-28 15:32:35,797 | INFO | max output length: 726
2026-01-28 15:32:35,797 | INFO | min output length: 72
2026-01-28 15:34:06,415 | INFO | end detected at 585
2026-01-28 15:34:06,417 | INFO | -632.85 * 0.5 = -316.43 for decoder
2026-01-28 15:34:06,417 | INFO | -81.58 * 0.5 = -40.79 for ctc
2026-01-28 15:34:06,417 | INFO | total log probability: -357.22
2026-01-28 15:34:06,417 | INFO | normalized log probability: -0.62
2026-01-28 15:34:06,417 | INFO | total number of ended hypotheses: 204
2026-01-28 15:34:06,424 | INFO | best hypo: et<space>i<space>m'a<space>dit<space>écoutez<space>tout<space>ça<space>n'est<space>pas<space>sérieux<space>i<space>faut<space>que<space>tu<space>deviennes<space>scri<space>guerle<space>par<space>exemple<space>bien<space>travailler<space>avec<space>moi<space>c'était<space>une<space>autre<space>époque<space>où<space>on<space>pouvait<space>faire<space>ça<space>aujourd'hui<space>euh<space>je<space>ne<space>crois<space>pas<space>que<space>ce<space>serait<space>possible<space>mais<space>à<space>l'époque<space>euh<space>personne<space>ne<space>savait<space>que<space>je<space>n'avais<space>jamais<space>fait<space>ce<space>métier<space>sauf<space>lui<space>tout<space>était<space>possible<space>parce<space>que<space>époque<space>là<space>oui<space>le<space>cinéma<space>était<space>beaucoup<space>moins<space>n'était<space>pas<space>une<space>organisation<space>origine<space>que<space>ce<space>e<space>le<space>l'est<space>aujour<space>'hui<space>c'est<space>peut<space>être<space>à<space>ce<space>mome<space>t<space>là<space>que<space>vous<space>avez<space>rat<space>une<space>autre<space>carrière<space>car<space>euh<space>on<space>vous<space>avait<space>proposé<space>de<space>tourner<space>dans<space>un<space>film<space>oui

2026-01-28 15:34:06,427 | INFO | speech length: 42400
2026-01-28 15:34:06,473 | INFO | decoder input length: 65
2026-01-28 15:34:06,473 | INFO | max output length: 65
2026-01-28 15:34:06,473 | INFO | min output length: 6
2026-01-28 15:34:12,894 | INFO | adding <eos> in the last position in the loop
2026-01-28 15:34:12,902 | INFO | no hypothesis. Finish decoding.
2026-01-28 15:34:12,903 | INFO | -32.42 * 0.5 = -16.21 for decoder
2026-01-28 15:34:12,903 | INFO | -107.56 * 0.5 = -53.78 for ctc
2026-01-28 15:34:12,903 | INFO | total log probability: -69.99
2026-01-28 15:34:12,903 | INFO | normalized log probability: -1.15
2026-01-28 15:34:12,903 | INFO | total number of ended hypotheses: 57
2026-01-28 15:34:12,904 | INFO | best hypo: crois<space>pas<space>j'ai<space>raté<space>me<space>carrière<space>vraiment<space>été<space>pas<space>faite<space>pour

2026-01-28 15:34:12,905 | INFO | speech length: 173920
2026-01-28 15:34:12,963 | INFO | decoder input length: 271
2026-01-28 15:34:12,963 | INFO | max output length: 271
2026-01-28 15:34:12,963 | INFO | min output length: 27
2026-01-28 15:34:43,160 | INFO | end detected at 241
2026-01-28 15:34:43,162 | INFO | -33.00 * 0.5 = -16.50 for decoder
2026-01-28 15:34:43,163 | INFO | -31.51 * 0.5 = -15.75 for ctc
2026-01-28 15:34:43,163 | INFO | total log probability: -32.26
2026-01-28 15:34:43,163 | INFO | normalized log probability: -0.14
2026-01-28 15:34:43,163 | INFO | total number of ended hypotheses: 228
2026-01-28 15:34:43,166 | INFO | best hypo: non<space>mais<space>metons<space>que<space>vous<space>ayez<space>tourné<space>dans<space>ce<space>film<space>oui<space>que<space>le<space>film<space>était<space>du<space>succès<space>soyez<space>devenu<space>une<space>venette<space>vous<space>étiez<space>normalement<space>entraînée<space>je<space>ne<space>serai<space>pas<space>devenu<space>une<space>vedette<space>je<space>crois<space>véritablement<space>que<space>je<space>n'étais<space>pas<space>douée<space>pour<space>s

2026-01-28 15:34:43,169 | INFO | speech length: 119040
2026-01-28 15:34:43,229 | INFO | decoder input length: 185
2026-01-28 15:34:43,230 | INFO | max output length: 185
2026-01-28 15:34:43,230 | INFO | min output length: 18
2026-01-28 15:34:56,656 | INFO | end detected at 115
2026-01-28 15:34:56,658 | INFO | -10.86 * 0.5 =  -5.43 for decoder
2026-01-28 15:34:56,658 | INFO |  -5.22 * 0.5 =  -2.61 for ctc
2026-01-28 15:34:56,658 | INFO | total log probability: -8.04
2026-01-28 15:34:56,658 | INFO | normalized log probability: -0.08
2026-01-28 15:34:56,658 | INFO | total number of ended hypotheses: 219
2026-01-28 15:34:56,660 | INFO | best hypo: pas<space>du<space>tout<space>je<space>ne<space>suis<space>pas<space>du<space>tout<space>quelqu'un<space>de<space>d'extroverti<space>et<space>je<space>ne<space>l'étais<space>surtout<space>pas<space>à<space>cet<space>âge<space>là

2026-01-28 15:34:56,663 | INFO | speech length: 10880
2026-01-28 15:34:56,716 | INFO | decoder input length: 16
2026-01-28 15:34:56,716 | INFO | max output length: 16
2026-01-28 15:34:56,716 | INFO | min output length: 1
2026-01-28 15:34:57,823 | INFO | end detected at 11
2026-01-28 15:34:57,825 | INFO |  -0.63 * 0.5 =  -0.31 for decoder
2026-01-28 15:34:57,825 | INFO |  -0.42 * 0.5 =  -0.21 for ctc
2026-01-28 15:34:57,825 | INFO | total log probability: -0.52
2026-01-28 15:34:57,825 | INFO | normalized log probability: -0.13
2026-01-28 15:34:57,825 | INFO | total number of ended hypotheses: 198
2026-01-28 15:34:57,825 | INFO | best hypo: et

2026-01-28 15:34:57,828 | INFO | speech length: 465600
2026-01-28 15:34:57,885 | INFO | decoder input length: 727
2026-01-28 15:34:57,885 | INFO | max output length: 727
2026-01-28 15:34:57,885 | INFO | min output length: 72
2026-01-28 15:36:48,317 | INFO | end detected at 624
2026-01-28 15:36:48,319 | INFO | -717.26 * 0.5 = -358.63 for decoder
2026-01-28 15:36:48,319 | INFO | -139.67 * 0.5 = -69.83 for ctc
2026-01-28 15:36:48,319 | INFO | total log probability: -428.46
2026-01-28 15:36:48,319 | INFO | normalized log probability: -0.69
2026-01-28 15:36:48,319 | INFO | total number of ended hypotheses: 180
2026-01-28 15:36:48,327 | INFO | best hypo: je<space>crois<space>que<space>c'avait<space>aucune<space>déqualité<space>ni<space>des<space>défauts<space>qui<space>font<space>un<space>une<space>bonne<space>camédienne<space>mais<space>il<space>y<space>a<space>quand<space>même<space>dans<space>la<space>vie<space>des<space>enchaînements<space>normaux<space>car<space>euh<space>vous<space>avez<space>tourné<space>avec<space>jean<space>renoir<space>je<space>veux<space>dire<space>que<space>oui<space>vous<space>avez<space>travaillé<space>avec<space>jean<space>renoir<space>la<space>grande<space>illusion<space>il<space>y<space>avait<space>jacques<space>becker<space>jacques<space>becker<space>ensuite<space>c'était<space>autre<space>chose<space>car<space>vous<space>avez<space>travail<space>travaillé<space>directement<space>avec<space>lui<space>oui<space>c'est<space>à<space>dire<space>qun<space>nous<space>avons<space>été<space>tous<space>les<space>deux<space>nous<space>avons<space>tous<space>les<space>deux<space>collabores<space>aves<space>es<space>aves<space>des<space>nons<space>et<space>nous<space>nous<space>nous<space>avons<space>eu<space>beaucoup<space>d'aitiés<space>l'un<space>pour<space>l'autre<space>à<space>des<space>don<space>des<space>grandes<space>illusion<space>pour<space>des<space>raisons<space>que<space>j'ai

2026-01-28 15:36:48,329 | INFO | speech length: 240160
2026-01-28 15:36:48,376 | INFO | decoder input length: 374
2026-01-28 15:36:48,376 | INFO | max output length: 374
2026-01-28 15:36:48,376 | INFO | min output length: 37
2026-01-28 15:37:19,714 | INFO | end detected at 246
2026-01-28 15:37:19,717 | INFO | -28.18 * 0.5 = -14.09 for decoder
2026-01-28 15:37:19,717 | INFO | -10.25 * 0.5 =  -5.13 for ctc
2026-01-28 15:37:19,717 | INFO | total log probability: -19.22
2026-01-28 15:37:19,717 | INFO | normalized log probability: -0.08
2026-01-28 15:37:19,717 | INFO | total number of ended hypotheses: 231
2026-01-28 15:37:19,720 | INFO | best hypo: j'ai<space>déjà<space>dit<space>une<space>fois<space>je<space>m'excuse<space>de<space>les<space>répéter<space>c'est<space>vous<space>les<space>connaissez<space>euh<space>c'est<space>que<space>il<space>y<space>avait<space>beaucoup<space>de<space>choses<space>communes<space>entre<space>nous<space>nous<space>étions<space>tous<space>les<space>deux<space>d'origine<space>bourgeoise<space>élevées<space>un<space>peu<space>de<space>la<space>même<space>manière<space>euh<space>c'est<space>à<space>dire

2026-01-28 15:37:19,723 | INFO | speech length: 251838
2026-01-28 15:37:19,766 | INFO | decoder input length: 392
2026-01-28 15:37:19,767 | INFO | max output length: 392
2026-01-28 15:37:19,767 | INFO | min output length: 39
2026-01-28 15:37:57,584 | INFO | end detected at 306
2026-01-28 15:37:57,586 | INFO | -41.40 * 0.5 = -20.70 for decoder
2026-01-28 15:37:57,586 | INFO | -15.76 * 0.5 =  -7.88 for ctc
2026-01-28 15:37:57,587 | INFO | total log probability: -28.58
2026-01-28 15:37:57,587 | INFO | normalized log probability: -0.10
2026-01-28 15:37:57,587 | INFO | total number of ended hypotheses: 210
2026-01-28 15:37:57,590 | INFO | best hypo: disons<space>d'une<space>façon<space>un<space>peu<space>britannique<space>dans<space>le<space>comportement<space>n'est<space>ce<space>pas<space>euh<space>on<space>ne<space>pleure<space>pas<space>euh<space>on<space>se<space>tient<space>toujours<space>euh<space>vous<space>voyez<space>ce<space>je<space>dirais<space>ni<space>ét<space>ni<space>que<space>suis<space>un<space>petit<space>peu<space>rigide<space>à<space>cet<space>égard<space>nous<space>avions<space>donc<space>ça<space>en<space>commun<space>et<space>nous<space>étions<space>tous<space>les<space>deux<space>extraordinairement<space>pauvres<space>à<space>ce<space>moment<space>là

2026-01-28 15:37:57,593 | INFO | speech length: 46560
2026-01-28 15:37:57,638 | INFO | decoder input length: 72
2026-01-28 15:37:57,638 | INFO | max output length: 72
2026-01-28 15:37:57,638 | INFO | min output length: 7
2026-01-28 15:38:02,292 | INFO | end detected at 46
2026-01-28 15:38:02,293 | INFO |  -3.25 * 0.5 =  -1.62 for decoder
2026-01-28 15:38:02,293 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 15:38:02,293 | INFO | total log probability: -1.62
2026-01-28 15:38:02,294 | INFO | normalized log probability: -0.04
2026-01-28 15:38:02,294 | INFO | total number of ended hypotheses: 166
2026-01-28 15:38:02,294 | INFO | best hypo: euh<space>en<space>contact<space>avec<space>ce<space>milieu<space>de<space>cinéma

2026-01-28 15:38:02,296 | INFO | speech length: 123360
2026-01-28 15:38:02,334 | INFO | decoder input length: 192
2026-01-28 15:38:02,335 | INFO | max output length: 192
2026-01-28 15:38:02,335 | INFO | min output length: 19
2026-01-28 15:38:19,065 | INFO | end detected at 156
2026-01-28 15:38:19,068 | INFO | -14.10 * 0.5 =  -7.05 for decoder
2026-01-28 15:38:19,068 | INFO | -13.09 * 0.5 =  -6.55 for ctc
2026-01-28 15:38:19,068 | INFO | total log probability: -13.60
2026-01-28 15:38:19,068 | INFO | normalized log probability: -0.09
2026-01-28 15:38:19,068 | INFO | total number of ended hypotheses: 225
2026-01-28 15:38:19,070 | INFO | best hypo: euh<space>très<space>très<space>exhibitionniste<space>en<space>matière<space>d'argent<space>et<space>nous<space>avions<space>tous<space>les<space>deux<space>la<space>passion<space>de<space>de<space>la<space>musique<space>et<space>nous<space>jouions<space>tous<space>les<space>deux<space>du<space>piano

2026-01-28 15:38:19,084 | INFO | Chunk: 0 | WER=9.523810 | S=2 D=0 I=0
2026-01-28 15:38:19,085 | INFO | Chunk: 1 | WER=7.692308 | S=1 D=0 I=0
2026-01-28 15:38:19,085 | INFO | Chunk: 2 | WER=20.000000 | S=3 D=0 I=0
2026-01-28 15:38:19,087 | INFO | Chunk: 3 | WER=6.000000 | S=2 D=1 I=0
2026-01-28 15:38:19,088 | INFO | Chunk: 4 | WER=33.333333 | S=8 D=2 I=0
2026-01-28 15:38:19,089 | INFO | Chunk: 5 | WER=7.692308 | S=1 D=0 I=1
2026-01-28 15:38:19,089 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:38:19,089 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:38:19,094 | INFO | Chunk: 8 | WER=16.049383 | S=5 D=3 I=5
2026-01-28 15:38:19,096 | INFO | Chunk: 9 | WER=19.230769 | S=8 D=1 I=1
2026-01-28 15:38:19,097 | INFO | Chunk: 10 | WER=18.000000 | S=7 D=2 I=0
2026-01-28 15:38:19,098 | INFO | Chunk: 11 | WER=23.529412 | S=3 D=0 I=1
2026-01-28 15:38:19,101 | INFO | Chunk: 12 | WER=8.695652 | S=4 D=0 I=2
2026-01-28 15:38:19,102 | INFO | Chunk: 13 | WER=40.000000 | S=5 D=1 I=0
2026-01-28 15:38:19,102 | INFO | Chunk: 14 | WER=42.307692 | S=9 D=1 I=1
2026-01-28 15:38:19,103 | INFO | Chunk: 15 | WER=9.090909 | S=1 D=0 I=0
2026-01-28 15:38:19,103 | INFO | Chunk: 16 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:38:19,106 | INFO | Chunk: 17 | WER=5.714286 | S=3 D=1 I=0
2026-01-28 15:38:19,106 | INFO | Chunk: 18 | WER=14.285714 | S=0 D=0 I=1
2026-01-28 15:38:19,107 | INFO | Chunk: 19 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:38:19,107 | INFO | Chunk: 20 | WER=7.692308 | S=0 D=0 I=1
2026-01-28 15:38:19,107 | INFO | Chunk: 21 | WER=20.000000 | S=0 D=0 I=1
2026-01-28 15:38:19,109 | INFO | Chunk: 22 | WER=11.764706 | S=2 D=1 I=1
2026-01-28 15:38:19,109 | INFO | Chunk: 23 | WER=25.000000 | S=6 D=0 I=1
2026-01-28 15:38:19,117 | INFO | Chunk: 24 | WER=18.181818 | S=7 D=4 I=9
2026-01-28 15:38:19,125 | INFO | Chunk: 25 | WER=15.966387 | S=12 D=2 I=5
2026-01-28 15:38:19,134 | INFO | Chunk: 26 | WER=15.517241 | S=10 D=2 I=6
2026-01-28 15:38:19,138 | INFO | Chunk: 27 | WER=10.256410 | S=3 D=2 I=3
2026-01-28 15:38:19,140 | INFO | Chunk: 28 | WER=5.263158 | S=2 D=0 I=1
2026-01-28 15:38:19,140 | INFO | Chunk: 29 | WER=45.454545 | S=4 D=1 I=0
2026-01-28 15:38:19,141 | INFO | Chunk: 30 | WER=29.629630 | S=6 D=2 I=0
2026-01-28 15:38:19,150 | INFO | Chunk: 31 | WER=22.656250 | S=13 D=8 I=8
2026-01-28 15:38:19,151 | INFO | Chunk: 32 | WER=36.000000 | S=4 D=5 I=0
2026-01-28 15:38:19,157 | INFO | Chunk: 33 | WER=32.075472 | S=28 D=1 I=5
2026-01-28 15:38:19,165 | INFO | Chunk: 34 | WER=17.699115 | S=14 D=0 I=6
2026-01-28 15:38:19,165 | INFO | Chunk: 35 | WER=44.444444 | S=2 D=6 I=0
2026-01-28 15:38:19,167 | INFO | Chunk: 36 | WER=25.531915 | S=6 D=5 I=1
2026-01-28 15:38:19,168 | INFO | Chunk: 37 | WER=4.000000 | S=1 D=0 I=0
2026-01-28 15:38:19,168 | INFO | Chunk: 38 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:38:19,176 | INFO | Chunk: 39 | WER=24.778761 | S=18 D=2 I=8
2026-01-28 15:38:19,178 | INFO | Chunk: 40 | WER=12.500000 | S=4 D=0 I=2
2026-01-28 15:38:19,180 | INFO | Chunk: 41 | WER=21.311475 | S=6 D=4 I=3
2026-01-28 15:38:19,180 | INFO | Chunk: 42 | WER=14.285714 | S=0 D=0 I=1
2026-01-28 15:38:19,181 | INFO | Chunk: 43 | WER=3.703704 | S=0 D=0 I=1
2026-01-28 15:38:21,281 | INFO | File: Rhap-D2001.wav | WER=17.659244 | S=211 D=56 I=74
2026-01-28 15:38:21,282 | INFO | ------------------------------
2026-01-28 15:38:21,282 | INFO | Conf ester Done!
2026-01-28 15:45:29,974 | INFO | Chunk: 0 | WER=23.809524 | S=3 D=0 I=2
2026-01-28 15:45:29,974 | INFO | Chunk: 1 | WER=15.384615 | S=1 D=1 I=0
2026-01-28 15:45:29,975 | INFO | Chunk: 2 | WER=13.333333 | S=2 D=0 I=0
2026-01-28 15:45:29,978 | INFO | Chunk: 3 | WER=16.000000 | S=6 D=2 I=0
2026-01-28 15:45:29,979 | INFO | Chunk: 4 | WER=6.666667 | S=2 D=0 I=0
2026-01-28 15:45:29,980 | INFO | Chunk: 5 | WER=3.846154 | S=1 D=0 I=0
2026-01-28 15:45:29,981 | INFO | Chunk: 6 | WER=41.666667 | S=4 D=0 I=1
2026-01-28 15:45:29,982 | INFO | Chunk: 7 | WER=5.263158 | S=0 D=1 I=0
2026-01-28 15:45:29,988 | INFO | Chunk: 8 | WER=11.111111 | S=6 D=2 I=1
2026-01-28 15:45:29,991 | INFO | Chunk: 9 | WER=9.615385 | S=5 D=0 I=0
2026-01-28 15:45:29,994 | INFO | Chunk: 10 | WER=14.000000 | S=5 D=1 I=1
2026-01-28 15:45:29,995 | INFO | Chunk: 11 | WER=23.529412 | S=3 D=1 I=0
2026-01-28 15:45:29,999 | INFO | Chunk: 12 | WER=20.289855 | S=9 D=2 I=3
2026-01-28 15:45:30,000 | INFO | Chunk: 13 | WER=13.333333 | S=1 D=1 I=0
2026-01-28 15:45:30,001 | INFO | Chunk: 14 | WER=23.076923 | S=3 D=3 I=0
2026-01-28 15:45:30,002 | INFO | Chunk: 15 | WER=9.090909 | S=1 D=0 I=0
2026-01-28 15:45:30,003 | INFO | Chunk: 16 | WER=14.285714 | S=2 D=0 I=0
2026-01-28 15:45:30,007 | INFO | Chunk: 17 | WER=18.571429 | S=4 D=8 I=1
2026-01-28 15:45:30,008 | INFO | Chunk: 18 | WER=14.285714 | S=1 D=0 I=0
2026-01-28 15:45:30,008 | INFO | Chunk: 19 | WER=6.250000 | S=0 D=1 I=0
2026-01-28 15:45:30,009 | INFO | Chunk: 20 | WER=15.384615 | S=0 D=0 I=2
2026-01-28 15:45:30,009 | INFO | Chunk: 21 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:45:30,011 | INFO | Chunk: 22 | WER=8.823529 | S=2 D=1 I=0
2026-01-28 15:45:30,013 | INFO | Chunk: 23 | WER=21.428571 | S=6 D=0 I=0
2026-01-28 15:45:30,023 | INFO | Chunk: 24 | WER=15.454545 | S=9 D=3 I=5
2026-01-28 15:45:30,035 | INFO | Chunk: 25 | WER=18.487395 | S=17 D=4 I=1
2026-01-28 15:45:30,045 | INFO | Chunk: 26 | WER=11.206897 | S=3 D=6 I=4
2026-01-28 15:45:30,051 | INFO | Chunk: 27 | WER=12.820513 | S=6 D=3 I=1
2026-01-28 15:45:30,054 | INFO | Chunk: 28 | WER=5.263158 | S=1 D=1 I=1
2026-01-28 15:45:30,055 | INFO | Chunk: 29 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:45:30,056 | INFO | Chunk: 30 | WER=18.518519 | S=4 D=1 I=0
2026-01-28 15:45:30,068 | INFO | Chunk: 31 | WER=25.000000 | S=15 D=11 I=6
2026-01-28 15:45:30,070 | INFO | Chunk: 32 | WER=24.000000 | S=1 D=5 I=0
2026-01-28 15:45:30,078 | INFO | Chunk: 33 | WER=30.188679 | S=12 D=17 I=3
2026-01-28 15:45:30,088 | INFO | Chunk: 34 | WER=9.734513 | S=9 D=2 I=0
2026-01-28 15:45:30,089 | INFO | Chunk: 35 | WER=16.666667 | S=0 D=1 I=2
2026-01-28 15:45:30,091 | INFO | Chunk: 36 | WER=29.787234 | S=8 D=4 I=2
2026-01-28 15:45:30,092 | INFO | Chunk: 37 | WER=8.000000 | S=1 D=1 I=0
2026-01-28 15:45:30,093 | INFO | Chunk: 38 | WER=100.000000 | S=0 D=0 I=1
2026-01-28 15:45:30,103 | INFO | Chunk: 39 | WER=18.584071 | S=14 D=3 I=4
2026-01-28 15:45:30,106 | INFO | Chunk: 40 | WER=18.750000 | S=5 D=2 I=2
2026-01-28 15:45:30,109 | INFO | Chunk: 41 | WER=32.786885 | S=12 D=7 I=1
2026-01-28 15:45:30,110 | INFO | Chunk: 42 | WER=14.285714 | S=0 D=0 I=1
2026-01-28 15:45:30,111 | INFO | Chunk: 43 | WER=29.629630 | S=4 D=3 I=1
2026-01-28 15:45:32,315 | INFO | File: Rhap-D2001.wav | WER=17.037804 | S=191 D=95 I=43
2026-01-28 15:45:32,323 | INFO | ------------------------------
2026-01-28 15:45:32,324 | INFO | hmm_tdnn Done!
2026-01-28 15:45:32,628 | INFO | ==================================Rhap-D2002.wav=========================================
2026-01-28 15:45:32,971 | INFO | Using rVAD model
2026-01-28 15:45:58,296 | INFO | Chunk: 0 | WER=32.584270 | S=10 D=17 I=2
2026-01-28 15:45:58,300 | INFO | Chunk: 1 | WER=49.230769 | S=15 D=17 I=0
2026-01-28 15:45:58,300 | INFO | Chunk: 2 | WER=15.384615 | S=1 D=1 I=0
2026-01-28 15:45:58,307 | INFO | Chunk: 3 | WER=38.235294 | S=16 D=22 I=1
2026-01-28 15:45:58,315 | INFO | Chunk: 4 | WER=40.540541 | S=19 D=26 I=0
2026-01-28 15:45:58,315 | INFO | Chunk: 5 | WER=16.666667 | S=1 D=1 I=0
2026-01-28 15:45:58,321 | INFO | Chunk: 6 | WER=25.581395 | S=11 D=10 I=1
2026-01-28 15:45:58,323 | INFO | Chunk: 7 | WER=11.764706 | S=2 D=2 I=0
2026-01-28 15:45:58,327 | INFO | Chunk: 8 | WER=45.333333 | S=17 D=17 I=0
2026-01-28 15:45:58,327 | INFO | Chunk: 9 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:45:58,328 | INFO | Chunk: 10 | WER=75.000000 | S=5 D=1 I=0
2026-01-28 15:45:58,329 | INFO | Chunk: 11 | WER=41.463415 | S=8 D=9 I=0
2026-01-28 15:45:58,331 | INFO | Chunk: 12 | WER=70.731707 | S=18 D=11 I=0
2026-01-28 15:45:58,331 | INFO | Chunk: 13 | WER=37.500000 | S=2 D=4 I=0
2026-01-28 15:45:58,332 | INFO | Chunk: 14 | WER=66.666667 | S=3 D=5 I=0
2026-01-28 15:45:58,333 | INFO | Chunk: 15 | WER=43.333333 | S=9 D=4 I=0
2026-01-28 15:45:58,333 | INFO | Chunk: 16 | WER=33.333333 | S=3 D=1 I=0
2026-01-28 15:45:58,334 | INFO | Chunk: 17 | WER=58.823529 | S=7 D=13 I=0
2026-01-28 15:45:58,340 | INFO | Chunk: 18 | WER=35.164835 | S=13 D=19 I=0
2026-01-28 15:45:58,806 | INFO | File: Rhap-D2002.wav | WER=38.819523 | S=160 D=179 I=3
2026-01-28 15:45:58,807 | INFO | ------------------------------
2026-01-28 15:45:58,807 | INFO | w2vec vad chunk Done!
2026-01-28 15:46:25,511 | INFO | Chunk: 0 | WER=84.269663 | S=1 D=74 I=0
2026-01-28 15:46:25,514 | INFO | Chunk: 1 | WER=55.384615 | S=9 D=26 I=1
2026-01-28 15:46:25,515 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:46:25,517 | INFO | Chunk: 3 | WER=85.294118 | S=2 D=85 I=0
2026-01-28 15:46:25,520 | INFO | Chunk: 4 | WER=84.684685 | S=9 D=85 I=0
2026-01-28 15:46:25,521 | INFO | Chunk: 5 | WER=58.333333 | S=1 D=6 I=0
2026-01-28 15:46:25,524 | INFO | Chunk: 6 | WER=65.116279 | S=5 D=51 I=0
2026-01-28 15:46:25,525 | INFO | Chunk: 7 | WER=23.529412 | S=2 D=4 I=2
2026-01-28 15:46:25,528 | INFO | Chunk: 8 | WER=65.333333 | S=7 D=41 I=1
2026-01-28 15:46:25,529 | INFO | Chunk: 9 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:46:25,529 | INFO | Chunk: 10 | WER=75.000000 | S=4 D=0 I=2
2026-01-28 15:46:25,530 | INFO | Chunk: 11 | WER=73.170732 | S=1 D=29 I=0
2026-01-28 15:46:25,531 | INFO | Chunk: 12 | WER=82.926829 | S=2 D=32 I=0
2026-01-28 15:46:25,531 | INFO | Chunk: 13 | WER=31.250000 | S=2 D=2 I=1
2026-01-28 15:46:25,532 | INFO | Chunk: 14 | WER=41.666667 | S=4 D=1 I=0
2026-01-28 15:46:25,533 | INFO | Chunk: 15 | WER=36.666667 | S=7 D=3 I=1
2026-01-28 15:46:25,533 | INFO | Chunk: 16 | WER=41.666667 | S=2 D=3 I=0
2026-01-28 15:46:25,534 | INFO | Chunk: 17 | WER=79.411765 | S=4 D=23 I=0
2026-01-28 15:46:25,538 | INFO | Chunk: 18 | WER=62.637363 | S=7 D=48 I=2
2026-01-28 15:46:25,786 | INFO | File: Rhap-D2002.wav | WER=66.969353 | S=69 D=512 I=9
2026-01-28 15:46:25,786 | INFO | ------------------------------
2026-01-28 15:46:25,787 | INFO | whisper med Done!
2026-01-28 15:47:18,503 | INFO | Chunk: 0 | WER=56.179775 | S=10 D=39 I=1
2026-01-28 15:47:18,524 | INFO | Chunk: 1 | WER=29.230769 | S=4 D=15 I=0
2026-01-28 15:47:18,525 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:47:18,528 | INFO | Chunk: 3 | WER=85.294118 | S=2 D=85 I=0
2026-01-28 15:47:18,533 | INFO | Chunk: 4 | WER=86.486486 | S=8 D=88 I=0
2026-01-28 15:47:18,534 | INFO | Chunk: 5 | WER=8.333333 | S=1 D=0 I=0
2026-01-28 15:47:18,540 | INFO | Chunk: 6 | WER=54.651163 | S=6 D=40 I=1
2026-01-28 15:47:18,542 | INFO | Chunk: 7 | WER=14.705882 | S=1 D=4 I=0
2026-01-28 15:47:18,548 | INFO | Chunk: 8 | WER=49.333333 | S=11 D=25 I=1
2026-01-28 15:47:18,549 | INFO | Chunk: 9 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 15:47:18,549 | INFO | Chunk: 10 | WER=100.000000 | S=3 D=5 I=0
2026-01-28 15:47:18,552 | INFO | Chunk: 11 | WER=26.829268 | S=3 D=8 I=0
2026-01-28 15:47:18,553 | INFO | Chunk: 12 | WER=80.487805 | S=4 D=29 I=0
2026-01-28 15:47:18,554 | INFO | Chunk: 13 | WER=62.500000 | S=0 D=10 I=0
2026-01-28 15:47:18,555 | INFO | Chunk: 14 | WER=66.666667 | S=6 D=2 I=0
2026-01-28 15:47:18,556 | INFO | Chunk: 15 | WER=63.333333 | S=1 D=17 I=1
2026-01-28 15:47:18,557 | INFO | Chunk: 16 | WER=41.666667 | S=2 D=2 I=1
2026-01-28 15:47:18,559 | INFO | Chunk: 17 | WER=32.352941 | S=3 D=8 I=0
2026-01-28 15:47:18,565 | INFO | Chunk: 18 | WER=58.241758 | S=8 D=45 I=0
2026-01-28 15:47:19,017 | INFO | File: Rhap-D2002.wav | WER=56.640182 | S=74 D=421 I=4
2026-01-28 15:47:19,017 | INFO | ------------------------------
2026-01-28 15:47:19,017 | INFO | whisper large Done!
2026-01-28 15:47:19,193 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 15:47:19,231 | INFO | Vocabulary size: 350
2026-01-28 15:47:20,214 | INFO | Gradient checkpoint layers: []
2026-01-28 15:47:20,967 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 15:47:20,972 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 15:47:20,973 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 15:47:20,973 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 15:47:20,974 | INFO | speech length: 444160
2026-01-28 15:47:21,028 | INFO | decoder input length: 693
2026-01-28 15:47:21,028 | INFO | max output length: 693
2026-01-28 15:47:21,029 | INFO | min output length: 69
2026-01-28 15:47:47,542 | INFO | end detected at 203
2026-01-28 15:47:47,545 | INFO | -484.44 * 0.5 = -242.22 for decoder
2026-01-28 15:47:47,545 | INFO | -168.11 * 0.5 = -84.05 for ctc
2026-01-28 15:47:47,545 | INFO | total log probability: -326.28
2026-01-28 15:47:47,545 | INFO | normalized log probability: -1.66
2026-01-28 15:47:47,545 | INFO | total number of ended hypotheses: 189
2026-01-28 15:47:47,547 | INFO | best hypo: ▁je▁suis▁pas▁du▁tout▁inacharnée▁de▁l'actualité▁littéraire▁et▁je▁suis▁peintre▁épouvantablement▁éclectique▁et▁on▁ne▁le▁sava▁vraiment▁de▁deux▁romans▁étrangers▁et▁quelques▁romans▁en▁contemporains▁mais▁aussi▁des▁essuais▁mais▁un▁peu▁de▁bandes▁destinées▁mais▁pas▁malulgrés▁sur▁les▁jardins▁et▁de▁cuisines▁ons▁étudiés▁à▁ces▁et▁ce▁traits▁très▁variés▁ons▁regarder▁ça▁prêts▁mais▁je▁trouverais▁pas▁de▁lible▁sur▁ségolème▁roy▁ou▁nicolas▁sarkozy▁un▁anon

2026-01-28 15:47:47,551 | INFO | speech length: 255040
2026-01-28 15:47:47,592 | INFO | decoder input length: 398
2026-01-28 15:47:47,592 | INFO | max output length: 398
2026-01-28 15:47:47,592 | INFO | min output length: 39
2026-01-28 15:47:59,679 | INFO | end detected at 144
2026-01-28 15:47:59,682 | INFO | -271.01 * 0.5 = -135.50 for decoder
2026-01-28 15:47:59,682 | INFO | -154.49 * 0.5 = -77.24 for ctc
2026-01-28 15:47:59,682 | INFO | total log probability: -212.75
2026-01-28 15:47:59,682 | INFO | normalized log probability: -1.60
2026-01-28 15:47:59,682 | INFO | total number of ended hypotheses: 184
2026-01-28 15:47:59,684 | INFO | best hypo: ▁dans▁ça▁des▁livres▁politiques▁afin▁de▁messer▁simum▁par▁les▁mêmes▁pas▁glyphes▁politiques▁l'avons▁parlés▁de▁people▁mêléliphes▁politiques▁n'en▁sachant▁libre▁je▁n'ai▁pas▁beaucoup▁à▁quand▁même▁les▁meilleurs▁fins▁journée▁en▁sens▁les▁philosophes▁qu▁peuvent▁intervenant▁y▁compris▁dans▁ce▁champ▁là

2026-01-28 15:47:59,687 | INFO | speech length: 85440
2026-01-28 15:47:59,739 | INFO | decoder input length: 133
2026-01-28 15:47:59,739 | INFO | max output length: 133
2026-01-28 15:47:59,739 | INFO | min output length: 13
2026-01-28 15:48:01,480 | INFO | end detected at 29
2026-01-28 15:48:01,483 | INFO |  -2.84 * 0.5 =  -1.42 for decoder
2026-01-28 15:48:01,483 | INFO |  -3.08 * 0.5 =  -1.54 for ctc
2026-01-28 15:48:01,483 | INFO | total log probability: -2.96
2026-01-28 15:48:01,483 | INFO | normalized log probability: -0.12
2026-01-28 15:48:01,483 | INFO | total number of ended hypotheses: 168
2026-01-28 15:48:01,483 | INFO | best hypo: ▁je▁permets▁la▁vie▁courte▁non▁plus▁quand▁même▁en▁même▁temps

2026-01-28 15:48:01,486 | INFO | speech length: 465440
2026-01-28 15:48:01,560 | INFO | decoder input length: 726
2026-01-28 15:48:01,560 | INFO | max output length: 726
2026-01-28 15:48:01,560 | INFO | min output length: 72
2026-01-28 15:48:33,600 | INFO | end detected at 197
2026-01-28 15:48:33,602 | INFO | -537.50 * 0.5 = -268.75 for decoder
2026-01-28 15:48:33,602 | INFO | -164.99 * 0.5 = -82.49 for ctc
2026-01-28 15:48:33,602 | INFO | total log probability: -351.24
2026-01-28 15:48:33,602 | INFO | normalized log probability: -1.84
2026-01-28 15:48:33,602 | INFO | total number of ended hypotheses: 177
2026-01-28 15:48:33,605 | INFO | best hypo: ▁est▁ce▁que▁vous▁disez▁en▁ce▁moment▁je▁cherche▁le▁livre▁que▁dit▁pascal▁ferrand▁je▁relie▁livre▁alors▁vraiment▁une▁mêlée▁fétiche▁qu'à▁l'objet▁d'une▁herbe▁déjà▁deux▁fois▁qui▁est▁un▁vive▁garante▁de▁murakamie▁qui▁l'appeelle▁au▁sud▁de▁la▁frontière▁avec▁l'ouest▁du▁soleil▁et▁en▁fait▁de▁c'estlasque▁pour▁la▁chose▁plume▁j'écrime▁sur▁laquelle▁je▁travaille▁en▁ce▁moment▁un'univers▁du▁mirachaamille▁mais▁excrêmement▁plus▁précieux▁amosphériquement▁comme▁ça

2026-01-28 15:48:33,609 | INFO | speech length: 458880
2026-01-28 15:48:33,667 | INFO | decoder input length: 716
2026-01-28 15:48:33,667 | INFO | max output length: 716
2026-01-28 15:48:33,667 | INFO | min output length: 71
2026-01-28 15:49:12,404 | INFO | end detected at 224
2026-01-28 15:49:12,406 | INFO | -613.09 * 0.5 = -306.55 for decoder
2026-01-28 15:49:12,406 | INFO | -184.73 * 0.5 = -92.36 for ctc
2026-01-28 15:49:12,406 | INFO | total log probability: -398.91
2026-01-28 15:49:12,406 | INFO | normalized log probability: -1.82
2026-01-28 15:49:12,406 | INFO | total number of ended hypotheses: 146
2026-01-28 15:49:12,409 | INFO | best hypo: ▁donc▁je▁viens▁de▁relire▁cela▁mais▁adore▁oui▁à▁peu▁patouche▁je▁n'en▁sa▁soulais▁viens▁sortir▁le▁dernier▁cercueil▁de▁nouvelles▁sous▁l'aveugle▁femme▁en▁d'armes▁mais▁autrement▁et▁voilà▁sur▁une▁relire▁ronique▁de▁l'aiseur▁sors▁la▁costume▁ton▁sauvage▁carcat▁sur▁le▁rivage▁pute▁quand▁elle▁un▁peu▁son▁chefs▁d'oeuvre▁enfin▁et▁pour▁l'inctant▁est▁chaud▁et▁et▁celui▁là▁là▁aussi▁de▁la▁frontière▁que▁j'adore▁littéralement▁que▁je▁'obre▁absolument▁magnifique▁et▁qui▁mene▁et▁qui▁m'obsède▁vachement

2026-01-28 15:49:12,411 | INFO | speech length: 53280
2026-01-28 15:49:12,453 | INFO | decoder input length: 82
2026-01-28 15:49:12,453 | INFO | max output length: 82
2026-01-28 15:49:12,453 | INFO | min output length: 8
2026-01-28 15:49:16,269 | INFO | end detected at 31
2026-01-28 15:49:16,271 | INFO |  -1.97 * 0.5 =  -0.99 for decoder
2026-01-28 15:49:16,271 | INFO |  -2.41 * 0.5 =  -1.21 for ctc
2026-01-28 15:49:16,271 | INFO | total log probability: -2.19
2026-01-28 15:49:16,271 | INFO | normalized log probability: -0.09
2026-01-28 15:49:16,271 | INFO | total number of ended hypotheses: 168
2026-01-28 15:49:16,272 | INFO | best hypo: ▁très▁très▁difficile▁à▁définir▁il▁y▁a▁toujours▁une

2026-01-28 15:49:16,274 | INFO | speech length: 455840
2026-01-28 15:49:16,327 | INFO | decoder input length: 711
2026-01-28 15:49:16,327 | INFO | max output length: 711
2026-01-28 15:49:16,327 | INFO | min output length: 71
2026-01-28 15:49:52,148 | INFO | end detected at 213
2026-01-28 15:49:52,151 | INFO | -597.74 * 0.5 = -298.87 for decoder
2026-01-28 15:49:52,151 | INFO | -181.58 * 0.5 = -90.79 for ctc
2026-01-28 15:49:52,151 | INFO | total log probability: -389.66
2026-01-28 15:49:52,151 | INFO | normalized log probability: -1.89
2026-01-28 15:49:52,151 | INFO | total number of ended hypotheses: 172
2026-01-28 15:49:52,154 | INFO | best hypo: ▁plus▁ou▁moins▁là▁ou▁plus▁ou▁moins▁prégnante▁une▁espèce▁d'ambiance▁fantastique▁de▁choses▁complètement▁irréelles▁ou▁surnaturelles▁dans▁d'un▁casqu'un▁rivage▁par▁exemple▁un▁des▁personnages▁principals▁'à▁un▁vieil▁homme▁'as▁ait▁été▁soucips▁pendant▁la▁guerre▁à▁le▁pouvoir▁parler▁ou▁char▁par▁exemple▁'est▁chosement▁comme▁ça▁bon▁qu'ils▁sont▁d'une▁à▁l'ocsasion▁d'une▁très▁grande▁fantaisie▁d'une▁très▁grande▁léingèreté▁d'un▁humaurtement▁incroyable

2026-01-28 15:49:52,158 | INFO | speech length: 195200
2026-01-28 15:49:52,230 | INFO | decoder input length: 304
2026-01-28 15:49:52,230 | INFO | max output length: 304
2026-01-28 15:49:52,230 | INFO | min output length: 30
2026-01-28 15:50:05,763 | INFO | end detected at 79
2026-01-28 15:50:05,765 | INFO | -59.02 * 0.5 = -29.51 for decoder
2026-01-28 15:50:05,765 | INFO | -40.30 * 0.5 = -20.15 for ctc
2026-01-28 15:50:05,765 | INFO | total log probability: -49.66
2026-01-28 15:50:05,765 | INFO | normalized log probability: -0.70
2026-01-28 15:50:05,765 | INFO | total number of ended hypotheses: 188
2026-01-28 15:50:05,766 | INFO | best hypo: ▁et▁à▁la▁fois▁tout▁ça▁et▁dans▁un▁mélange▁extrêmement▁singulier▁facoumage▁n'est▁pas▁tellement▁rencontré▁ailleurs▁des▁choses▁d'une▁granderudité▁et▁d'une▁grandeur

2026-01-28 15:50:05,768 | INFO | speech length: 348960
2026-01-28 15:50:05,820 | INFO | decoder input length: 544
2026-01-28 15:50:05,820 | INFO | max output length: 544
2026-01-28 15:50:05,820 | INFO | min output length: 54
2026-01-28 15:50:29,710 | INFO | end detected at 165
2026-01-28 15:50:29,713 | INFO | -410.88 * 0.5 = -205.44 for decoder
2026-01-28 15:50:29,713 | INFO | -109.68 * 0.5 = -54.84 for ctc
2026-01-28 15:50:29,713 | INFO | total log probability: -260.28
2026-01-28 15:50:29,713 | INFO | normalized log probability: -1.62
2026-01-28 15:50:29,713 | INFO | total number of ended hypotheses: 151
2026-01-28 15:50:29,715 | INFO | best hypo: ▁pas▁exemple▁à▁la▁sexualité▁et▁les▁choses▁et▁les▁descriptions▁extrêmement▁crues▁extrêmement▁belles▁mais▁valassades▁simpson▁le▁mélange▁très▁très▁étrange▁et▁qui▁moi▁ne▁me▁je▁sais▁pas▁pourquoi▁me▁rabertse▁les▁grngea▁dorça▁quoi▁je▁suis▁même▁au▁moment▁où▁paraît▁d'étranges▁fantaacstiques▁la▁grême▁qui▁frappent▁contre▁les▁kntela▁le▁précis▁aux▁auditeurs

2026-01-28 15:50:29,718 | INFO | speech length: 45760
2026-01-28 15:50:29,794 | INFO | decoder input length: 71
2026-01-28 15:50:29,794 | INFO | max output length: 71
2026-01-28 15:50:29,794 | INFO | min output length: 7
2026-01-28 15:50:31,324 | INFO | end detected at 27
2026-01-28 15:50:31,326 | INFO |  -1.60 * 0.5 =  -0.80 for decoder
2026-01-28 15:50:31,326 | INFO |  -0.98 * 0.5 =  -0.49 for ctc
2026-01-28 15:50:31,326 | INFO | total log probability: -1.29
2026-01-28 15:50:31,326 | INFO | normalized log probability: -0.06
2026-01-28 15:50:31,326 | INFO | total number of ended hypotheses: 172
2026-01-28 15:50:31,327 | INFO | best hypo: ▁on▁peut▁avoir▁un▁aperçu▁de▁l'écriture

2026-01-28 15:50:31,329 | INFO | speech length: 68640
2026-01-28 15:50:31,368 | INFO | decoder input length: 106
2026-01-28 15:50:31,368 | INFO | max output length: 106
2026-01-28 15:50:31,368 | INFO | min output length: 10
2026-01-28 15:50:32,844 | INFO | end detected at 31
2026-01-28 15:50:32,845 | INFO | -14.79 * 0.5 =  -7.39 for decoder
2026-01-28 15:50:32,846 | INFO | -16.61 * 0.5 =  -8.30 for ctc
2026-01-28 15:50:32,846 | INFO | total log probability: -15.70
2026-01-28 15:50:32,846 | INFO | normalized log probability: -0.68
2026-01-28 15:50:32,846 | INFO | total number of ended hypotheses: 190
2026-01-28 15:50:32,846 | INFO | best hypo: ▁demeura▁camille▁ah▁on▁est▁en▁quoi▁je▁pour▁aller

2026-01-28 15:50:32,848 | INFO | speech length: 152480
2026-01-28 15:50:32,909 | INFO | decoder input length: 237
2026-01-28 15:50:32,909 | INFO | max output length: 237
2026-01-28 15:50:32,910 | INFO | min output length: 23
2026-01-28 15:50:46,517 | INFO | end detected at 94
2026-01-28 15:50:46,519 | INFO | -30.25 * 0.5 = -15.13 for decoder
2026-01-28 15:50:46,519 | INFO | -46.34 * 0.5 = -23.17 for ctc
2026-01-28 15:50:46,520 | INFO | total log probability: -38.30
2026-01-28 15:50:46,520 | INFO | normalized log probability: -0.45
2026-01-28 15:50:46,520 | INFO | total number of ended hypotheses: 197
2026-01-28 15:50:46,521 | INFO | best hypo: ▁vous▁voudrez▁entendre▁que▁vous▁disiez▁sur▁le▁sexe▁la▁sexualité▁de▁lui▁ah▁bon▁ah▁oui▁elle▁a▁attendez▁non▁là▁ça▁c'est▁pas▁dans▁celui▁là▁ce▁que▁ça▁me▁renversa▁si▁donne▁plutonnant

2026-01-28 15:50:46,525 | INFO | speech length: 275360
2026-01-28 15:50:46,583 | INFO | decoder input length: 429
2026-01-28 15:50:46,583 | INFO | max output length: 429
2026-01-28 15:50:46,583 | INFO | min output length: 42
2026-01-28 15:50:58,015 | INFO | end detected at 72
2026-01-28 15:50:58,017 | INFO | -85.78 * 0.5 = -42.89 for decoder
2026-01-28 15:50:58,017 | INFO | -60.18 * 0.5 = -30.09 for ctc
2026-01-28 15:50:58,017 | INFO | total log probability: -72.98
2026-01-28 15:50:58,017 | INFO | normalized log probability: -1.12
2026-01-28 15:50:58,017 | INFO | total number of ended hypotheses: 165
2026-01-28 15:50:58,018 | INFO | best hypo: ▁là▁au▁sud▁alors▁bougé▁partout▁et▁nous▁attendons▁le▁vouloeur▁tenant▁vous▁lire▁tout▁comme▁ça▁ça▁valère▁très▁bien▁ça▁et▁tu▁te▁attablir▁mais

2026-01-28 15:50:58,021 | INFO | speech length: 60960
2026-01-28 15:50:58,070 | INFO | decoder input length: 94
2026-01-28 15:50:58,070 | INFO | max output length: 94
2026-01-28 15:50:58,070 | INFO | min output length: 9
2026-01-28 15:51:00,969 | INFO | end detected at 43
2026-01-28 15:51:00,970 | INFO |  -8.22 * 0.5 =  -4.11 for decoder
2026-01-28 15:51:00,970 | INFO |  -7.38 * 0.5 =  -3.69 for ctc
2026-01-28 15:51:00,970 | INFO | total log probability: -7.80
2026-01-28 15:51:00,971 | INFO | normalized log probability: -0.20
2026-01-28 15:51:00,971 | INFO | total number of ended hypotheses: 160
2026-01-28 15:51:00,971 | INFO | best hypo: ▁elle▁vous▁avez▁une▁petite▁pudeur▁la▁mouche▁d'une▁fille▁extrêmement▁publique▁dans▁le▁fond

2026-01-28 15:51:00,973 | INFO | speech length: 34720
2026-01-28 15:51:01,012 | INFO | decoder input length: 53
2026-01-28 15:51:01,012 | INFO | max output length: 53
2026-01-28 15:51:01,012 | INFO | min output length: 5
2026-01-28 15:51:03,873 | INFO | end detected at 28
2026-01-28 15:51:03,874 | INFO |  -5.01 * 0.5 =  -2.51 for decoder
2026-01-28 15:51:03,874 | INFO | -14.93 * 0.5 =  -7.46 for ctc
2026-01-28 15:51:03,874 | INFO | total log probability: -9.97
2026-01-28 15:51:03,874 | INFO | normalized log probability: -0.47
2026-01-28 15:51:03,874 | INFO | total number of ended hypotheses: 178
2026-01-28 15:51:03,875 | INFO | best hypo: ▁non▁mais▁même▁pas▁d'enfant▁je▁t'ai▁accusé

2026-01-28 15:51:03,877 | INFO | speech length: 70720
2026-01-28 15:51:03,930 | INFO | decoder input length: 110
2026-01-28 15:51:03,930 | INFO | max output length: 110
2026-01-28 15:51:03,930 | INFO | min output length: 11
2026-01-28 15:51:06,558 | INFO | end detected at 58
2026-01-28 15:51:06,560 | INFO | -23.00 * 0.5 = -11.50 for decoder
2026-01-28 15:51:06,560 | INFO | -24.11 * 0.5 = -12.05 for ctc
2026-01-28 15:51:06,560 | INFO | total log probability: -23.55
2026-01-28 15:51:06,560 | INFO | normalized log probability: -0.48
2026-01-28 15:51:06,560 | INFO | total number of ended hypotheses: 180
2026-01-28 15:51:06,561 | INFO | best hypo: ▁c'est▁bo▁qui▁dite▁c'est▁lui▁qu'il▁écrit▁allons▁la▁mais▁bien▁sûr▁c'est▁totalement▁lui▁s'est▁passe▁assez

2026-01-28 15:51:06,563 | INFO | speech length: 101280
2026-01-28 15:51:06,604 | INFO | decoder input length: 157
2026-01-28 15:51:06,604 | INFO | max output length: 157
2026-01-28 15:51:06,604 | INFO | min output length: 15
2026-01-28 15:51:09,974 | INFO | end detected at 35
2026-01-28 15:51:09,976 | INFO | -13.94 * 0.5 =  -6.97 for decoder
2026-01-28 15:51:09,976 | INFO | -10.84 * 0.5 =  -5.42 for ctc
2026-01-28 15:51:09,976 | INFO | total log probability: -12.39
2026-01-28 15:51:09,976 | INFO | normalized log probability: -0.44
2026-01-28 15:51:09,976 | INFO | total number of ended hypotheses: 163
2026-01-28 15:51:09,976 | INFO | best hypo: ▁pareille▁une▁incogralité▁oui▁afin▁un▁exercice▁un▁peu▁difficile

2026-01-28 15:51:09,978 | INFO | speech length: 118080
2026-01-28 15:51:10,047 | INFO | decoder input length: 184
2026-01-28 15:51:10,047 | INFO | max output length: 184
2026-01-28 15:51:10,048 | INFO | min output length: 18
2026-01-28 15:51:16,955 | INFO | end detected at 61
2026-01-28 15:51:16,957 | INFO | -18.31 * 0.5 =  -9.16 for decoder
2026-01-28 15:51:16,958 | INFO | -30.88 * 0.5 = -15.44 for ctc
2026-01-28 15:51:16,958 | INFO | total log probability: -24.60
2026-01-28 15:51:16,958 | INFO | normalized log probability: -0.46
2026-01-28 15:51:16,958 | INFO | total number of ended hypotheses: 183
2026-01-28 15:51:16,958 | INFO | best hypo: ▁alors▁comment▁nous▁assommes▁ces▁calvire▁à▁l'avancée▁c'est▁le▁au▁sud▁de▁la▁frontière▁elle▁lui▁susseleil▁page▁cent▁quatre▁vingt▁onze

2026-01-28 15:51:16,961 | INFO | speech length: 449440
2026-01-28 15:51:17,012 | INFO | decoder input length: 701
2026-01-28 15:51:17,013 | INFO | max output length: 701
2026-01-28 15:51:17,013 | INFO | min output length: 70
2026-01-28 15:51:54,601 | INFO | end detected at 208
2026-01-28 15:51:54,603 | INFO | -528.97 * 0.5 = -264.49 for decoder
2026-01-28 15:51:54,603 | INFO | -141.70 * 0.5 = -70.85 for ctc
2026-01-28 15:51:54,603 | INFO | total log probability: -335.34
2026-01-28 15:51:54,603 | INFO | normalized log probability: -1.67
2026-01-28 15:51:54,603 | INFO | total number of ended hypotheses: 142
2026-01-28 15:51:54,606 | INFO | best hypo: ▁c'est▁défaite▁ces▁deux▁un▁amour▁d'enfance▁moins▁de▁dix▁douze▁ans▁entre▁un▁jeune▁homme▁et▁une▁jeune▁femme▁qui▁finissent▁par▁se▁retrouver▁alors▁qu'ils▁ont▁trente▁cinq▁ou▁quarante▁ans▁et▁un▁bouleversent▁complètement▁leurs▁titres▁le▁racontée▁comme▁ça▁ce▁nulle▁mais▁juges▁rives▁eux▁ces▁eus▁pour▁'ontempêt▁ils▁s'adresse▁ce▁moment▁la▁première▁fois▁où▁il▁feront▁l'amouron▁faites▁ensemble▁puis▁quand▁ils▁ét▁jeunes▁évidemment▁c'est▁pas▁d'actualité

2026-01-28 15:51:54,619 | INFO | Chunk: 0 | WER=52.808989 | S=29 D=15 I=3
2026-01-28 15:51:54,622 | INFO | Chunk: 1 | WER=67.692308 | S=26 D=16 I=2
2026-01-28 15:51:54,622 | INFO | Chunk: 2 | WER=23.076923 | S=2 D=1 I=0
2026-01-28 15:51:54,627 | INFO | Chunk: 3 | WER=53.921569 | S=33 D=20 I=2
2026-01-28 15:51:54,634 | INFO | Chunk: 4 | WER=53.153153 | S=37 D=20 I=2
2026-01-28 15:51:54,634 | INFO | Chunk: 5 | WER=16.666667 | S=0 D=2 I=0
2026-01-28 15:51:54,638 | INFO | Chunk: 6 | WER=32.558140 | S=18 D=9 I=1
2026-01-28 15:51:54,639 | INFO | Chunk: 7 | WER=38.235294 | S=7 D=6 I=0
2026-01-28 15:51:54,642 | INFO | Chunk: 8 | WER=53.333333 | S=20 D=18 I=2
2026-01-28 15:51:54,643 | INFO | Chunk: 9 | WER=11.111111 | S=0 D=1 I=0
2026-01-28 15:51:54,643 | INFO | Chunk: 10 | WER=87.500000 | S=5 D=0 I=2
2026-01-28 15:51:54,644 | INFO | Chunk: 11 | WER=39.024390 | S=10 D=5 I=1
2026-01-28 15:51:54,645 | INFO | Chunk: 12 | WER=73.170732 | S=16 D=14 I=0
2026-01-28 15:51:54,646 | INFO | Chunk: 13 | WER=43.750000 | S=3 D=2 I=2
2026-01-28 15:51:54,646 | INFO | Chunk: 14 | WER=58.333333 | S=5 D=2 I=0
2026-01-28 15:51:54,647 | INFO | Chunk: 15 | WER=50.000000 | S=9 D=6 I=0
2026-01-28 15:51:54,647 | INFO | Chunk: 16 | WER=41.666667 | S=3 D=2 I=0
2026-01-28 15:51:54,648 | INFO | Chunk: 17 | WER=55.882353 | S=10 D=9 I=0
2026-01-28 15:51:54,653 | INFO | Chunk: 18 | WER=39.560440 | S=27 D=9 I=0
2026-01-28 15:51:55,035 | INFO | File: Rhap-D2002.wav | WER=48.581158 | S=264 D=152 I=12
2026-01-28 15:51:55,035 | INFO | ------------------------------
2026-01-28 15:51:55,036 | INFO | Conf cv Done!
2026-01-28 15:51:55,244 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 15:51:55,273 | INFO | Vocabulary size: 47
2026-01-28 15:51:56,699 | INFO | Gradient checkpoint layers: []
2026-01-28 15:51:57,716 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 15:51:57,722 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 15:51:57,723 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 15:51:57,724 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 15:51:57,727 | INFO | speech length: 444160
2026-01-28 15:51:57,783 | INFO | decoder input length: 693
2026-01-28 15:51:57,783 | INFO | max output length: 693
2026-01-28 15:51:57,783 | INFO | min output length: 69
2026-01-28 15:53:31,244 | INFO | end detected at 498
2026-01-28 15:53:31,246 | INFO | -345.15 * 0.5 = -172.58 for decoder
2026-01-28 15:53:31,246 | INFO | -46.66 * 0.5 = -23.33 for ctc
2026-01-28 15:53:31,246 | INFO | total log probability: -195.91
2026-01-28 15:53:31,247 | INFO | normalized log probability: -0.40
2026-01-28 15:53:31,247 | INFO | total number of ended hypotheses: 171
2026-01-28 15:53:31,253 | INFO | best hypo: je<space>suis<space>pas<space>du<space>tout<space>inacharné<space>de<space>de<space>l'actualité<space>littéraire<space>et<space>je<space>suis<space>euh<space>je<space>sais<space>pas<space>quoi<space>épouvantablement<space>éclectique<space>quoi<space>donc<space>ça<space>va<space>vraiment<space>de<space>de<space>romans<space>étrangers<space>un<space>quelques<space>romans<space>en<space>français<space>contemporains<space>mais<space>aussi<space>des<space>essais<space>mais<space>un<space>peu<space>de<space>bande<space>dessinée<space>mais<space>pas<space>mal<space>de<space>livres<space>sur<space>les<space>jardins<space>euh<space>un<space>des<space>livres<space>de<space>cuisine<space>moi<space>je<space>veux<space>dire<space>c'est<space>c'est<space>c'est<space>très<space>très<space>dardique<space>on<space>va<space>regarder<space>ça<space>deprès<space>euh<space>mais<space>je<space>trouveras<space>pas<space>de<space>livre<space>sur<space>ségolène<space>royal<space>on<space>nicolas<space>sarkozy<space>ah<space>non

2026-01-28 15:53:31,255 | INFO | speech length: 255040
2026-01-28 15:53:31,300 | INFO | decoder input length: 398
2026-01-28 15:53:31,300 | INFO | max output length: 398
2026-01-28 15:53:31,300 | INFO | min output length: 39
2026-01-28 15:54:25,939 | INFO | end detected at 310
2026-01-28 15:54:25,942 | INFO | -36.43 * 0.5 = -18.22 for decoder
2026-01-28 15:54:25,942 | INFO | -41.81 * 0.5 = -20.91 for ctc
2026-01-28 15:54:25,943 | INFO | total log probability: -39.12
2026-01-28 15:54:25,943 | INFO | normalized log probability: -0.13
2026-01-28 15:54:25,943 | INFO | total number of ended hypotheses: 186
2026-01-28 15:54:25,948 | INFO | best hypo: dans<space>ça<space>des<space>livres<space>politiques<space>enfin<space>nous<space>mais<space>ça<space>ça<space>m'on<space>me<space>parlait<space>même<space>pas<space>de<space>livre<space>politique<space>vous<space>me<space>parlez<space>de<space>people<space>euh<space>mais<space>les<space>livres<space>politiques<space>tout<space>ça<space>j'en<space>ai<space>j'en<space>ai<space>pas<space>beaucoup<space>quand<space>même<space>mais<space>enfin<space>j'en<space>ai<space>en<space>sens<space>de<space>philosophes<space>qui<space>peuvent<space>intervenir<space>y<space>compris<space>sur<space>ce<space>champ<space>là<space>quoi<space>voilà

2026-01-28 15:54:25,952 | INFO | speech length: 85440
2026-01-28 15:54:25,997 | INFO | decoder input length: 133
2026-01-28 15:54:25,997 | INFO | max output length: 133
2026-01-28 15:54:25,997 | INFO | min output length: 13
2026-01-28 15:54:32,335 | INFO | end detected at 67
2026-01-28 15:54:32,336 | INFO |  -5.73 * 0.5 =  -2.87 for decoder
2026-01-28 15:54:32,336 | INFO |  -3.94 * 0.5 =  -1.97 for ctc
2026-01-28 15:54:32,336 | INFO | total log probability: -4.84
2026-01-28 15:54:32,336 | INFO | normalized log probability: -0.08
2026-01-28 15:54:32,336 | INFO | total number of ended hypotheses: 166
2026-01-28 15:54:32,337 | INFO | best hypo: non<space>mais<space>la<space>vie<space>est<space>course<space>non<space>plus<space>quand<space>même<space>en<space>même<space>temps

2026-01-28 15:54:32,339 | INFO | speech length: 465440
2026-01-28 15:54:32,382 | INFO | decoder input length: 726
2026-01-28 15:54:32,382 | INFO | max output length: 726
2026-01-28 15:54:32,382 | INFO | min output length: 72
2026-01-28 15:55:43,431 | INFO | end detected at 477
2026-01-28 15:55:43,432 | INFO | -307.12 * 0.5 = -153.56 for decoder
2026-01-28 15:55:43,432 | INFO | -76.29 * 0.5 = -38.14 for ctc
2026-01-28 15:55:43,432 | INFO | total log probability: -191.70
2026-01-28 15:55:43,432 | INFO | normalized log probability: -0.41
2026-01-28 15:55:43,432 | INFO | total number of ended hypotheses: 154
2026-01-28 15:55:43,438 | INFO | best hypo: qu'est<space>ce<space>que<space>vous<space>lisez<space>en<space>ce<space>moment<space>je<space>cherche<space>le<space>livre<space>que<space>lit<space>pascal<space>ferrand<space>ah<space>ben<space>la<space>françaisje<space>relie<space>euh<space>un<space>demi<space>livre<space>alors<space>vraiment<space>une<space>mélu<space>fétiche<space>qui<space>a<space>ce<space>que<space>j'ai<space>du<space>né<space>déjà<space>deux<space>fois<space>qui<space>est<space>un<space>livre<space>garanti<space>euh<space>murakami<space>qui<space>s'appelle<space>au<space>sud<space>de<space>la<space>frontière<space>à<space>l'ouest<space>du<space>soleil<space>et<space>en<space>fait<space>c'est<space>parce<space>que<space>l<space>pour<space>le<space>la<space>chose<space>plus<space>j'écris<space>sur<space>laquelle<space>je<space>travaille<space>en<space>ce<space>moment<space>l'univers<space>de<space>murakakami<space>e<space>h<space>mais<space>extrêmement<space>précieux<space>atmosphériquement<space>comme<space>ça

2026-01-28 15:55:43,441 | INFO | speech length: 458880
2026-01-28 15:55:43,508 | INFO | decoder input length: 716
2026-01-28 15:55:43,508 | INFO | max output length: 716
2026-01-28 15:55:43,509 | INFO | min output length: 71
2026-01-28 15:56:56,637 | INFO | end detected at 516
2026-01-28 15:56:56,638 | INFO | -431.37 * 0.5 = -215.68 for decoder
2026-01-28 15:56:56,638 | INFO | -104.09 * 0.5 = -52.04 for ctc
2026-01-28 15:56:56,638 | INFO | total log probability: -267.73
2026-01-28 15:56:56,638 | INFO | normalized log probability: -0.52
2026-01-28 15:56:56,638 | INFO | total number of ended hypotheses: 168
2026-01-28 15:56:56,645 | INFO | best hypo: donc<space>je<space>viens<space>de<space>relire<space>euh<space>ceux<space>là<space>mais<space>la<space>tense<space>oui<space>y<space>a<space>à<space>peu<space>près<space>tout<space>je<space>dans<space>sa<space>cela<space>il<space>vient<space>de<space>sortir<space>le<space>dernier<space>c'est<space>arcueil<space>de<space>nouvelles<space>sol<space>aveugle<space>femme<space>en<space>dorme<space>mais<space>autrement<space>euh<space>et<space>voilà<space>je<space>viens<space>de<space>relire<space>chronique<space>de<space>l'oiseur<space>sort<space>la<space>constitution<space>sauvage<space>kafka<space>sur<space>un<space>rivage<space>quand<space>même<space>un<space>peu<space>son<space>chef<space>d'oeuvre<space>enfin<space>pour<space>l'instant<space>chaud<space>et<space>n<space>et<space>celui<space>là<space>là<space>aussi<space>de<space>la<space>frontière<space>que<space>j'adore<space>ligéralement<space>que<space>je<space>trouve<space>un<space>mon<space>dime<space>absolument<space>euh<space>magnifique<space>et<space>qui<space>me<space>t<space>qui<space>m'obsède<space>vachement

2026-01-28 15:56:56,648 | INFO | speech length: 53280
2026-01-28 15:56:56,690 | INFO | decoder input length: 82
2026-01-28 15:56:56,690 | INFO | max output length: 82
2026-01-28 15:56:56,690 | INFO | min output length: 8
2026-01-28 15:57:01,366 | INFO | end detected at 63
2026-01-28 15:57:01,368 | INFO |  -5.03 * 0.5 =  -2.51 for decoder
2026-01-28 15:57:01,368 | INFO |  -2.75 * 0.5 =  -1.37 for ctc
2026-01-28 15:57:01,368 | INFO | total log probability: -3.89
2026-01-28 15:57:01,369 | INFO | normalized log probability: -0.07
2026-01-28 15:57:01,369 | INFO | total number of ended hypotheses: 180
2026-01-28 15:57:01,369 | INFO | best hypo: c'est<space>très<space>très<space>difficile<space>à<space>définir<space>il<space>y<space>a<space>toujours<space>une

2026-01-28 15:57:01,372 | INFO | speech length: 455840
2026-01-28 15:57:01,410 | INFO | decoder input length: 711
2026-01-28 15:57:01,410 | INFO | max output length: 711
2026-01-28 15:57:01,410 | INFO | min output length: 71
2026-01-28 15:58:00,610 | INFO | end detected at 486
2026-01-28 15:58:00,613 | INFO | -317.57 * 0.5 = -158.78 for decoder
2026-01-28 15:58:00,613 | INFO | -64.12 * 0.5 = -32.06 for ctc
2026-01-28 15:58:00,613 | INFO | total log probability: -190.85
2026-01-28 15:58:00,613 | INFO | normalized log probability: -0.40
2026-01-28 15:58:00,613 | INFO | total number of ended hypotheses: 179
2026-01-28 15:58:00,620 | INFO | best hypo: plus<space>ou<space>moins<space>là<space>ou<space>plus<space>ou<space>moins<space>imprégnante<space>et<space>une<space>espèce<space>d'ambiance<space>euh<space>fantastique<space>de<space>choses<space>euh<space>complètement<space>euh<space>irréelles<space>ou<space>surnaturelles<space>dans<space>dans<space>kafka<space>sur<space>un<space>rivage<space>par<space>exemple<space>un<space>des<space>personnages<space>principal<space>qui<space>a<space>un<space>vieil<space>homme<space>euh<space>qui<space>avec<space>des<space>soucis<space>pendant<space>la<space>guerre<space>à<space>le<space>pouvoir<space>de<space>parler<space>ou<space>chard<space>par<space>exemple<space>c'est<space>des<space>choses<space>comme<space>ça<space>donc<space>qui<space>sont<space>d'une<space>à<space>à<space>l'occason<space>d'une<space>très<space>grande<space>fantaisie<space>d'une<space>très<space>grande<space>légèreté<space>une<space>ie<space>y<space>une<space>humour<space>absolunt<space>incroyable

2026-01-28 15:58:00,624 | INFO | speech length: 195200
2026-01-28 15:58:00,678 | INFO | decoder input length: 304
2026-01-28 15:58:00,678 | INFO | max output length: 304
2026-01-28 15:58:00,678 | INFO | min output length: 30
2026-01-28 15:58:12,799 | INFO | end detected at 182
2026-01-28 15:58:12,802 | INFO | -15.07 * 0.5 =  -7.53 for decoder
2026-01-28 15:58:12,802 | INFO |  -3.89 * 0.5 =  -1.95 for ctc
2026-01-28 15:58:12,802 | INFO | total log probability: -9.48
2026-01-28 15:58:12,802 | INFO | normalized log probability: -0.05
2026-01-28 15:58:12,802 | INFO | total number of ended hypotheses: 193
2026-01-28 15:58:12,804 | INFO | best hypo: et<space>à<space>la<space>fois<space>tout<space>ça<space>est<space>un<space>dans<space>un<space>mélange<space>extrêmement<space>singulier<space>enfin<space>que<space>moi<space>j'ai<space>pas<space>tellement<space>rencontré<space>ailleurs<space>de<space>choses<space>d'une<space>grande<space>crudité<space>et<space>de<space>et<space>d'une<space>grande<space>euh

2026-01-28 15:58:12,807 | INFO | speech length: 348960
2026-01-28 15:58:12,858 | INFO | decoder input length: 544
2026-01-28 15:58:12,858 | INFO | max output length: 544
2026-01-28 15:58:12,858 | INFO | min output length: 54
2026-01-28 15:59:04,844 | INFO | end detected at 392
2026-01-28 15:59:04,847 | INFO | -53.20 * 0.5 = -26.60 for decoder
2026-01-28 15:59:04,847 | INFO | -35.33 * 0.5 = -17.66 for ctc
2026-01-28 15:59:04,847 | INFO | total log probability: -44.27
2026-01-28 15:59:04,847 | INFO | normalized log probability: -0.12
2026-01-28 15:59:04,847 | INFO | total number of ended hypotheses: 221
2026-01-28 15:59:04,852 | INFO | best hypo: euh<space>par<space>exemple<space>c'est<space>la<space>sexualité<space>il<space>y<space>a<space>une<space>chose<space>d'exception<space>extrêmement<space>crue<space>extrêmement<space>belle<space>mais<space>voilà<space>ça<space>c'est<space>une<space>sorte<space>de<space>mélange<space>très<space>très<space>très<space>étrange<space>et<space>qui<space>moi<space>me<space>je<space>sais<space>pas<space>pourquoi<space>me<space>renverse<space>également<space>fin<space>j'adore<space>ça<space>quoi<space>je<space>suis<space>au<space>moment<space>où<space>vous<space>parlez<space>d'étranges<space>fantastiques<space>il<space>y<space>a<space>de<space>la<space>grène<space>qui<space>frappe<space>contre<space>les<space>fenêtres<space>là<space>je<space>le<space>précise<space>aux<space>auditeurs

2026-01-28 15:59:04,855 | INFO | speech length: 45760
2026-01-28 15:59:04,900 | INFO | decoder input length: 71
2026-01-28 15:59:04,900 | INFO | max output length: 71
2026-01-28 15:59:04,900 | INFO | min output length: 7
2026-01-28 15:59:06,793 | INFO | end detected at 50
2026-01-28 15:59:06,794 | INFO |  -3.68 * 0.5 =  -1.84 for decoder
2026-01-28 15:59:06,794 | INFO |  -0.10 * 0.5 =  -0.05 for ctc
2026-01-28 15:59:06,794 | INFO | total log probability: -1.89
2026-01-28 15:59:06,794 | INFO | normalized log probability: -0.04
2026-01-28 15:59:06,794 | INFO | total number of ended hypotheses: 140
2026-01-28 15:59:06,795 | INFO | best hypo: euh<space>on<space>peut<space>avoir<space>un<space>aperçu<space>de<space>de<space>l'écriture

2026-01-28 15:59:06,796 | INFO | speech length: 68640
2026-01-28 15:59:06,848 | INFO | decoder input length: 106
2026-01-28 15:59:06,848 | INFO | max output length: 106
2026-01-28 15:59:06,848 | INFO | min output length: 10
2026-01-28 15:59:11,010 | INFO | end detected at 54
2026-01-28 15:59:11,014 | INFO | -15.06 * 0.5 =  -7.53 for decoder
2026-01-28 15:59:11,015 | INFO | -32.44 * 0.5 = -16.22 for ctc
2026-01-28 15:59:11,015 | INFO | total log probability: -23.75
2026-01-28 15:59:11,015 | INFO | normalized log probability: -0.53
2026-01-28 15:59:11,015 | INFO | total number of ended hypotheses: 207
2026-01-28 15:59:11,016 | INFO | best hypo: de<space>me<space>racunir<space>dans<space>les<space>enquoi<space>je<space>pour<space>mieux

2026-01-28 15:59:11,020 | INFO | speech length: 152480
2026-01-28 15:59:11,074 | INFO | decoder input length: 237
2026-01-28 15:59:11,074 | INFO | max output length: 237
2026-01-28 15:59:11,074 | INFO | min output length: 23
2026-01-28 15:59:30,887 | INFO | end detected at 198
2026-01-28 15:59:30,889 | INFO | -22.30 * 0.5 = -11.15 for decoder
2026-01-28 15:59:30,890 | INFO | -21.12 * 0.5 = -10.56 for ctc
2026-01-28 15:59:30,890 | INFO | total log probability: -21.71
2026-01-28 15:59:30,890 | INFO | normalized log probability: -0.11
2026-01-28 15:59:30,890 | INFO | total number of ended hypotheses: 208
2026-01-28 15:59:30,892 | INFO | best hypo: je<space>voudrais<space>en<space>pense<space>que<space>vous<space>disiez<space>sur<space>le<space>sexe<space>la<space>sexualité<space>de<space>lui<space>ah<space>bon<space>ah<space>oui<space>alors<space>attendez<space>non<space>alors<space>ça<space>c'est<space>pas<space>dans<space>ce<space>lui<space>là<space>parce<space>que<space>c'est<space>mais<space>alors<space>ça<space>c'est<space>donc<space>plutôt<space>non

2026-01-28 15:59:30,895 | INFO | speech length: 275360
2026-01-28 15:59:30,949 | INFO | decoder input length: 429
2026-01-28 15:59:30,949 | INFO | max output length: 429
2026-01-28 15:59:30,949 | INFO | min output length: 42
2026-01-28 15:59:55,490 | INFO | end detected at 193
2026-01-28 15:59:55,492 | INFO | -55.30 * 0.5 = -27.65 for decoder
2026-01-28 15:59:55,492 | INFO | -81.52 * 0.5 = -40.76 for ctc
2026-01-28 15:59:55,492 | INFO | total log probability: -68.41
2026-01-28 15:59:55,493 | INFO | normalized log probability: -0.38
2026-01-28 15:59:55,493 | INFO | total number of ended hypotheses: 199
2026-01-28 15:59:55,495 | INFO | best hypo: mais<space>au<space>sud<space>alors<space>vous<space>bougez<space>pas<space>tous<space>les<space>temps<space>vous<space>attendez<space>je<space>suis<space>nous<space>vouloir<space>tellement<space>vous<space>lire<space>un<space>peu<space>comme<space>ça<space>ça<space>va<space>l'air<space>très<space>bien<space>ça<space>oui<space>tout<space>ça<space>être<space>un<space>très<space>mir<space>mais

2026-01-28 15:59:55,498 | INFO | speech length: 60960
2026-01-28 15:59:55,541 | INFO | decoder input length: 94
2026-01-28 15:59:55,542 | INFO | max output length: 94
2026-01-28 15:59:55,542 | INFO | min output length: 9
2026-01-28 16:00:00,875 | INFO | adding <eos> in the last position in the loop
2026-01-28 16:00:00,884 | INFO | end detected at 93
2026-01-28 16:00:00,885 | INFO |  -9.89 * 0.5 =  -4.94 for decoder
2026-01-28 16:00:00,886 | INFO | -14.09 * 0.5 =  -7.05 for ctc
2026-01-28 16:00:00,886 | INFO | total log probability: -11.99
2026-01-28 16:00:00,886 | INFO | normalized log probability: -0.14
2026-01-28 16:00:00,886 | INFO | total number of ended hypotheses: 198
2026-01-28 16:00:00,887 | INFO | best hypo: bah<space>vous<space>avez<space>une<space>petite<space>pudeur<space>là<space>moi<space>j'ai<space>une<space>fille<space>extrêmement<space>punique<space>dans<space>le<space>fond

2026-01-28 16:00:00,889 | INFO | speech length: 34720
2026-01-28 16:00:00,948 | INFO | decoder input length: 53
2026-01-28 16:00:00,948 | INFO | max output length: 53
2026-01-28 16:00:00,948 | INFO | min output length: 5
2026-01-28 16:00:04,151 | INFO | adding <eos> in the last position in the loop
2026-01-28 16:00:04,165 | INFO | no hypothesis. Finish decoding.
2026-01-28 16:00:04,167 | INFO | -11.00 * 0.5 =  -5.50 for decoder
2026-01-28 16:00:04,168 | INFO | -16.85 * 0.5 =  -8.42 for ctc
2026-01-28 16:00:04,168 | INFO | total log probability: -13.92
2026-01-28 16:00:04,168 | INFO | normalized log probability: -0.29
2026-01-28 16:00:04,168 | INFO | total number of ended hypotheses: 150
2026-01-28 16:00:04,169 | INFO | best hypo: non<space>mais<space>même<space>pas<space>enfant<space>je<space>suis<space>ça<space>très<space>pudit

2026-01-28 16:00:04,172 | INFO | speech length: 70720
2026-01-28 16:00:04,225 | INFO | decoder input length: 110
2026-01-28 16:00:04,225 | INFO | max output length: 110
2026-01-28 16:00:04,225 | INFO | min output length: 11
2026-01-28 16:00:16,440 | INFO | adding <eos> in the last position in the loop
2026-01-28 16:00:16,450 | INFO | no hypothesis. Finish decoding.
2026-01-28 16:00:16,451 | INFO | -27.49 * 0.5 = -13.75 for decoder
2026-01-28 16:00:16,452 | INFO | -74.15 * 0.5 = -37.07 for ctc
2026-01-28 16:00:16,452 | INFO | total log probability: -50.82
2026-01-28 16:00:16,452 | INFO | normalized log probability: -0.47
2026-01-28 16:00:16,452 | INFO | total number of ended hypotheses: 118
2026-01-28 16:00:16,453 | INFO | best hypo: c'est<space>pas<space>vous<space>qui<space>dites<space>c'est<space>lui<space>il<space>écrit<space>ah<space>non<space>non<space>mais<space>bien<space>sûr<space>c'st<space>totalement<space>lui<space>c'est<space>pas<space>ça<space>c'est

2026-01-28 16:00:16,456 | INFO | speech length: 101280
2026-01-28 16:00:16,505 | INFO | decoder input length: 157
2026-01-28 16:00:16,505 | INFO | max output length: 157
2026-01-28 16:00:16,505 | INFO | min output length: 15
2026-01-28 16:00:22,514 | INFO | end detected at 84
2026-01-28 16:00:22,516 | INFO | -15.10 * 0.5 =  -7.55 for decoder
2026-01-28 16:00:22,516 | INFO | -11.39 * 0.5 =  -5.69 for ctc
2026-01-28 16:00:22,517 | INFO | total log probability: -13.24
2026-01-28 16:00:22,517 | INFO | normalized log probability: -0.17
2026-01-28 16:00:22,517 | INFO | total number of ended hypotheses: 201
2026-01-28 16:00:22,518 | INFO | best hypo: ça<space>paraît<space>une<space>euh<space>de<space>ma<space>comburité<space>de<space>oui<space>enfin<space>un<space>exercice<space>un<space>peu<space>difficile

2026-01-28 16:00:22,521 | INFO | speech length: 118080
2026-01-28 16:00:22,577 | INFO | decoder input length: 184
2026-01-28 16:00:22,577 | INFO | max output length: 184
2026-01-28 16:00:22,577 | INFO | min output length: 18
2026-01-28 16:00:36,949 | INFO | end detected at 135
2026-01-28 16:00:36,952 | INFO | -23.64 * 0.5 = -11.82 for decoder
2026-01-28 16:00:36,952 | INFO | -31.78 * 0.5 = -15.89 for ctc
2026-01-28 16:00:36,952 | INFO | total log probability: -27.71
2026-01-28 16:00:36,952 | INFO | normalized log probability: -0.22
2026-01-28 16:00:36,952 | INFO | total number of ended hypotheses: 177
2026-01-28 16:00:36,954 | INFO | best hypo: alors<space>comment<space>cette<space>ces<space>cagires<space>alors<space>donc<space>c'est<space>c'est<space>le<space>aussi<space>de<space>la<space>frontière<space>elle<space>va<space>suscérer<space>page<space>cent<space>quatre<space>vingt<space>onze

2026-01-28 16:00:36,957 | INFO | speech length: 449440
2026-01-28 16:00:37,010 | INFO | decoder input length: 701
2026-01-28 16:00:37,010 | INFO | max output length: 701
2026-01-28 16:00:37,010 | INFO | min output length: 70
2026-01-28 16:01:37,696 | INFO | end detected at 502
2026-01-28 16:01:37,698 | INFO | -533.51 * 0.5 = -266.75 for decoder
2026-01-28 16:01:37,698 | INFO | -72.96 * 0.5 = -36.48 for ctc
2026-01-28 16:01:37,698 | INFO | total log probability: -303.23
2026-01-28 16:01:37,698 | INFO | normalized log probability: -0.61
2026-01-28 16:01:37,698 | INFO | total number of ended hypotheses: 163
2026-01-28 16:01:37,706 | INFO | best hypo: c'est<space>fait<space>c'est<space>deux<space>un<space>amour<space>d'enfance<space>moins<space>de<space>dix<space>douze<space>ans<space>entre<space>un<space>jeune<space>homme<space>et<space>une<space>jeune<space>femme<space>qui<space>finissent<space>par<space>se<space>retrouver<space>euh<space>alors<space>qu'ils<space>ont<space>trente<space>cinq<space>ou<space>quarante<space>ans<space>et<space>ça<space>bouleversent<space>complètement<space>leur<space>vie<space>alors<space>raconter<space>comme<space>ça<space>c'est<space>nul<space>mais<space>euh<space>je<space>je<space>arrive<space>je<space>peux<space>c'est<space>jeste<space>pour<space>donc<space>çais<space>c'est<space>je<space>pas<space>çadoit<space>être<space>une<space>momon<space>première<space>fois<space>ou<space>ils<space>front<space>l'amour<space>en<space>fait<space>ensemble<space>puisque<space>quand<space>ils<space>étaient<space>e<space>h<space>jeunes<space>évidemment<space>c'était<space>pas<space>d'actualité<space>pour<space>cent<space>hesitaion

2026-01-28 16:01:37,722 | INFO | Chunk: 0 | WER=17.977528 | S=10 D=1 I=5
2026-01-28 16:01:37,726 | INFO | Chunk: 1 | WER=26.153846 | S=11 D=5 I=1
2026-01-28 16:01:37,727 | INFO | Chunk: 2 | WER=7.692308 | S=1 D=0 I=0
2026-01-28 16:01:37,734 | INFO | Chunk: 3 | WER=31.372549 | S=20 D=9 I=3
2026-01-28 16:01:37,744 | INFO | Chunk: 4 | WER=38.738739 | S=23 D=14 I=6
2026-01-28 16:01:37,744 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 16:01:37,750 | INFO | Chunk: 6 | WER=24.418605 | S=11 D=3 I=7
2026-01-28 16:01:37,752 | INFO | Chunk: 7 | WER=2.941176 | S=0 D=0 I=1
2026-01-28 16:01:37,757 | INFO | Chunk: 8 | WER=22.666667 | S=12 D=3 I=2
2026-01-28 16:01:37,758 | INFO | Chunk: 9 | WER=11.111111 | S=0 D=0 I=1
2026-01-28 16:01:37,758 | INFO | Chunk: 10 | WER=87.500000 | S=4 D=1 I=2
2026-01-28 16:01:37,760 | INFO | Chunk: 11 | WER=21.951220 | S=6 D=1 I=2
2026-01-28 16:01:37,762 | INFO | Chunk: 12 | WER=43.902439 | S=11 D=5 I=2
2026-01-28 16:01:37,762 | INFO | Chunk: 13 | WER=37.500000 | S=3 D=1 I=2
2026-01-28 16:01:37,763 | INFO | Chunk: 14 | WER=41.666667 | S=3 D=2 I=0
2026-01-28 16:01:37,763 | INFO | Chunk: 15 | WER=16.666667 | S=2 D=3 I=0
2026-01-28 16:01:37,764 | INFO | Chunk: 16 | WER=41.666667 | S=2 D=0 I=3
2026-01-28 16:01:37,765 | INFO | Chunk: 17 | WER=50.000000 | S=7 D=10 I=0
2026-01-28 16:01:37,771 | INFO | Chunk: 18 | WER=25.274725 | S=12 D=2 I=9
2026-01-28 16:01:38,322 | INFO | File: Rhap-D2002.wav | WER=26.901249 | S=141 D=55 I=41
2026-01-28 16:01:38,322 | INFO | ------------------------------
2026-01-28 16:01:38,323 | INFO | Conf ester Done!
2026-01-28 16:06:17,708 | INFO | Chunk: 0 | WER=25.842697 | S=13 D=7 I=3
2026-01-28 16:06:17,712 | INFO | Chunk: 1 | WER=38.461538 | S=14 D=9 I=2
2026-01-28 16:06:17,712 | INFO | Chunk: 2 | WER=15.384615 | S=0 D=2 I=0
2026-01-28 16:06:17,720 | INFO | Chunk: 3 | WER=35.294118 | S=18 D=14 I=4
2026-01-28 16:06:17,728 | INFO | Chunk: 4 | WER=41.441441 | S=21 D=23 I=2
2026-01-28 16:06:17,729 | INFO | Chunk: 5 | WER=16.666667 | S=1 D=1 I=0
2026-01-28 16:06:17,735 | INFO | Chunk: 6 | WER=24.418605 | S=13 D=7 I=1
2026-01-28 16:06:17,736 | INFO | Chunk: 7 | WER=17.647059 | S=3 D=2 I=1
2026-01-28 16:06:17,741 | INFO | Chunk: 8 | WER=28.000000 | S=12 D=7 I=2
2026-01-28 16:06:17,741 | INFO | Chunk: 9 | WER=11.111111 | S=0 D=1 I=0
2026-01-28 16:06:17,742 | INFO | Chunk: 10 | WER=87.500000 | S=6 D=1 I=0
2026-01-28 16:06:17,743 | INFO | Chunk: 11 | WER=29.268293 | S=7 D=4 I=1
2026-01-28 16:06:17,745 | INFO | Chunk: 12 | WER=80.487805 | S=19 D=14 I=0
2026-01-28 16:06:17,745 | INFO | Chunk: 13 | WER=18.750000 | S=2 D=1 I=0
2026-01-28 16:06:17,746 | INFO | Chunk: 14 | WER=66.666667 | S=3 D=5 I=0
2026-01-28 16:06:17,747 | INFO | Chunk: 15 | WER=13.333333 | S=4 D=0 I=0
2026-01-28 16:06:17,747 | INFO | Chunk: 16 | WER=8.333333 | S=0 D=1 I=0
2026-01-28 16:06:17,749 | INFO | Chunk: 17 | WER=47.058824 | S=6 D=9 I=1
2026-01-28 16:06:17,755 | INFO | Chunk: 18 | WER=20.879121 | S=15 D=3 I=1
2026-01-28 16:06:18,268 | INFO | File: Rhap-D2002.wav | WER=32.122588 | S=158 D=109 I=16
2026-01-28 16:06:18,269 | INFO | ------------------------------
2026-01-28 16:06:18,269 | INFO | hmm_tdnn Done!
2026-01-28 16:06:18,473 | INFO | ==================================Rhap-D2003.wav=========================================
2026-01-28 16:06:18,743 | INFO | Using rVAD model
2026-01-28 16:06:48,412 | INFO | Chunk: 0 | WER=44.444444 | S=10 D=22 I=0
2026-01-28 16:06:48,418 | INFO | Chunk: 1 | WER=37.864078 | S=18 D=20 I=1
2026-01-28 16:06:48,423 | INFO | Chunk: 2 | WER=53.465347 | S=37 D=17 I=0
2026-01-28 16:06:48,424 | INFO | Chunk: 3 | WER=30.769231 | S=3 D=5 I=0
2026-01-28 16:06:48,428 | INFO | Chunk: 4 | WER=31.521739 | S=14 D=14 I=1
2026-01-28 16:06:48,431 | INFO | Chunk: 5 | WER=32.786885 | S=10 D=8 I=2
2026-01-28 16:06:48,433 | INFO | Chunk: 6 | WER=35.937500 | S=15 D=8 I=0
2026-01-28 16:06:48,439 | INFO | Chunk: 7 | WER=38.317757 | S=18 D=23 I=0
2026-01-28 16:06:48,439 | INFO | Chunk: 8 | WER=10.000000 | S=0 D=0 I=1
2026-01-28 16:06:48,447 | INFO | Chunk: 9 | WER=40.000000 | S=18 D=32 I=2
2026-01-28 16:06:48,449 | INFO | Chunk: 10 | WER=23.636364 | S=11 D=1 I=1
2026-01-28 16:06:48,452 | INFO | Chunk: 11 | WER=28.571429 | S=10 D=11 I=1
2026-01-28 16:06:48,458 | INFO | Chunk: 12 | WER=16.831683 | S=11 D=6 I=0
2026-01-28 16:06:48,459 | INFO | Chunk: 13 | WER=28.571429 | S=5 D=5 I=0
2026-01-28 16:06:48,468 | INFO | Chunk: 14 | WER=37.142857 | S=26 D=25 I=1
2026-01-28 16:06:48,474 | INFO | Chunk: 15 | WER=40.000000 | S=27 D=15 I=0
2026-01-28 16:06:49,243 | INFO | File: Rhap-D2003.wav | WER=35.574668 | S=233 D=212 I=10
2026-01-28 16:06:49,244 | INFO | ------------------------------
2026-01-28 16:06:49,244 | INFO | w2vec vad chunk Done!
2026-01-28 16:07:16,302 | INFO | Chunk: 0 | WER=86.111111 | S=2 D=60 I=0
2026-01-28 16:07:16,305 | INFO | Chunk: 1 | WER=77.669903 | S=2 D=78 I=0
2026-01-28 16:07:16,307 | INFO | Chunk: 2 | WER=76.237624 | S=3 D=74 I=0
2026-01-28 16:07:16,307 | INFO | Chunk: 3 | WER=23.076923 | S=3 D=3 I=0
2026-01-28 16:07:16,309 | INFO | Chunk: 4 | WER=90.217391 | S=1 D=82 I=0
2026-01-28 16:07:16,311 | INFO | Chunk: 5 | WER=37.704918 | S=4 D=18 I=1
2026-01-28 16:07:16,312 | INFO | Chunk: 6 | WER=87.500000 | S=6 D=46 I=4
2026-01-28 16:07:16,314 | INFO | Chunk: 7 | WER=92.523364 | S=3 D=96 I=0
2026-01-28 16:07:16,314 | INFO | Chunk: 8 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 16:07:16,317 | INFO | Chunk: 9 | WER=83.846154 | S=7 D=102 I=0
2026-01-28 16:07:16,318 | INFO | Chunk: 10 | WER=47.272727 | S=5 D=21 I=0
2026-01-28 16:07:16,320 | INFO | Chunk: 11 | WER=75.324675 | S=4 D=53 I=1
2026-01-28 16:07:16,322 | INFO | Chunk: 12 | WER=79.207921 | S=0 D=80 I=0
2026-01-28 16:07:16,323 | INFO | Chunk: 13 | WER=22.857143 | S=5 D=3 I=0
2026-01-28 16:07:16,328 | INFO | Chunk: 14 | WER=73.571429 | S=18 D=83 I=2
2026-01-28 16:07:16,330 | INFO | Chunk: 15 | WER=81.904762 | S=0 D=86 I=0
2026-01-28 16:07:16,731 | INFO | File: Rhap-D2003.wav | WER=74.433151 | S=67 D=881 I=4
2026-01-28 16:07:16,732 | INFO | ------------------------------
2026-01-28 16:07:16,732 | INFO | whisper med Done!
2026-01-28 16:08:12,005 | INFO | Chunk: 0 | WER=83.333333 | S=2 D=58 I=0
2026-01-28 16:08:12,009 | INFO | Chunk: 1 | WER=72.815534 | S=7 D=68 I=0
2026-01-28 16:08:12,012 | INFO | Chunk: 2 | WER=74.257426 | S=0 D=75 I=0
2026-01-28 16:08:12,013 | INFO | Chunk: 3 | WER=19.230769 | S=3 D=2 I=0
2026-01-28 16:08:12,018 | INFO | Chunk: 4 | WER=55.434783 | S=21 D=30 I=0
2026-01-28 16:08:12,022 | INFO | Chunk: 5 | WER=27.868852 | S=9 D=8 I=0
2026-01-28 16:08:12,024 | INFO | Chunk: 6 | WER=85.937500 | S=6 D=45 I=4
2026-01-28 16:08:12,026 | INFO | Chunk: 7 | WER=91.588785 | S=3 D=95 I=0
2026-01-28 16:08:12,026 | INFO | Chunk: 8 | WER=10.000000 | S=0 D=0 I=1
2026-01-28 16:08:12,030 | INFO | Chunk: 9 | WER=85.384615 | S=8 D=102 I=1
2026-01-28 16:08:12,033 | INFO | Chunk: 10 | WER=27.272727 | S=9 D=1 I=5
2026-01-28 16:08:12,038 | INFO | Chunk: 11 | WER=32.467532 | S=10 D=12 I=3
2026-01-28 16:08:12,041 | INFO | Chunk: 12 | WER=71.287129 | S=1 D=71 I=0
2026-01-28 16:08:12,043 | INFO | Chunk: 13 | WER=17.142857 | S=6 D=0 I=0
2026-01-28 16:08:12,048 | INFO | Chunk: 14 | WER=72.857143 | S=3 D=99 I=0
2026-01-28 16:08:12,052 | INFO | Chunk: 15 | WER=67.619048 | S=5 D=66 I=0
2026-01-28 16:08:12,594 | INFO | File: Rhap-D2003.wav | WER=65.363565 | S=98 D=728 I=10
2026-01-28 16:08:12,594 | INFO | ------------------------------
2026-01-28 16:08:12,595 | INFO | whisper large Done!
2026-01-28 16:08:12,797 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 16:08:12,846 | INFO | Vocabulary size: 350
2026-01-28 16:08:13,873 | INFO | Gradient checkpoint layers: []
2026-01-28 16:08:14,631 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 16:08:14,636 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 16:08:14,637 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 16:08:14,637 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 16:08:14,638 | INFO | speech length: 250240
2026-01-28 16:08:14,697 | INFO | decoder input length: 390
2026-01-28 16:08:14,697 | INFO | max output length: 390
2026-01-28 16:08:14,697 | INFO | min output length: 39
2026-01-28 16:08:27,569 | INFO | end detected at 147
2026-01-28 16:08:27,570 | INFO | -228.91 * 0.5 = -114.46 for decoder
2026-01-28 16:08:27,571 | INFO | -102.49 * 0.5 = -51.24 for ctc
2026-01-28 16:08:27,571 | INFO | total log probability: -165.70
2026-01-28 16:08:27,571 | INFO | normalized log probability: -1.16
2026-01-28 16:08:27,571 | INFO | total number of ended hypotheses: 154
2026-01-28 16:08:27,573 | INFO | best hypo: ▁si▁peu▁délicat▁donc▁dans▁le▁doute▁l'attaque▁que▁l'arbitre▁ericsson▁s'est▁abstenu▁fallait▁avec▁messie▁ou▁chercher▁corner▁qui▁va▁trouver▁le▁corner▁ou▁là▁toujous▁l'assime▁orien▁sur▁tout▁cas▁d'une▁concentation▁d'une▁prétentité▁de▁bête▁ébriscueil▁la▁poussé▁le▁ballan▁trop▁loin▁mais▁si▁et▁a▁une▁protestation

2026-01-28 16:08:27,576 | INFO | speech length: 450240
2026-01-28 16:08:27,624 | INFO | decoder input length: 703
2026-01-28 16:08:27,625 | INFO | max output length: 703
2026-01-28 16:08:27,625 | INFO | min output length: 70
2026-01-28 16:08:58,618 | INFO | end detected at 250
2026-01-28 16:08:58,620 | INFO | -606.00 * 0.5 = -303.00 for decoder
2026-01-28 16:08:58,620 | INFO | -306.36 * 0.5 = -153.18 for ctc
2026-01-28 16:08:58,620 | INFO | total log probability: -456.18
2026-01-28 16:08:58,621 | INFO | normalized log probability: -1.89
2026-01-28 16:08:58,621 | INFO | total number of ended hypotheses: 195
2026-01-28 16:08:58,624 | INFO | best hypo: ▁d'inquévalence▁de▁sématophogène▁se▁sera▁un▁coup▁fond▁en▁acceptant▁au▁départ▁leur▁armée▁revient▁la▁première▁faute▁avec▁un▁avertissement▁effectivement▁pour▁dira▁la▁chaleur▁eugène▁don▁pour▁cette▁acction▁litigieuse▁salu▁cinquante▁de▁cinquante▁e▁obstruction▁de▁carbiainse▁mais▁il▁semblerait▁que▁gourkuuse▁erait▁perlue▁le▁ballon▁en▁raison▁pour▁la▁pelle▁peutt▁masction▁n'a▁siffe▁de▁pénctif▁à▁l'orgaince▁après▁un▁cartonss▁signal▁tout▁de▁même▁c'este▁un▁peut▁e▁bêtante▁enfint▁bon▁le▁couplerante▁qui▁va▁être▁uhé▁nonss

2026-01-28 16:08:58,627 | INFO | speech length: 451840
2026-01-28 16:08:58,696 | INFO | decoder input length: 705
2026-01-28 16:08:58,696 | INFO | max output length: 705
2026-01-28 16:08:58,696 | INFO | min output length: 70
2026-01-28 16:09:46,407 | INFO | end detected at 235
2026-01-28 16:09:46,409 | INFO | -639.88 * 0.5 = -319.94 for decoder
2026-01-28 16:09:46,409 | INFO | -195.81 * 0.5 = -97.91 for ctc
2026-01-28 16:09:46,409 | INFO | total log probability: -417.85
2026-01-28 16:09:46,409 | INFO | normalized log probability: -1.82
2026-01-28 16:09:46,409 | INFO | total number of ended hypotheses: 171
2026-01-28 16:09:46,412 | INFO | best hypo: ▁il▁est▁derrière▁le▁ballon▁et▁sa▁tête▁à▁lui▁qui▁atterrit▁sont▁deux▁argentins▁et▁l'autre▁argentin▁cérodrigueuse▁rodrigas▁maxiordre▁le▁piédroit▁de▁rodriguez▁ou▁pied▁gauche▁de▁messie▁ce▁sera▁le▁droit▁de▁rodrigues▁à▁vallois▁à▁sa▁fases▁de▁réparation▁sur▁la▁tête▁argentin▁et▁l'ourne▁de▁mexel▁entre▁de▁brouxès▁sa▁ballon▁de▁la▁tête▁en▁corner▁ils▁saute▁est▁plus▁hauts▁que▁les▁atattachants▁en▁argentin▁et▁ils▁saute▁le▁plus▁haut▁s▁qu'ainguero▁et▁il▁expédit▁ce▁ballons▁en▁corner▁n'y▁avait▁pas▁d'autre▁solution

2026-01-28 16:09:46,415 | INFO | speech length: 126880
2026-01-28 16:09:46,474 | INFO | decoder input length: 197
2026-01-28 16:09:46,474 | INFO | max output length: 197
2026-01-28 16:09:46,474 | INFO | min output length: 19
2026-01-28 16:09:50,025 | INFO | end detected at 61
2026-01-28 16:09:50,028 | INFO |  -7.99 * 0.5 =  -3.99 for decoder
2026-01-28 16:09:50,028 | INFO | -12.69 * 0.5 =  -6.35 for ctc
2026-01-28 16:09:50,028 | INFO | total log probability: -10.34
2026-01-28 16:09:50,028 | INFO | normalized log probability: -0.19
2026-01-28 16:09:50,028 | INFO | total number of ended hypotheses: 183
2026-01-28 16:09:50,029 | INFO | best hypo: ▁et▁c'est▁lionel▁messi▁la▁main▁sur▁le▁côté▁droit▁qui▁va▁tirer▁ce▁cornet▁ne▁le▁tira▁pas▁d'ailleurs▁pour▁regrouler▁à▁deux

2026-01-28 16:09:50,031 | INFO | speech length: 410080
2026-01-28 16:09:50,079 | INFO | decoder input length: 640
2026-01-28 16:09:50,079 | INFO | max output length: 640
2026-01-28 16:09:50,079 | INFO | min output length: 64
2026-01-28 16:10:14,511 | INFO | end detected at 215
2026-01-28 16:10:14,513 | INFO | -641.51 * 0.5 = -320.75 for decoder
2026-01-28 16:10:14,513 | INFO | -254.49 * 0.5 = -127.25 for ctc
2026-01-28 16:10:14,513 | INFO | total log probability: -448.00
2026-01-28 16:10:14,513 | INFO | normalized log probability: -2.15
2026-01-28 16:10:14,513 | INFO | total number of ended hypotheses: 162
2026-01-28 16:10:14,516 | INFO | best hypo: ▁avec▁guédriguez▁mais▁c'est▁messies▁sur▁son▁bi▁gauche▁intention▁mais▁ci▁qui▁change▁pour▁papa▁maintenant▁sur▁santé▁gauche▁également▁par▁kilomètres▁le▁ballon▁dans▁la▁pi▁boîte▁surface▁de▁réparation▁avec▁aguero▁s'écutiérrez▁apppe▁à▁là▁et▁là▁et▁'o▁stepppendant'ae▁à▁la▁l▁suite▁est▁ce▁ballon▁a▁craîché▁par▁les▁argentins▁à▁la▁défense▁de▁cette▁équipe▁de▁france▁et▁s'éguthérèse▁est▁frappe▁à▁environ▁à▁deux▁mètres▁la▁ballon▁à▁la▁niche▁de▁sa▁manda▁'

2026-01-28 16:10:14,518 | INFO | speech length: 324640
2026-01-28 16:10:14,570 | INFO | decoder input length: 506
2026-01-28 16:10:14,570 | INFO | max output length: 506
2026-01-28 16:10:14,570 | INFO | min output length: 50
2026-01-28 16:10:28,850 | INFO | end detected at 147
2026-01-28 16:10:28,852 | INFO | -325.00 * 0.5 = -162.50 for decoder
2026-01-28 16:10:28,852 | INFO | -135.83 * 0.5 = -67.92 for ctc
2026-01-28 16:10:28,852 | INFO | total log probability: -230.42
2026-01-28 16:10:28,852 | INFO | normalized log probability: -1.65
2026-01-28 16:10:28,852 | INFO | total number of ended hypotheses: 140
2026-01-28 16:10:28,854 | INFO | best hypo: ▁peut▁être▁facile▁c'est▁vrai▁mais▁le▁ballon▁était▁en▁points▁sur▁la▁poitrine▁de▁mandenda▁il▁a▁a▁pu▁le▁capter▁tout▁à▁fait▁normalement▁attention▁là▁une▁attaque▁d'aguerrero▁qui▁est▁seule▁car▁'on▁n'achevante▁d'aimpération▁magixèse▁de▁ne▁peut▁pas▁le▁contrer▁mais▁il▁le▁laide▁jouer▁il▁sanera▁à▁héro▁pour▁qui▁est▁madrie

2026-01-28 16:10:28,856 | INFO | speech length: 435840
2026-01-28 16:10:28,902 | INFO | decoder input length: 680
2026-01-28 16:10:28,902 | INFO | max output length: 680
2026-01-28 16:10:28,902 | INFO | min output length: 68
2026-01-28 16:10:52,872 | INFO | end detected at 185
2026-01-28 16:10:52,874 | INFO | -484.29 * 0.5 = -242.14 for decoder
2026-01-28 16:10:52,874 | INFO | -266.91 * 0.5 = -133.45 for ctc
2026-01-28 16:10:52,874 | INFO | total log probability: -375.60
2026-01-28 16:10:52,874 | INFO | normalized log probability: -2.11
2026-01-28 16:10:52,875 | INFO | total number of ended hypotheses: 150
2026-01-28 16:10:52,877 | INFO | best hypo: ▁au▁élu▁le▁hors▁du▁verdoyage▁du▁pied▁duc▁vient▁de▁guérer▁l'un▁dreux▁c'est▁guthérèse▁qui▁vient▁alors▁la▁but▁du▁pédroit▁et▁le▁côté▁droit▁est▁complètement▁seul▁puis▁l'é▁'arislètement▁seul▁puis▁armée▁s'entir▁main▁tenant▁plongé▁et▁on▁il▁prait▁pas▁grand'chose▁l'ain▁sur▁ce▁tir▁et▁'un▁grès▁guthiérèse'aque▁ce▁but▁alors▁que▁c'é▁au▁moment▁l'équipe▁de▁france

2026-01-28 16:10:52,881 | INFO | speech length: 447200
2026-01-28 16:10:52,942 | INFO | decoder input length: 698
2026-01-28 16:10:52,942 | INFO | max output length: 698
2026-01-28 16:10:52,942 | INFO | min output length: 69
2026-01-28 16:11:24,870 | INFO | end detected at 254
2026-01-28 16:11:24,871 | INFO | -561.02 * 0.5 = -280.51 for decoder
2026-01-28 16:11:24,872 | INFO | -204.61 * 0.5 = -102.31 for ctc
2026-01-28 16:11:24,872 | INFO | total log probability: -382.82
2026-01-28 16:11:24,872 | INFO | normalized log probability: -1.54
2026-01-28 16:11:24,872 | INFO | total number of ended hypotheses: 143
2026-01-28 16:11:24,875 | INFO | best hypo: ▁a▁dominez▁un▁peu▁globalement▁enfin▁quoiqu'aye▁avant▁l'écrire▁de▁la▁chambre▁là▁j'aij'allai▁menre▁pas▁tout▁à▁fait▁d'accord▁avec▁comparé▁depuis▁cinq▁minutes▁effectivement▁l'équipe▁de▁france▁de▁commençait▁de▁nouveau▁à▁reculer▁pour▁commencer▁à▁lâcher▁un▁emprise▁sur▁l'adversaire▁les▁argentins▁sans▁les▁ordres▁de▁mahadonnains▁à▁jouer▁un▁peu▁plus▁haut▁ma▁voyenn'adon▁l'asaire▁de▁crcins▁demandant▁les▁joueurs▁de▁monter▁et▁donc▁l'équipe▁de▁france▁et▁mener▁un▁à▁zéro▁à▁là▁qui▁change▁les▁données▁à▁du▁problèmes▁l'és▁dématch▁de▁cette▁importance

2026-01-28 16:11:24,877 | INFO | speech length: 52000
2026-01-28 16:11:24,925 | INFO | decoder input length: 80
2026-01-28 16:11:24,925 | INFO | max output length: 80
2026-01-28 16:11:24,925 | INFO | min output length: 8
2026-01-28 16:11:26,052 | INFO | end detected at 26
2026-01-28 16:11:26,053 | INFO |  -3.38 * 0.5 =  -1.69 for decoder
2026-01-28 16:11:26,053 | INFO |  -1.91 * 0.5 =  -0.95 for ctc
2026-01-28 16:11:26,053 | INFO | total log probability: -2.65
2026-01-28 16:11:26,053 | INFO | normalized log probability: -0.12
2026-01-28 16:11:26,053 | INFO | total number of ended hypotheses: 148
2026-01-28 16:11:26,053 | INFO | best hypo: ▁avec▁de▁tels▁joueurs▁ma▁canton▁mène▁à▁la▁marque

2026-01-28 16:11:26,055 | INFO | speech length: 466560
2026-01-28 16:11:26,103 | INFO | decoder input length: 728
2026-01-28 16:11:26,104 | INFO | max output length: 728
2026-01-28 16:11:26,104 | INFO | min output length: 72
2026-01-28 16:11:58,950 | INFO | end detected at 270
2026-01-28 16:11:58,952 | INFO | -837.43 * 0.5 = -418.71 for decoder
2026-01-28 16:11:58,952 | INFO | -375.16 * 0.5 = -187.58 for ctc
2026-01-28 16:11:58,952 | INFO | total log probability: -606.30
2026-01-28 16:11:58,952 | INFO | normalized log probability: -2.31
2026-01-28 16:11:58,952 | INFO | total number of ended hypotheses: 167
2026-01-28 16:11:58,955 | INFO | best hypo: ▁on▁n'est▁plus▁confiant▁et▁insodique▁que▁renaît▁tranquille▁attention▁quand▁l'attaque▁française▁avec▁l'intérieur▁il▁frappe▁de▁loi▁mais▁dans▁les▁nuages▁alors▁je▁crois▁quellai▁là▁et▁il▁a▁plante▁et▁son▁pied▁dans▁le▁gazon▁alors▁hécutiérès▁dans▁la▁c'héphémala▁la▁frappe▁de▁guthia▁est▁vraiment▁seule▁à▁côté▁droit▁là▁une▁rerreur▁et▁défensive▁en▁tou▁comme▁une▁rerreur▁et▁le▁moine▁de▁marquage▁maison▁surtout▁et▁il▁frappe▁la▁nule▁et▁il▁avant▁un▁peu▁de▁chancement▁cars▁c'a▁dans▁un▁troupe▁de▁rire▁et▁qu'il▁rait▁héroylandai▁impppet▁albaldant▁côté▁entre▁manda▁et▁son▁pote▁droit

2026-01-28 16:11:58,958 | INFO | speech length: 278880
2026-01-28 16:11:59,004 | INFO | decoder input length: 435
2026-01-28 16:11:59,004 | INFO | max output length: 435
2026-01-28 16:11:59,004 | INFO | min output length: 43
2026-01-28 16:12:11,069 | INFO | end detected at 127
2026-01-28 16:12:11,070 | INFO | -202.21 * 0.5 = -101.10 for decoder
2026-01-28 16:12:11,070 | INFO | -58.63 * 0.5 = -29.32 for ctc
2026-01-28 16:12:11,070 | INFO | total log probability: -130.42
2026-01-28 16:12:11,070 | INFO | normalized log probability: -1.09
2026-01-28 16:12:11,070 | INFO | total number of ended hypotheses: 176
2026-01-28 16:12:11,072 | INFO | best hypo: ▁alors▁le▁dégagement▁maintenant▁de▁carriso▁le▁dégagement▁argentin▁en▁est▁à▁quatre▁minutes▁de▁la▁mi▁temps▁même▁pas▁et▁les▁argentins▁mènent▁à▁zéro▁un▁elka▁un▁alca▁qui▁vaut▁donner▁le▁reco▁de▁gauche▁là▁bas▁pour▁tirer▁henri▁mais▁c'est▁manquer▁'est▁manquer▁et▁s'est▁récupéré▁bien▁sûr

2026-01-28 16:12:11,074 | INFO | speech length: 295680
2026-01-28 16:12:11,111 | INFO | decoder input length: 461
2026-01-28 16:12:11,112 | INFO | max output length: 461
2026-01-28 16:12:11,112 | INFO | min output length: 46
2026-01-28 16:12:30,460 | INFO | end detected at 173
2026-01-28 16:12:30,462 | INFO | -265.94 * 0.5 = -132.97 for decoder
2026-01-28 16:12:30,462 | INFO | -61.33 * 0.5 = -30.67 for ctc
2026-01-28 16:12:30,462 | INFO | total log probability: -163.64
2026-01-28 16:12:30,462 | INFO | normalized log probability: -1.00
2026-01-28 16:12:30,462 | INFO | total number of ended hypotheses: 144
2026-01-28 16:12:30,465 | INFO | best hypo: ▁par▁l'équipe▁argentine▁ils▁sont▁de▁nouveau▁en▁possession▁les▁argentins▁du▁ballon▁et▁saravid▁comme▁en▁début▁de▁match▁et▁pour▁l'instant▁l'équipe▁de▁france▁n'arrive▁pas▁à▁récupérer▁ce▁ballon▁elle▁est▁obligée▁de▁commettre▁les▁photes▁finales▁aux▁habitations▁axio▁rodriguez▁et▁s'est▁un▁coupe▁pron▁dans▁le▁camp▁l'équipe▁de▁france▁comme▁en▁début▁de▁rencontres▁français

2026-01-28 16:12:30,470 | INFO | speech length: 446560
2026-01-28 16:12:30,537 | INFO | decoder input length: 697
2026-01-28 16:12:30,537 | INFO | max output length: 697
2026-01-28 16:12:30,537 | INFO | min output length: 69
2026-01-28 16:12:58,340 | INFO | end detected at 210
2026-01-28 16:12:58,341 | INFO | -532.00 * 0.5 = -266.00 for decoder
2026-01-28 16:12:58,341 | INFO | -147.60 * 0.5 = -73.80 for ctc
2026-01-28 16:12:58,341 | INFO | total log probability: -339.80
2026-01-28 16:12:58,341 | INFO | normalized log probability: -1.66
2026-01-28 16:12:58,341 | INFO | total number of ended hypotheses: 176
2026-01-28 16:12:58,344 | INFO | best hypo: ▁qui▁sont▁dépassés▁par▁les▁événements▁ils▁se▁sont▁bien▁repris▁ensuite▁mais▁là▁ils▁lâchent▁de▁nouveau▁prise▁alors▁que▁nous▁sommes▁trois▁minutes▁de▁la▁mi▁temps▁et▁alors▁que▁le▁ballon▁maintenant▁est▁dans▁les▁pieds▁et▁la▁bas▁droite▁de▁je▁pense▁c'est▁majrano▁peut▁être▁c'était▁lui▁et▁le▁ballon▁est▁sortie▁et▁pour▁l'équipe▁de▁la▁france▁là▁nonent▁pour▁les▁argentins▁sortie▁en▁touge▁pour▁les▁argentins▁avec▁zannettier▁qui▁va▁faire▁cette▁touche▁là▁bas▁et▁le▁jouheur▁qui▁joue▁depuis▁nées▁à▁allinter

2026-01-28 16:12:58,347 | INFO | speech length: 139200
2026-01-28 16:12:58,394 | INFO | decoder input length: 217
2026-01-28 16:12:58,394 | INFO | max output length: 217
2026-01-28 16:12:58,394 | INFO | min output length: 21
2026-01-28 16:13:02,906 | INFO | end detected at 74
2026-01-28 16:13:02,908 | INFO | -18.36 * 0.5 =  -9.18 for decoder
2026-01-28 16:13:02,908 | INFO | -26.51 * 0.5 = -13.25 for ctc
2026-01-28 16:13:02,908 | INFO | total log probability: -22.43
2026-01-28 16:13:02,908 | INFO | normalized log probability: -0.33
2026-01-28 16:13:02,908 | INFO | total number of ended hypotheses: 195
2026-01-28 16:13:02,909 | INFO | best hypo: ▁alors▁que▁le▁ballon▁est▁revenu▁maintenant▁dans▁le▁carou▁à▁nouveau▁dans▁les▁piozzanetti▁gravational▁doit▁tout▁près▁de▁la▁ligne▁de▁touges▁et▁il▁donne▁un▁gaga

2026-01-28 16:13:02,911 | INFO | speech length: 621280
2026-01-28 16:13:02,958 | INFO | decoder input length: 970
2026-01-28 16:13:02,958 | INFO | max output length: 970
2026-01-28 16:13:02,958 | INFO | min output length: 97
2026-01-28 16:14:00,410 | INFO | end detected at 364
2026-01-28 16:14:00,411 | INFO | -957.17 * 0.5 = -478.58 for decoder
2026-01-28 16:14:00,411 | INFO | -427.77 * 0.5 = -213.89 for ctc
2026-01-28 16:14:00,411 | INFO | total log probability: -692.47
2026-01-28 16:14:00,411 | INFO | normalized log probability: -1.93
2026-01-28 16:14:00,411 | INFO | total number of ended hypotheses: 165
2026-01-28 16:14:00,416 | INFO | best hypo: ▁agago▁est▁effectivement▁une▁démontrée▁corrective▁et▁contrée▁à▁heinzewein▁ce▁qui▁change▁de▁côté▁on▁n'a▁pas▁pu▁passer▁à▁droite▁tomber▁ici▁à▁gauche▁avec▁papain▁maintenant▁très▁offensif▁et▁pas▁mal▁de▁tout▁ce▁qui▁fait▁penser▁à▁vicente▁là▁il▁est▁petit▁et▁très▁vif▁très▁offensif▁et▁il▁courts▁dans▁son▁couloir▁le▁dénomet▁papa▁le▁ballon▁récupéré▁par▁exe▁o▁maintenant▁les▁fant▁qui▁sont▁obligés▁de▁courir▁après▁ce▁ballon▁et▁il▁encore▁papasstement▁ifshe▁'im▁média▁qui▁accélère▁maintenant▁qu'a▁tropé▁à▁gwado▁agoano▁en▁retrait▁pour▁gako▁gwako▁pour▁e▁de▁nouveau▁par▁les▁brapaalons▁contt▁mais▁'a▁contin▁de▁cirer▁'és▁maintenant▁'a▁là▁là▁la▁mais▁cit▁techéniquement▁'▁fantainsins▁et▁ils▁viente▁de▁s'achapper▁de▁sortier▁d'un▁ph▁tendu▁par▁un▁jouans

2026-01-28 16:14:00,419 | INFO | speech length: 469120
2026-01-28 16:14:00,467 | INFO | decoder input length: 732
2026-01-28 16:14:00,467 | INFO | max output length: 732
2026-01-28 16:14:00,468 | INFO | min output length: 73
2026-01-28 16:14:32,969 | INFO | end detected at 256
2026-01-28 16:14:32,971 | INFO | -689.87 * 0.5 = -344.93 for decoder
2026-01-28 16:14:32,971 | INFO | -314.41 * 0.5 = -157.21 for ctc
2026-01-28 16:14:32,971 | INFO | total log probability: -502.14
2026-01-28 16:14:32,971 | INFO | normalized log probability: -2.02
2026-01-28 16:14:32,971 | INFO | total number of ended hypotheses: 192
2026-01-28 16:14:32,974 | INFO | best hypo: ▁il▁peut▁donner▁derrière▁ou▁heinzey▁à▁récupérer▁ce▁ballon▁et▁les▁cartes▁à▁bas▁côtés▁droit▁zanettif▁fon▁zanetti▁alors▁que▁heinze▁s'ékahame▁aissez▁est▁extraordinaire▁et▁seul▁patron▁de▁la▁défense▁et▁jolien▁sur▁l'haya▁et▁a▁une▁mauvaise▁séquence▁pour▁l'équipe▁de▁francea▁actuelle▁qui▁se▁fait▁la▁vraie▁malaia▁déjà▁avantette▁de▁nouve▁durès▁le▁buateur▁justetement▁à▁la▁quarante▁mincutes▁ils▁rendre▁sa▁fortte▁de▁réparie▁pour▁l'in▁de▁la▁récy▁mais▁ce▁qui▁rendrens▁de▁réparration▁stée▁impoca▁de▁française▁qui▁tente▁de▁repouser▁les▁vion

2026-01-28 16:14:32,987 | INFO | Chunk: 0 | WER=52.777778 | S=21 D=16 I=1
2026-01-28 16:14:32,993 | INFO | Chunk: 1 | WER=61.165049 | S=43 D=18 I=2
2026-01-28 16:14:32,998 | INFO | Chunk: 2 | WER=47.524752 | S=31 D=11 I=6
2026-01-28 16:14:32,999 | INFO | Chunk: 3 | WER=26.923077 | S=5 D=1 I=1
2026-01-28 16:14:33,004 | INFO | Chunk: 4 | WER=50.000000 | S=28 D=13 I=5
2026-01-28 16:14:33,007 | INFO | Chunk: 5 | WER=36.065574 | S=17 D=2 I=3
2026-01-28 16:14:33,010 | INFO | Chunk: 6 | WER=62.500000 | S=27 D=2 I=11
2026-01-28 16:14:33,016 | INFO | Chunk: 7 | WER=46.728972 | S=34 D=11 I=5
2026-01-28 16:14:33,016 | INFO | Chunk: 8 | WER=20.000000 | S=2 D=0 I=0
2026-01-28 16:14:33,025 | INFO | Chunk: 9 | WER=56.923077 | S=40 D=28 I=6
2026-01-28 16:14:33,027 | INFO | Chunk: 10 | WER=34.545455 | S=12 D=4 I=3
2026-01-28 16:14:33,030 | INFO | Chunk: 11 | WER=29.870130 | S=11 D=11 I=1
2026-01-28 16:14:33,036 | INFO | Chunk: 12 | WER=23.762376 | S=10 D=10 I=4
2026-01-28 16:14:33,037 | INFO | Chunk: 13 | WER=34.285714 | S=6 D=6 I=0
2026-01-28 16:14:33,048 | INFO | Chunk: 14 | WER=50.000000 | S=51 D=11 I=8
2026-01-28 16:14:33,054 | INFO | Chunk: 15 | WER=60.952381 | S=51 D=11 I=2
2026-01-28 16:14:33,902 | INFO | File: Rhap-D2003.wav | WER=46.989836 | S=390 D=154 I=57
2026-01-28 16:14:33,902 | INFO | ------------------------------
2026-01-28 16:14:33,902 | INFO | Conf cv Done!
2026-01-28 16:14:34,083 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 16:14:34,106 | INFO | Vocabulary size: 47
2026-01-28 16:14:35,387 | INFO | Gradient checkpoint layers: []
2026-01-28 16:14:36,506 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 16:14:36,515 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 16:14:36,515 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 16:14:36,516 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 16:14:36,520 | INFO | speech length: 250240
2026-01-28 16:14:36,594 | INFO | decoder input length: 390
2026-01-28 16:14:36,594 | INFO | max output length: 390
2026-01-28 16:14:36,594 | INFO | min output length: 39
2026-01-28 16:15:15,323 | INFO | end detected at 360
2026-01-28 16:15:15,325 | INFO | -75.76 * 0.5 = -37.88 for decoder
2026-01-28 16:15:15,325 | INFO | -157.39 * 0.5 = -78.69 for ctc
2026-01-28 16:15:15,326 | INFO | total log probability: -116.57
2026-01-28 16:15:15,326 | INFO | normalized log probability: -0.34
2026-01-28 16:15:15,326 | INFO | total number of ended hypotheses: 224
2026-01-28 16:15:15,330 | INFO | best hypo: petit<space>peu<space>délicat<space>donc<space>euh<space>dans<space>le<space>doute<space>tel<space>que<space>l'arbitre<space>euh<space>ericson<space>s'est<space>absolu<space>fallait<space>avec<space>messier<space>on<space>va<space>chercher<space>le<space>cornaire<space>qui<space>va<space>trouver<space>le<space>corner<space>où<space>la<space>touche<space>ou<space>la<space>sortie<space>du<space>nu<space>don<space>bien<space>sûr<space>bien<space>sûr<space>en<space>tout<space>cas<space>il<space>a<space>une<space>concertation<space>et<space>pour<space>tenter<space>de<space>but<space>puisque<space>il<space>a<space>poussé<space>le<space>balan<space>trop<space>loin<space>mais<space>ici<space>et<space>y<space>a<space>une<space>euh<space>protestation

2026-01-28 16:15:15,333 | INFO | speech length: 450240
2026-01-28 16:15:15,376 | INFO | decoder input length: 703
2026-01-28 16:15:15,376 | INFO | max output length: 703
2026-01-28 16:15:15,376 | INFO | min output length: 70
2026-01-28 16:16:28,867 | INFO | end detected at 573
2026-01-28 16:16:28,868 | INFO | -505.01 * 0.5 = -252.51 for decoder
2026-01-28 16:16:28,868 | INFO | -251.92 * 0.5 = -125.96 for ctc
2026-01-28 16:16:28,868 | INFO | total log probability: -378.47
2026-01-28 16:16:28,868 | INFO | normalized log probability: -0.67
2026-01-28 16:16:28,868 | INFO | total number of ended hypotheses: 148
2026-01-28 16:16:28,877 | INFO | best hypo: euh<space>d'ag<space>alors<space>ce<space>sera<space>tout<space>fort<space>eugène<space>ce<space>sera<space>un<space>coufort<space>hein<space>c'est<space>peut<space>être<space>au<space>départ<space>la<space>harvée<space>revient<space>la<space>première<space>faute<space>avec<space>un<space>aertissement<space>effectivement<space>pour<space>dire<space>à<space>la<space>alors<space>eugène<space>donc<space>pour<space>cette<space>action<space>itigieux<space>c'est<space>du<space>cinquante<space>cinquante<space>petite<space>obstruction<space>de<space>cabiane<space>zeu<space>mais<space>il<space>semplerait<space>que<space>gourcus<space>ait<space>perdu<space>le<space>ballon<space>en<space>raison<space>pour<space>laquelle<space>peut<space>être<space>monsieur<space>ritson<space>n'a<space>pas<space>cité<space>le<space>pénatie<space>alors<space>galan<space>çarès<space>un<space>cartron<space>je<space>vous<space>sigale<space>tout<space>de<space>même<space>euh<space>c'est<space>un<space>peu<space>embêtant<space>mais<space>enfin<space>bon<space>euh<space>le<space>cou<space>fran<space>qui<space>va<space>être<space>e<space>joué<space>non<space>pas<space>par<space>euh<space>mesie

2026-01-28 16:16:28,879 | INFO | speech length: 451840
2026-01-28 16:16:28,921 | INFO | decoder input length: 705
2026-01-28 16:16:28,921 | INFO | max output length: 705
2026-01-28 16:16:28,921 | INFO | min output length: 70
2026-01-28 16:17:33,711 | INFO | end detected at 526
2026-01-28 16:17:33,713 | INFO | -459.69 * 0.5 = -229.85 for decoder
2026-01-28 16:17:33,713 | INFO | -199.25 * 0.5 = -99.63 for ctc
2026-01-28 16:17:33,713 | INFO | total log probability: -329.47
2026-01-28 16:17:33,713 | INFO | normalized log probability: -0.64
2026-01-28 16:17:33,713 | INFO | total number of ended hypotheses: 178
2026-01-28 16:17:33,720 | INFO | best hypo: euh<space>il<space>est<space>derrière<space>le<space>valant<space>mais<space>c'est<space>peut<space>être<space>pas<space>lui<space>qui<space>va<space>tirer<space>son<space>deux<space>argentin<space>hein<space>et<space>l'autre<space>argentin<space>c'est<space>rodriguez<space>rodriguez<space>maxirodon<space>le<space>pied<space>droit<space>de<space>rodriguez<space>ou<space>piégauche<space>de<space>mexique<space>ce<space>sera<space>le<space>droit<space>de<space>rodriguez<space>zaballon<space>à<space>sa<space>fasse<space>de<space>réparation<space>à<space>la<space>tête<space>argentine<space>moi<space>autre<space>de<space>mexille<space>contre<space>le<space>succès<space>ce<space>ballon<space>dans<space>la<space>pête<space>en<space>corneir<space>il<space>chaute<space>plus<space>haut<space>euh<space>que<space>les<space>attaquants<space>argentins<space>ils<space>shaut<space>ent<space>plu<space>haut<space>au<space>si<space>kagheuro<space>et<space>lex<space>pédit<space>che<space>ballon<space>en<space>coron<space>eun<space>n'y<space>avait<space>pas<space>d'autre<space>solution

2026-01-28 16:17:33,723 | INFO | speech length: 126880
2026-01-28 16:17:33,762 | INFO | decoder input length: 197
2026-01-28 16:17:33,762 | INFO | max output length: 197
2026-01-28 16:17:33,762 | INFO | min output length: 19
2026-01-28 16:17:44,085 | INFO | end detected at 137
2026-01-28 16:17:44,088 | INFO | -16.82 * 0.5 =  -8.41 for decoder
2026-01-28 16:17:44,088 | INFO |  -8.01 * 0.5 =  -4.01 for ctc
2026-01-28 16:17:44,088 | INFO | total log probability: -12.42
2026-01-28 16:17:44,088 | INFO | normalized log probability: -0.10
2026-01-28 16:17:44,088 | INFO | total number of ended hypotheses: 234
2026-01-28 16:17:44,090 | INFO | best hypo: et<space>c'est<space>lionel<space>messi<space>là<space>bas<space>euh<space>sur<space>le<space>côté<space>droit<space>qui<space>va<space>tirer<space>ce<space>corneille<space>ne<space>le<space>tire<space>pas<space>d'ailleurs<space>pour<space>recordir<space>à<space>deux

2026-01-28 16:17:44,092 | INFO | speech length: 410080
2026-01-28 16:17:44,134 | INFO | decoder input length: 640
2026-01-28 16:17:44,134 | INFO | max output length: 640
2026-01-28 16:17:44,134 | INFO | min output length: 64
2026-01-28 16:18:42,436 | INFO | end detected at 482
2026-01-28 16:18:42,438 | INFO | -399.88 * 0.5 = -199.94 for decoder
2026-01-28 16:18:42,438 | INFO | -144.15 * 0.5 = -72.08 for ctc
2026-01-28 16:18:42,438 | INFO | total log probability: -272.02
2026-01-28 16:18:42,439 | INFO | normalized log probability: -0.58
2026-01-28 16:18:42,439 | INFO | total number of ended hypotheses: 184
2026-01-28 16:18:42,444 | INFO | best hypo: avec<space>euh<space>rodriguez<space>mais<space>c'est<space>merci<space>sur<space>son<space>biéroge<space>attention<space>euh<space>mais<space>ce<space>qui<space>change<space>pour<space>papa<space>maintenant<space>sur<space>son<space>pied<space>gauche<space>également<space>papa<space>qui<space>va<space>mettre<space>le<space>ballon<space>dans<space>la<space>petit<space>boîte<space>sur<space>face<space>de<space>réparation<space>avec<space>à<space>guéro<space>non<space>c'estous<space>tirez<space>la<space>trappe<space>à<space>là<space>tout<space>steve<space>mandant<space>d'a<space>à<space>la<space>suite<space>de<space>ce<space>balloir<space>raché<space>euh<space>par<space>les<space>euh<space>argentins<space>à<space>la<space>défense<space>de<space>cette<space>équipe<space>de<space>france<space>et<space>c'est<space>guû<space>tirrez<space>qui<space>frappe<space>à<space>environ<space>vingt<space>deux<space>mètres<space>le<space>ballon<space>dans<space>la<space>niche<space>de<space>ces<space>mondet

2026-01-28 16:18:42,447 | INFO | speech length: 324640
2026-01-28 16:18:42,492 | INFO | decoder input length: 506
2026-01-28 16:18:42,492 | INFO | max output length: 506
2026-01-28 16:18:42,492 | INFO | min output length: 50
2026-01-28 16:19:21,776 | INFO | end detected at 362
2026-01-28 16:19:21,778 | INFO | -158.43 * 0.5 = -79.22 for decoder
2026-01-28 16:19:21,778 | INFO | -82.49 * 0.5 = -41.24 for ctc
2026-01-28 16:19:21,779 | INFO | total log probability: -120.46
2026-01-28 16:19:21,779 | INFO | normalized log probability: -0.35
2026-01-28 16:19:21,779 | INFO | total number of ended hypotheses: 113
2026-01-28 16:19:21,784 | INFO | best hypo: peut<space>être<space>pas<space>facile<space>c'est<space>vrai<space>mais<space>le<space>ballon<space>était<space>en<space>point<space>sur<space>la<space>poitrine<space>de<space>mandandat<space>il<space>est<space>appu<space>le<space>capter<space>tout<space>à<space>fait<space>euh<space>normalement<space>attention<space>là<space>une<space>attaque<space>à<space>da<space>guerro<space>qui<space>est<space>celle<space>quand<space>on<space>a<space>se<space>passe<space>dans<space>l'imparation<space>mais<space>accesse<space>ne<space>peut<space>pas<space>le<space>contraire<space>il<space>le<space>laisse<space>se<space>jouer<space>il<space>chantera<space>guerro<space>pour<space>qui<space>oui<space>est<space>la<space>matière<space>de<space>faire

2026-01-28 16:19:21,786 | INFO | speech length: 435840
2026-01-28 16:19:21,833 | INFO | decoder input length: 680
2026-01-28 16:19:21,833 | INFO | max output length: 680
2026-01-28 16:19:21,833 | INFO | min output length: 68
2026-01-28 16:20:05,593 | INFO | end detected at 312
2026-01-28 16:20:05,596 | INFO | -96.90 * 0.5 = -48.45 for decoder
2026-01-28 16:20:05,596 | INFO | -92.56 * 0.5 = -46.28 for ctc
2026-01-28 16:20:05,596 | INFO | total log probability: -94.73
2026-01-28 16:20:05,596 | INFO | normalized log probability: -0.31
2026-01-28 16:20:05,596 | INFO | total number of ended hypotheses: 170
2026-01-28 16:20:05,600 | INFO | best hypo: oui<space>oui<space>bien<space>non<space>de<space>gutter<space>hein<space>c'est<space>gutiaze<space>du<space>p<space>droit<space>mais<space>le<space>côté<space>droit<space>complètement<space>seul<space>puis<space>de<space>gazi<space>complètement<space>seul<space>puis<space>a<space>mis<space>de<space>sentir<space>maintenant<space>on<space>a<space>plongé<space>mais<space>bon<space>il<space>pourrait<space>pas<space>grand<space>chose<space>là<space>sur<space>ce<space>tir<space>et<space>donc<space>euh<space>goutirez<space>marque<space>ce<space>but<space>alors<space>que<space>c'est<space>au<space>moment<space>où<space>l'équipe<space>de<space>france

2026-01-28 16:20:05,604 | INFO | speech length: 447200
2026-01-28 16:20:05,661 | INFO | decoder input length: 698
2026-01-28 16:20:05,662 | INFO | max output length: 698
2026-01-28 16:20:05,662 | INFO | min output length: 69
2026-01-28 16:21:16,333 | INFO | end detected at 595
2026-01-28 16:21:16,334 | INFO | -678.10 * 0.5 = -339.05 for decoder
2026-01-28 16:21:16,334 | INFO | -127.68 * 0.5 = -63.84 for ctc
2026-01-28 16:21:16,335 | INFO | total log probability: -402.89
2026-01-28 16:21:16,335 | INFO | normalized log probability: -0.68
2026-01-28 16:21:16,335 | INFO | total number of ended hypotheses: 157
2026-01-28 16:21:16,342 | INFO | best hypo: euh<space>dominer<space>un<space>peu<space>globalement<space>enfin<space>quoi<space>que<space>là<space>y<space>avait<space>les<space>guerre<space>d'achat<space>voilà<space>j'ai<space>j'ai<space>j'a<space>j'allai<space>vous<space>dire<space>pas<space>tout<space>à<space>fait<space>d'accord<space>avec<space>vous<space>passer<space>depuis<space>cinq<space>minutes<space>effectivement<space>l'équipe<space>de<space>france<space>commençait<space>de<space>nous<space>voir<space>reculer<space>voyez<space>commencer<space>à<space>lâcher<space>son<space>empris<space>sur<space>l'adversaire<space>rare<space>et<space>les<space>argentins<space>sous<space>les<space>ordres<space>de<space>mahagona<space>euh<space>jouait<space>un<space>peu<space>plus<space>haut<space>mais<space>on<space>voyez<space>mara<space>dans<space>l'affaire<space>de<space>grons<space>ses<space>demandant<space>ces<space>joueurs<space>de<space>de<space>monter<space>et<space>donc<space>l'équipe<space>de<space>france<space>'st<space>menée<space>un<space>à<space>zéro<space>vola<space>qui<space>change<space>les<space>données<space>uh<space>de<space>problème<space>parce<space>que<space>dn<space>des<space>matces<space>de<space>cette<space>importance

2026-01-28 16:21:16,344 | INFO | speech length: 52000
2026-01-28 16:21:16,383 | INFO | decoder input length: 80
2026-01-28 16:21:16,384 | INFO | max output length: 80
2026-01-28 16:21:16,384 | INFO | min output length: 8
2026-01-28 16:21:18,572 | INFO | end detected at 57
2026-01-28 16:21:18,573 | INFO |  -4.24 * 0.5 =  -2.12 for decoder
2026-01-28 16:21:18,574 | INFO |  -0.34 * 0.5 =  -0.17 for ctc
2026-01-28 16:21:18,574 | INFO | total log probability: -2.29
2026-01-28 16:21:18,574 | INFO | normalized log probability: -0.04
2026-01-28 16:21:18,574 | INFO | total number of ended hypotheses: 171
2026-01-28 16:21:18,574 | INFO | best hypo: avec<space>de<space>tels<space>joueurs<space>bah<space>quand<space>on<space>mène<space>à<space>la<space>marque

2026-01-28 16:21:18,576 | INFO | speech length: 466560
2026-01-28 16:21:18,626 | INFO | decoder input length: 728
2026-01-28 16:21:18,626 | INFO | max output length: 728
2026-01-28 16:21:18,626 | INFO | min output length: 72
2026-01-28 16:22:30,927 | INFO | end detected at 617
2026-01-28 16:22:30,929 | INFO | -772.74 * 0.5 = -386.37 for decoder
2026-01-28 16:22:30,929 | INFO | -239.41 * 0.5 = -119.71 for ctc
2026-01-28 16:22:30,929 | INFO | total log probability: -506.08
2026-01-28 16:22:30,930 | INFO | normalized log probability: -0.83
2026-01-28 16:22:30,930 | INFO | total number of ended hypotheses: 167
2026-01-28 16:22:30,942 | INFO | best hypo: euh<space>on<space>n'est<space>plus<space>confiant<space>et<space>on<space>se<space>dit<space>que<space>on<space>est<space>trang<space>qui<space>est<space>attachant<space>quand<space>l'attaque<space>française<space>avec<space>kerry<space>enri<space>il<space>fera<space>de<space>loin<space>mais<space>dans<space>les<space>nuages<space>alors<space>je<space>crois<space>que<space>là<space>il<space>a<space>il<space>a<space>planté<space>son<space>pied<space>dans<space>le<space>gazon<space>alors<space>et<space>qui<space>thierry<space>a<space>le<space>jet<space>fait<space>mal'ail<space>la<space>la<space>frappe<space>de<space>gutia<space>est<space>vraiment<space>seul<space>hein<space>côté<space>droit<space>là<space>y<space>a<space>une<space>erreur<space>défensive<space>en<space>tout<space>cas<space>une<space>erreur<space>pour<space>le<space>moins<space>de<space>de<space>marquage<space>mais<space>eh<space>surout<space>il<space>frapp<space>c'euneulle<space>et<space>i<space>aun<space>peu<space>de<space>hene<space>églement<space>car<space>c'est<space>dans<space>un<space>trop<space>de<space>sourie<space>hein<space>et<space>quil<space>trouve<space>he<space>herainguieuin<space>eu<space>heuieuie<space>aieui<space>côt<space>entre<space>mon<space>an<space>ai<space>et<space>son<space>pôteu<space>droit<space>heui<space>oui<space>oui<space>c'estvrai

2026-01-28 16:22:30,947 | INFO | speech length: 278880
2026-01-28 16:22:31,000 | INFO | decoder input length: 435
2026-01-28 16:22:31,000 | INFO | max output length: 435
2026-01-28 16:22:31,000 | INFO | min output length: 43
2026-01-28 16:23:00,076 | INFO | end detected at 295
2026-01-28 16:23:00,078 | INFO | -43.58 * 0.5 = -21.79 for decoder
2026-01-28 16:23:00,079 | INFO | -27.37 * 0.5 = -13.69 for ctc
2026-01-28 16:23:00,079 | INFO | total log probability: -35.48
2026-01-28 16:23:00,079 | INFO | normalized log probability: -0.12
2026-01-28 16:23:00,079 | INFO | total number of ended hypotheses: 173
2026-01-28 16:23:00,083 | INFO | best hypo: alors<space>le<space>dégagement<space>maintenant<space>de<space>cariso<space>euh<space>le<space>dégagement<space>argentin<space>on<space>est<space>à<space>quatre<space>minutes<space>de<space>la<space>mi<space>temps<space>même<space>pas<space>et<space>les<space>argentins<space>mènent<space>un<space>à<space>zéro<space>anelka<space>anelka<space>qui<space>veut<space>donner<space>sur<space>le<space>code<space>de<space>gauche<space>là<space>bas<space>pour<space>tierrer<space>en<space>haie<space>mais<space>c'est<space>manqué<space>c'est<space>manqué<space>et<space>s'y<space>récupérer<space>bien<space>sûr

2026-01-28 16:23:00,086 | INFO | speech length: 295680
2026-01-28 16:23:00,127 | INFO | decoder input length: 461
2026-01-28 16:23:00,127 | INFO | max output length: 461
2026-01-28 16:23:00,127 | INFO | min output length: 46
2026-01-28 16:23:37,556 | INFO | end detected at 394
2026-01-28 16:23:37,558 | INFO | -54.72 * 0.5 = -27.36 for decoder
2026-01-28 16:23:37,558 | INFO | -30.53 * 0.5 = -15.26 for ctc
2026-01-28 16:23:37,558 | INFO | total log probability: -42.63
2026-01-28 16:23:37,558 | INFO | normalized log probability: -0.11
2026-01-28 16:23:37,558 | INFO | total number of ended hypotheses: 194
2026-01-28 16:23:37,563 | INFO | best hypo: par<space>l'équipe<space>argentine<space>qui<space>sont<space>de<space>nouveau<space>en<space>possession<space>les<space>argentins<space>du<space>euh<space>ballon<space>et<space>ça<space>va<space>vide<space>comme<space>en<space>début<space>de<space>match<space>et<space>pour<space>l'instant<space>l'équipe<space>de<space>france<space>n'arrive<space>pas<space>à<space>récupérer<space>ce<space>ballon<space>elle<space>est<space>obligée<space>de<space>commettre<space>des<space>faute<space>et<space>euh<space>abidiale<space>oui<space>habilation<space>axio<space>rodriguez<space>et<space>c'est<space>un<space>coup<space>front<space>dans<space>le<space>camp<space>de<space>l'équipe<space>de<space>france<space>comme<space>au<space>en<space>début<space>de<space>rencontre<space>là<space>les<space>français

2026-01-28 16:23:37,565 | INFO | speech length: 446560
2026-01-28 16:23:37,606 | INFO | decoder input length: 697
2026-01-28 16:23:37,606 | INFO | max output length: 697
2026-01-28 16:23:37,606 | INFO | min output length: 69
2026-01-28 16:24:39,690 | INFO | end detected at 527
2026-01-28 16:24:39,691 | INFO | -436.69 * 0.5 = -218.35 for decoder
2026-01-28 16:24:39,691 | INFO | -134.78 * 0.5 = -67.39 for ctc
2026-01-28 16:24:39,691 | INFO | total log probability: -285.73
2026-01-28 16:24:39,691 | INFO | normalized log probability: -0.55
2026-01-28 16:24:39,691 | INFO | total number of ended hypotheses: 196
2026-01-28 16:24:39,698 | INFO | best hypo: qui<space>son<space>euh<space>dépassés<space>par<space>les<space>événements<space>ils<space>se<space>sont<space>bien<space>repris<space>ensuite<space>mais<space>là<space>ils<space>lâgent<space>de<space>nouveau<space>prise<space>alors<space>à<space>nous<space>sommes<space>àtrois<space>minutes<space>de<space>la<space>i<space>temps<space>et<space>oui<space>alor<space>que<space>nous<space>valons<space>maintenant<space>et<space>dans<space>les<space>pieds<space>là<space>bas<space>à<space>droite<space>de<space>je<space>pense<space>que<space>c'est<space>macher<space>anon<space>hum<space>euh<space>peut<space>peut<space>être<space>euh<space>oui<space>c'était<space>lui<space>et<space>le<space>ballon<space>est<space>sorti<space>pour<space>l'équipe<space>de<space>france<space>là<space>non<space>pour<space>les<space>argentins<space>sortis<space>en<space>touche<space>pour<space>les<space>argentins<space>avec<space>euh<space>an<space>éthe<space>qui<space>va<space>faire<space>cette<space>touche<space>là<space>bas<space>euh<space>les<space>joueur<space>qui<space>joun<space>depuis<space>des<space>années<space>à<space>à<space>linter

2026-01-28 16:24:39,701 | INFO | speech length: 139200
2026-01-28 16:24:39,738 | INFO | decoder input length: 217
2026-01-28 16:24:39,739 | INFO | max output length: 217
2026-01-28 16:24:39,739 | INFO | min output length: 21
2026-01-28 16:24:53,130 | INFO | end detected at 166
2026-01-28 16:24:53,132 | INFO | -23.28 * 0.5 = -11.64 for decoder
2026-01-28 16:24:53,132 | INFO | -22.67 * 0.5 = -11.33 for ctc
2026-01-28 16:24:53,132 | INFO | total log probability: -22.97
2026-01-28 16:24:53,132 | INFO | normalized log probability: -0.14
2026-01-28 16:24:53,132 | INFO | total number of ended hypotheses: 180
2026-01-28 16:24:53,134 | INFO | best hypo: alors<space>que<space>le<space>ballon<space>est<space>revenu<space>maintenant<space>dans<space>le<space>camp<space>où<space>à<space>nouveau<space>dans<space>les<space>pays<space>de<space>zanet<space>international<space>à<space>tout<space>près<space>de<space>la<space>ligne<space>de<space>touche<space>et<space>il<space>donne<space>à<space>gaga

2026-01-28 16:24:53,137 | INFO | speech length: 621280
2026-01-28 16:24:53,175 | INFO | decoder input length: 970
2026-01-28 16:24:53,175 | INFO | max output length: 970
2026-01-28 16:24:53,175 | INFO | min output length: 97
2026-01-28 16:27:00,409 | INFO | end detected at 798
2026-01-28 16:27:00,410 | INFO | -1204.89 * 0.5 = -602.44 for decoder
2026-01-28 16:27:00,411 | INFO | -325.88 * 0.5 = -162.94 for ctc
2026-01-28 16:27:00,411 | INFO | total log probability: -765.38
2026-01-28 16:27:00,411 | INFO | normalized log probability: -0.97
2026-01-28 16:27:00,411 | INFO | total number of ended hypotheses: 165
2026-01-28 16:27:00,422 | INFO | best hypo: à<space>gago<space>effectivement<space>et<space>qui<space>on<space>sait<space>qu'on<space>est<space>est<space>contré<space>à<space>insoan<space>ce<space>qui<space>change<space>de<space>côté<space>il<space>on<space>n'a<space>pas<space>pu<space>passer<space>à<space>droite<space>on<space>va<space>ici<space>à<space>gauche<space>avec<space>papa<space>maintenant<space>très<space>offensif<space>et<space>pas<space>mal<space>du<space>tout<space>ce<space>petit<space>il<space>fait<space>penser<space>à<space>bi<space>chentel<space>a<space>euh<space>il<space>est<space>petit<space>et<space>très<space>vif<space>très<space>offensif<space>et<space>il<space>est<space>court<space>dans<space>son<space>couloir<space>euh<space>le<space>dénommé<space>euh<space>papa<space>le<space>ballon<space>récupéré<space>par<space>gagne<space>de<space>maintenant<space>les<space>français<space>qui<space>sont<space>obligas<space>de<space>courir<space>après<space>ce<space>ballon<space>etreuncore<space>papa<space>justement<space>qui<space>franchait<space>les<space>imaiais<space>qui<space>ccélet<space>mainteant<space>qu<space>va<space>troura<space>coureuron<space>coron<space>tretrait<space>poureuh<space>gagon<space>gaco<space>pour<space>euh<space>eun<space>de<space>nouveaux<space>pa<space>p<space>pa<space>palalon<space>contré<space>mais<space>ça<space>continun<space>de<space>circleroui<space>ma<space>ci<space>mantenan<space>trale<space>là<space>là<space>là<space>là<space>masesil<space>techiquement<space>c'est<space>fantastiqu<space>heun<space>il<space>vient<space>de<space>s'échapper<space>de<space>sortir<space>d'un<space>page<space>tendu<space>par<space>tra<space>joueur<space>français

2026-01-28 16:27:00,425 | INFO | speech length: 469120
2026-01-28 16:27:00,470 | INFO | decoder input length: 732
2026-01-28 16:27:00,470 | INFO | max output length: 732
2026-01-28 16:27:00,470 | INFO | min output length: 73
2026-01-28 16:28:11,067 | INFO | end detected at 567
2026-01-28 16:28:11,068 | INFO | -476.07 * 0.5 = -238.03 for decoder
2026-01-28 16:28:11,068 | INFO | -266.99 * 0.5 = -133.49 for ctc
2026-01-28 16:28:11,068 | INFO | total log probability: -371.53
2026-01-28 16:28:11,069 | INFO | normalized log probability: -0.66
2026-01-28 16:28:11,069 | INFO | total number of ended hypotheses: 145
2026-01-28 16:28:11,075 | INFO | best hypo: il<space>peut<space>donner<space>derrière<space>où<space>euh<space>aïnze<space>a<space>récupéré<space>un<space>chevallon<space>et<space>l'écarte<space>là<space>bas<space>côté<space>droits<space>anetifs<space>pour<space>janette<space>alors<space>que<space>euh<space>henzey<space>c'est<space>quand<space>même<space>assez<space>extraordinaire<space>c'est<space>le<space>patron<space>de<space>la<space>défense<space>et<space>et<space>et<space>je<space>viens<space>cher<space>alors<space>il<space>y<space>a<space>y<space>a<space>une<space>mauvaise<space>séquence<space>hein<space>pour<space>l'équipe<space>de<space>france<space>actuellement<space>qui<space>se<space>fait<space>la<space>vraie<space>malade<space>ah<space>hors<space>perque<space>de<space>nouveau<space>du<space>tierres<space>le<space>tbuteur<space>justement<space>à<space>ala<space>qureteminte<space>il<space>rendrait<space>sa<space>pace<space>de<space>répartion<space>pour<space>leun<space>me<space>icimais<space>ces<space>qun<space>tes<space>tes<space>es<space>tes<space>de<space>réparains<space>t<space>eun<space>francain<space>deun<space>français<space>qui<space>tentet<space>e<space>eus<space>es<space>es<space>écarts

2026-01-28 16:28:11,088 | INFO | Chunk: 0 | WER=40.277778 | S=20 D=5 I=4
2026-01-28 16:28:11,095 | INFO | Chunk: 1 | WER=36.893204 | S=25 D=4 I=9
2026-01-28 16:28:11,101 | INFO | Chunk: 2 | WER=41.584158 | S=31 D=6 I=5
2026-01-28 16:28:11,102 | INFO | Chunk: 3 | WER=23.076923 | S=3 D=1 I=2
2026-01-28 16:28:11,107 | INFO | Chunk: 4 | WER=41.304348 | S=22 D=7 I=9
2026-01-28 16:28:11,110 | INFO | Chunk: 5 | WER=47.540984 | S=21 D=0 I=8
2026-01-28 16:28:11,112 | INFO | Chunk: 6 | WER=43.750000 | S=18 D=6 I=4
2026-01-28 16:28:11,120 | INFO | Chunk: 7 | WER=38.317757 | S=25 D=4 I=12
2026-01-28 16:28:11,120 | INFO | Chunk: 8 | WER=10.000000 | S=0 D=0 I=1
2026-01-28 16:28:11,129 | INFO | Chunk: 9 | WER=46.923077 | S=47 D=7 I=7
2026-01-28 16:28:11,132 | INFO | Chunk: 10 | WER=21.818182 | S=8 D=1 I=3
2026-01-28 16:28:11,135 | INFO | Chunk: 11 | WER=16.883117 | S=8 D=3 I=2
2026-01-28 16:28:11,142 | INFO | Chunk: 12 | WER=25.742574 | S=17 D=2 I=7
2026-01-28 16:28:11,143 | INFO | Chunk: 13 | WER=22.857143 | S=5 D=3 I=0
2026-01-28 16:28:11,154 | INFO | Chunk: 14 | WER=45.000000 | S=40 D=5 I=18
2026-01-28 16:28:11,161 | INFO | Chunk: 15 | WER=51.428571 | S=42 D=4 I=8
2026-01-28 16:28:12,116 | INFO | File: Rhap-D2003.wav | WER=38.076622 | S=334 D=56 I=97
2026-01-28 16:28:12,116 | INFO | ------------------------------
2026-01-28 16:28:12,116 | INFO | Conf ester Done!
2026-01-28 16:33:57,925 | INFO | Chunk: 0 | WER=41.666667 | S=12 D=14 I=4
2026-01-28 16:33:57,934 | INFO | Chunk: 1 | WER=29.126214 | S=20 D=7 I=3
2026-01-28 16:33:57,942 | INFO | Chunk: 2 | WER=42.574257 | S=22 D=14 I=7
2026-01-28 16:33:57,943 | INFO | Chunk: 3 | WER=30.769231 | S=4 D=2 I=2
2026-01-28 16:33:57,950 | INFO | Chunk: 4 | WER=41.304348 | S=24 D=9 I=5
2026-01-28 16:33:57,953 | INFO | Chunk: 5 | WER=42.622951 | S=19 D=2 I=5
2026-01-28 16:33:57,957 | INFO | Chunk: 6 | WER=45.312500 | S=21 D=7 I=1
2026-01-28 16:33:57,967 | INFO | Chunk: 7 | WER=32.710280 | S=20 D=4 I=11
2026-01-28 16:33:57,967 | INFO | Chunk: 8 | WER=10.000000 | S=0 D=0 I=1
2026-01-28 16:33:57,980 | INFO | Chunk: 9 | WER=36.153846 | S=28 D=13 I=6
2026-01-28 16:33:57,983 | INFO | Chunk: 10 | WER=23.636364 | S=9 D=4 I=0
2026-01-28 16:33:57,988 | INFO | Chunk: 11 | WER=18.181818 | S=6 D=5 I=3
2026-01-28 16:33:57,996 | INFO | Chunk: 12 | WER=22.772277 | S=16 D=2 I=5
2026-01-28 16:33:57,998 | INFO | Chunk: 13 | WER=37.142857 | S=10 D=3 I=0
2026-01-28 16:33:58,013 | INFO | Chunk: 14 | WER=31.428571 | S=32 D=4 I=8
2026-01-28 16:33:58,022 | INFO | Chunk: 15 | WER=29.523810 | S=22 D=2 I=7
2026-01-28 16:33:59,253 | INFO | File: Rhap-D2003.wav | WER=33.229085 | S=265 D=92 I=68
2026-01-28 16:33:59,253 | INFO | ------------------------------
2026-01-28 16:33:59,253 | INFO | hmm_tdnn Done!
2026-01-28 16:33:59,433 | INFO | ==================================Rhap-D2004.wav=========================================
2026-01-28 16:33:59,667 | INFO | Using rVAD model
2026-01-28 16:34:21,155 | INFO | Chunk: 0 | WER=35.714286 | S=3 D=2 I=0
2026-01-28 16:34:21,156 | INFO | Chunk: 1 | WER=50.000000 | S=0 D=5 I=0
2026-01-28 16:34:21,160 | INFO | Chunk: 2 | WER=32.500000 | S=7 D=18 I=1
2026-01-28 16:34:21,166 | INFO | Chunk: 3 | WER=21.590909 | S=9 D=10 I=0
2026-01-28 16:34:21,170 | INFO | Chunk: 4 | WER=28.169014 | S=11 D=8 I=1
2026-01-28 16:34:21,171 | INFO | Chunk: 5 | WER=0.000000 | S=1 D=0 I=10
2026-01-28 16:34:21,174 | INFO | Chunk: 6 | WER=35.937500 | S=12 D=11 I=0
2026-01-28 16:34:21,181 | INFO | Chunk: 7 | WER=14.583333 | S=8 D=6 I=0
2026-01-28 16:34:21,182 | INFO | Chunk: 8 | WER=20.689655 | S=1 D=5 I=0
2026-01-28 16:34:21,183 | INFO | Chunk: 9 | WER=15.789474 | S=1 D=2 I=0
2026-01-28 16:34:21,186 | INFO | Chunk: 10 | WER=33.333333 | S=7 D=13 I=0
2026-01-28 16:34:21,193 | INFO | Chunk: 11 | WER=31.372549 | S=11 D=20 I=1
2026-01-28 16:34:21,193 | INFO | Chunk: 12 | WER=33.333333 | S=3 D=0 I=0
2026-01-28 16:34:21,194 | INFO | Chunk: 13 | WER=30.434783 | S=2 D=5 I=0
2026-01-28 16:34:21,201 | INFO | Chunk: 14 | WER=18.947368 | S=9 D=9 I=0
2026-01-28 16:34:21,720 | INFO | File: Rhap-D2004.wav | WER=27.631579 | S=86 D=112 I=12
2026-01-28 16:34:21,720 | INFO | ------------------------------
2026-01-28 16:34:21,721 | INFO | w2vec vad chunk Done!
2026-01-28 16:34:42,464 | INFO | Chunk: 0 | WER=21.428571 | S=3 D=0 I=0
2026-01-28 16:34:42,464 | INFO | Chunk: 1 | WER=20.000000 | S=0 D=1 I=1
2026-01-28 16:34:42,467 | INFO | Chunk: 2 | WER=55.000000 | S=14 D=30 I=0
2026-01-28 16:34:42,469 | INFO | Chunk: 3 | WER=78.409091 | S=1 D=68 I=0
2026-01-28 16:34:42,471 | INFO | Chunk: 4 | WER=57.746479 | S=9 D=32 I=0
2026-01-28 16:34:42,471 | INFO | Chunk: 5 | WER=0.000000 | S=1 D=0 I=2
2026-01-28 16:34:42,472 | INFO | Chunk: 6 | WER=68.750000 | S=0 D=44 I=0
2026-01-28 16:34:42,475 | INFO | Chunk: 7 | WER=69.791667 | S=4 D=60 I=3
2026-01-28 16:34:42,476 | INFO | Chunk: 8 | WER=10.344828 | S=0 D=3 I=0
2026-01-28 16:34:42,476 | INFO | Chunk: 9 | WER=15.789474 | S=1 D=2 I=0
2026-01-28 16:34:42,478 | INFO | Chunk: 10 | WER=53.333333 | S=3 D=29 I=0
2026-01-28 16:34:42,479 | INFO | Chunk: 11 | WER=91.176471 | S=0 D=93 I=0
2026-01-28 16:34:42,479 | INFO | Chunk: 12 | WER=22.222222 | S=2 D=0 I=0
2026-01-28 16:34:42,480 | INFO | Chunk: 13 | WER=17.391304 | S=2 D=2 I=0
2026-01-28 16:34:42,482 | INFO | Chunk: 14 | WER=72.631579 | S=6 D=63 I=0
2026-01-28 16:34:42,622 | INFO | File: Rhap-D2004.wav | WER=62.763158 | S=47 D=425 I=5
2026-01-28 16:34:42,622 | INFO | ------------------------------
2026-01-28 16:34:42,622 | INFO | whisper med Done!
2026-01-28 16:35:14,103 | INFO | Chunk: 0 | WER=35.714286 | S=4 D=1 I=0
2026-01-28 16:35:14,103 | INFO | Chunk: 1 | WER=40.000000 | S=2 D=1 I=1
2026-01-28 16:35:14,105 | INFO | Chunk: 2 | WER=72.500000 | S=2 D=56 I=0
2026-01-28 16:35:14,108 | INFO | Chunk: 3 | WER=51.136364 | S=15 D=30 I=0
2026-01-28 16:35:14,111 | INFO | Chunk: 4 | WER=45.070423 | S=17 D=15 I=0
2026-01-28 16:35:14,111 | INFO | Chunk: 5 | WER=0.000000 | S=1 D=0 I=8
2026-01-28 16:35:14,113 | INFO | Chunk: 6 | WER=70.312500 | S=1 D=44 I=0
2026-01-28 16:35:14,116 | INFO | Chunk: 7 | WER=58.333333 | S=4 D=52 I=0
2026-01-28 16:35:14,116 | INFO | Chunk: 8 | WER=10.344828 | S=0 D=3 I=0
2026-01-28 16:35:14,117 | INFO | Chunk: 9 | WER=26.315789 | S=2 D=3 I=0
2026-01-28 16:35:14,118 | INFO | Chunk: 10 | WER=53.333333 | S=2 D=29 I=1
2026-01-28 16:35:14,120 | INFO | Chunk: 11 | WER=95.098039 | S=3 D=94 I=0
2026-01-28 16:35:14,120 | INFO | Chunk: 12 | WER=22.222222 | S=1 D=1 I=0
2026-01-28 16:35:14,120 | INFO | Chunk: 13 | WER=30.434783 | S=2 D=5 I=0
2026-01-28 16:35:14,123 | INFO | Chunk: 14 | WER=69.473684 | S=6 D=60 I=0
2026-01-28 16:35:14,279 | INFO | File: Rhap-D2004.wav | WER=60.789474 | S=73 D=386 I=3
2026-01-28 16:35:14,279 | INFO | ------------------------------
2026-01-28 16:35:14,280 | INFO | whisper large Done!
2026-01-28 16:35:14,455 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 16:35:14,493 | INFO | Vocabulary size: 350
2026-01-28 16:35:15,445 | INFO | Gradient checkpoint layers: []
2026-01-28 16:35:16,216 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 16:35:16,222 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 16:35:16,222 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 16:35:16,223 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 16:35:16,223 | INFO | speech length: 52160
2026-01-28 16:35:16,282 | INFO | decoder input length: 81
2026-01-28 16:35:16,282 | INFO | max output length: 81
2026-01-28 16:35:16,282 | INFO | min output length: 8
2026-01-28 16:35:18,112 | INFO | end detected at 41
2026-01-28 16:35:18,113 | INFO |  -6.24 * 0.5 =  -3.12 for decoder
2026-01-28 16:35:18,114 | INFO |  -8.83 * 0.5 =  -4.41 for ctc
2026-01-28 16:35:18,114 | INFO | total log probability: -7.53
2026-01-28 16:35:18,114 | INFO | normalized log probability: -0.21
2026-01-28 16:35:18,114 | INFO | total number of ended hypotheses: 160
2026-01-28 16:35:18,115 | INFO | best hypo: ▁et▁vous▁appartenez▁à▁ce▁qu'on▁appelle▁les▁flying▁docters▁de▁l'amrai

2026-01-28 16:35:18,119 | INFO | speech length: 42080
2026-01-28 16:35:18,229 | INFO | decoder input length: 65
2026-01-28 16:35:18,245 | INFO | max output length: 65
2026-01-28 16:35:18,245 | INFO | min output length: 6
2026-01-28 16:35:19,741 | INFO | end detected at 28
2026-01-28 16:35:19,743 | INFO | -11.92 * 0.5 =  -5.96 for decoder
2026-01-28 16:35:19,743 | INFO |  -4.20 * 0.5 =  -2.10 for ctc
2026-01-28 16:35:19,743 | INFO | total log probability: -8.06
2026-01-28 16:35:19,743 | INFO | normalized log probability: -0.40
2026-01-28 16:35:19,744 | INFO | total number of ended hypotheses: 213
2026-01-28 16:35:19,744 | INFO | best hypo: ▁asociation▁des▁médecins▁et▁de▁de▁recherche

2026-01-28 16:35:19,746 | INFO | speech length: 453920
2026-01-28 16:35:19,811 | INFO | decoder input length: 708
2026-01-28 16:35:19,812 | INFO | max output length: 708
2026-01-28 16:35:19,812 | INFO | min output length: 70
2026-01-28 16:35:42,429 | INFO | end detected at 183
2026-01-28 16:35:42,431 | INFO | -433.66 * 0.5 = -216.83 for decoder
2026-01-28 16:35:42,431 | INFO | -161.77 * 0.5 = -80.88 for ctc
2026-01-28 16:35:42,431 | INFO | total log probability: -297.71
2026-01-28 16:35:42,432 | INFO | normalized log probability: -1.68
2026-01-28 16:35:42,432 | INFO | total number of ended hypotheses: 185
2026-01-28 16:35:42,434 | INFO | best hypo: ▁nairobi▁au▁kenya▁eh▁oui▁j'ai▁commencé▁par▁à▁travailler▁comme▁médecin▁de▁campagne▁à▁un▁endroit▁qui▁s'appelait▁olcalo▁juste▁au▁sud▁de▁l'équateur▁et▁non▁j'y▁suis▁restait▁pendant▁quatorze▁ans▁en▁sillonnant▁le▁à▁la▁région▁avec▁mais▁peu▁giots▁subces▁suivent▁qui▁sont▁vraiment▁les▁meilleure▁voiatures▁sur▁le▁kenya▁à▁s'ou▁dire▁je▁c'en▁vient▁de▁s'en▁suis▁à▁la▁vingt▁deuxième▁puis▁suis▁là

2026-01-28 16:35:42,437 | INFO | speech length: 461760
2026-01-28 16:35:42,482 | INFO | decoder input length: 721
2026-01-28 16:35:42,482 | INFO | max output length: 721
2026-01-28 16:35:42,482 | INFO | min output length: 72
2026-01-28 16:36:08,917 | INFO | end detected at 206
2026-01-28 16:36:08,920 | INFO | -533.62 * 0.5 = -266.81 for decoder
2026-01-28 16:36:08,920 | INFO | -154.14 * 0.5 = -77.07 for ctc
2026-01-28 16:36:08,920 | INFO | total log probability: -343.88
2026-01-28 16:36:08,920 | INFO | normalized log probability: -1.71
2026-01-28 16:36:08,920 | INFO | total number of ended hypotheses: 161
2026-01-28 16:36:08,923 | INFO | best hypo: ▁et▁j'avais▁une▁circonscription▁à▁un▁rayon▁d'action▁d'à▁peu▁près▁cent▁kilomètres▁tout▁autour▁de▁cet▁endroit▁je▁suis▁arrivé▁au▁kenya▁je▁voulais▁parler▁d'abord▁pour▁le▁gouvernement▁mais▁les▁postes▁qui▁m'aurt▁peu▁proposés▁ne▁me▁plaisaient▁pas▁beaucoupr▁excepter▁mars'abit▁qui▁était▁une▁espèces▁de▁l'▁dans▁le▁désert▁du▁tout▁au▁nord▁du▁kenya▁et▁le▁désert▁du▁kennya▁'aurais▁beaucoupt▁aimé▁aller▁mais▁le▁commsaire▁du▁district▁le▁préfet▁du▁lieu

2026-01-28 16:36:08,926 | INFO | speech length: 302880
2026-01-28 16:36:08,983 | INFO | decoder input length: 472
2026-01-28 16:36:08,983 | INFO | max output length: 472
2026-01-28 16:36:08,983 | INFO | min output length: 47
2026-01-28 16:36:26,768 | INFO | end detected at 170
2026-01-28 16:36:26,771 | INFO | -315.45 * 0.5 = -157.72 for decoder
2026-01-28 16:36:26,771 | INFO | -104.71 * 0.5 = -52.35 for ctc
2026-01-28 16:36:26,771 | INFO | total log probability: -210.08
2026-01-28 16:36:26,771 | INFO | normalized log probability: -1.28
2026-01-28 16:36:26,771 | INFO | total number of ended hypotheses: 168
2026-01-28 16:36:26,774 | INFO | best hypo: ▁ne▁voulez▁pas▁de▁femme▁non▁mariée▁parce▁qu'elle▁le▁dio▁elle▁corromprend▁tous▁mes▁petits▁officiers▁de▁district▁ah▁je▁se▁sommes▁connaître▁elle▁n'y▁rien▁du▁tout▁de▁sorte▁que▁je▁n'ai▁pas▁pu▁aller▁là▁comme▁j'ai▁décidai▁de▁m'installer▁à▁mon▁compte▁j'ai▁trouvé▁cetendroit▁où▁haloù▁j'vais▁besoin▁d'un▁médecin▁fut▁rester▁là▁quatorze▁ans

2026-01-28 16:36:26,777 | INFO | speech length: 228480
2026-01-28 16:36:26,834 | INFO | decoder input length: 356
2026-01-28 16:36:26,834 | INFO | max output length: 356
2026-01-28 16:36:26,834 | INFO | min output length: 35
2026-01-28 16:36:30,888 | INFO | end detected at 42
2026-01-28 16:36:30,890 | INFO | -40.08 * 0.5 = -20.04 for decoder
2026-01-28 16:36:30,890 | INFO | -109.86 * 0.5 = -54.93 for ctc
2026-01-28 16:36:30,890 | INFO | total log probability: -74.97
2026-01-28 16:36:30,891 | INFO | normalized log probability: -2.03
2026-01-28 16:36:30,891 | INFO | total number of ended hypotheses: 160
2026-01-28 16:36:30,891 | INFO | best hypo: ▁il▁a▁eu▁une▁femme▁rien▁à▁l'instantana▁l'école▁et▁à▁parallèles

2026-01-28 16:36:30,894 | INFO | speech length: 295200
2026-01-28 16:36:30,942 | INFO | decoder input length: 460
2026-01-28 16:36:30,942 | INFO | max output length: 460
2026-01-28 16:36:30,942 | INFO | min output length: 46
2026-01-28 16:36:44,048 | INFO | end detected at 144
2026-01-28 16:36:44,050 | INFO | -299.38 * 0.5 = -149.69 for decoder
2026-01-28 16:36:44,050 | INFO | -135.88 * 0.5 = -67.94 for ctc
2026-01-28 16:36:44,050 | INFO | total log probability: -217.63
2026-01-28 16:36:44,050 | INFO | normalized log probability: -1.61
2026-01-28 16:36:44,050 | INFO | total number of ended hypotheses: 195
2026-01-28 16:36:44,052 | INFO | best hypo: ▁j'ai▁affaire▁un▁petit▁retournant▁en▁arrière▁mais▁puisque▁finalement▁ça▁fille▁quarante▁huit▁ans▁que▁vous▁êtes▁au▁kenya▁quarante▁huit▁ans▁ouiches▁arrivé▁pour▁la▁première▁fois▁quarante▁neuf▁jus▁de▁par▁ans▁j'aiais▁passsé▁deux▁semaines▁et▁je▁trouveille▁que▁cetta▁payés▁formidable▁je▁suis▁venu▁en▁insttallers▁cinq▁ans

2026-01-28 16:36:44,054 | INFO | speech length: 410560
2026-01-28 16:36:44,097 | INFO | decoder input length: 641
2026-01-28 16:36:44,097 | INFO | max output length: 641
2026-01-28 16:36:44,098 | INFO | min output length: 64
2026-01-28 16:37:09,325 | INFO | end detected at 224
2026-01-28 16:37:09,327 | INFO | -618.74 * 0.5 = -309.37 for decoder
2026-01-28 16:37:09,327 | INFO | -210.05 * 0.5 = -105.03 for ctc
2026-01-28 16:37:09,328 | INFO | total log probability: -414.40
2026-01-28 16:37:09,328 | INFO | normalized log probability: -1.89
2026-01-28 16:37:09,328 | INFO | total number of ended hypotheses: 184
2026-01-28 16:37:09,331 | INFO | best hypo: ▁en▁fait▁vous▁êtes▁liés▁à▁la▁france▁puisque▁vous▁êtes▁française▁vous▁êtes▁dans▁l'est▁de▁la▁france▁vous▁êtes▁nés▁à▁quel▁endroits▁je▁suis▁né▁à▁cannes▁pendant▁la▁guerre▁à▁la▁fin▁de▁la▁guerre▁du▁vic▁mon▁père▁est▁au▁fond▁ou▁nous▁avons▁été▁charsché▁d'as▁d'alphace▁ou▁naturellement▁plus▁ête▁né▁à▁cannes▁et▁puis▁finalement▁que▁vous▁avez▁vous▁être▁partie▁verd▁mulhouse▁on▁est▁repartie▁vers▁à▁mulhous▁mes▁les▁souvenirs▁les▁plus▁ancienson▁je▁crois▁des▁souvenirs▁de▁mulhouse'asement▁pas▁de▁cannes

2026-01-28 16:37:09,333 | INFO | speech length: 120640
2026-01-28 16:37:09,371 | INFO | decoder input length: 188
2026-01-28 16:37:09,371 | INFO | max output length: 188
2026-01-28 16:37:09,371 | INFO | min output length: 18
2026-01-28 16:37:13,092 | INFO | end detected at 66
2026-01-28 16:37:13,094 | INFO |  -5.22 * 0.5 =  -2.61 for decoder
2026-01-28 16:37:13,094 | INFO |  -6.93 * 0.5 =  -3.47 for ctc
2026-01-28 16:37:13,094 | INFO | total log probability: -6.08
2026-01-28 16:37:13,094 | INFO | normalized log probability: -0.10
2026-01-28 16:37:13,094 | INFO | total number of ended hypotheses: 177
2026-01-28 16:37:13,095 | INFO | best hypo: ▁et▁puis▁j'ai▁fait▁mes▁études▁au▁lycée▁de▁mulhouse▁et▁j'ai▁été▁deux▁ans▁en▁angleterre▁en▁trente▁trente▁et▁un▁trente▁deux

2026-01-28 16:37:13,097 | INFO | speech length: 64480
2026-01-28 16:37:13,140 | INFO | decoder input length: 100
2026-01-28 16:37:13,141 | INFO | max output length: 100
2026-01-28 16:37:13,141 | INFO | min output length: 10
2026-01-28 16:37:15,590 | INFO | end detected at 52
2026-01-28 16:37:15,592 | INFO | -11.95 * 0.5 =  -5.98 for decoder
2026-01-28 16:37:15,592 | INFO |  -4.34 * 0.5 =  -2.17 for ctc
2026-01-28 16:37:15,592 | INFO | total log probability: -8.15
2026-01-28 16:37:15,592 | INFO | normalized log probability: -0.17
2026-01-28 16:37:15,592 | INFO | total number of ended hypotheses: 169
2026-01-28 16:37:15,593 | INFO | best hypo: ▁c'est▁là▁que▁j'ai▁j'ai▁bien▁appris▁l'anglais▁ce▁qui▁m'a▁beaucoup▁aidé▁naturellement

2026-01-28 16:37:15,595 | INFO | speech length: 261120
2026-01-28 16:37:15,643 | INFO | decoder input length: 407
2026-01-28 16:37:15,643 | INFO | max output length: 407
2026-01-28 16:37:15,643 | INFO | min output length: 40
2026-01-28 16:37:26,679 | INFO | end detected at 131
2026-01-28 16:37:26,681 | INFO | -202.31 * 0.5 = -101.15 for decoder
2026-01-28 16:37:26,681 | INFO | -73.01 * 0.5 = -36.50 for ctc
2026-01-28 16:37:26,681 | INFO | total log probability: -137.66
2026-01-28 16:37:26,681 | INFO | normalized log probability: -1.12
2026-01-28 16:37:26,681 | INFO | total number of ended hypotheses: 138
2026-01-28 16:37:26,683 | INFO | best hypo: ▁et▁puis▁j'ai▁fini▁mes▁études▁à▁mulhouse▁j'ai▁j'ai▁fait▁mon▁pcb▁à▁paris▁je▁n'ai▁retourné▁à▁à▁strasbourg▁mais▁j'ai▁lâché▁sage▁je▁suis▁resté▁à▁paris▁sois▁qu'on▁as▁goûté▁paris▁aurait▁pu▁aller▁à▁la▁province▁des▁études▁mais▁pour▁les▁études

2026-01-28 16:37:26,685 | INFO | speech length: 468320
2026-01-28 16:37:26,731 | INFO | decoder input length: 731
2026-01-28 16:37:26,731 | INFO | max output length: 731
2026-01-28 16:37:26,731 | INFO | min output length: 73
2026-01-28 16:37:56,342 | INFO | end detected at 246
2026-01-28 16:37:56,344 | INFO | -621.90 * 0.5 = -310.95 for decoder
2026-01-28 16:37:56,344 | INFO | -168.02 * 0.5 = -84.01 for ctc
2026-01-28 16:37:56,344 | INFO | total log probability: -394.96
2026-01-28 16:37:56,344 | INFO | normalized log probability: -1.66
2026-01-28 16:37:56,344 | INFO | total number of ended hypotheses: 182
2026-01-28 16:37:56,347 | INFO | best hypo: ▁elle▁aura▁là▁vous▁visé▁des▁études▁de▁médecine▁déjà▁j'ai▁fait▁mes▁études▁de▁médecine▁à▁paris▁et▁après▁la▁guerre▁je▁suis▁venu▁passionner▁toujours▁depuis▁alors▁depuis▁avant▁la▁guerre▁et▁j'avais▁rencontrée▁à▁mon▁fred▁herêtement▁fred▁qui▁venait▁faire▁des▁conférces▁à▁mulhouse▁et▁est▁ce▁un▁type▁le▁passionnant▁et▁j'ai▁toujours▁là▁tous▁livres▁et▁j'avais▁absrolument▁enviet▁d'aller▁à▁la▁cordre▁de▁l'afrique▁la▁'hiopie▁la▁somalie▁là▁à▁l'arabie▁tous▁ça▁'était▁là▁aller

2026-01-28 16:37:56,350 | INFO | speech length: 56320
2026-01-28 16:37:56,387 | INFO | decoder input length: 87
2026-01-28 16:37:56,387 | INFO | max output length: 87
2026-01-28 16:37:56,388 | INFO | min output length: 8
2026-01-28 16:37:57,553 | INFO | end detected at 24
2026-01-28 16:37:57,554 | INFO |  -5.28 * 0.5 =  -2.64 for decoder
2026-01-28 16:37:57,554 | INFO |  -3.86 * 0.5 =  -1.93 for ctc
2026-01-28 16:37:57,554 | INFO | total log probability: -4.57
2026-01-28 16:37:57,554 | INFO | normalized log probability: -0.24
2026-01-28 16:37:57,554 | INFO | total number of ended hypotheses: 160
2026-01-28 16:37:57,555 | INFO | best hypo: ▁et▁à▁cavalet▁pardigon▁nous▁allions▁en▁été

2026-01-28 16:37:57,556 | INFO | speech length: 135200
2026-01-28 16:37:57,602 | INFO | decoder input length: 210
2026-01-28 16:37:57,602 | INFO | max output length: 210
2026-01-28 16:37:57,602 | INFO | min output length: 21
2026-01-28 16:38:00,823 | INFO | end detected at 53
2026-01-28 16:38:00,825 | INFO |  -9.21 * 0.5 =  -4.60 for decoder
2026-01-28 16:38:00,825 | INFO | -14.91 * 0.5 =  -7.46 for ctc
2026-01-28 16:38:00,825 | INFO | total log probability: -12.06
2026-01-28 16:38:00,825 | INFO | normalized log probability: -0.26
2026-01-28 16:38:00,825 | INFO | total number of ended hypotheses: 193
2026-01-28 16:38:00,826 | INFO | best hypo: ▁il▁y▁avait▁une▁famille▁ami▁à▁nous▁les▁bess▁et▁qui▁avait▁un▁empire▁commercial▁je▁veux▁dire▁surtout▁à▁aden

2026-01-28 16:38:00,828 | INFO | speech length: 425440
2026-01-28 16:38:00,872 | INFO | decoder input length: 664
2026-01-28 16:38:00,872 | INFO | max output length: 664
2026-01-28 16:38:00,873 | INFO | min output length: 66
2026-01-28 16:38:24,240 | INFO | end detected at 202
2026-01-28 16:38:24,242 | INFO | -479.65 * 0.5 = -239.83 for decoder
2026-01-28 16:38:24,242 | INFO | -188.58 * 0.5 = -94.29 for ctc
2026-01-28 16:38:24,242 | INFO | total log probability: -334.12
2026-01-28 16:38:24,242 | INFO | normalized log probability: -1.70
2026-01-28 16:38:24,242 | INFO | total number of ended hypotheses: 162
2026-01-28 16:38:24,245 | INFO | best hypo: ▁et▁après▁la▁guerre▁il▁nous▁m'a▁invités▁à▁venir▁à▁aden▁et▁je▁reste▁là▁presque▁un▁anj'ai▁travaillé▁à▁l'hôpital▁d'adennes▁en▁m'a▁demandé▁le▁fait▁mais▁vous▁étiez▁auprès▁de▁femmes▁ou▁naturellement▁parce▁que▁la▁seule▁la▁seule▁la▁médec▁sa▁femme▁elle▁avait▁été▁oée▁malade▁et▁elle▁rentrée▁en▁angleterre▁alors▁comme▁ça▁j'io▁travaillée▁pendant▁jus▁qu'au▁moment▁où▁elle▁avait▁remplanante▁qui▁avait▁venue▁d'angleterre▁et▁moi▁je▁cherais▁en▁ethiopie

2026-01-28 16:38:24,252 | INFO | Chunk: 0 | WER=21.428571 | S=3 D=0 I=0
2026-01-28 16:38:24,252 | INFO | Chunk: 1 | WER=40.000000 | S=1 D=3 I=0
2026-01-28 16:38:24,256 | INFO | Chunk: 2 | WER=38.750000 | S=22 D=6 I=3
2026-01-28 16:38:24,261 | INFO | Chunk: 3 | WER=27.272727 | S=15 D=7 I=2
2026-01-28 16:38:24,264 | INFO | Chunk: 4 | WER=40.845070 | S=18 D=7 I=4
2026-01-28 16:38:24,264 | INFO | Chunk: 5 | WER=0.000000 | S=1 D=0 I=13
2026-01-28 16:38:24,267 | INFO | Chunk: 6 | WER=53.125000 | S=23 D=10 I=1
2026-01-28 16:38:24,272 | INFO | Chunk: 7 | WER=32.291667 | S=23 D=4 I=4
2026-01-28 16:38:24,273 | INFO | Chunk: 8 | WER=10.344828 | S=0 D=3 I=0
2026-01-28 16:38:24,274 | INFO | Chunk: 9 | WER=5.263158 | S=1 D=0 I=0
2026-01-28 16:38:24,276 | INFO | Chunk: 10 | WER=31.666667 | S=10 D=8 I=1
2026-01-28 16:38:24,281 | INFO | Chunk: 11 | WER=41.176471 | S=20 D=17 I=5
2026-01-28 16:38:24,282 | INFO | Chunk: 12 | WER=22.222222 | S=1 D=1 I=0
2026-01-28 16:38:24,282 | INFO | Chunk: 13 | WER=13.043478 | S=2 D=1 I=0
2026-01-28 16:38:24,287 | INFO | Chunk: 14 | WER=41.052632 | S=21 D=13 I=5
2026-01-28 16:38:24,587 | INFO | File: Rhap-D2004.wav | WER=36.447368 | S=162 D=78 I=37
2026-01-28 16:38:24,587 | INFO | ------------------------------
2026-01-28 16:38:24,588 | INFO | Conf cv Done!
2026-01-28 16:38:24,723 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 16:38:24,747 | INFO | Vocabulary size: 47
2026-01-28 16:38:25,892 | INFO | Gradient checkpoint layers: []
2026-01-28 16:38:26,643 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 16:38:26,648 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 16:38:26,649 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 16:38:26,649 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 16:38:26,652 | INFO | speech length: 52160
2026-01-28 16:38:26,697 | INFO | decoder input length: 81
2026-01-28 16:38:26,697 | INFO | max output length: 81
2026-01-28 16:38:26,697 | INFO | min output length: 8
2026-01-28 16:38:30,083 | INFO | end detected at 77
2026-01-28 16:38:30,085 | INFO |  -9.33 * 0.5 =  -4.67 for decoder
2026-01-28 16:38:30,085 | INFO | -12.46 * 0.5 =  -6.23 for ctc
2026-01-28 16:38:30,085 | INFO | total log probability: -10.90
2026-01-28 16:38:30,085 | INFO | normalized log probability: -0.16
2026-01-28 16:38:30,085 | INFO | total number of ended hypotheses: 198
2026-01-28 16:38:30,086 | INFO | best hypo: et<space>vous<space>appartenez<space>à<space>ce<space>qu'on<space>appelle<space>les<space>flindoks<space>euh<space>s<space>de<space>l'ameray

2026-01-28 16:38:30,089 | INFO | speech length: 42080
2026-01-28 16:38:30,124 | INFO | decoder input length: 65
2026-01-28 16:38:30,124 | INFO | max output length: 65
2026-01-28 16:38:30,124 | INFO | min output length: 6
2026-01-28 16:38:32,284 | INFO | end detected at 59
2026-01-28 16:38:32,286 | INFO |  -4.52 * 0.5 =  -2.26 for decoder
2026-01-28 16:38:32,286 | INFO |  -0.66 * 0.5 =  -0.33 for ctc
2026-01-28 16:38:32,286 | INFO | total log probability: -2.59
2026-01-28 16:38:32,287 | INFO | normalized log probability: -0.05
2026-01-28 16:38:32,287 | INFO | total number of ended hypotheses: 204
2026-01-28 16:38:32,287 | INFO | best hypo: association<space>des<space>médecins<space>et<space>de<space>de<space>de<space>recherche<space>qui

2026-01-28 16:38:32,289 | INFO | speech length: 453920
2026-01-28 16:38:32,335 | INFO | decoder input length: 708
2026-01-28 16:38:32,335 | INFO | max output length: 708
2026-01-28 16:38:32,335 | INFO | min output length: 70
2026-01-28 16:39:23,194 | INFO | end detected at 419
2026-01-28 16:39:23,196 | INFO | -262.70 * 0.5 = -131.35 for decoder
2026-01-28 16:39:23,196 | INFO | -34.00 * 0.5 = -17.00 for ctc
2026-01-28 16:39:23,196 | INFO | total log probability: -148.35
2026-01-28 16:39:23,196 | INFO | normalized log probability: -0.36
2026-01-28 16:39:23,196 | INFO | total number of ended hypotheses: 173
2026-01-28 16:39:23,201 | INFO | best hypo: nairobi<space>au<space>kenya<space>eh<space>oui<space>euh<space>j'ai<space>commencé<space>par<space>euh<space>travailler<space>comme<space>médecin<space>de<space>campagne<space>à<space>un<space>endroit<space>qui<space>s'appelait<space>ol<space>talou<space>juste<space>au<space>sud<space>de<space>l'équateur<space>et<space>j'y<space>suis<space>resté<space>pendant<space>quatorze<space>ans<space>comme<space>euh<space>euh<space>sillonnant<space>le<space>la<space>la<space>région<space>a<space>v<space>avec<space>euh<space>mes<space>peugeots<space>successives<space>qui<space>sont<space>vraiment<space>les<space>meilleures<space>voitures<space>pour<space>le<space>kenya<space>ça<space>je<space>veux<space>dire<space>je<space>crois<space>que<space>jen<space>j'en<space>suis<space>la<space>la<space>vingt<space>vingt<space>deuxième<space>puis<space>jesuis<space>là

2026-01-28 16:39:23,204 | INFO | speech length: 461760
2026-01-28 16:39:23,242 | INFO | decoder input length: 721
2026-01-28 16:39:23,242 | INFO | max output length: 721
2026-01-28 16:39:23,242 | INFO | min output length: 72
2026-01-28 16:40:11,583 | INFO | end detected at 476
2026-01-28 16:40:11,584 | INFO | -302.64 * 0.5 = -151.32 for decoder
2026-01-28 16:40:11,584 | INFO | -35.17 * 0.5 = -17.58 for ctc
2026-01-28 16:40:11,584 | INFO | total log probability: -168.91
2026-01-28 16:40:11,584 | INFO | normalized log probability: -0.36
2026-01-28 16:40:11,584 | INFO | total number of ended hypotheses: 160
2026-01-28 16:40:11,590 | INFO | best hypo: et<space>j'avais<space>une<space>circonscription<space>un<space>un<space>rayon<space>d'action<space>d'à<space>peu<space>près<space>euh<space>cent<space>kilomètres<space>tout<space>autour<space>de<space>cet<space>endroit<space>je<space>suis<space>arrivé<space>euh<space>au<space>kenya<space>je<space>voulais<space>travailler<space>d'abord<space>pour<space>le<space>gouvernement<space>mais<space>les<space>postes<space>qui<space>m'ont<space>f<space>euh<space>proposé<space>ne<space>me<space>plaisaient<space>pas<space>beaucoup<space>exceptait<space>marshabitz<space>qui<space>était<space>une<space>espèce<space>de<space>d'hile<space>euh<space>dans<space>le<space>désert<space>tout<space>au<space>nord<space>du<space>kenya<space>dans<space>des<space>heures<space>du<space>cagnava<space>j'aurais<space>beaucoup<space>aimé<space>aller<space>mais<space>le<space>le<space>die<space>le<space>commissaire<space>de<space>district<space>le<space>préfet<space>du<space>lieu

2026-01-28 16:40:11,592 | INFO | speech length: 302880
2026-01-28 16:40:11,641 | INFO | decoder input length: 472
2026-01-28 16:40:11,641 | INFO | max output length: 472
2026-01-28 16:40:11,641 | INFO | min output length: 47
2026-01-28 16:40:37,792 | INFO | end detected at 349
2026-01-28 16:40:37,794 | INFO | -46.53 * 0.5 = -23.27 for decoder
2026-01-28 16:40:37,794 | INFO | -34.73 * 0.5 = -17.37 for ctc
2026-01-28 16:40:37,794 | INFO | total log probability: -40.63
2026-01-28 16:40:37,794 | INFO | normalized log probability: -0.12
2026-01-28 16:40:37,794 | INFO | total number of ended hypotheses: 188
2026-01-28 16:40:37,798 | INFO | best hypo: ne<space>voulaient<space>pas<space>de<space>de<space>femmes<space>non<space>mariées<space>parce<space>que<space>dieu<space>ou<space>a<space>cor<space>euh<space>corromprend<space>tous<space>mes<space>petits<space>euh<space>officiers<space>districts<space>euh<space>se<space>semble<space>connaître<space>ni<space>rien<space>du<space>tout<space>de<space>sorte<space>que<space>je<space>n'ai<space>pas<space>pu<space>aller<space>là<space>alors<space>j'ai<space>décidé<space>de<space>m'installer<space>à<space>mon<space>compte<space>et<space>j'ai<space>trouvé<space>cet<space>endroit<space>au<space>calou<space>j'avais<space>besoin<space>d'un<space>médecin<space>je<space>suis<space>acheté<space>là<space>quatorze<space>ans

2026-01-28 16:40:37,801 | INFO | speech length: 228480
2026-01-28 16:40:37,863 | INFO | decoder input length: 356
2026-01-28 16:40:37,863 | INFO | max output length: 356
2026-01-28 16:40:37,863 | INFO | min output length: 35
2026-01-28 16:40:52,037 | INFO | end detected at 146
2026-01-28 16:40:52,038 | INFO | -58.92 * 0.5 = -29.46 for decoder
2026-01-28 16:40:52,038 | INFO | -181.55 * 0.5 = -90.77 for ctc
2026-01-28 16:40:52,038 | INFO | total log probability: -120.23
2026-01-28 16:40:52,038 | INFO | normalized log probability: -0.89
2026-01-28 16:40:52,038 | INFO | total number of ended hypotheses: 59
2026-01-28 16:40:52,040 | INFO | best hypo: y<space>a<space>allez<space>on<space>a<space>les<space>les<space>yeux<space>y<space>allez<space>on<space>va<space>y<space>aller<space>allez<space>on<space>va<space>y<space>aller<space>voyez<space>allez<space>vous<space>allez<space>vous<space>allez<space>le<space>faire<space>monter<space>la<space>y<space>allez<space>me

2026-01-28 16:40:52,042 | INFO | speech length: 295200
2026-01-28 16:40:52,093 | INFO | decoder input length: 460
2026-01-28 16:40:52,093 | INFO | max output length: 460
2026-01-28 16:40:52,093 | INFO | min output length: 46
2026-01-28 16:41:18,043 | INFO | end detected at 354
2026-01-28 16:41:18,046 | INFO | -40.80 * 0.5 = -20.40 for decoder
2026-01-28 16:41:18,046 | INFO | -16.76 * 0.5 =  -8.38 for ctc
2026-01-28 16:41:18,046 | INFO | total log probability: -28.78
2026-01-28 16:41:18,046 | INFO | normalized log probability: -0.08
2026-01-28 16:41:18,046 | INFO | total number of ended hypotheses: 226
2026-01-28 16:41:18,051 | INFO | best hypo: alors<space>on<space>va<space>faire<space>un<space>petit<space>retourne<space>en<space>euh<space>en<space>arrière<space>oui<space>puisque<space>finalement<space>ça<space>fit<space>quarante<space>huit<space>ans<space>que<space>vous<space>êtes<space>au<space>kenya<space>quarante<space>huit<space>ans<space>oui<space>oui<space>je<space>suis<space>arrivé<space>la<space>première<space>fois<space>en<space>quarante<space>neuf<space>euh<space>juste<space>par<space>hasard<space>je<space>j'y<space>ai<space>passé<space>euh<space>deux<space>semaines<space>j'ai<space>trouvé<space>que<space>c'était<space>un<space>pays<space>formidable<space>je<space>suis<space>venu<space>m'y<space>installer<space>frès<space>cinquante

2026-01-28 16:41:18,054 | INFO | speech length: 410560
2026-01-28 16:41:18,097 | INFO | decoder input length: 641
2026-01-28 16:41:18,097 | INFO | max output length: 641
2026-01-28 16:41:18,097 | INFO | min output length: 64
2026-01-28 16:42:02,062 | INFO | end detected at 497
2026-01-28 16:42:02,063 | INFO | -316.11 * 0.5 = -158.06 for decoder
2026-01-28 16:42:02,063 | INFO | -42.66 * 0.5 = -21.33 for ctc
2026-01-28 16:42:02,063 | INFO | total log probability: -179.38
2026-01-28 16:42:02,063 | INFO | normalized log probability: -0.37
2026-01-28 16:42:02,063 | INFO | total number of ended hypotheses: 140
2026-01-28 16:42:02,069 | INFO | best hypo: alors<space>en<space>fait<space>vous<space>êtes<space>lié<space>à<space>la<space>france<space>puisque<space>vous<space>êtes<space>française<space>vous<space>êtes<space>de<space>l'est<space>de<space>la<space>france<space>vous<space>êtes<space>né<space>à<space>quel<space>endroit<space>je<space>suis<space>né<space>à<space>cannes<space>pendant<space>la<space>guerre<space>à<space>la<space>fin<space>de<space>la<space>guerre<space>de<space>dix<space>huit<space>mon<space>père<space>était<space>au<space>fond<space>euh<space>nous<space>avons<space>été<space>chassé<space>da<space>d'alsace<space>naturellement<space>vous<space>êtes<space>né<space>à<space>cannes<space>et<space>puis<space>euh<space>finalement<space>vous<space>avez<space>vous<space>êtes<space>parti<space>vers<space>melouse<space>alors<space>on<space>est<space>reparti<space>vers<space>melouse<space>mes<space>souvenirs<space>des<space>plus<space>anciens<space>son<space>je<space>crois<space>des<space>souvenirs<space>de<space>boulouse<space>n<space>s<space>sûrement<space>pas<space>de<space>cans

2026-01-28 16:42:02,072 | INFO | speech length: 120640
2026-01-28 16:42:02,129 | INFO | decoder input length: 188
2026-01-28 16:42:02,129 | INFO | max output length: 188
2026-01-28 16:42:02,129 | INFO | min output length: 18
2026-01-28 16:42:14,387 | INFO | end detected at 139
2026-01-28 16:42:14,391 | INFO | -16.63 * 0.5 =  -8.31 for decoder
2026-01-28 16:42:14,391 | INFO |  -3.09 * 0.5 =  -1.55 for ctc
2026-01-28 16:42:14,391 | INFO | total log probability: -9.86
2026-01-28 16:42:14,391 | INFO | normalized log probability: -0.07
2026-01-28 16:42:14,392 | INFO | total number of ended hypotheses: 182
2026-01-28 16:42:14,394 | INFO | best hypo: et<space>puis<space>j'ai<space>fait<space>mes<space>études<space>au<space>lycée<space>de<space>euh<space>de<space>mulhouse<space>et<space>j'ai<space>été<space>deux<space>ans<space>en<space>en<space>angleterre<space>en<space>en<space>trente<space>trente<space>et<space>un<space>trente<space>deux

2026-01-28 16:42:14,398 | INFO | speech length: 64480
2026-01-28 16:42:14,453 | INFO | decoder input length: 100
2026-01-28 16:42:14,453 | INFO | max output length: 100
2026-01-28 16:42:14,453 | INFO | min output length: 10
2026-01-28 16:42:20,718 | INFO | end detected at 90
2026-01-28 16:42:20,721 | INFO |  -7.05 * 0.5 =  -3.52 for decoder
2026-01-28 16:42:20,721 | INFO |  -2.52 * 0.5 =  -1.26 for ctc
2026-01-28 16:42:20,721 | INFO | total log probability: -4.78
2026-01-28 16:42:20,721 | INFO | normalized log probability: -0.06
2026-01-28 16:42:20,721 | INFO | total number of ended hypotheses: 191
2026-01-28 16:42:20,723 | INFO | best hypo: c'est<space>là<space>que<space>j'ai<space>j'ai<space>bien<space>appris<space>l'anglais<space>ce<space>qui<space>m'a<space>beaucoup<space>aidé<space>naturellement

2026-01-28 16:42:20,727 | INFO | speech length: 261120
2026-01-28 16:42:20,775 | INFO | decoder input length: 407
2026-01-28 16:42:20,775 | INFO | max output length: 407
2026-01-28 16:42:20,775 | INFO | min output length: 40
2026-01-28 16:42:39,969 | INFO | end detected at 263
2026-01-28 16:42:39,972 | INFO | -37.06 * 0.5 = -18.53 for decoder
2026-01-28 16:42:39,972 | INFO | -21.56 * 0.5 = -10.78 for ctc
2026-01-28 16:42:39,972 | INFO | total log probability: -29.31
2026-01-28 16:42:39,972 | INFO | normalized log probability: -0.11
2026-01-28 16:42:39,972 | INFO | total number of ended hypotheses: 189
2026-01-28 16:42:39,975 | INFO | best hypo: et<space>puis<space>j'ai<space>fini<space>mes<space>études<space>à<space>moulouse<space>j'ai<space>j'ai<space>fait<space>mon<space>pcb<space>à<space>paris<space>je<space>voulais<space>retourner<space>à<space>à<space>strasbourg<space>mais<space>j'ai<space>lâché<space>ça<space>je<space>suis<space>resté<space>à<space>paris<space>une<space>fois<space>qu'on<space>a<space>goûté<space>paris<space>on<space>va<space>pu<space>aller<space>à<space>commencer<space>pour<space>des<space>études<space>donc<space>pour<space>les<space>études<space>sur<space>sur<space>vous

2026-01-28 16:42:39,978 | INFO | speech length: 468320
2026-01-28 16:42:40,043 | INFO | decoder input length: 731
2026-01-28 16:42:40,043 | INFO | max output length: 731
2026-01-28 16:42:40,043 | INFO | min output length: 73
2026-01-28 16:43:53,720 | INFO | end detected at 530
2026-01-28 16:43:53,724 | INFO | -624.15 * 0.5 = -312.07 for decoder
2026-01-28 16:43:53,724 | INFO | -73.33 * 0.5 = -36.66 for ctc
2026-01-28 16:43:53,724 | INFO | total log probability: -348.74
2026-01-28 16:43:53,724 | INFO | normalized log probability: -0.67
2026-01-28 16:43:53,724 | INFO | total number of ended hypotheses: 202
2026-01-28 16:43:53,735 | INFO | best hypo: et<space>alors<space>là<space>vous<space>faisiez<space>des<space>études<space>de<space>médecine<space>déjà<space>alors<space>j'ai<space>fait<space>mes<space>études<space>de<space>médecine<space>à<space>paris<space>et<space>après<space>la<space>guerre<space>je<space>suis<space>venue<space>euh<space>parce<space>qu'<space>y<space>avait<space>toujours<space>deux<space>p<space>alors<space>depuis<space>avant<space>la<space>guerre<space>j'avais<space>rencontré<space>euh<space>à<space>mon<space>fred<space>henry<space>de<space>monfreid<space>qui<space>venait<space>de<space>faire<space>des<space>conférences<space>à<space>à<space>mulhouse<space>et<space>et<space>et<space>c'est<space>un<space>type<space>euh<space>passionnant<space>t<space>j'ai<space>toujours<space>lu<space>tous<space>ces<space>livres<space>et<space>j'avai<space>absolument<space>envie<space>d'aller<space>dans<space>la<space>corne<space>de<space>l'afrique<space>e<space>e<space>l'éthiopie<space>la<space>somalie<space>euh<space>l'aur<space>à<space>l'arabi<space>tout<space>ça<space>c'était<space>c<space>est<space>là<space>j'ai<space>y<space>alles

2026-01-28 16:43:53,740 | INFO | speech length: 56320
2026-01-28 16:43:53,797 | INFO | decoder input length: 87
2026-01-28 16:43:53,797 | INFO | max output length: 87
2026-01-28 16:43:53,798 | INFO | min output length: 8
2026-01-28 16:43:56,093 | INFO | end detected at 51
2026-01-28 16:43:56,095 | INFO |  -4.47 * 0.5 =  -2.24 for decoder
2026-01-28 16:43:56,095 | INFO |  -4.15 * 0.5 =  -2.08 for ctc
2026-01-28 16:43:56,095 | INFO | total log probability: -4.31
2026-01-28 16:43:56,095 | INFO | normalized log probability: -0.10
2026-01-28 16:43:56,095 | INFO | total number of ended hypotheses: 194
2026-01-28 16:43:56,096 | INFO | best hypo: et<space>à<space>cavaler<space>pardillon<space>nous<space>allions<space>en<space>été

2026-01-28 16:43:56,098 | INFO | speech length: 135200
2026-01-28 16:43:56,138 | INFO | decoder input length: 210
2026-01-28 16:43:56,138 | INFO | max output length: 210
2026-01-28 16:43:56,138 | INFO | min output length: 21
2026-01-28 16:44:03,040 | INFO | end detected at 125
2026-01-28 16:44:03,043 | INFO | -16.03 * 0.5 =  -8.02 for decoder
2026-01-28 16:44:03,043 | INFO |  -7.18 * 0.5 =  -3.59 for ctc
2026-01-28 16:44:03,043 | INFO | total log probability: -11.61
2026-01-28 16:44:03,043 | INFO | normalized log probability: -0.10
2026-01-28 16:44:03,043 | INFO | total number of ended hypotheses: 202
2026-01-28 16:44:03,045 | INFO | best hypo: il<space>y<space>avait<space>une<space>famille<space>amie<space>à<space>nous<space>les<space>bèses<space>et<space>qui<space>avait<space>un<space>un<space>empire<space>commercial<space>je<space>veux<space>dire<space>surtout<space>à<space>à<space>adhènes

2026-01-28 16:44:03,049 | INFO | speech length: 425440
2026-01-28 16:44:03,152 | INFO | decoder input length: 664
2026-01-28 16:44:03,152 | INFO | max output length: 664
2026-01-28 16:44:03,152 | INFO | min output length: 66
2026-01-28 16:44:56,978 | INFO | end detected at 466
2026-01-28 16:44:56,979 | INFO | -356.55 * 0.5 = -178.27 for decoder
2026-01-28 16:44:56,979 | INFO | -24.95 * 0.5 = -12.47 for ctc
2026-01-28 16:44:56,980 | INFO | total log probability: -190.75
2026-01-28 16:44:56,980 | INFO | normalized log probability: -0.42
2026-01-28 16:44:56,980 | INFO | total number of ended hypotheses: 178
2026-01-28 16:44:56,985 | INFO | best hypo: et<space>après<space>la<space>guerre<space>et<space>nous<space>m'a<space>invité<space>à<space>venir<space>à<space>à<space>adhènes<space>et<space>je<space>suivais<space>cela<space>presque<space>un<space>an<space>j'ai<space>travaillé<space>à<space>l'hôpital<space>d'aden<space>on<space>m'a<space>demandé<space>de<space>le<space>faire<space>vous<space>étiez<space>auprès<space>des<space>femmes<space>euh<space>ah<space>oui<space>naturellement<space>parce<space>que<space>la<space>seule<space>euh<space>la<space>seule<space>médecin<space>femme<space>euh<space>avait<space>été<space>euh<space>tombée<space>malade<space>était<space>rentrée<space>en<space>angleterre<space>alors<space>comme<space>ça<space>j'ai<space>travaillé<space>pendant<space>jusqu'au<space>moment<space>où<space>il<space>y<space>a<space>eu<space>une<space>remplaçante<space>qui<space>est<space>qui<space>est<space>venu<space>d'angleterre<space>moi<space>je<space>suis<space>allé<space>en<space>éthiopie

2026-01-28 16:44:56,994 | INFO | Chunk: 0 | WER=35.714286 | S=4 D=0 I=1
2026-01-28 16:44:56,994 | INFO | Chunk: 1 | WER=10.000000 | S=0 D=1 I=0
2026-01-28 16:44:56,998 | INFO | Chunk: 2 | WER=25.000000 | S=7 D=6 I=7
2026-01-28 16:44:57,003 | INFO | Chunk: 3 | WER=19.318182 | S=11 D=2 I=4
2026-01-28 16:44:57,006 | INFO | Chunk: 4 | WER=29.577465 | S=16 D=3 I=2
2026-01-28 16:44:57,007 | INFO | Chunk: 5 | WER=0.000000 | S=1 D=0 I=31
2026-01-28 16:44:57,010 | INFO | Chunk: 6 | WER=14.062500 | S=6 D=0 I=3
2026-01-28 16:44:57,015 | INFO | Chunk: 7 | WER=21.875000 | S=17 D=1 I=3
2026-01-28 16:44:57,016 | INFO | Chunk: 8 | WER=10.344828 | S=0 D=1 I=2
2026-01-28 16:44:57,017 | INFO | Chunk: 9 | WER=5.263158 | S=1 D=0 I=0
2026-01-28 16:44:57,019 | INFO | Chunk: 10 | WER=21.666667 | S=8 D=4 I=1
2026-01-28 16:44:57,025 | INFO | Chunk: 11 | WER=26.470588 | S=15 D=1 I=11
2026-01-28 16:44:57,026 | INFO | Chunk: 12 | WER=33.333333 | S=2 D=1 I=0
2026-01-28 16:44:57,026 | INFO | Chunk: 13 | WER=13.043478 | S=2 D=0 I=1
2026-01-28 16:44:57,032 | INFO | Chunk: 14 | WER=16.842105 | S=11 D=3 I=2
2026-01-28 16:44:57,366 | INFO | File: Rhap-D2004.wav | WER=25.263158 | S=100 D=23 I=69
2026-01-28 16:44:57,367 | INFO | ------------------------------
2026-01-28 16:44:57,367 | INFO | Conf ester Done!
2026-01-28 16:48:15,254 | INFO | Chunk: 0 | WER=28.571429 | S=3 D=0 I=1
2026-01-28 16:48:15,255 | INFO | Chunk: 1 | WER=30.000000 | S=1 D=1 I=1
2026-01-28 16:48:15,260 | INFO | Chunk: 2 | WER=20.000000 | S=8 D=5 I=3
2026-01-28 16:48:15,265 | INFO | Chunk: 3 | WER=22.727273 | S=14 D=3 I=3
2026-01-28 16:48:15,268 | INFO | Chunk: 4 | WER=25.352113 | S=11 D=7 I=0
2026-01-28 16:48:15,268 | INFO | Chunk: 5 | WER=0.000000 | S=1 D=0 I=0
2026-01-28 16:48:15,271 | INFO | Chunk: 6 | WER=23.437500 | S=7 D=6 I=2
2026-01-28 16:48:15,276 | INFO | Chunk: 7 | WER=18.750000 | S=12 D=3 I=3
2026-01-28 16:48:15,277 | INFO | Chunk: 8 | WER=10.344828 | S=0 D=3 I=0
2026-01-28 16:48:15,278 | INFO | Chunk: 9 | WER=21.052632 | S=2 D=2 I=0
2026-01-28 16:48:15,280 | INFO | Chunk: 10 | WER=28.333333 | S=9 D=7 I=1
2026-01-28 16:48:15,288 | INFO | Chunk: 11 | WER=21.568627 | S=13 D=7 I=2
2026-01-28 16:48:15,288 | INFO | Chunk: 12 | WER=44.444444 | S=4 D=0 I=0
2026-01-28 16:48:15,289 | INFO | Chunk: 13 | WER=26.086957 | S=2 D=1 I=3
2026-01-28 16:48:15,294 | INFO | Chunk: 14 | WER=18.947368 | S=12 D=6 I=0
2026-01-28 16:48:15,598 | INFO | File: Rhap-D2004.wav | WER=22.236842 | S=98 D=51 I=20
2026-01-28 16:48:15,598 | INFO | ------------------------------
2026-01-28 16:48:15,599 | INFO | hmm_tdnn Done!
2026-01-28 16:48:15,781 | INFO | ==================================Rhap-D2005.wav=========================================
2026-01-28 16:48:16,023 | INFO | Using rVAD model
2026-01-28 16:48:35,070 | INFO | Chunk: 0 | WER=11.842105 | S=5 D=4 I=0
2026-01-28 16:48:35,072 | INFO | Chunk: 1 | WER=4.761905 | S=2 D=0 I=0
2026-01-28 16:48:35,076 | INFO | Chunk: 2 | WER=1.265823 | S=1 D=0 I=0
2026-01-28 16:48:35,079 | INFO | Chunk: 3 | WER=15.714286 | S=5 D=4 I=2
2026-01-28 16:48:35,079 | INFO | Chunk: 4 | WER=33.333333 | S=1 D=0 I=1
2026-01-28 16:48:35,080 | INFO | Chunk: 5 | WER=10.000000 | S=1 D=1 I=0
2026-01-28 16:48:35,081 | INFO | Chunk: 6 | WER=14.634146 | S=3 D=3 I=0
2026-01-28 16:48:35,082 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 16:48:35,082 | INFO | Chunk: 8 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 16:48:35,083 | INFO | Chunk: 9 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 16:48:35,083 | INFO | Chunk: 10 | WER=7.692308 | S=1 D=0 I=0
2026-01-28 16:48:35,084 | INFO | Chunk: 11 | WER=20.833333 | S=3 D=1 I=1
2026-01-28 16:48:35,084 | INFO | Chunk: 12 | WER=100.000000 | S=1 D=0 I=0
2026-01-28 16:48:35,084 | INFO | Chunk: 13 | WER=33.333333 | S=3 D=2 I=2
2026-01-28 16:48:35,085 | INFO | Chunk: 14 | WER=11.111111 | S=2 D=0 I=0
2026-01-28 16:48:35,085 | INFO | Chunk: 15 | WER=30.769231 | S=1 D=2 I=1
2026-01-28 16:48:35,086 | INFO | Chunk: 16 | WER=32.432432 | S=5 D=7 I=0
2026-01-28 16:48:35,087 | INFO | Chunk: 17 | WER=75.000000 | S=2 D=0 I=1
2026-01-28 16:48:35,087 | INFO | Chunk: 18 | WER=40.000000 | S=2 D=1 I=1
2026-01-28 16:48:35,087 | INFO | Chunk: 19 | WER=33.333333 | S=0 D=0 I=1
2026-01-28 16:48:35,088 | INFO | Chunk: 20 | WER=22.222222 | S=4 D=3 I=1
2026-01-28 16:48:35,090 | INFO | Chunk: 21 | WER=15.909091 | S=3 D=3 I=1
2026-01-28 16:48:35,090 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 16:48:35,090 | INFO | Chunk: 23 | WER=75.000000 | S=1 D=3 I=2
2026-01-28 16:48:35,091 | INFO | Chunk: 24 | WER=4.000000 | S=1 D=0 I=0
2026-01-28 16:48:35,092 | INFO | Chunk: 25 | WER=29.729730 | S=5 D=5 I=1
2026-01-28 16:48:35,093 | INFO | Chunk: 26 | WER=41.666667 | S=7 D=2 I=1
2026-01-28 16:48:35,096 | INFO | Chunk: 27 | WER=20.895522 | S=7 D=6 I=1
2026-01-28 16:48:35,096 | INFO | Chunk: 28 | WER=38.888889 | S=2 D=5 I=0
2026-01-28 16:48:35,098 | INFO | Chunk: 29 | WER=19.642857 | S=7 D=4 I=0
2026-01-28 16:48:35,099 | INFO | Chunk: 30 | WER=15.384615 | S=4 D=2 I=0
2026-01-28 16:48:35,100 | INFO | Chunk: 31 | WER=100.000000 | S=1 D=0 I=0
2026-01-28 16:48:35,102 | INFO | Chunk: 32 | WER=19.672131 | S=6 D=6 I=0
2026-01-28 16:48:35,566 | INFO | File: Rhap-D2005.wav | WER=17.179215 | S=89 D=60 I=13
2026-01-28 16:48:35,567 | INFO | ------------------------------
2026-01-28 16:48:35,567 | INFO | w2vec vad chunk Done!
2026-01-28 16:49:21,783 | INFO | Chunk: 0 | WER=52.631579 | S=4 D=36 I=0
2026-01-28 16:49:21,785 | INFO | Chunk: 1 | WER=16.666667 | S=2 D=5 I=0
2026-01-28 16:49:21,787 | INFO | Chunk: 2 | WER=54.430380 | S=1 D=42 I=0
2026-01-28 16:49:21,789 | INFO | Chunk: 3 | WER=68.571429 | S=3 D=44 I=1
2026-01-28 16:49:21,789 | INFO | Chunk: 4 | WER=16.666667 | S=1 D=0 I=0
2026-01-28 16:49:21,790 | INFO | Chunk: 5 | WER=10.000000 | S=2 D=0 I=0
2026-01-28 16:49:21,791 | INFO | Chunk: 6 | WER=31.707317 | S=5 D=8 I=0
2026-01-28 16:49:21,791 | INFO | Chunk: 7 | WER=11.111111 | S=0 D=2 I=0
2026-01-28 16:49:21,792 | INFO | Chunk: 8 | WER=9.523810 | S=1 D=1 I=0
2026-01-28 16:49:21,792 | INFO | Chunk: 9 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 16:49:21,792 | INFO | Chunk: 10 | WER=15.384615 | S=2 D=0 I=0
2026-01-28 16:49:21,793 | INFO | Chunk: 11 | WER=12.500000 | S=1 D=2 I=0
2026-01-28 16:49:21,793 | INFO | Chunk: 12 | WER=300.000000 | S=1 D=0 I=2
2026-01-28 16:49:21,794 | INFO | Chunk: 13 | WER=28.571429 | S=2 D=1 I=3
2026-01-28 16:49:21,794 | INFO | Chunk: 14 | WER=5.555556 | S=1 D=0 I=0
2026-01-28 16:49:21,795 | INFO | Chunk: 15 | WER=30.769231 | S=0 D=2 I=2
2026-01-28 16:49:21,795 | INFO | Chunk: 16 | WER=75.675676 | S=2 D=26 I=0
2026-01-28 16:49:21,795 | INFO | Chunk: 17 | WER=75.000000 | S=2 D=0 I=1
2026-01-28 16:49:21,796 | INFO | Chunk: 18 | WER=10.000000 | S=0 D=1 I=0
2026-01-28 16:49:21,796 | INFO | Chunk: 19 | WER=66.666667 | S=0 D=0 I=2
2026-01-28 16:49:21,797 | INFO | Chunk: 20 | WER=13.888889 | S=3 D=2 I=0
2026-01-28 16:49:21,798 | INFO | Chunk: 21 | WER=36.363636 | S=0 D=16 I=0
2026-01-28 16:49:21,798 | INFO | Chunk: 22 | WER=133.333333 | S=1 D=0 I=3
2026-01-28 16:49:21,799 | INFO | Chunk: 23 | WER=50.000000 | S=0 D=3 I=1
2026-01-28 16:49:21,799 | INFO | Chunk: 24 | WER=24.000000 | S=4 D=0 I=2
2026-01-28 16:49:21,800 | INFO | Chunk: 25 | WER=51.351351 | S=0 D=19 I=0
2026-01-28 16:49:21,801 | INFO | Chunk: 26 | WER=37.500000 | S=5 D=3 I=1
2026-01-28 16:49:21,803 | INFO | Chunk: 27 | WER=58.208955 | S=12 D=27 I=0
2026-01-28 16:49:21,803 | INFO | Chunk: 28 | WER=33.333333 | S=1 D=5 I=0
2026-01-28 16:49:21,805 | INFO | Chunk: 29 | WER=35.714286 | S=3 D=16 I=1
2026-01-28 16:49:21,806 | INFO | Chunk: 30 | WER=33.333333 | S=4 D=9 I=0
2026-01-28 16:49:21,806 | INFO | Chunk: 31 | WER=100.000000 | S=1 D=0 I=0
2026-01-28 16:49:21,808 | INFO | Chunk: 32 | WER=81.967213 | S=5 D=44 I=1
2026-01-28 16:49:22,242 | INFO | File: Rhap-D2005.wav | WER=41.675504 | S=75 D=306 I=12
2026-01-28 16:49:22,242 | INFO | ------------------------------
2026-01-28 16:49:22,242 | INFO | whisper med Done!
2026-01-28 16:50:27,234 | INFO | Chunk: 0 | WER=59.210526 | S=4 D=41 I=0
2026-01-28 16:50:27,237 | INFO | Chunk: 1 | WER=4.761905 | S=2 D=0 I=0
2026-01-28 16:50:27,241 | INFO | Chunk: 2 | WER=36.708861 | S=3 D=26 I=0
2026-01-28 16:50:27,244 | INFO | Chunk: 3 | WER=50.000000 | S=7 D=27 I=1
2026-01-28 16:50:27,244 | INFO | Chunk: 4 | WER=16.666667 | S=1 D=0 I=0
2026-01-28 16:50:27,245 | INFO | Chunk: 5 | WER=5.000000 | S=0 D=1 I=0
2026-01-28 16:50:27,246 | INFO | Chunk: 6 | WER=24.390244 | S=6 D=4 I=0
2026-01-28 16:50:27,247 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 16:50:27,248 | INFO | Chunk: 8 | WER=4.761905 | S=1 D=0 I=0
2026-01-28 16:50:27,248 | INFO | Chunk: 9 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 16:50:27,249 | INFO | Chunk: 10 | WER=15.384615 | S=2 D=0 I=0
2026-01-28 16:50:27,250 | INFO | Chunk: 11 | WER=4.166667 | S=0 D=1 I=0
2026-01-28 16:50:27,250 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 16:50:27,250 | INFO | Chunk: 13 | WER=28.571429 | S=2 D=2 I=2
2026-01-28 16:50:27,251 | INFO | Chunk: 14 | WER=5.555556 | S=1 D=0 I=0
2026-01-28 16:50:27,252 | INFO | Chunk: 15 | WER=30.769231 | S=1 D=1 I=2
2026-01-28 16:50:27,252 | INFO | Chunk: 16 | WER=72.972973 | S=1 D=26 I=0
2026-01-28 16:50:27,253 | INFO | Chunk: 17 | WER=75.000000 | S=2 D=0 I=1
2026-01-28 16:50:27,253 | INFO | Chunk: 18 | WER=40.000000 | S=0 D=4 I=0
2026-01-28 16:50:27,253 | INFO | Chunk: 19 | WER=66.666667 | S=0 D=0 I=2
2026-01-28 16:50:27,255 | INFO | Chunk: 20 | WER=16.666667 | S=5 D=1 I=0
2026-01-28 16:50:27,257 | INFO | Chunk: 21 | WER=18.181818 | S=0 D=8 I=0
2026-01-28 16:50:27,257 | INFO | Chunk: 22 | WER=66.666667 | S=0 D=0 I=2
2026-01-28 16:50:27,257 | INFO | Chunk: 23 | WER=50.000000 | S=0 D=3 I=1
2026-01-28 16:50:27,258 | INFO | Chunk: 24 | WER=12.000000 | S=1 D=1 I=1
2026-01-28 16:50:27,259 | INFO | Chunk: 25 | WER=59.459459 | S=2 D=20 I=0
2026-01-28 16:50:27,260 | INFO | Chunk: 26 | WER=29.166667 | S=6 D=1 I=0
2026-01-28 16:50:27,263 | INFO | Chunk: 27 | WER=41.791045 | S=5 D=23 I=0
2026-01-28 16:50:27,264 | INFO | Chunk: 28 | WER=38.888889 | S=2 D=5 I=0
2026-01-28 16:50:27,266 | INFO | Chunk: 29 | WER=42.857143 | S=8 D=16 I=0
2026-01-28 16:50:27,268 | INFO | Chunk: 30 | WER=10.256410 | S=2 D=2 I=0
2026-01-28 16:50:27,268 | INFO | Chunk: 31 | WER=100.000000 | S=1 D=0 I=0
2026-01-28 16:50:27,270 | INFO | Chunk: 32 | WER=78.688525 | S=3 D=45 I=0
2026-01-28 16:50:27,764 | INFO | File: Rhap-D2005.wav | WER=35.630965 | S=70 D=256 I=10
2026-01-28 16:50:27,764 | INFO | ------------------------------
2026-01-28 16:50:27,765 | INFO | whisper large Done!
2026-01-28 16:50:27,969 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 16:50:28,019 | INFO | Vocabulary size: 350
2026-01-28 16:50:28,954 | INFO | Gradient checkpoint layers: []
2026-01-28 16:50:29,710 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 16:50:29,714 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 16:50:29,714 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 16:50:29,715 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 16:50:29,715 | INFO | speech length: 371520
2026-01-28 16:50:29,769 | INFO | decoder input length: 580
2026-01-28 16:50:29,769 | INFO | max output length: 580
2026-01-28 16:50:29,769 | INFO | min output length: 58
2026-01-28 16:50:53,184 | INFO | end detected at 199
2026-01-28 16:50:53,186 | INFO | -394.92 * 0.5 = -197.46 for decoder
2026-01-28 16:50:53,186 | INFO | -99.69 * 0.5 = -49.84 for ctc
2026-01-28 16:50:53,186 | INFO | total log probability: -247.30
2026-01-28 16:50:53,186 | INFO | normalized log probability: -1.28
2026-01-28 16:50:53,186 | INFO | total number of ended hypotheses: 162
2026-01-28 16:50:53,189 | INFO | best hypo: ▁et▁écrivain▁conseiller▁spécial▁du▁président▁françois▁mitterrand▁banquier▁du▁monde▁jacques▁atalli▁a▁usé▁de▁tous▁les▁pouvoirs▁dans▁les▁coulisses▁de▁l'histoire▁il▁s'évolue▁comme▁un▁homme▁d'idée▁et▁un▁homme▁de▁concepts▁de▁tallients▁séduisent▁un▁large▁publics▁considéré▁par▁certains▁comme▁un▁des▁escrits▁et▁plus▁vifs▁il▁est▁l'auteur▁de▁nombreux▁dest▁sellers▁comme▁mille▁quatre▁cent▁quatre▁vingt▁douze▁ou▁le▁dictionnaire▁du▁vingt▁huitième▁siècle

2026-01-28 16:50:53,193 | INFO | speech length: 220800
2026-01-28 16:50:53,250 | INFO | decoder input length: 344
2026-01-28 16:50:53,250 | INFO | max output length: 344
2026-01-28 16:50:53,250 | INFO | min output length: 34
2026-01-28 16:51:01,351 | INFO | end detected at 107
2026-01-28 16:51:01,354 | INFO | -177.06 * 0.5 = -88.53 for decoder
2026-01-28 16:51:01,354 | INFO | -37.45 * 0.5 = -18.72 for ctc
2026-01-28 16:51:01,354 | INFO | total log probability: -107.25
2026-01-28 16:51:01,354 | INFO | normalized log probability: -1.06
2026-01-28 16:51:01,354 | INFO | total number of ended hypotheses: 183
2026-01-28 16:51:01,356 | INFO | best hypo: ▁perçu▁comme▁un▁surdoué▁de▁sa▁génération▁son▁action▁s'étend▁aux▁allées▁du▁pouvoir▁il▁devient▁le▁sherpa▁du▁président▁le▁porteur▁de▁valise▁le▁conseiller▁influent▁du▁prince▁négocient▁au▁plus▁haut▁niveau▁et▁les▁hommes▁et▁les▁états

2026-01-28 16:51:01,358 | INFO | speech length: 443200
2026-01-28 16:51:01,407 | INFO | decoder input length: 692
2026-01-28 16:51:01,408 | INFO | max output length: 692
2026-01-28 16:51:01,408 | INFO | min output length: 69
2026-01-28 16:51:28,103 | INFO | end detected at 210
2026-01-28 16:51:28,104 | INFO | -645.22 * 0.5 = -322.61 for decoder
2026-01-28 16:51:28,104 | INFO | -177.17 * 0.5 = -88.58 for ctc
2026-01-28 16:51:28,104 | INFO | total log probability: -411.19
2026-01-28 16:51:28,104 | INFO | normalized log probability: -2.01
2026-01-28 16:51:28,104 | INFO | total number of ended hypotheses: 166
2026-01-28 16:51:28,107 | INFO | best hypo: ▁devenu▁l'acteur▁principal▁de▁ses▁idées▁il▁s'expose▁directement▁comme▁concepteur▁et▁président▁d'une▁banque▁internationale▁pour▁la▁reconstruction▁d'un▁monde▁qui▁vaceille▁la▁russie▁et▁le▁bloc▁de▁l'est▁les▁enjeux▁sont▁consaidérables▁et▁la▁tourmente▁le▁qu'il▁génère▁va▁l'emporter▁il▁démis▁sionne▁et▁se▁reôtire▁aujourd'huire▁il▁continue▁de▁son▁actativité▁de▁confseils▁et▁se▁cons'acre▁toujours▁à▁l'ecture▁et▁à▁l'histoire▁qui▁ne▁l'a▁pas▁toujours▁reconnu

2026-01-28 16:51:28,110 | INFO | speech length: 431520
2026-01-28 16:51:28,161 | INFO | decoder input length: 673
2026-01-28 16:51:28,161 | INFO | max output length: 673
2026-01-28 16:51:28,161 | INFO | min output length: 67
2026-01-28 16:51:49,792 | INFO | end detected at 174
2026-01-28 16:51:49,794 | INFO | -375.43 * 0.5 = -187.72 for decoder
2026-01-28 16:51:49,794 | INFO | -105.42 * 0.5 = -52.71 for ctc
2026-01-28 16:51:49,794 | INFO | total log probability: -240.43
2026-01-28 16:51:49,794 | INFO | normalized log probability: -1.43
2026-01-28 16:51:49,794 | INFO | total number of ended hypotheses: 161
2026-01-28 16:51:49,797 | INFO | best hypo: ▁mais▁tu▁n'ai▁à▁alger▁en▁mille▁neuf▁cent▁quarante▁trois▁non▁que▁j'ai▁connu▁l'algérie▁heureuse▁je▁ni▁en▁premier▁novembre▁donc▁mon▁anniversaire▁aïncide▁avec▁le▁début▁de▁l'in'insurrection▁algérienne▁et▁j'ai▁été▁d'une▁famille▁très▁réunie▁très▁aisée▁à▁mon▁père▁en▁un▁autobdacte▁ven▁d'un▁milieu▁très▁pauvre▁ayant▁fait▁aucune▁étude▁mais▁ayant▁passion▁de▁littérature▁du▁savoir▁ayant

2026-01-28 16:51:49,800 | INFO | speech length: 32000
2026-01-28 16:51:49,844 | INFO | decoder input length: 49
2026-01-28 16:51:49,845 | INFO | max output length: 49
2026-01-28 16:51:49,845 | INFO | min output length: 4
2026-01-28 16:51:51,199 | INFO | end detected at 29
2026-01-28 16:51:51,202 | INFO |  -2.36 * 0.5 =  -1.18 for decoder
2026-01-28 16:51:51,202 | INFO |  -4.51 * 0.5 =  -2.25 for ctc
2026-01-28 16:51:51,202 | INFO | total log probability: -3.43
2026-01-28 16:51:51,202 | INFO | normalized log probability: -0.14
2026-01-28 16:51:51,202 | INFO | total number of ended hypotheses: 164
2026-01-28 16:51:51,203 | INFO | best hypo: ▁il▁accumulait▁un▁énorme▁savoir▁d'autodidacte

2026-01-28 16:51:51,205 | INFO | speech length: 68960
2026-01-28 16:51:51,263 | INFO | decoder input length: 107
2026-01-28 16:51:51,263 | INFO | max output length: 107
2026-01-28 16:51:51,263 | INFO | min output length: 10
2026-01-28 16:51:55,132 | INFO | end detected at 46
2026-01-28 16:51:55,133 | INFO |  -3.16 * 0.5 =  -1.58 for decoder
2026-01-28 16:51:55,134 | INFO |  -1.01 * 0.5 =  -0.50 for ctc
2026-01-28 16:51:55,134 | INFO | total log probability: -2.09
2026-01-28 16:51:55,134 | INFO | normalized log probability: -0.05
2026-01-28 16:51:55,134 | INFO | total number of ended hypotheses: 146
2026-01-28 16:51:55,135 | INFO | best hypo: ▁et▁j'ai▁vécu▁avec▁mon▁frère▁jumeau▁et▁ma▁soeur▁dans▁une▁ambiance▁où▁le▁livre▁était▁sacré

2026-01-28 16:51:55,138 | INFO | speech length: 180960
2026-01-28 16:51:55,191 | INFO | decoder input length: 282
2026-01-28 16:51:55,191 | INFO | max output length: 282
2026-01-28 16:51:55,191 | INFO | min output length: 28
2026-01-28 16:52:02,188 | INFO | end detected at 81
2026-01-28 16:52:02,189 | INFO | -50.50 * 0.5 = -25.25 for decoder
2026-01-28 16:52:02,190 | INFO | -25.40 * 0.5 = -12.70 for ctc
2026-01-28 16:52:02,190 | INFO | total log probability: -37.95
2026-01-28 16:52:02,190 | INFO | normalized log probability: -0.50
2026-01-28 16:52:02,190 | INFO | total number of ended hypotheses: 166
2026-01-28 16:52:02,191 | INFO | best hypo: ▁et▁où▁le▁travail▁était▁la▁moindre▁des▁choses▁nous▁sommes▁venus▁en▁france▁mille▁neuf▁cent▁cinquante▁six▁chevailles▁douze▁ans▁et▁je▁suis▁entré▁au▁mes▁études▁jançon▁de▁saillis▁à▁paris

2026-01-28 16:52:02,193 | INFO | speech length: 85280
2026-01-28 16:52:02,239 | INFO | decoder input length: 132
2026-01-28 16:52:02,240 | INFO | max output length: 132
2026-01-28 16:52:02,240 | INFO | min output length: 13
2026-01-28 16:52:04,706 | INFO | end detected at 49
2026-01-28 16:52:04,707 | INFO |  -3.42 * 0.5 =  -1.71 for decoder
2026-01-28 16:52:04,707 | INFO |  -3.41 * 0.5 =  -1.70 for ctc
2026-01-28 16:52:04,708 | INFO | total log probability: -3.41
2026-01-28 16:52:04,708 | INFO | normalized log probability: -0.08
2026-01-28 16:52:04,708 | INFO | total number of ended hypotheses: 168
2026-01-28 16:52:04,708 | INFO | best hypo: ▁et▁avec▁une▁ambition▁qui▁commençait▁à▁se▁cristalliser▁déjà▁autour▁de▁l'idée▁d'écrire▁et▁d'eux

2026-01-28 16:52:04,710 | INFO | speech length: 107040
2026-01-28 16:52:04,754 | INFO | decoder input length: 166
2026-01-28 16:52:04,754 | INFO | max output length: 166
2026-01-28 16:52:04,754 | INFO | min output length: 16
2026-01-28 16:52:07,931 | INFO | end detected at 58
2026-01-28 16:52:07,933 | INFO |  -5.49 * 0.5 =  -2.74 for decoder
2026-01-28 16:52:07,933 | INFO |  -6.19 * 0.5 =  -3.10 for ctc
2026-01-28 16:52:07,933 | INFO | total log probability: -5.84
2026-01-28 16:52:07,933 | INFO | normalized log probability: -0.11
2026-01-28 16:52:07,933 | INFO | total number of ended hypotheses: 174
2026-01-28 16:52:07,934 | INFO | best hypo: ▁cherchez▁j'ai▁toujours▁pensé▁qu'on▁n'est▁pas▁sur▁terre▁pour▁être▁heureux▁mais▁pour▁moi▁essaie▁d'être▁utile

2026-01-28 16:52:07,936 | INFO | speech length: 24640
2026-01-28 16:52:07,982 | INFO | decoder input length: 38
2026-01-28 16:52:07,982 | INFO | max output length: 38
2026-01-28 16:52:07,982 | INFO | min output length: 3
2026-01-28 16:52:08,881 | INFO | end detected at 23
2026-01-28 16:52:08,883 | INFO |  -1.91 * 0.5 =  -0.96 for decoder
2026-01-28 16:52:08,883 | INFO |  -0.29 * 0.5 =  -0.14 for ctc
2026-01-28 16:52:08,883 | INFO | total log probability: -1.10
2026-01-28 16:52:08,883 | INFO | normalized log probability: -0.06
2026-01-28 16:52:08,883 | INFO | total number of ended hypotheses: 164
2026-01-28 16:52:08,883 | INFO | best hypo: ▁le▁bonheur▁n'est▁pas▁bon▁problème

2026-01-28 16:52:08,885 | INFO | speech length: 44640
2026-01-28 16:52:08,926 | INFO | decoder input length: 69
2026-01-28 16:52:08,927 | INFO | max output length: 69
2026-01-28 16:52:08,927 | INFO | min output length: 6
2026-01-28 16:52:10,493 | INFO | end detected at 37
2026-01-28 16:52:10,496 | INFO |  -3.74 * 0.5 =  -1.87 for decoder
2026-01-28 16:52:10,496 | INFO |  -5.74 * 0.5 =  -2.87 for ctc
2026-01-28 16:52:10,496 | INFO | total log probability: -4.74
2026-01-28 16:52:10,496 | INFO | normalized log probability: -0.14
2026-01-28 16:52:10,496 | INFO | total number of ended hypotheses: 163
2026-01-28 16:52:10,497 | INFO | best hypo: ▁je▁servirai▁quelque▁chose▁et▁quelque▁chose▁qui▁me▁préoccupe▁tout▁le▁temps

2026-01-28 16:52:10,500 | INFO | speech length: 135360
2026-01-28 16:52:10,651 | INFO | decoder input length: 211
2026-01-28 16:52:10,652 | INFO | max output length: 211
2026-01-28 16:52:10,652 | INFO | min output length: 21
2026-01-28 16:52:13,970 | INFO | end detected at 54
2026-01-28 16:52:13,971 | INFO | -12.27 * 0.5 =  -6.14 for decoder
2026-01-28 16:52:13,971 | INFO |  -2.41 * 0.5 =  -1.21 for ctc
2026-01-28 16:52:13,971 | INFO | total log probability: -7.34
2026-01-28 16:52:13,971 | INFO | normalized log probability: -0.15
2026-01-28 16:52:13,971 | INFO | total number of ended hypotheses: 161
2026-01-28 16:52:13,972 | INFO | best hypo: ▁et▁faire▁avancer▁le▁savoir▁ou▁faire▁avancer▁la▁littérature▁ou▁créer▁des▁choses▁qui▁durent▁qui▁servent▁m'a▁paru▁être

2026-01-28 16:52:13,974 | INFO | speech length: 9600
2026-01-28 16:52:14,002 | INFO | decoder input length: 14
2026-01-28 16:52:14,002 | INFO | max output length: 14
2026-01-28 16:52:14,002 | INFO | min output length: 1
2026-01-28 16:52:14,456 | INFO | end detected at 12
2026-01-28 16:52:14,458 | INFO |  -0.60 * 0.5 =  -0.30 for decoder
2026-01-28 16:52:14,458 | INFO |  -7.62 * 0.5 =  -3.81 for ctc
2026-01-28 16:52:14,458 | INFO | total log probability: -4.11
2026-01-28 16:52:14,458 | INFO | normalized log probability: -0.51
2026-01-28 16:52:14,458 | INFO | total number of ended hypotheses: 159
2026-01-28 16:52:14,458 | INFO | best hypo: ▁essentiel

2026-01-28 16:52:14,460 | INFO | speech length: 93760
2026-01-28 16:52:14,500 | INFO | decoder input length: 146
2026-01-28 16:52:14,500 | INFO | max output length: 146
2026-01-28 16:52:14,500 | INFO | min output length: 14
2026-01-28 16:52:17,805 | INFO | end detected at 62
2026-01-28 16:52:17,807 | INFO |  -9.78 * 0.5 =  -4.89 for decoder
2026-01-28 16:52:17,807 | INFO |  -9.48 * 0.5 =  -4.74 for ctc
2026-01-28 16:52:17,807 | INFO | total log probability: -9.63
2026-01-28 16:52:17,807 | INFO | normalized log probability: -0.18
2026-01-28 16:52:17,807 | INFO | total number of ended hypotheses: 172
2026-01-28 16:52:17,808 | INFO | best hypo: ▁et▁très▁tôt▁vous▁avez▁eu▁cette▁conviction▁profonde▁très▁tôt▁des▁disons▁de▁quinzee▁ans▁j'ai▁envie▁d'être

2026-01-28 16:52:17,810 | INFO | speech length: 73920
2026-01-28 16:52:17,858 | INFO | decoder input length: 115
2026-01-28 16:52:17,858 | INFO | max output length: 115
2026-01-28 16:52:17,858 | INFO | min output length: 11
2026-01-28 16:52:20,046 | INFO | end detected at 46
2026-01-28 16:52:20,047 | INFO |  -5.26 * 0.5 =  -2.63 for decoder
2026-01-28 16:52:20,047 | INFO |  -4.91 * 0.5 =  -2.45 for ctc
2026-01-28 16:52:20,048 | INFO | total log probability: -5.08
2026-01-28 16:52:20,048 | INFO | normalized log probability: -0.12
2026-01-28 16:52:20,048 | INFO | total number of ended hypotheses: 166
2026-01-28 16:52:20,048 | INFO | best hypo: ▁ma▁famille▁on▁a▁toujours▁fait▁de▁la▁littérature▁et▁de▁la▁recherche▁quelque▁chose▁d'essentiel

2026-01-28 16:52:20,050 | INFO | speech length: 46240
2026-01-28 16:52:20,091 | INFO | decoder input length: 71
2026-01-28 16:52:20,091 | INFO | max output length: 71
2026-01-28 16:52:20,091 | INFO | min output length: 7
2026-01-28 16:52:21,449 | INFO | end detected at 29
2026-01-28 16:52:21,451 | INFO |  -4.02 * 0.5 =  -2.01 for decoder
2026-01-28 16:52:21,451 | INFO |  -2.62 * 0.5 =  -1.31 for ctc
2026-01-28 16:52:21,451 | INFO | total log probability: -3.32
2026-01-28 16:52:21,451 | INFO | normalized log probability: -0.14
2026-01-28 16:52:21,451 | INFO | total number of ended hypotheses: 166
2026-01-28 16:52:21,451 | INFO | best hypo: ▁je▁suis▁pas▁en▁révolte▁avec▁mon▁environnement▁que▁je▁voulus

2026-01-28 16:52:21,453 | INFO | speech length: 194240
2026-01-28 16:52:21,502 | INFO | decoder input length: 303
2026-01-28 16:52:21,502 | INFO | max output length: 303
2026-01-28 16:52:21,502 | INFO | min output length: 30
2026-01-28 16:52:26,481 | INFO | end detected at 69
2026-01-28 16:52:26,484 | INFO | -24.98 * 0.5 = -12.49 for decoder
2026-01-28 16:52:26,484 | INFO |  -7.56 * 0.5 =  -3.78 for ctc
2026-01-28 16:52:26,484 | INFO | total log probability: -16.27
2026-01-28 16:52:26,484 | INFO | normalized log probability: -0.25
2026-01-28 16:52:26,484 | INFO | total number of ended hypotheses: 180
2026-01-28 16:52:26,485 | INFO | best hypo: ▁gay▁était▁le▁le▁poids▁de▁de▁la▁religion▁de▁votre▁communauté▁culturelle▁des▁et▁la▁langue▁maternelle▁de▁mon▁père▁et▁la▁langue▁maternelle▁de▁ma▁mère▁était▁l'arabe

2026-01-28 16:52:26,487 | INFO | speech length: 16640
2026-01-28 16:52:26,539 | INFO | decoder input length: 25
2026-01-28 16:52:26,539 | INFO | max output length: 25
2026-01-28 16:52:26,539 | INFO | min output length: 2
2026-01-28 16:52:27,132 | INFO | end detected at 15
2026-01-28 16:52:27,133 | INFO |  -1.19 * 0.5 =  -0.60 for decoder
2026-01-28 16:52:27,133 | INFO |  -3.75 * 0.5 =  -1.88 for ctc
2026-01-28 16:52:27,133 | INFO | total log probability: -2.47
2026-01-28 16:52:27,133 | INFO | normalized log probability: -0.25
2026-01-28 16:52:27,133 | INFO | total number of ended hypotheses: 169
2026-01-28 16:52:27,134 | INFO | best hypo: ▁même▁s'il▁parlait▁elle

2026-01-28 16:52:27,135 | INFO | speech length: 46240
2026-01-28 16:52:27,172 | INFO | decoder input length: 71
2026-01-28 16:52:27,172 | INFO | max output length: 71
2026-01-28 16:52:27,172 | INFO | min output length: 7
2026-01-28 16:52:28,882 | INFO | end detected at 34
2026-01-28 16:52:28,886 | INFO |  -2.74 * 0.5 =  -1.37 for decoder
2026-01-28 16:52:28,887 | INFO |  -5.32 * 0.5 =  -2.66 for ctc
2026-01-28 16:52:28,887 | INFO | total log probability: -4.03
2026-01-28 16:52:28,887 | INFO | normalized log probability: -0.15
2026-01-28 16:52:28,887 | INFO | total number of ended hypotheses: 207
2026-01-28 16:52:28,888 | INFO | best hypo: ▁parfaitement▁sans▁accent▁sans▁même▁aucun▁accent▁pied▁noir

2026-01-28 16:52:28,891 | INFO | speech length: 16000
2026-01-28 16:52:29,000 | INFO | decoder input length: 24
2026-01-28 16:52:29,000 | INFO | max output length: 24
2026-01-28 16:52:29,000 | INFO | min output length: 2
2026-01-28 16:52:29,572 | INFO | end detected at 15
2026-01-28 16:52:29,573 | INFO |  -0.94 * 0.5 =  -0.47 for decoder
2026-01-28 16:52:29,574 | INFO |  -1.11 * 0.5 =  -0.56 for ctc
2026-01-28 16:52:29,574 | INFO | total log probability: -1.02
2026-01-28 16:52:29,574 | INFO | normalized log probability: -0.09
2026-01-28 16:52:29,574 | INFO | total number of ended hypotheses: 148
2026-01-28 16:52:29,574 | INFO | best hypo: ▁entre▁eux▁ils▁parlaient

2026-01-28 16:52:29,575 | INFO | speech length: 169600
2026-01-28 16:52:29,630 | INFO | decoder input length: 264
2026-01-28 16:52:29,630 | INFO | max output length: 264
2026-01-28 16:52:29,630 | INFO | min output length: 26
2026-01-28 16:52:36,011 | INFO | end detected at 99
2026-01-28 16:52:36,012 | INFO | -38.70 * 0.5 = -19.35 for decoder
2026-01-28 16:52:36,013 | INFO | -16.50 * 0.5 =  -8.25 for ctc
2026-01-28 16:52:36,013 | INFO | total log probability: -27.60
2026-01-28 16:52:36,013 | INFO | normalized log probability: -0.30
2026-01-28 16:52:36,013 | INFO | total number of ended hypotheses: 187
2026-01-28 16:52:36,014 | INFO | best hypo: ▁quelles▁étaient▁vos▁vos▁lectures▁enfants▁je▁lisais▁tout▁j'ai▁lu▁beaucoup▁beaucoup▁d'abord▁la▁littérature▁française▁du▁dix▁huitième▁du▁neuvième▁j'étais▁très▁impressionné▁par▁le▁théâtre▁de▁marivaux▁qui▁était

2026-01-28 16:52:36,017 | INFO | speech length: 156800
2026-01-28 16:52:36,068 | INFO | decoder input length: 244
2026-01-28 16:52:36,068 | INFO | max output length: 244
2026-01-28 16:52:36,069 | INFO | min output length: 24
2026-01-28 16:52:43,425 | INFO | end detected at 99
2026-01-28 16:52:43,428 | INFO | -39.65 * 0.5 = -19.82 for decoder
2026-01-28 16:52:43,428 | INFO | -17.51 * 0.5 =  -8.76 for ctc
2026-01-28 16:52:43,428 | INFO | total log probability: -28.58
2026-01-28 16:52:43,428 | INFO | normalized log probability: -0.31
2026-01-28 16:52:43,428 | INFO | total number of ended hypotheses: 205
2026-01-28 16:52:43,430 | INFO | best hypo: ▁pourtant▁pour▁moi▁et▁puis▁je▁me▁suis▁mis▁un▁peu▁plus▁tardvers▁dix▁huit▁ans▁à▁découvrir▁la▁littérature▁étrangère▁c'était▁une▁partie▁assez▁secondaire▁comme▁un▁demande▁d'activité▁parce▁que▁je▁fais▁beaucoup▁de▁piano

2026-01-28 16:52:43,434 | INFO | speech length: 16320
2026-01-28 16:52:43,487 | INFO | decoder input length: 25
2026-01-28 16:52:43,487 | INFO | max output length: 25
2026-01-28 16:52:43,487 | INFO | min output length: 2
2026-01-28 16:52:44,198 | INFO | end detected at 16
2026-01-28 16:52:44,200 | INFO |  -2.93 * 0.5 =  -1.46 for decoder
2026-01-28 16:52:44,200 | INFO |  -1.84 * 0.5 =  -0.92 for ctc
2026-01-28 16:52:44,200 | INFO | total log probability: -2.38
2026-01-28 16:52:44,200 | INFO | normalized log probability: -0.22
2026-01-28 16:52:44,200 | INFO | total number of ended hypotheses: 160
2026-01-28 16:52:44,200 | INFO | best hypo: ▁énormément▁de▁musique

2026-01-28 16:52:44,202 | INFO | speech length: 39360
2026-01-28 16:52:44,248 | INFO | decoder input length: 61
2026-01-28 16:52:44,248 | INFO | max output length: 61
2026-01-28 16:52:44,248 | INFO | min output length: 6
2026-01-28 16:52:45,089 | INFO | end detected at 18
2026-01-28 16:52:45,090 | INFO |  -2.73 * 0.5 =  -1.36 for decoder
2026-01-28 16:52:45,090 | INFO |  -2.34 * 0.5 =  -1.17 for ctc
2026-01-28 16:52:45,090 | INFO | total log probability: -2.53
2026-01-28 16:52:45,090 | INFO | normalized log probability: -0.19
2026-01-28 16:52:45,090 | INFO | total number of ended hypotheses: 166
2026-01-28 16:52:45,091 | INFO | best hypo: ▁et▁mon▁idée▁c'était▁de

2026-01-28 16:52:45,092 | INFO | speech length: 88480
2026-01-28 16:52:45,129 | INFO | decoder input length: 137
2026-01-28 16:52:45,129 | INFO | max output length: 137
2026-01-28 16:52:45,129 | INFO | min output length: 13
2026-01-28 16:52:48,328 | INFO | end detected at 62
2026-01-28 16:52:48,329 | INFO |  -5.86 * 0.5 =  -2.93 for decoder
2026-01-28 16:52:48,329 | INFO |  -8.48 * 0.5 =  -4.24 for ctc
2026-01-28 16:52:48,329 | INFO | total log probability: -7.17
2026-01-28 16:52:48,329 | INFO | normalized log probability: -0.13
2026-01-28 16:52:48,329 | INFO | total number of ended hypotheses: 180
2026-01-28 16:52:48,330 | INFO | best hypo: ▁faire▁des▁études▁de▁mathématiques▁puisque▁c'était▁le▁domaine▁dans▁lequel▁je▁me▁trouvais▁être▁le▁plus▁doué▁pourra▁ensuite▁revenir▁le▁littéraire

2026-01-28 16:52:48,333 | INFO | speech length: 168960
2026-01-28 16:52:48,376 | INFO | decoder input length: 263
2026-01-28 16:52:48,376 | INFO | max output length: 263
2026-01-28 16:52:48,376 | INFO | min output length: 26
2026-01-28 16:52:55,182 | INFO | end detected at 100
2026-01-28 16:52:55,184 | INFO | -88.09 * 0.5 = -44.05 for decoder
2026-01-28 16:52:55,184 | INFO | -54.90 * 0.5 = -27.45 for ctc
2026-01-28 16:52:55,184 | INFO | total log probability: -71.50
2026-01-28 16:52:55,184 | INFO | normalized log probability: -0.78
2026-01-28 16:52:55,184 | INFO | total number of ended hypotheses: 186
2026-01-28 16:52:55,185 | INFO | best hypo: ▁c'était▁une▁enfance▁heureuse▁oui▁très▁heureuse▁déjà▁au▁lycée▁vous▁distinguez▁par▁vos▁talents▁intellectuels▁non▁je▁j'avais▁la▁réputation▁d'être▁très▁bon▁élèves▁et▁j'ai▁pas▁à▁trop

2026-01-28 16:52:55,188 | INFO | speech length: 85600
2026-01-28 16:52:55,227 | INFO | decoder input length: 133
2026-01-28 16:52:55,227 | INFO | max output length: 133
2026-01-28 16:52:55,227 | INFO | min output length: 13
2026-01-28 16:52:58,065 | INFO | end detected at 56
2026-01-28 16:52:58,066 | INFO | -16.94 * 0.5 =  -8.47 for decoder
2026-01-28 16:52:58,066 | INFO | -21.36 * 0.5 = -10.68 for ctc
2026-01-28 16:52:58,066 | INFO | total log probability: -19.15
2026-01-28 16:52:58,066 | INFO | normalized log probability: -0.39
2026-01-28 16:52:58,066 | INFO | total number of ended hypotheses: 171
2026-01-28 16:52:58,067 | INFO | best hypo: ▁je▁veux▁la▁réputation▁d'être▁douée▁mais▁je▁faisais▁pas▁grand▁chose▁j'étais▁toujours▁les▁premiers▁de▁la▁classe▁mais

2026-01-28 16:52:58,069 | INFO | speech length: 289119
2026-01-28 16:52:58,131 | INFO | decoder input length: 451
2026-01-28 16:52:58,131 | INFO | max output length: 451
2026-01-28 16:52:58,131 | INFO | min output length: 45
2026-01-28 16:53:14,350 | INFO | end detected at 163
2026-01-28 16:53:14,352 | INFO | -389.71 * 0.5 = -194.86 for decoder
2026-01-28 16:53:14,352 | INFO | -151.37 * 0.5 = -75.68 for ctc
2026-01-28 16:53:14,352 | INFO | total log probability: -270.54
2026-01-28 16:53:14,352 | INFO | normalized log probability: -1.75
2026-01-28 16:53:14,352 | INFO | total number of ended hypotheses: 173
2026-01-28 16:53:14,354 | INFO | best hypo: ▁y▁a▁leur▁donc▁aller▁faire▁toutes▁les▁plus▁grandes▁écoles▁françaises▁polytechniques▁les▁nains▁l'école▁d'émie▁alors▁pourquoi▁se▁diriger▁vers▁ces▁grandes▁écoles▁qu'elle▁a▁le▁projet▁est▁à▁l'époque▁oi▁de▁ne▁pas▁faire▁la▁soologie▁de▁l'histoire▁ou▁de▁la▁ma▁cher▁dans▁ma▁génération▁c'était▁à▁peu▁près▁vidente▁ca▁onnibourramale▁de▁ne▁pas▁gaspe

2026-01-28 16:53:14,357 | INFO | speech length: 50240
2026-01-28 16:53:14,398 | INFO | decoder input length: 78
2026-01-28 16:53:14,398 | INFO | max output length: 78
2026-01-28 16:53:14,398 | INFO | min output length: 7
2026-01-28 16:53:15,706 | INFO | end detected at 30
2026-01-28 16:53:15,707 | INFO |  -4.58 * 0.5 =  -2.29 for decoder
2026-01-28 16:53:15,708 | INFO |  -6.06 * 0.5 =  -3.03 for ctc
2026-01-28 16:53:15,708 | INFO | total log probability: -5.32
2026-01-28 16:53:15,708 | INFO | normalized log probability: -0.21
2026-01-28 16:53:15,708 | INFO | total number of ended hypotheses: 165
2026-01-28 16:53:15,708 | INFO | best hypo: ▁il▁se▁trouve▁qu'à▁partir▁de▁la▁matelem▁je▁me▁suis▁trouvé

2026-01-28 16:53:15,710 | INFO | speech length: 218400
2026-01-28 16:53:15,776 | INFO | decoder input length: 340
2026-01-28 16:53:15,776 | INFO | max output length: 340
2026-01-28 16:53:15,776 | INFO | min output length: 34
2026-01-28 16:53:25,813 | INFO | end detected at 138
2026-01-28 16:53:25,814 | INFO | -157.92 * 0.5 = -78.96 for decoder
2026-01-28 16:53:25,814 | INFO | -43.56 * 0.5 = -21.78 for ctc
2026-01-28 16:53:25,814 | INFO | total log probability: -100.74
2026-01-28 16:53:25,814 | INFO | normalized log probability: -0.76
2026-01-28 16:53:25,814 | INFO | total number of ended hypotheses: 168
2026-01-28 16:53:25,816 | INFO | best hypo: ▁très▁bon▁matin▁et▁donc▁pas▁question▁de▁gaspillier▁stalan▁quitta▁à▁savoir▁très▁vite▁dès▁ce▁moment▁là▁que▁j'offrai▁pas▁un▁métier▁de▁mathématiques▁je▁serais▁jamais▁ingénieur▁mais▁je▁voulais▁aller▁à▁bout▁au▁bout▁de▁sa▁voix▁ni▁puis▁j'avais▁toujours▁à▁présu▁à▁l'esprit▁lewis▁carroll

2026-01-28 16:53:25,819 | INFO | speech length: 190080
2026-01-28 16:53:25,870 | INFO | decoder input length: 296
2026-01-28 16:53:25,870 | INFO | max output length: 296
2026-01-28 16:53:25,871 | INFO | min output length: 29
2026-01-28 16:53:32,888 | INFO | end detected at 100
2026-01-28 16:53:32,891 | INFO | -67.98 * 0.5 = -33.99 for decoder
2026-01-28 16:53:32,891 | INFO | -13.37 * 0.5 =  -6.68 for ctc
2026-01-28 16:53:32,891 | INFO | total log probability: -40.67
2026-01-28 16:53:32,891 | INFO | normalized log probability: -0.43
2026-01-28 16:53:32,891 | INFO | total number of ended hypotheses: 181
2026-01-28 16:53:32,892 | INFO | best hypo: ▁mathématicien▁écrivain▁et▁en▁même▁temps▁je▁voyais▁les▁mathématiques▁comme▁ça▁c'est▁à▁dire▁comme▁l'autre▁côté▁du▁miroir▁qui▁fait▁découvrir▁un▁univers▁fantastique▁et▁mathématique▁je▁vous▁les▁aiz▁vu▁toujours▁comme▁très

2026-01-28 16:53:32,895 | INFO | speech length: 12160
2026-01-28 16:53:32,937 | INFO | decoder input length: 18
2026-01-28 16:53:32,938 | INFO | max output length: 18
2026-01-28 16:53:32,938 | INFO | min output length: 1
2026-01-28 16:53:33,290 | INFO | end detected at 9
2026-01-28 16:53:33,292 | INFO |  -0.68 * 0.5 =  -0.34 for decoder
2026-01-28 16:53:33,292 | INFO |  -0.10 * 0.5 =  -0.05 for ctc
2026-01-28 16:53:33,292 | INFO | total log probability: -0.39
2026-01-28 16:53:33,292 | INFO | normalized log probability: -0.08
2026-01-28 16:53:33,292 | INFO | total number of ended hypotheses: 142
2026-01-28 16:53:33,292 | INFO | best hypo: ▁poétique

2026-01-28 16:53:33,294 | INFO | speech length: 239040
2026-01-28 16:53:33,359 | INFO | decoder input length: 373
2026-01-28 16:53:33,359 | INFO | max output length: 373
2026-01-28 16:53:33,359 | INFO | min output length: 37
2026-01-28 16:53:47,813 | INFO | end detected at 132
2026-01-28 16:53:47,814 | INFO | -245.75 * 0.5 = -122.87 for decoder
2026-01-28 16:53:47,814 | INFO | -118.13 * 0.5 = -59.06 for ctc
2026-01-28 16:53:47,814 | INFO | total log probability: -181.94
2026-01-28 16:53:47,814 | INFO | normalized log probability: -1.42
2026-01-28 16:53:47,814 | INFO | total number of ended hypotheses: 144
2026-01-28 16:53:47,816 | INFO | best hypo: ▁et▁est▁ce▁qui▁à▁ce▁moment▁là▁un▁projet▁de▁de▁faire▁quelques▁suje'avais▁non▁je▁suis▁un▁intellectuel▁je▁l'ai▁toujours▁été▁j'ai▁jamais▁eu▁envie▁de▁faire▁une▁carrière▁politique▁j'ai▁boujours▁comme▁envie▁et▁de▁faire▁évancers▁le▁monde▁mes▁idée▁qui▁'a▁les▁mettre▁en▁oeuvre▁moi▁même

2026-01-28 16:53:47,833 | INFO | Chunk: 0 | WER=18.421053 | S=10 D=2 I=2
2026-01-28 16:53:47,835 | INFO | Chunk: 1 | WER=9.523810 | S=3 D=1 I=0
2026-01-28 16:53:47,841 | INFO | Chunk: 2 | WER=16.455696 | S=9 D=0 I=4
2026-01-28 16:53:47,845 | INFO | Chunk: 3 | WER=31.428571 | S=12 D=4 I=6
2026-01-28 16:53:47,846 | INFO | Chunk: 4 | WER=33.333333 | S=1 D=0 I=1
2026-01-28 16:53:47,846 | INFO | Chunk: 5 | WER=5.000000 | S=0 D=1 I=0
2026-01-28 16:53:47,848 | INFO | Chunk: 6 | WER=24.390244 | S=3 D=7 I=0
2026-01-28 16:53:47,849 | INFO | Chunk: 7 | WER=11.111111 | S=1 D=0 I=1
2026-01-28 16:53:47,849 | INFO | Chunk: 8 | WER=14.285714 | S=2 D=0 I=1
2026-01-28 16:53:47,850 | INFO | Chunk: 9 | WER=14.285714 | S=1 D=0 I=0
2026-01-28 16:53:47,850 | INFO | Chunk: 10 | WER=23.076923 | S=3 D=0 I=0
2026-01-28 16:53:47,851 | INFO | Chunk: 11 | WER=8.333333 | S=0 D=2 I=0
2026-01-28 16:53:47,851 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 16:53:47,852 | INFO | Chunk: 13 | WER=28.571429 | S=2 D=2 I=2
2026-01-28 16:53:47,853 | INFO | Chunk: 14 | WER=5.555556 | S=0 D=1 I=0
2026-01-28 16:53:47,853 | INFO | Chunk: 15 | WER=46.153846 | S=4 D=2 I=0
2026-01-28 16:53:47,855 | INFO | Chunk: 16 | WER=27.027027 | S=4 D=6 I=0
2026-01-28 16:53:47,855 | INFO | Chunk: 17 | WER=75.000000 | S=2 D=0 I=1
2026-01-28 16:53:47,855 | INFO | Chunk: 18 | WER=10.000000 | S=0 D=1 I=0
2026-01-28 16:53:47,855 | INFO | Chunk: 19 | WER=33.333333 | S=0 D=0 I=1
2026-01-28 16:53:47,857 | INFO | Chunk: 20 | WER=8.333333 | S=2 D=1 I=0
2026-01-28 16:53:47,859 | INFO | Chunk: 21 | WER=27.272727 | S=7 D=5 I=0
2026-01-28 16:53:47,859 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 16:53:47,860 | INFO | Chunk: 23 | WER=50.000000 | S=0 D=3 I=1
2026-01-28 16:53:47,861 | INFO | Chunk: 24 | WER=16.000000 | S=3 D=1 I=0
2026-01-28 16:53:47,862 | INFO | Chunk: 25 | WER=27.027027 | S=3 D=5 I=2
2026-01-28 16:53:47,863 | INFO | Chunk: 26 | WER=29.166667 | S=3 D=3 I=1
2026-01-28 16:53:47,867 | INFO | Chunk: 27 | WER=41.791045 | S=17 D=6 I=5
2026-01-28 16:53:47,868 | INFO | Chunk: 28 | WER=33.333333 | S=1 D=5 I=0
2026-01-28 16:53:47,871 | INFO | Chunk: 29 | WER=26.785714 | S=11 D=3 I=1
2026-01-28 16:53:47,873 | INFO | Chunk: 30 | WER=15.384615 | S=5 D=1 I=0
2026-01-28 16:53:47,873 | INFO | Chunk: 31 | WER=100.000000 | S=1 D=0 I=0
2026-01-28 16:53:47,876 | INFO | Chunk: 32 | WER=31.147541 | S=9 D=7 I=3
2026-01-28 16:53:48,516 | INFO | File: Rhap-D2005.wav | WER=23.011665 | S=122 D=66 I=29
2026-01-28 16:53:48,516 | INFO | ------------------------------
2026-01-28 16:53:48,516 | INFO | Conf cv Done!
2026-01-28 16:53:48,724 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 16:53:48,753 | INFO | Vocabulary size: 47
2026-01-28 16:53:49,646 | INFO | Gradient checkpoint layers: []
2026-01-28 16:53:50,435 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 16:53:50,439 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 16:53:50,440 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 16:53:50,440 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 16:53:50,443 | INFO | speech length: 371520
2026-01-28 16:53:50,495 | INFO | decoder input length: 580
2026-01-28 16:53:50,495 | INFO | max output length: 580
2026-01-28 16:53:50,495 | INFO | min output length: 58
2026-01-28 16:54:29,245 | INFO | end detected at 444
2026-01-28 16:54:29,246 | INFO | -158.96 * 0.5 = -79.48 for decoder
2026-01-28 16:54:29,246 | INFO | -21.87 * 0.5 = -10.93 for ctc
2026-01-28 16:54:29,247 | INFO | total log probability: -90.42
2026-01-28 16:54:29,247 | INFO | normalized log probability: -0.21
2026-01-28 16:54:29,247 | INFO | total number of ended hypotheses: 175
2026-01-28 16:54:29,252 | INFO | best hypo: écrivain<space>conseiller<space>spécial<space>du<space>président<space>françois<space>mitterrand<space>banquier<space>du<space>monde<space>jacques<space>attali<space>a<space>usé<space>de<space>tous<space>les<space>pouvoirs<space>dans<space>les<space>coulisses<space>de<space>l'histoire<space>il<space>s'est<space>voulu<space>comme<space>un<space>homme<space>d'idées<space>un<space>homme<space>de<space>concept<space>ces<space>talents<space>séduisent<space>un<space>large<space>public<space>considéré<space>par<space>certains<space>comme<space>un<space>des<space>esprits<space>les<space>plus<space>vifs<space>il<space>est<space>l'auteur<space>de<space>nombreux<space>bescelleurs<space>comme<space>mille<space>quatre<space>cent<space>quatre<space>vingt<space>douze<space>où<space>le<space>dictionnaire<space>de<space>vingt<space>et<space>unième<space>siècle

2026-01-28 16:54:29,255 | INFO | speech length: 220800
2026-01-28 16:54:29,293 | INFO | decoder input length: 344
2026-01-28 16:54:29,293 | INFO | max output length: 344
2026-01-28 16:54:29,293 | INFO | min output length: 34
2026-01-28 16:54:44,904 | INFO | end detected at 238
2026-01-28 16:54:44,906 | INFO | -20.39 * 0.5 = -10.19 for decoder
2026-01-28 16:54:44,906 | INFO |  -3.47 * 0.5 =  -1.74 for ctc
2026-01-28 16:54:44,906 | INFO | total log probability: -11.93
2026-01-28 16:54:44,906 | INFO | normalized log probability: -0.05
2026-01-28 16:54:44,906 | INFO | total number of ended hypotheses: 173
2026-01-28 16:54:44,909 | INFO | best hypo: perçu<space>comme<space>un<space>surdoué<space>de<space>sa<space>génération<space>son<space>action<space>s'étend<space>aux<space>allées<space>du<space>pouvoir<space>il<space>devient<space>le<space>cherpa<space>du<space>président<space>le<space>porteur<space>de<space>valises<space>le<space>conseiller<space>influent<space>du<space>prince<space>qui<space>négocie<space>au<space>plus<space>haut<space>niveau<space>avec<space>les<space>hommes<space>et<space>les<space>états

2026-01-28 16:54:44,911 | INFO | speech length: 443200
2026-01-28 16:54:44,948 | INFO | decoder input length: 692
2026-01-28 16:54:44,948 | INFO | max output length: 692
2026-01-28 16:54:44,948 | INFO | min output length: 69
2026-01-28 16:55:34,903 | INFO | end detected at 442
2026-01-28 16:55:34,906 | INFO | -266.53 * 0.5 = -133.27 for decoder
2026-01-28 16:55:34,906 | INFO |  -2.11 * 0.5 =  -1.06 for ctc
2026-01-28 16:55:34,906 | INFO | total log probability: -134.32
2026-01-28 16:55:34,906 | INFO | normalized log probability: -0.31
2026-01-28 16:55:34,906 | INFO | total number of ended hypotheses: 189
2026-01-28 16:55:34,913 | INFO | best hypo: devenu<space>l'acteur<space>principal<space>de<space>ces<space>idées<space>il<space>s'expose<space>directement<space>comme<space>concepteur<space>et<space>président<space>d'une<space>banque<space>internationale<space>pour<space>la<space>reconstruction<space>d'un<space>monde<space>qui<space>vacille<space>euh<space>la<space>russie<space>et<space>le<space>bloc<space>de<space>l'est<space>les<space>enjeux<space>sont<space>considérables<space>et<space>la<space>tourmente<space>qu'il<space>génère<space>va<space>l'emporter<space>il<space>démissionne<space>et<space>se<space>retire<space>aujourd'hui<space>il<space>continue<space>son<space>activité<space>de<space>conseil<space>et<space>se<space>consacre<space>toujours<space>à<space>l'écriture<space>et<space>à<space>l'histoire<space>qui<space>ne<space>l'a<space>pas<space>toujours<space>reconnue

2026-01-28 16:55:34,916 | INFO | speech length: 431520
2026-01-28 16:55:34,965 | INFO | decoder input length: 673
2026-01-28 16:55:34,965 | INFO | max output length: 673
2026-01-28 16:55:34,965 | INFO | min output length: 67
2026-01-28 16:56:21,935 | INFO | end detected at 405
2026-01-28 16:56:21,938 | INFO | -265.66 * 0.5 = -132.83 for decoder
2026-01-28 16:56:21,938 | INFO | -37.55 * 0.5 = -18.77 for ctc
2026-01-28 16:56:21,938 | INFO | total log probability: -151.61
2026-01-28 16:56:21,938 | INFO | normalized log probability: -0.39
2026-01-28 16:56:21,939 | INFO | total number of ended hypotheses: 211
2026-01-28 16:56:21,944 | INFO | best hypo: je<space>n'ai<space>à<space>alger<space>en<space>mille<space>neuf<space>cent<space>quarante<space>trois<space>donc<space>euh<space>j'ai<space>connu<space>l'algérie<space>heureuse<space>je<space>suis<space>en<space>premier<space>novembre<space>hein<space>donc<space>euh<space>mon<space>anniversaire<space>coïncide<space>avec<space>le<space>début<space>de<space>l'insurrection<space>algérienne<space>et<space>j'ai<space>été<space>d'une<space>famille<space>très<space>unie<space>très<space>aisée<space>euh<space>mon<space>père<space>était<space>notre<space>didacte<space>venant<space>dans<space>ma<space>milieu<space>très<space>pauvre<space>et<space>en<space>fait<space>aucune<space>étude<space>mais<space>ayant<space>une<space>passion<space>de<space>la<space>littérature<space>du<space>savoir<space>et

2026-01-28 16:56:21,948 | INFO | speech length: 32000
2026-01-28 16:56:21,995 | INFO | decoder input length: 49
2026-01-28 16:56:21,996 | INFO | max output length: 49
2026-01-28 16:56:21,996 | INFO | min output length: 4
2026-01-28 16:56:23,932 | INFO | adding <eos> in the last position in the loop
2026-01-28 16:56:23,942 | INFO | no hypothesis. Finish decoding.
2026-01-28 16:56:23,943 | INFO |  -6.53 * 0.5 =  -3.27 for decoder
2026-01-28 16:56:23,943 | INFO |  -4.97 * 0.5 =  -2.49 for ctc
2026-01-28 16:56:23,943 | INFO | total log probability: -5.75
2026-01-28 16:56:23,943 | INFO | normalized log probability: -0.12
2026-01-28 16:56:23,943 | INFO | total number of ended hypotheses: 87
2026-01-28 16:56:23,944 | INFO | best hypo: qui<space>a<space>cumulé<space>un<space>énorme<space>savoir<space>d'autres<space>didactes

2026-01-28 16:56:23,945 | INFO | speech length: 68960
2026-01-28 16:56:23,990 | INFO | decoder input length: 107
2026-01-28 16:56:23,990 | INFO | max output length: 107
2026-01-28 16:56:23,990 | INFO | min output length: 10
2026-01-28 16:56:28,546 | INFO | end detected at 100
2026-01-28 16:56:28,547 | INFO |  -9.63 * 0.5 =  -4.82 for decoder
2026-01-28 16:56:28,547 | INFO |  -3.84 * 0.5 =  -1.92 for ctc
2026-01-28 16:56:28,547 | INFO | total log probability: -6.73
2026-01-28 16:56:28,547 | INFO | normalized log probability: -0.07
2026-01-28 16:56:28,547 | INFO | total number of ended hypotheses: 177
2026-01-28 16:56:28,548 | INFO | best hypo: et<space>j'ai<space>vécu<space>avec<space>mon<space>frère<space>je<space>mot<space>et<space>ma<space>soeur<space>dans<space>dans<space>une<space>ambiance<space>où<space>le<space>livre<space>était<space>sacré

2026-01-28 16:56:28,550 | INFO | speech length: 180960
2026-01-28 16:56:28,586 | INFO | decoder input length: 282
2026-01-28 16:56:28,586 | INFO | max output length: 282
2026-01-28 16:56:28,586 | INFO | min output length: 28
2026-01-28 16:56:40,255 | INFO | end detected at 202
2026-01-28 16:56:40,256 | INFO | -21.26 * 0.5 = -10.63 for decoder
2026-01-28 16:56:40,257 | INFO |  -7.68 * 0.5 =  -3.84 for ctc
2026-01-28 16:56:40,257 | INFO | total log probability: -14.47
2026-01-28 16:56:40,257 | INFO | normalized log probability: -0.07
2026-01-28 16:56:40,257 | INFO | total number of ended hypotheses: 157
2026-01-28 16:56:40,259 | INFO | best hypo: et<space>où<space>euh<space>le<space>travail<space>était<space>la<space>moindre<space>des<space>choses<space>nous<space>sommes<space>venus<space>en<space>france<space>euh<space>en<space>mille<space>neuf<space>cent<space>cinquante<space>six<space>j'avais<space>douze<space>ans<space>et<space>je<space>suis<space>entré<space>au<space>fait<space>mes<space>études<space>au<space>janson<space>de<space>seilly<space>à<space>paris

2026-01-28 16:56:40,261 | INFO | speech length: 85280
2026-01-28 16:56:40,302 | INFO | decoder input length: 132
2026-01-28 16:56:40,302 | INFO | max output length: 132
2026-01-28 16:56:40,302 | INFO | min output length: 13
2026-01-28 16:56:45,018 | INFO | end detected at 101
2026-01-28 16:56:45,020 | INFO |  -7.96 * 0.5 =  -3.98 for decoder
2026-01-28 16:56:45,020 | INFO |  -0.76 * 0.5 =  -0.38 for ctc
2026-01-28 16:56:45,020 | INFO | total log probability: -4.36
2026-01-28 16:56:45,020 | INFO | normalized log probability: -0.05
2026-01-28 16:56:45,020 | INFO | total number of ended hypotheses: 183
2026-01-28 16:56:45,021 | INFO | best hypo: et<space>avec<space>une<space>euh<space>ambition<space>qui<space>commençait<space>à<space>se<space>cristalliser<space>déjà<space>autour<space>de<space>l'idée<space>d'écrire<space>et<space>de

2026-01-28 16:56:45,024 | INFO | speech length: 107040
2026-01-28 16:56:45,060 | INFO | decoder input length: 166
2026-01-28 16:56:45,060 | INFO | max output length: 166
2026-01-28 16:56:45,061 | INFO | min output length: 16
2026-01-28 16:56:50,407 | INFO | end detected at 114
2026-01-28 16:56:50,409 | INFO |  -8.60 * 0.5 =  -4.30 for decoder
2026-01-28 16:56:50,409 | INFO |  -0.43 * 0.5 =  -0.21 for ctc
2026-01-28 16:56:50,409 | INFO | total log probability: -4.51
2026-01-28 16:56:50,409 | INFO | normalized log probability: -0.04
2026-01-28 16:56:50,409 | INFO | total number of ended hypotheses: 179
2026-01-28 16:56:50,411 | INFO | best hypo: chercher<space>j'ai<space>toujours<space>pensé<space>qu'on<space>n'est<space>pas<space>sur<space>terre<space>pour<space>être<space>heureux<space>mais<space>pour<space>euh<space>essayer<space>d'être<space>utile

2026-01-28 16:56:50,413 | INFO | speech length: 24640
2026-01-28 16:56:50,465 | INFO | decoder input length: 38
2026-01-28 16:56:50,465 | INFO | max output length: 38
2026-01-28 16:56:50,465 | INFO | min output length: 3
2026-01-28 16:56:52,616 | INFO | adding <eos> in the last position in the loop
2026-01-28 16:56:52,631 | INFO | no hypothesis. Finish decoding.
2026-01-28 16:56:52,632 | INFO |  -2.74 * 0.5 =  -1.37 for decoder
2026-01-28 16:56:52,632 | INFO |  -0.04 * 0.5 =  -0.02 for ctc
2026-01-28 16:56:52,633 | INFO | total log probability: -1.39
2026-01-28 16:56:52,633 | INFO | normalized log probability: -0.04
2026-01-28 16:56:52,633 | INFO | total number of ended hypotheses: 137
2026-01-28 16:56:52,634 | INFO | best hypo: le<space>bonheur<space>n'est<space>pas<space>mon<space>problème

2026-01-28 16:56:52,636 | INFO | speech length: 44640
2026-01-28 16:56:52,689 | INFO | decoder input length: 69
2026-01-28 16:56:52,689 | INFO | max output length: 69
2026-01-28 16:56:52,690 | INFO | min output length: 6
2026-01-28 16:56:56,888 | INFO | adding <eos> in the last position in the loop
2026-01-28 16:56:56,901 | INFO | no hypothesis. Finish decoding.
2026-01-28 16:56:56,902 | INFO | -18.67 * 0.5 =  -9.33 for decoder
2026-01-28 16:56:56,902 | INFO | -22.87 * 0.5 = -11.43 for ctc
2026-01-28 16:56:56,902 | INFO | total log probability: -20.77
2026-01-28 16:56:56,903 | INFO | normalized log probability: -0.31
2026-01-28 16:56:56,903 | INFO | total number of ended hypotheses: 78
2026-01-28 16:56:56,904 | INFO | best hypo: servir<space>à<space>quelque<space>chose<space>et<space>quelue<space>chose<space>qui<space>me<space>préoce<space>tout<space>le<space>temps

2026-01-28 16:56:56,907 | INFO | speech length: 135360
2026-01-28 16:56:56,955 | INFO | decoder input length: 211
2026-01-28 16:56:56,955 | INFO | max output length: 211
2026-01-28 16:56:56,955 | INFO | min output length: 21
2026-01-28 16:57:04,500 | INFO | end detected at 138
2026-01-28 16:57:04,501 | INFO | -11.01 * 0.5 =  -5.50 for decoder
2026-01-28 16:57:04,501 | INFO |  -0.68 * 0.5 =  -0.34 for ctc
2026-01-28 16:57:04,502 | INFO | total log probability: -5.84
2026-01-28 16:57:04,502 | INFO | normalized log probability: -0.04
2026-01-28 16:57:04,502 | INFO | total number of ended hypotheses: 179
2026-01-28 16:57:04,504 | INFO | best hypo: et<space>faire<space>avancer<space>le<space>savoir<space>ou<space>faire<space>avancer<space>la<space>littérature<space>ou<space>créer<space>des<space>des<space>choses<space>qui<space>durent<space>euh<space>qui<space>servent<space>euh<space>m'a<space>paru<space>être<space>euh

2026-01-28 16:57:04,506 | INFO | speech length: 9600
2026-01-28 16:57:04,536 | INFO | decoder input length: 14
2026-01-28 16:57:04,536 | INFO | max output length: 14
2026-01-28 16:57:04,536 | INFO | min output length: 1
2026-01-28 16:57:04,976 | INFO | adding <eos> in the last position in the loop
2026-01-28 16:57:04,985 | INFO | no hypothesis. Finish decoding.
2026-01-28 16:57:04,987 | INFO |  -1.81 * 0.5 =  -0.90 for decoder
2026-01-28 16:57:04,987 | INFO |  -0.58 * 0.5 =  -0.29 for ctc
2026-01-28 16:57:04,987 | INFO | total log probability: -1.19
2026-01-28 16:57:04,987 | INFO | normalized log probability: -0.11
2026-01-28 16:57:04,987 | INFO | total number of ended hypotheses: 126
2026-01-28 16:57:04,987 | INFO | best hypo: essentiel

2026-01-28 16:57:04,989 | INFO | speech length: 93760
2026-01-28 16:57:05,046 | INFO | decoder input length: 146
2026-01-28 16:57:05,047 | INFO | max output length: 146
2026-01-28 16:57:05,047 | INFO | min output length: 14
2026-01-28 16:57:14,323 | INFO | end detected at 118
2026-01-28 16:57:14,325 | INFO |  -9.70 * 0.5 =  -4.85 for decoder
2026-01-28 16:57:14,325 | INFO |  -0.86 * 0.5 =  -0.43 for ctc
2026-01-28 16:57:14,325 | INFO | total log probability: -5.28
2026-01-28 16:57:14,325 | INFO | normalized log probability: -0.05
2026-01-28 16:57:14,326 | INFO | total number of ended hypotheses: 178
2026-01-28 16:57:14,328 | INFO | best hypo: et<space>très<space>tôt<space>vous<space>avez<space>eu<space>cette<space>conviction<space>profonde<space>très<space>tôt<space>des<space>disons<space>de<space>quinze<space>seize<space>ans<space>j'ai<space>eu<space>envie<space>d'être

2026-01-28 16:57:14,332 | INFO | speech length: 73920
2026-01-28 16:57:14,387 | INFO | decoder input length: 115
2026-01-28 16:57:14,387 | INFO | max output length: 115
2026-01-28 16:57:14,387 | INFO | min output length: 11
2026-01-28 16:57:18,626 | INFO | end detected at 103
2026-01-28 16:57:18,628 | INFO |  -7.87 * 0.5 =  -3.93 for decoder
2026-01-28 16:57:18,628 | INFO |  -0.34 * 0.5 =  -0.17 for ctc
2026-01-28 16:57:18,628 | INFO | total log probability: -4.10
2026-01-28 16:57:18,628 | INFO | normalized log probability: -0.04
2026-01-28 16:57:18,628 | INFO | total number of ended hypotheses: 179
2026-01-28 16:57:18,630 | INFO | best hypo: ma<space>famille<space>on<space>a<space>toujours<space>fait<space>euh<space>de<space>la<space>littérature<space>et<space>de<space>la<space>recherche<space>quelque<space>chose<space>d'essentiel

2026-01-28 16:57:18,632 | INFO | speech length: 46240
2026-01-28 16:57:18,680 | INFO | decoder input length: 71
2026-01-28 16:57:18,680 | INFO | max output length: 71
2026-01-28 16:57:18,680 | INFO | min output length: 7
2026-01-28 16:57:21,422 | INFO | adding <eos> in the last position in the loop
2026-01-28 16:57:21,432 | INFO | end detected at 70
2026-01-28 16:57:21,433 | INFO |  -5.49 * 0.5 =  -2.74 for decoder
2026-01-28 16:57:21,434 | INFO |  -2.06 * 0.5 =  -1.03 for ctc
2026-01-28 16:57:21,434 | INFO | total log probability: -3.77
2026-01-28 16:57:21,434 | INFO | normalized log probability: -0.06
2026-01-28 16:57:21,434 | INFO | total number of ended hypotheses: 200
2026-01-28 16:57:21,435 | INFO | best hypo: j'ai<space>c'est<space>pas<space>en<space>révolte<space>avec<space>mon<space>environnement<space>que<space>j'ai<space>voulu

2026-01-28 16:57:21,437 | INFO | speech length: 194240
2026-01-28 16:57:21,478 | INFO | decoder input length: 303
2026-01-28 16:57:21,478 | INFO | max output length: 303
2026-01-28 16:57:21,479 | INFO | min output length: 30
2026-01-28 16:57:32,743 | INFO | end detected at 181
2026-01-28 16:57:32,745 | INFO | -15.48 * 0.5 =  -7.74 for decoder
2026-01-28 16:57:32,745 | INFO |  -1.95 * 0.5 =  -0.98 for ctc
2026-01-28 16:57:32,745 | INFO | total log probability: -8.72
2026-01-28 16:57:32,745 | INFO | normalized log probability: -0.05
2026-01-28 16:57:32,745 | INFO | total number of ended hypotheses: 172
2026-01-28 16:57:32,747 | INFO | best hypo: quel<space>a<space>été<space>le<space>le<space>le<space>poids<space>de<space>de<space>de<space>la<space>religion<space>de<space>votre<space>communauté<space>euh<space>culturelle<space>et<space>euh<space>et<space>la<space>langue<space>maternelle<space>de<space>mon<space>père<space>et<space>la<space>langue<space>maternelle<space>de<space>ma<space>mère<space>était<space>l'arabe

2026-01-28 16:57:32,750 | INFO | speech length: 16640
2026-01-28 16:57:32,792 | INFO | decoder input length: 25
2026-01-28 16:57:32,793 | INFO | max output length: 25
2026-01-28 16:57:32,793 | INFO | min output length: 2
2026-01-28 16:57:33,598 | INFO | adding <eos> in the last position in the loop
2026-01-28 16:57:33,608 | INFO | no hypothesis. Finish decoding.
2026-01-28 16:57:33,609 | INFO |  -1.81 * 0.5 =  -0.91 for decoder
2026-01-28 16:57:33,609 | INFO |  -2.70 * 0.5 =  -1.35 for ctc
2026-01-28 16:57:33,609 | INFO | total log probability: -2.26
2026-01-28 16:57:33,609 | INFO | normalized log probability: -0.12
2026-01-28 16:57:33,609 | INFO | total number of ended hypotheses: 155
2026-01-28 16:57:33,610 | INFO | best hypo: même<space>s'il<space>parlait

2026-01-28 16:57:33,611 | INFO | speech length: 46240
2026-01-28 16:57:33,652 | INFO | decoder input length: 71
2026-01-28 16:57:33,652 | INFO | max output length: 71
2026-01-28 16:57:33,652 | INFO | min output length: 7
2026-01-28 16:57:36,081 | INFO | end detected at 66
2026-01-28 16:57:36,082 | INFO |  -4.89 * 0.5 =  -2.44 for decoder
2026-01-28 16:57:36,082 | INFO |  -0.21 * 0.5 =  -0.11 for ctc
2026-01-28 16:57:36,082 | INFO | total log probability: -2.55
2026-01-28 16:57:36,083 | INFO | normalized log probability: -0.04
2026-01-28 16:57:36,083 | INFO | total number of ended hypotheses: 154
2026-01-28 16:57:36,083 | INFO | best hypo: parfaitement<space>sans<space>accent<space>sans<space>même<space>aucun<space>accent<space>au<space>pied<space>noir

2026-01-28 16:57:36,085 | INFO | speech length: 16000
2026-01-28 16:57:36,122 | INFO | decoder input length: 24
2026-01-28 16:57:36,122 | INFO | max output length: 24
2026-01-28 16:57:36,122 | INFO | min output length: 2
2026-01-28 16:57:36,889 | INFO | adding <eos> in the last position in the loop
2026-01-28 16:57:36,896 | INFO | no hypothesis. Finish decoding.
2026-01-28 16:57:36,897 | INFO |  -2.15 * 0.5 =  -1.07 for decoder
2026-01-28 16:57:36,897 | INFO |  -0.95 * 0.5 =  -0.47 for ctc
2026-01-28 16:57:36,897 | INFO | total log probability: -1.55
2026-01-28 16:57:36,897 | INFO | normalized log probability: -0.06
2026-01-28 16:57:36,897 | INFO | total number of ended hypotheses: 65
2026-01-28 16:57:36,898 | INFO | best hypo: entre<space>eux<space>ils<space>parlaient<sos/eos>

2026-01-28 16:57:36,898 | WARNING | best hypo length: 24 == max output length: 24
2026-01-28 16:57:36,898 | WARNING | decoding may be stopped by the max output length limitation, please consider to increase the maxlenratio.
2026-01-28 16:57:36,899 | INFO | speech length: 169600
2026-01-28 16:57:36,940 | INFO | decoder input length: 264
2026-01-28 16:57:36,941 | INFO | max output length: 264
2026-01-28 16:57:36,941 | INFO | min output length: 26
2026-01-28 16:57:48,858 | INFO | end detected at 223
2026-01-28 16:57:48,861 | INFO | -20.36 * 0.5 = -10.18 for decoder
2026-01-28 16:57:48,861 | INFO |  -3.92 * 0.5 =  -1.96 for ctc
2026-01-28 16:57:48,861 | INFO | total log probability: -12.14
2026-01-28 16:57:48,861 | INFO | normalized log probability: -0.06
2026-01-28 16:57:48,861 | INFO | total number of ended hypotheses: 212
2026-01-28 16:57:48,864 | INFO | best hypo: quelles<space>étaient<space>vos<space>vos<space>lectures<space>en<space>f<space>au<space>fond<space>je<space>je<space>lisais<space>tout<space>j'ai<space>lu<space>beaucoup<space>beaucoup<space>d'abord<space>la<space>littérature<space>française<space>du<space>dix<space>huitième<space>dix<space>neuvième<space>j'étais<space>très<space>impressionné<space>par<space>le<space>théâtre<space>de<space>marivo<space>qui<space>était

2026-01-28 16:57:48,867 | INFO | speech length: 156800
2026-01-28 16:57:48,904 | INFO | decoder input length: 244
2026-01-28 16:57:48,905 | INFO | max output length: 244
2026-01-28 16:57:48,905 | INFO | min output length: 24
2026-01-28 16:58:00,650 | INFO | end detected at 236
2026-01-28 16:58:00,651 | INFO | -18.99 * 0.5 =  -9.49 for decoder
2026-01-28 16:58:00,651 | INFO | -10.74 * 0.5 =  -5.37 for ctc
2026-01-28 16:58:00,651 | INFO | total log probability: -14.86
2026-01-28 16:58:00,651 | INFO | normalized log probability: -0.06
2026-01-28 16:58:00,652 | INFO | total number of ended hypotheses: 180
2026-01-28 16:58:00,655 | INFO | best hypo: pourtant<space>pour<space>moi<space>et<space>puis<space>je<space>me<space>suis<space>mis<space>un<space>peu<space>plus<space>tard<space>c'est<space>à<space>dire<space>vers<space>dix<space>huit<space>ans<space>à<space>découvrir<space>la<space>littérature<space>étrangère<space>c'était<space>une<space>partie<space>assez<space>secondaire<space>quand<space>même<space>de<space>mon<space>activité<space>parce<space>que<space>je<space>faisais<space>beaucoup<space>de<space>pianos

2026-01-28 16:58:00,657 | INFO | speech length: 16320
2026-01-28 16:58:00,692 | INFO | decoder input length: 25
2026-01-28 16:58:00,692 | INFO | max output length: 25
2026-01-28 16:58:00,692 | INFO | min output length: 2
2026-01-28 16:58:01,486 | INFO | adding <eos> in the last position in the loop
2026-01-28 16:58:01,494 | INFO | no hypothesis. Finish decoding.
2026-01-28 16:58:01,495 | INFO |  -2.04 * 0.5 =  -1.02 for decoder
2026-01-28 16:58:01,495 | INFO |  -0.08 * 0.5 =  -0.04 for ctc
2026-01-28 16:58:01,495 | INFO | total log probability: -1.06
2026-01-28 16:58:01,495 | INFO | normalized log probability: -0.04
2026-01-28 16:58:01,495 | INFO | total number of ended hypotheses: 98
2026-01-28 16:58:01,496 | INFO | best hypo: énormément<space>de<space>musiques

2026-01-28 16:58:01,497 | INFO | speech length: 39360
2026-01-28 16:58:01,534 | INFO | decoder input length: 61
2026-01-28 16:58:01,534 | INFO | max output length: 61
2026-01-28 16:58:01,534 | INFO | min output length: 6
2026-01-28 16:58:03,113 | INFO | end detected at 42
2026-01-28 16:58:03,115 | INFO |  -3.49 * 0.5 =  -1.75 for decoder
2026-01-28 16:58:03,116 | INFO |  -5.26 * 0.5 =  -2.63 for ctc
2026-01-28 16:58:03,116 | INFO | total log probability: -4.37
2026-01-28 16:58:03,116 | INFO | normalized log probability: -0.14
2026-01-28 16:58:03,116 | INFO | total number of ended hypotheses: 195
2026-01-28 16:58:03,116 | INFO | best hypo: et<space>et<space>et<space>m<space>mon<space>idée<space>c'était<space>de

2026-01-28 16:58:03,119 | INFO | speech length: 88480
2026-01-28 16:58:03,162 | INFO | decoder input length: 137
2026-01-28 16:58:03,162 | INFO | max output length: 137
2026-01-28 16:58:03,162 | INFO | min output length: 13
2026-01-28 16:58:08,952 | INFO | adding <eos> in the last position in the loop
2026-01-28 16:58:08,959 | INFO | no hypothesis. Finish decoding.
2026-01-28 16:58:08,960 | INFO | -27.55 * 0.5 = -13.77 for decoder
2026-01-28 16:58:08,960 | INFO | -61.55 * 0.5 = -30.77 for ctc
2026-01-28 16:58:08,960 | INFO | total log probability: -44.55
2026-01-28 16:58:08,960 | INFO | normalized log probability: -0.33
2026-01-28 16:58:08,960 | INFO | total number of ended hypotheses: 67
2026-01-28 16:58:08,962 | INFO | best hypo: faire<space>des<space>études<space>de<space>mathématiques<space>puisque<space>c'était<space>ledomaine<space>dans<space>lequel<space>je<space>me<space>trouvai<space>être<space>le<space>plus<space>loué<space>pour<space>ensuite<space>revenir<space>le<space>terrain

2026-01-28 16:58:08,963 | INFO | speech length: 168960
2026-01-28 16:58:09,007 | INFO | decoder input length: 263
2026-01-28 16:58:09,007 | INFO | max output length: 263
2026-01-28 16:58:09,007 | INFO | min output length: 26
2026-01-28 16:58:20,001 | INFO | end detected at 201
2026-01-28 16:58:20,004 | INFO | -15.69 * 0.5 =  -7.85 for decoder
2026-01-28 16:58:20,004 | INFO |  -6.44 * 0.5 =  -3.22 for ctc
2026-01-28 16:58:20,004 | INFO | total log probability: -11.07
2026-01-28 16:58:20,004 | INFO | normalized log probability: -0.06
2026-01-28 16:58:20,004 | INFO | total number of ended hypotheses: 220
2026-01-28 16:58:20,007 | INFO | best hypo: c'était<space>une<space>enfance<space>heureuse<space>oui<space>très<space>heureuse<space>déjà<space>au<space>lycée<space>vous<space>distinguez<space>par<space>vos<space>vos<space>vos<space>talents<space>intellectuels<space>non<space>j'étais<space>euh<space>j'ai<space>vu<space>la<space>réputation<space>d'être<space>très<space>bon<space>élève<space>et<space>j'étais<space>pas

2026-01-28 16:58:20,009 | INFO | speech length: 85600
2026-01-28 16:58:20,050 | INFO | decoder input length: 133
2026-01-28 16:58:20,050 | INFO | max output length: 133
2026-01-28 16:58:20,050 | INFO | min output length: 13
2026-01-28 16:58:26,326 | INFO | adding <eos> in the last position in the loop
2026-01-28 16:58:26,335 | INFO | no hypothesis. Finish decoding.
2026-01-28 16:58:26,337 | INFO | -27.90 * 0.5 = -13.95 for decoder
2026-01-28 16:58:26,337 | INFO | -71.42 * 0.5 = -35.71 for ctc
2026-01-28 16:58:26,337 | INFO | total log probability: -49.66
2026-01-28 16:58:26,337 | INFO | normalized log probability: -0.38
2026-01-28 16:58:26,337 | INFO | total number of ended hypotheses: 110
2026-01-28 16:58:26,339 | INFO | best hypo: je<space>vais<space>la<space>réputation<space>d'êtrel<space>oui<space>mais<space>je<space>faisais<space>pas<space>grand<space>chose<space>j'étais<space>toujours<space>euh<space>on<space>les<space>c'est<space>un<space>premie<space>de<space>la<space>classe<space>mais

2026-01-28 16:58:26,341 | INFO | speech length: 289119
2026-01-28 16:58:26,395 | INFO | decoder input length: 451
2026-01-28 16:58:26,396 | INFO | max output length: 451
2026-01-28 16:58:26,396 | INFO | min output length: 45
2026-01-28 16:59:16,258 | INFO | end detected at 388
2026-01-28 16:59:16,261 | INFO | -39.15 * 0.5 = -19.57 for decoder
2026-01-28 16:59:16,261 | INFO | -23.19 * 0.5 = -11.60 for ctc
2026-01-28 16:59:16,261 | INFO | total log probability: -31.17
2026-01-28 16:59:16,262 | INFO | normalized log probability: -0.08
2026-01-28 16:59:16,262 | INFO | total number of ended hypotheses: 206
2026-01-28 16:59:16,269 | INFO | best hypo: et<space>alors<space>donc<space>vous<space>allez<space>faire<space>toutes<space>les<space>plus<space>grandes<space>écoles<space>françaises<space>polytechniques<space>les<space>nains<space>à<space>l'école<space>des<space>mines<space>alors<space>euh<space>pourquoi<space>se<space>diriger<space>vers<space>ces<space>grandes<space>écoles<space>quel<space>était<space>le<space>projet<space>à<space>l'époque<space>pourquoi<space>ne<space>pas<space>faire<space>de<space>la<space>sociologie<space>de<space>l'histoire<space>ou<space>de<space>la<space>recherche<space>dans<space>ma<space>génération<space>euh<space>c'était<space>à<space>peu<space>près<space>évident<space>quand<space>on<space>était<space>bon<space>en<space>mat<space>de<space>ne<space>pas<space>gaspiller<space>cetal

2026-01-28 16:59:16,274 | INFO | speech length: 50240
2026-01-28 16:59:16,329 | INFO | decoder input length: 78
2026-01-28 16:59:16,329 | INFO | max output length: 78
2026-01-28 16:59:16,329 | INFO | min output length: 7
2026-01-28 16:59:20,878 | INFO | adding <eos> in the last position in the loop
2026-01-28 16:59:20,888 | INFO | no hypothesis. Finish decoding.
2026-01-28 16:59:20,890 | INFO | -10.97 * 0.5 =  -5.49 for decoder
2026-01-28 16:59:20,890 | INFO | -33.73 * 0.5 = -16.87 for ctc
2026-01-28 16:59:20,890 | INFO | total log probability: -22.35
2026-01-28 16:59:20,890 | INFO | normalized log probability: -0.31
2026-01-28 16:59:20,890 | INFO | total number of ended hypotheses: 187
2026-01-28 16:59:20,891 | INFO | best hypo: ça<space>se<space>devrait<space>i<space>se<space>trouve<space>qu'à<space>partir<space>de<space>la<space>matelam<space>je<space>me<space>suis<space>trouvé

2026-01-28 16:59:20,893 | INFO | speech length: 218400
2026-01-28 16:59:20,936 | INFO | decoder input length: 340
2026-01-28 16:59:20,936 | INFO | max output length: 340
2026-01-28 16:59:20,936 | INFO | min output length: 34
2026-01-28 16:59:39,286 | INFO | end detected at 300
2026-01-28 16:59:39,288 | INFO | -31.94 * 0.5 = -15.97 for decoder
2026-01-28 16:59:39,288 | INFO | -20.48 * 0.5 = -10.24 for ctc
2026-01-28 16:59:39,288 | INFO | total log probability: -26.21
2026-01-28 16:59:39,288 | INFO | normalized log probability: -0.09
2026-01-28 16:59:39,288 | INFO | total number of ended hypotheses: 198
2026-01-28 16:59:39,292 | INFO | best hypo: très<space>bon<space>mate<space>et<space>donc<space>euh<space>pas<space>question<space>de<space>gaspiller<space>ce<space>talent<space>quitte<space>à<space>à<space>savoir<space>très<space>vite<space>dès<space>à<space>ce<space>moment<space>là<space>que<space>je<space>ferais<space>pas<space>un<space>métier<space>des<space>mathématiques<space>je<space>serais<space>jamais<space>ingénieur<space>mais<space>je<space>voulais<space>aller<space>à<space>bout<space>au<space>bout<space>de<space>ce<space>savoir<space>et<space>puis<space>j'avais<space>toujours<space>présent<space>à<space>l'esprit<space>les<space>louises<space>carol

2026-01-28 16:59:39,295 | INFO | speech length: 190080
2026-01-28 16:59:39,345 | INFO | decoder input length: 296
2026-01-28 16:59:39,345 | INFO | max output length: 296
2026-01-28 16:59:39,345 | INFO | min output length: 29
2026-01-28 16:59:52,804 | INFO | end detected at 237
2026-01-28 16:59:52,807 | INFO | -19.52 * 0.5 =  -9.76 for decoder
2026-01-28 16:59:52,807 | INFO |  -6.68 * 0.5 =  -3.34 for ctc
2026-01-28 16:59:52,807 | INFO | total log probability: -13.10
2026-01-28 16:59:52,807 | INFO | normalized log probability: -0.06
2026-01-28 16:59:52,807 | INFO | total number of ended hypotheses: 224
2026-01-28 16:59:52,810 | INFO | best hypo: mathématicien<space>écrivain<space>et<space>en<space>même<space>temps<space>je<space>voyais<space>les<space>mathématiques<space>comme<space>ça<space>c'est<space>à<space>dire<space>comme<space>l'autre<space>côté<space>du<space>miroir<space>qui<space>fait<space>découvrir<space>un<space>univers<space>euh<space>fantastique<space>et<space>mathématique<space>je<space>le<space>vous<space>je<space>les<space>ai<space>vu<space>toujours<space>comme<space>euh<space>très

2026-01-28 16:59:52,813 | INFO | speech length: 12160
2026-01-28 16:59:52,849 | INFO | decoder input length: 18
2026-01-28 16:59:52,849 | INFO | max output length: 18
2026-01-28 16:59:52,849 | INFO | min output length: 1
2026-01-28 16:59:53,367 | INFO | end detected at 15
2026-01-28 16:59:53,368 | INFO |  -0.92 * 0.5 =  -0.46 for decoder
2026-01-28 16:59:53,368 | INFO |  -0.35 * 0.5 =  -0.17 for ctc
2026-01-28 16:59:53,368 | INFO | total log probability: -0.63
2026-01-28 16:59:53,368 | INFO | normalized log probability: -0.06
2026-01-28 16:59:53,368 | INFO | total number of ended hypotheses: 163
2026-01-28 16:59:53,368 | INFO | best hypo: poétique

2026-01-28 16:59:53,370 | INFO | speech length: 239040
2026-01-28 16:59:53,423 | INFO | decoder input length: 373
2026-01-28 16:59:53,424 | INFO | max output length: 373
2026-01-28 16:59:53,424 | INFO | min output length: 37
2026-01-28 17:00:20,707 | INFO | end detected at 298
2026-01-28 17:00:20,710 | INFO | -28.60 * 0.5 = -14.30 for decoder
2026-01-28 17:00:20,710 | INFO |  -5.38 * 0.5 =  -2.69 for ctc
2026-01-28 17:00:20,710 | INFO | total log probability: -16.99
2026-01-28 17:00:20,710 | INFO | normalized log probability: -0.06
2026-01-28 17:00:20,710 | INFO | total number of ended hypotheses: 189
2026-01-28 17:00:20,714 | INFO | best hypo: et<space>est<space>ce<space>qu'<space>y<space>a<space>à<space>ce<space>moment<space>là<space>j<space>un<space>projet<space>de<space>de<space>de<space>faire<space>quelque<space>chose<space>j'avais<space>non<space>je<space>suis<space>un<space>intellectuel<space>je<space>l'ai<space>toujours<space>été<space>j'ai<space>jamais<space>eu<space>envie<space>de<space>faire<space>une<space>carrière<space>politique<space>j'ai<space>toujours<space>envie<space>de<space>faire<space>euh<space>avancer<space>le<space>monde<space>par<space>mes<space>idées<space>qui<space>est<space>allé<space>mettre<space>en<space>oeuvre<space>euh<space>moi<space>même

2026-01-28 17:00:20,727 | INFO | Chunk: 0 | WER=6.578947 | S=4 D=1 I=0
2026-01-28 17:00:20,729 | INFO | Chunk: 1 | WER=2.380952 | S=1 D=0 I=0
2026-01-28 17:00:20,733 | INFO | Chunk: 2 | WER=3.797468 | S=2 D=0 I=1
2026-01-28 17:00:20,736 | INFO | Chunk: 3 | WER=25.714286 | S=9 D=2 I=7
2026-01-28 17:00:20,736 | INFO | Chunk: 4 | WER=83.333333 | S=2 D=0 I=3
2026-01-28 17:00:20,737 | INFO | Chunk: 5 | WER=10.000000 | S=1 D=0 I=1
2026-01-28 17:00:20,738 | INFO | Chunk: 6 | WER=14.634146 | S=1 D=3 I=2
2026-01-28 17:00:20,739 | INFO | Chunk: 7 | WER=5.555556 | S=0 D=0 I=1
2026-01-28 17:00:20,739 | INFO | Chunk: 8 | WER=4.761905 | S=0 D=0 I=1
2026-01-28 17:00:20,740 | INFO | Chunk: 9 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:00:20,740 | INFO | Chunk: 10 | WER=23.076923 | S=3 D=0 I=0
2026-01-28 17:00:20,741 | INFO | Chunk: 11 | WER=12.500000 | S=1 D=0 I=2
2026-01-28 17:00:20,741 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:00:20,741 | INFO | Chunk: 13 | WER=14.285714 | S=1 D=0 I=2
2026-01-28 17:00:20,742 | INFO | Chunk: 14 | WER=11.111111 | S=0 D=1 I=1
2026-01-28 17:00:20,742 | INFO | Chunk: 15 | WER=7.692308 | S=0 D=0 I=1
2026-01-28 17:00:20,743 | INFO | Chunk: 16 | WER=18.918919 | S=2 D=3 I=2
2026-01-28 17:00:20,744 | INFO | Chunk: 17 | WER=50.000000 | S=2 D=0 I=0
2026-01-28 17:00:20,744 | INFO | Chunk: 18 | WER=20.000000 | S=0 D=1 I=1
2026-01-28 17:00:20,744 | INFO | Chunk: 19 | WER=100.000000 | S=0 D=0 I=3
2026-01-28 17:00:20,745 | INFO | Chunk: 20 | WER=13.888889 | S=2 D=0 I=3
2026-01-28 17:00:20,747 | INFO | Chunk: 21 | WER=4.545455 | S=2 D=0 I=0
2026-01-28 17:00:20,747 | INFO | Chunk: 22 | WER=33.333333 | S=1 D=0 I=0
2026-01-28 17:00:20,747 | INFO | Chunk: 23 | WER=12.500000 | S=0 D=0 I=1
2026-01-28 17:00:20,748 | INFO | Chunk: 24 | WER=28.000000 | S=5 D=2 I=0
2026-01-28 17:00:20,749 | INFO | Chunk: 25 | WER=13.513514 | S=1 D=2 I=2
2026-01-28 17:00:20,750 | INFO | Chunk: 26 | WER=50.000000 | S=7 D=1 I=4
2026-01-28 17:00:20,753 | INFO | Chunk: 27 | WER=13.432836 | S=5 D=0 I=4
2026-01-28 17:00:20,754 | INFO | Chunk: 28 | WER=33.333333 | S=4 D=2 I=0
2026-01-28 17:00:20,756 | INFO | Chunk: 29 | WER=12.500000 | S=3 D=1 I=3
2026-01-28 17:00:20,757 | INFO | Chunk: 30 | WER=17.948718 | S=4 D=0 I=3
2026-01-28 17:00:20,757 | INFO | Chunk: 31 | WER=100.000000 | S=1 D=0 I=0
2026-01-28 17:00:20,760 | INFO | Chunk: 32 | WER=19.672131 | S=6 D=2 I=4
2026-01-28 17:00:21,260 | INFO | File: Rhap-D2005.wav | WER=14.740191 | S=72 D=18 I=49
2026-01-28 17:00:21,260 | INFO | ------------------------------
2026-01-28 17:00:21,261 | INFO | Conf ester Done!
2026-01-28 17:04:16,199 | INFO | Chunk: 0 | WER=3.947368 | S=2 D=0 I=1
2026-01-28 17:04:16,202 | INFO | Chunk: 1 | WER=2.380952 | S=1 D=0 I=0
2026-01-28 17:04:16,207 | INFO | Chunk: 2 | WER=1.265823 | S=1 D=0 I=0
2026-01-28 17:04:16,212 | INFO | Chunk: 3 | WER=14.285714 | S=3 D=4 I=3
2026-01-28 17:04:16,212 | INFO | Chunk: 4 | WER=33.333333 | S=0 D=0 I=2
2026-01-28 17:04:16,213 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:04:16,215 | INFO | Chunk: 6 | WER=17.073171 | S=5 D=2 I=0
2026-01-28 17:04:16,215 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:04:16,216 | INFO | Chunk: 8 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:04:16,216 | INFO | Chunk: 9 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:04:16,217 | INFO | Chunk: 10 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:04:16,218 | INFO | Chunk: 11 | WER=12.500000 | S=2 D=1 I=0
2026-01-28 17:04:16,218 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:04:16,219 | INFO | Chunk: 13 | WER=38.095238 | S=4 D=1 I=3
2026-01-28 17:04:16,219 | INFO | Chunk: 14 | WER=16.666667 | S=2 D=1 I=0
2026-01-28 17:04:16,220 | INFO | Chunk: 15 | WER=7.692308 | S=0 D=0 I=1
2026-01-28 17:04:16,221 | INFO | Chunk: 16 | WER=18.918919 | S=2 D=5 I=0
2026-01-28 17:04:16,222 | INFO | Chunk: 17 | WER=100.000000 | S=2 D=0 I=2
2026-01-28 17:04:16,222 | INFO | Chunk: 18 | WER=20.000000 | S=1 D=1 I=0
2026-01-28 17:04:16,222 | INFO | Chunk: 19 | WER=66.666667 | S=0 D=0 I=2
2026-01-28 17:04:16,224 | INFO | Chunk: 20 | WER=5.555556 | S=1 D=0 I=1
2026-01-28 17:04:16,226 | INFO | Chunk: 21 | WER=2.272727 | S=1 D=0 I=0
2026-01-28 17:04:16,226 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:04:16,227 | INFO | Chunk: 23 | WER=62.500000 | S=0 D=4 I=1
2026-01-28 17:04:16,228 | INFO | Chunk: 24 | WER=12.000000 | S=2 D=1 I=0
2026-01-28 17:04:16,229 | INFO | Chunk: 25 | WER=29.729730 | S=6 D=4 I=1
2026-01-28 17:04:16,230 | INFO | Chunk: 26 | WER=33.333333 | S=4 D=3 I=1
2026-01-28 17:04:16,235 | INFO | Chunk: 27 | WER=10.447761 | S=3 D=0 I=4
2026-01-28 17:04:16,235 | INFO | Chunk: 28 | WER=27.777778 | S=4 D=1 I=0
2026-01-28 17:04:16,238 | INFO | Chunk: 29 | WER=5.357143 | S=1 D=2 I=0
2026-01-28 17:04:16,240 | INFO | Chunk: 30 | WER=15.384615 | S=4 D=0 I=2
2026-01-28 17:04:16,240 | INFO | Chunk: 31 | WER=200.000000 | S=1 D=0 I=1
2026-01-28 17:04:16,244 | INFO | Chunk: 32 | WER=21.311475 | S=6 D=5 I=2
2026-01-28 17:04:16,895 | INFO | File: Rhap-D2005.wav | WER=12.089077 | S=64 D=29 I=21
2026-01-28 17:04:16,895 | INFO | ------------------------------
2026-01-28 17:04:16,895 | INFO | hmm_tdnn Done!
2026-01-28 17:04:17,071 | INFO | ==================================Rhap-D2006.wav=========================================
2026-01-28 17:04:17,255 | INFO | Using rVAD model
2026-01-28 17:04:28,584 | INFO | Chunk: 0 | WER=33.333333 | S=1 D=1 I=0
2026-01-28 17:04:28,585 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:04:28,585 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:04:28,587 | INFO | Chunk: 3 | WER=18.518519 | S=5 D=0 I=0
2026-01-28 17:04:28,589 | INFO | Chunk: 4 | WER=20.454545 | S=7 D=2 I=0
2026-01-28 17:04:28,590 | INFO | Chunk: 5 | WER=0.000000 | S=1 D=0 I=0
2026-01-28 17:04:28,590 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:04:28,590 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:04:28,591 | INFO | Chunk: 8 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:04:28,591 | INFO | Chunk: 9 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:04:28,593 | INFO | Chunk: 10 | WER=5.000000 | S=2 D=0 I=0
2026-01-28 17:04:28,594 | INFO | Chunk: 11 | WER=11.111111 | S=1 D=0 I=0
2026-01-28 17:04:28,594 | INFO | Chunk: 12 | WER=39.130435 | S=2 D=7 I=0
2026-01-28 17:04:28,595 | INFO | Chunk: 13 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:04:28,595 | INFO | Chunk: 14 | WER=50.000000 | S=3 D=1 I=0
2026-01-28 17:04:28,636 | INFO | File: Rhap-D2006.wav | WER=13.445378 | S=22 D=10 I=0
2026-01-28 17:04:28,636 | INFO | ------------------------------
2026-01-28 17:04:28,637 | INFO | w2vec vad chunk Done!
2026-01-28 17:04:47,493 | INFO | Chunk: 0 | WER=33.333333 | S=2 D=0 I=0
2026-01-28 17:04:47,494 | INFO | Chunk: 1 | WER=15.789474 | S=3 D=0 I=0
2026-01-28 17:04:47,494 | INFO | Chunk: 2 | WER=66.666667 | S=2 D=0 I=0
2026-01-28 17:04:47,496 | INFO | Chunk: 3 | WER=7.407407 | S=2 D=0 I=0
2026-01-28 17:04:47,498 | INFO | Chunk: 4 | WER=11.363636 | S=5 D=0 I=0
2026-01-28 17:04:47,498 | INFO | Chunk: 5 | WER=0.000000 | S=1 D=0 I=4
2026-01-28 17:04:47,498 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:04:47,499 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:04:47,499 | INFO | Chunk: 8 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:04:47,500 | INFO | Chunk: 9 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:04:47,501 | INFO | Chunk: 10 | WER=22.500000 | S=1 D=8 I=0
2026-01-28 17:04:47,502 | INFO | Chunk: 11 | WER=22.222222 | S=1 D=0 I=1
2026-01-28 17:04:47,503 | INFO | Chunk: 12 | WER=34.782609 | S=1 D=7 I=0
2026-01-28 17:04:47,503 | INFO | Chunk: 13 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:04:47,504 | INFO | Chunk: 14 | WER=75.000000 | S=3 D=3 I=0
2026-01-28 17:04:47,543 | INFO | File: Rhap-D2006.wav | WER=18.067227 | S=21 D=17 I=5
2026-01-28 17:04:47,543 | INFO | ------------------------------
2026-01-28 17:04:47,544 | INFO | whisper med Done!
2026-01-28 17:05:12,058 | INFO | Chunk: 0 | WER=50.000000 | S=2 D=0 I=1
2026-01-28 17:05:12,058 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:05:12,059 | INFO | Chunk: 2 | WER=66.666667 | S=2 D=0 I=0
2026-01-28 17:05:12,060 | INFO | Chunk: 3 | WER=14.814815 | S=4 D=0 I=0
2026-01-28 17:05:12,061 | INFO | Chunk: 4 | WER=15.909091 | S=4 D=0 I=3
2026-01-28 17:05:12,061 | INFO | Chunk: 5 | WER=0.000000 | S=1 D=0 I=2
2026-01-28 17:05:12,062 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:05:12,062 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:05:12,062 | INFO | Chunk: 8 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:05:12,063 | INFO | Chunk: 9 | WER=33.333333 | S=2 D=0 I=0
2026-01-28 17:05:12,064 | INFO | Chunk: 10 | WER=20.000000 | S=0 D=8 I=0
2026-01-28 17:05:12,064 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:05:12,065 | INFO | Chunk: 12 | WER=8.695652 | S=1 D=1 I=0
2026-01-28 17:05:12,065 | INFO | Chunk: 13 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:05:12,066 | INFO | Chunk: 14 | WER=62.500000 | S=5 D=0 I=0
2026-01-28 17:05:12,096 | INFO | File: Rhap-D2006.wav | WER=15.126050 | S=20 D=9 I=7
2026-01-28 17:05:12,096 | INFO | ------------------------------
2026-01-28 17:05:12,096 | INFO | whisper large Done!
2026-01-28 17:05:12,272 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 17:05:12,310 | INFO | Vocabulary size: 350
2026-01-28 17:05:13,510 | INFO | Gradient checkpoint layers: []
2026-01-28 17:05:14,294 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 17:05:14,300 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 17:05:14,300 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 17:05:14,301 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 17:05:14,301 | INFO | speech length: 30400
2026-01-28 17:05:14,350 | INFO | decoder input length: 47
2026-01-28 17:05:14,350 | INFO | max output length: 47
2026-01-28 17:05:14,350 | INFO | min output length: 4
2026-01-28 17:05:15,404 | INFO | end detected at 22
2026-01-28 17:05:15,405 | INFO |  -2.18 * 0.5 =  -1.09 for decoder
2026-01-28 17:05:15,405 | INFO |  -0.98 * 0.5 =  -0.49 for ctc
2026-01-28 17:05:15,406 | INFO | total log probability: -1.58
2026-01-28 17:05:15,406 | INFO | normalized log probability: -0.09
2026-01-28 17:05:15,406 | INFO | total number of ended hypotheses: 147
2026-01-28 17:05:15,406 | INFO | best hypo: ▁ce▁programme▁monsieur▁le▁premier▁ministre

2026-01-28 17:05:15,410 | INFO | speech length: 88800
2026-01-28 17:05:15,459 | INFO | decoder input length: 138
2026-01-28 17:05:15,459 | INFO | max output length: 138
2026-01-28 17:05:15,459 | INFO | min output length: 13
2026-01-28 17:05:18,304 | INFO | end detected at 56
2026-01-28 17:05:18,306 | INFO |  -4.24 * 0.5 =  -2.12 for decoder
2026-01-28 17:05:18,306 | INFO |  -1.88 * 0.5 =  -0.94 for ctc
2026-01-28 17:05:18,306 | INFO | total log probability: -3.06
2026-01-28 17:05:18,306 | INFO | normalized log probability: -0.06
2026-01-28 17:05:18,306 | INFO | total number of ended hypotheses: 171
2026-01-28 17:05:18,307 | INFO | best hypo: ▁il▁comporte▁un▁certain▁nombre▁de▁projets▁d'autant▁plus▁intéressants▁à▁nos▁yeux▁que▁nous▁les▁proposons▁depuis▁longtemps

2026-01-28 17:05:18,309 | INFO | speech length: 24160
2026-01-28 17:05:18,344 | INFO | decoder input length: 37
2026-01-28 17:05:18,344 | INFO | max output length: 37
2026-01-28 17:05:18,344 | INFO | min output length: 3
2026-01-28 17:05:19,028 | INFO | end detected at 17
2026-01-28 17:05:19,030 | INFO |  -0.77 * 0.5 =  -0.39 for decoder
2026-01-28 17:05:19,030 | INFO |  -3.41 * 0.5 =  -1.71 for ctc
2026-01-28 17:05:19,030 | INFO | total log probability: -2.09
2026-01-28 17:05:19,030 | INFO | normalized log probability: -0.19
2026-01-28 17:05:19,030 | INFO | total number of ended hypotheses: 173
2026-01-28 17:05:19,031 | INFO | best hypo: ▁relèvement▁du▁smic

2026-01-28 17:05:19,032 | INFO | speech length: 187520
2026-01-28 17:05:19,082 | INFO | decoder input length: 292
2026-01-28 17:05:19,082 | INFO | max output length: 292
2026-01-28 17:05:19,083 | INFO | min output length: 29
2026-01-28 17:05:25,616 | INFO | end detected at 93
2026-01-28 17:05:25,618 | INFO | -46.69 * 0.5 = -23.34 for decoder
2026-01-28 17:05:25,618 | INFO | -10.42 * 0.5 =  -5.21 for ctc
2026-01-28 17:05:25,618 | INFO | total log probability: -28.55
2026-01-28 17:05:25,618 | INFO | normalized log probability: -0.32
2026-01-28 17:05:25,618 | INFO | total number of ended hypotheses: 161
2026-01-28 17:05:25,619 | INFO | best hypo: ▁amnistie▁pour▁les▁commerçants▁et▁les▁artisans▁aides▁aux▁chômeurs▁partiels▁égalisation▁des▁salaires▁féminins▁et▁masculins▁participation▁accrue▁des▁cadres▁au▁conseil▁d'administration▁des▁sociétés

2026-01-28 17:05:25,622 | INFO | speech length: 295360
2026-01-28 17:05:25,666 | INFO | decoder input length: 461
2026-01-28 17:05:25,667 | INFO | max output length: 461
2026-01-28 17:05:25,667 | INFO | min output length: 46
2026-01-28 17:05:37,937 | INFO | end detected at 137
2026-01-28 17:05:37,938 | INFO | -243.42 * 0.5 = -121.71 for decoder
2026-01-28 17:05:37,939 | INFO | -75.45 * 0.5 = -37.73 for ctc
2026-01-28 17:05:37,939 | INFO | total log probability: -159.44
2026-01-28 17:05:37,939 | INFO | normalized log probability: -1.22
2026-01-28 17:05:37,939 | INFO | total number of ended hypotheses: 183
2026-01-28 17:05:37,940 | INFO | best hypo: ▁réforme▁du▁droit▁de▁licenciement▁nomination▁d'un▁médiateur▁entre▁l'administration▁et▁le▁public▁contrôle▁des▁dépenses▁électorales▁toute▁mesure▁ce▁qui▁dormait▁dans▁vos▁cartons▁et▁que▁vos▁est▁prédécessaires▁avaient▁énergiquement▁refusés▁et▁que▁l'ons▁retrouve▁dans▁le▁programmes▁commun▁de▁la▁go

2026-01-28 17:05:37,943 | INFO | speech length: 14240
2026-01-28 17:05:37,981 | INFO | decoder input length: 21
2026-01-28 17:05:37,981 | INFO | max output length: 21
2026-01-28 17:05:37,981 | INFO | min output length: 2
2026-01-28 17:05:38,368 | INFO | end detected at 10
2026-01-28 17:05:38,370 | INFO |  -3.41 * 0.5 =  -1.71 for decoder
2026-01-28 17:05:38,370 | INFO |  -5.92 * 0.5 =  -2.96 for ctc
2026-01-28 17:05:38,370 | INFO | total log probability: -4.67
2026-01-28 17:05:38,370 | INFO | normalized log probability: -1.17
2026-01-28 17:05:38,370 | INFO | total number of ended hypotheses: 173
2026-01-28 17:05:38,370 | INFO | best hypo: ▁il▁est

2026-01-28 17:05:38,372 | INFO | speech length: 83840
2026-01-28 17:05:38,408 | INFO | decoder input length: 130
2026-01-28 17:05:38,409 | INFO | max output length: 130
2026-01-28 17:05:38,409 | INFO | min output length: 13
2026-01-28 17:05:40,144 | INFO | end detected at 34
2026-01-28 17:05:40,146 | INFO |  -2.61 * 0.5 =  -1.30 for decoder
2026-01-28 17:05:40,146 | INFO |  -1.61 * 0.5 =  -0.81 for ctc
2026-01-28 17:05:40,146 | INFO | total log probability: -2.11
2026-01-28 17:05:40,146 | INFO | normalized log probability: -0.07
2026-01-28 17:05:40,146 | INFO | total number of ended hypotheses: 151
2026-01-28 17:05:40,147 | INFO | best hypo: ▁lorsque▁vous▁semblez▁mettre▁en▁doute▁notre▁amour▁des▁libertés

2026-01-28 17:05:40,148 | INFO | speech length: 56000
2026-01-28 17:05:40,185 | INFO | decoder input length: 87
2026-01-28 17:05:40,185 | INFO | max output length: 87
2026-01-28 17:05:40,185 | INFO | min output length: 8
2026-01-28 17:05:41,424 | INFO | end detected at 25
2026-01-28 17:05:41,426 | INFO |  -2.80 * 0.5 =  -1.40 for decoder
2026-01-28 17:05:41,426 | INFO |  -1.59 * 0.5 =  -0.79 for ctc
2026-01-28 17:05:41,426 | INFO | total log probability: -2.19
2026-01-28 17:05:41,426 | INFO | normalized log probability: -0.11
2026-01-28 17:05:41,426 | INFO | total number of ended hypotheses: 163
2026-01-28 17:05:41,427 | INFO | best hypo: ▁c'est▁un▁outrage▁que▁nous▁n'acceptons▁pas

2026-01-28 17:05:41,430 | INFO | speech length: 107360
2026-01-28 17:05:41,495 | INFO | decoder input length: 167
2026-01-28 17:05:41,495 | INFO | max output length: 167
2026-01-28 17:05:41,495 | INFO | min output length: 16
2026-01-28 17:05:44,696 | INFO | end detected at 50
2026-01-28 17:05:44,697 | INFO |  -4.80 * 0.5 =  -2.40 for decoder
2026-01-28 17:05:44,697 | INFO |  -2.43 * 0.5 =  -1.22 for ctc
2026-01-28 17:05:44,698 | INFO | total log probability: -3.62
2026-01-28 17:05:44,698 | INFO | normalized log probability: -0.08
2026-01-28 17:05:44,698 | INFO | total number of ended hypotheses: 151
2026-01-28 17:05:44,698 | INFO | best hypo: ▁nous▁sommes▁les▁héritiers▁de▁la▁tradition▁qui▁a▁restaurée▁dans▁ce▁pays▁la▁démocratie▁politique▁et▁sociale

2026-01-28 17:05:44,701 | INFO | speech length: 68481
2026-01-28 17:05:44,745 | INFO | decoder input length: 106
2026-01-28 17:05:44,745 | INFO | max output length: 106
2026-01-28 17:05:44,745 | INFO | min output length: 10
2026-01-28 17:05:46,484 | INFO | end detected at 31
2026-01-28 17:05:46,487 | INFO |  -1.93 * 0.5 =  -0.96 for decoder
2026-01-28 17:05:46,487 | INFO |  -0.55 * 0.5 =  -0.27 for ctc
2026-01-28 17:05:46,487 | INFO | total log probability: -1.24
2026-01-28 17:05:46,488 | INFO | normalized log probability: -0.05
2026-01-28 17:05:46,488 | INFO | total number of ended hypotheses: 171
2026-01-28 17:05:46,488 | INFO | best hypo: ▁toujours▁toujours▁contre▁les▁droites▁coalisées

2026-01-28 17:05:46,490 | INFO | speech length: 421120
2026-01-28 17:05:46,540 | INFO | decoder input length: 657
2026-01-28 17:05:46,540 | INFO | max output length: 657
2026-01-28 17:05:46,540 | INFO | min output length: 65
2026-01-28 17:06:03,192 | INFO | end detected at 119
2026-01-28 17:06:03,194 | INFO | -242.29 * 0.5 = -121.15 for decoder
2026-01-28 17:06:03,194 | INFO | -73.36 * 0.5 = -36.68 for ctc
2026-01-28 17:06:03,194 | INFO | total log probability: -157.83
2026-01-28 17:06:03,195 | INFO | normalized log probability: -1.40
2026-01-28 17:06:03,195 | INFO | total number of ended hypotheses: 161
2026-01-28 17:06:03,196 | INFO | best hypo: ▁nos▁combats▁pour▁la▁conquête▁du▁droit▁jalonnent▁l'histoire▁des▁deux▁derniers▁siècles▁c'est▁à▁ceux▁de▁votre▁tradition▁que▁nous▁avons▁arrassé▁le▁suffrage▁universel▁en▁la▁liberté▁d'as▁tociation▁que▁nous▁avons▁arraché▁la▁liberté▁d'as▁soitation

2026-01-28 17:06:03,200 | INFO | speech length: 53760
2026-01-28 17:06:03,247 | INFO | decoder input length: 83
2026-01-28 17:06:03,247 | INFO | max output length: 83
2026-01-28 17:06:03,247 | INFO | min output length: 8
2026-01-28 17:06:04,663 | INFO | end detected at 27
2026-01-28 17:06:04,664 | INFO |  -1.66 * 0.5 =  -0.83 for decoder
2026-01-28 17:06:04,664 | INFO |  -0.19 * 0.5 =  -0.09 for ctc
2026-01-28 17:06:04,664 | INFO | total log probability: -0.92
2026-01-28 17:06:04,664 | INFO | normalized log probability: -0.04
2026-01-28 17:06:04,664 | INFO | total number of ended hypotheses: 152
2026-01-28 17:06:04,665 | INFO | best hypo: ▁que▁nous▁avons▁arraché▁la▁liberté▁de▁la▁presse

2026-01-28 17:06:04,667 | INFO | speech length: 177120
2026-01-28 17:06:04,717 | INFO | decoder input length: 276
2026-01-28 17:06:04,717 | INFO | max output length: 276
2026-01-28 17:06:04,717 | INFO | min output length: 27
2026-01-28 17:06:10,336 | INFO | end detected at 70
2026-01-28 17:06:10,338 | INFO | -30.04 * 0.5 = -15.02 for decoder
2026-01-28 17:06:10,338 | INFO | -30.57 * 0.5 = -15.28 for ctc
2026-01-28 17:06:10,338 | INFO | total log probability: -30.30
2026-01-28 17:06:10,338 | INFO | normalized log probability: -0.48
2026-01-28 17:06:10,338 | INFO | total number of ended hypotheses: 168
2026-01-28 17:06:10,339 | INFO | best hypo: ▁le▁droit▁de▁grève▁le▁droit▁à▁l'insurrection▁second▁et▁second▁monsieur▁mythor▁monsolon▁monsieur▁habiton▁voulez▁vous▁continuer

2026-01-28 17:06:10,342 | INFO | speech length: 81440
2026-01-28 17:06:10,393 | INFO | decoder input length: 126
2026-01-28 17:06:10,393 | INFO | max output length: 126
2026-01-28 17:06:10,393 | INFO | min output length: 12
2026-01-28 17:06:12,742 | INFO | end detected at 38
2026-01-28 17:06:12,744 | INFO |  -5.62 * 0.5 =  -2.81 for decoder
2026-01-28 17:06:12,744 | INFO |  -5.38 * 0.5 =  -2.69 for ctc
2026-01-28 17:06:12,744 | INFO | total log probability: -5.50
2026-01-28 17:06:12,745 | INFO | normalized log probability: -0.17
2026-01-28 17:06:12,745 | INFO | total number of ended hypotheses: 183
2026-01-28 17:06:12,745 | INFO | best hypo: ▁mais▁monsieur▁le▁président▁il▁est▁naturel▁que▁la▁majorité▁se▁montre▁telle▁quelle▁est

2026-01-28 17:06:12,747 | INFO | speech length: 55360
2026-01-28 17:06:12,800 | INFO | decoder input length: 86
2026-01-28 17:06:12,800 | INFO | max output length: 86
2026-01-28 17:06:12,800 | INFO | min output length: 8
2026-01-28 17:06:15,347 | INFO | end detected at 32
2026-01-28 17:06:15,351 | INFO |  -6.62 * 0.5 =  -3.31 for decoder
2026-01-28 17:06:15,352 | INFO | -11.92 * 0.5 =  -5.96 for ctc
2026-01-28 17:06:15,352 | INFO | total log probability: -9.27
2026-01-28 17:06:15,352 | INFO | normalized log probability: -0.44
2026-01-28 17:06:15,352 | INFO | total number of ended hypotheses: 208
2026-01-28 17:06:15,353 | INFO | best hypo: ▁poursuivez▁monsieur▁le▁groupe▁coalisez▁que▁nous▁a

2026-01-28 17:06:15,363 | INFO | Chunk: 0 | WER=16.666667 | S=1 D=0 I=0
2026-01-28 17:06:15,364 | INFO | Chunk: 1 | WER=5.263158 | S=0 D=0 I=1
2026-01-28 17:06:15,364 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:06:15,366 | INFO | Chunk: 3 | WER=18.518519 | S=5 D=0 I=0
2026-01-28 17:06:15,370 | INFO | Chunk: 4 | WER=25.000000 | S=8 D=0 I=3
2026-01-28 17:06:15,370 | INFO | Chunk: 5 | WER=0.000000 | S=1 D=0 I=1
2026-01-28 17:06:15,371 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:06:15,371 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:06:15,372 | INFO | Chunk: 8 | WER=5.555556 | S=1 D=0 I=0
2026-01-28 17:06:15,373 | INFO | Chunk: 9 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:06:15,376 | INFO | Chunk: 10 | WER=15.000000 | S=3 D=0 I=3
2026-01-28 17:06:15,377 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:06:15,379 | INFO | Chunk: 12 | WER=43.478261 | S=7 D=3 I=0
2026-01-28 17:06:15,380 | INFO | Chunk: 13 | WER=12.500000 | S=1 D=1 I=0
2026-01-28 17:06:15,381 | INFO | Chunk: 14 | WER=50.000000 | S=4 D=0 I=0
2026-01-28 17:06:15,447 | INFO | File: Rhap-D2006.wav | WER=18.067227 | S=30 D=4 I=9
2026-01-28 17:06:15,448 | INFO | ------------------------------
2026-01-28 17:06:15,448 | INFO | Conf cv Done!
2026-01-28 17:06:15,636 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 17:06:15,684 | INFO | Vocabulary size: 47
2026-01-28 17:06:16,854 | INFO | Gradient checkpoint layers: []
2026-01-28 17:06:17,594 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 17:06:17,600 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 17:06:17,600 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 17:06:17,601 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 17:06:17,603 | INFO | speech length: 30400
2026-01-28 17:06:17,647 | INFO | decoder input length: 47
2026-01-28 17:06:17,647 | INFO | max output length: 47
2026-01-28 17:06:17,647 | INFO | min output length: 4
2026-01-28 17:06:19,621 | INFO | adding <eos> in the last position in the loop
2026-01-28 17:06:19,648 | INFO | no hypothesis. Finish decoding.
2026-01-28 17:06:19,649 | INFO |  -3.49 * 0.5 =  -1.75 for decoder
2026-01-28 17:06:19,650 | INFO |  -1.45 * 0.5 =  -0.72 for ctc
2026-01-28 17:06:19,650 | INFO | total log probability: -2.47
2026-01-28 17:06:19,650 | INFO | normalized log probability: -0.06
2026-01-28 17:06:19,650 | INFO | total number of ended hypotheses: 154
2026-01-28 17:06:19,651 | INFO | best hypo: le<space>programme<space>monsieur<space>le<space>premier<space>ministre

2026-01-28 17:06:19,653 | INFO | speech length: 88800
2026-01-28 17:06:19,719 | INFO | decoder input length: 138
2026-01-28 17:06:19,719 | INFO | max output length: 138
2026-01-28 17:06:19,719 | INFO | min output length: 13
2026-01-28 17:06:25,080 | INFO | end detected at 123
2026-01-28 17:06:25,081 | INFO |  -9.34 * 0.5 =  -4.67 for decoder
2026-01-28 17:06:25,081 | INFO |  -0.77 * 0.5 =  -0.38 for ctc
2026-01-28 17:06:25,081 | INFO | total log probability: -5.05
2026-01-28 17:06:25,081 | INFO | normalized log probability: -0.04
2026-01-28 17:06:25,081 | INFO | total number of ended hypotheses: 183
2026-01-28 17:06:25,083 | INFO | best hypo: comporte<space>un<space>certain<space>nombre<space>de<space>projets<space>d'autant<space>plus<space>intéressants<space>à<space>nos<space>yeux<space>que<space>nous<space>les<space>proposons<space>depuis<space>longtemps

2026-01-28 17:06:25,085 | INFO | speech length: 24160
2026-01-28 17:06:25,130 | INFO | decoder input length: 37
2026-01-28 17:06:25,130 | INFO | max output length: 37
2026-01-28 17:06:25,130 | INFO | min output length: 3
2026-01-28 17:06:26,028 | INFO | end detected at 25
2026-01-28 17:06:26,029 | INFO |  -1.51 * 0.5 =  -0.76 for decoder
2026-01-28 17:06:26,029 | INFO |  -0.03 * 0.5 =  -0.01 for ctc
2026-01-28 17:06:26,029 | INFO | total log probability: -0.77
2026-01-28 17:06:26,029 | INFO | normalized log probability: -0.04
2026-01-28 17:06:26,029 | INFO | total number of ended hypotheses: 165
2026-01-28 17:06:26,029 | INFO | best hypo: relèvement<space>du<space>smic

2026-01-28 17:06:26,031 | INFO | speech length: 187520
2026-01-28 17:06:26,069 | INFO | decoder input length: 292
2026-01-28 17:06:26,070 | INFO | max output length: 292
2026-01-28 17:06:26,070 | INFO | min output length: 29
2026-01-28 17:06:38,139 | INFO | end detected at 199
2026-01-28 17:06:38,141 | INFO | -15.55 * 0.5 =  -7.78 for decoder
2026-01-28 17:06:38,141 | INFO |  -0.93 * 0.5 =  -0.47 for ctc
2026-01-28 17:06:38,141 | INFO | total log probability: -8.24
2026-01-28 17:06:38,141 | INFO | normalized log probability: -0.04
2026-01-28 17:06:38,141 | INFO | total number of ended hypotheses: 177
2026-01-28 17:06:38,144 | INFO | best hypo: amnistie<space>pour<space>les<space>commerçants<space>et<space>les<space>artisans<space>aide<space>aux<space>chômeurs<space>partiels<space>égalisation<space>des<space>salaires<space>féminins<space>et<space>masculins<space>participation<space>accrue<space>des<space>cadres<space>au<space>conseil<space>d'administration<space>des<space>sociétés

2026-01-28 17:06:38,147 | INFO | speech length: 295360
2026-01-28 17:06:38,189 | INFO | decoder input length: 461
2026-01-28 17:06:38,189 | INFO | max output length: 461
2026-01-28 17:06:38,189 | INFO | min output length: 46
2026-01-28 17:07:04,452 | INFO | end detected at 290
2026-01-28 17:07:04,455 | INFO | -28.89 * 0.5 = -14.45 for decoder
2026-01-28 17:07:04,455 | INFO |  -5.07 * 0.5 =  -2.54 for ctc
2026-01-28 17:07:04,455 | INFO | total log probability: -16.98
2026-01-28 17:07:04,455 | INFO | normalized log probability: -0.06
2026-01-28 17:07:04,455 | INFO | total number of ended hypotheses: 212
2026-01-28 17:07:04,459 | INFO | best hypo: réforme<space>du<space>droit<space>de<space>licenciement<space>nomination<space>d'un<space>médiateur<space>entre<space>l'administration<space>et<space>le<space>public<space>contrôle<space>des<space>dépenses<space>électorales<space>toute<space>mesure<space>qui<space>dormait<space>dans<space>vos<space>cartons<space>que<space>vos<space>prédécesseurs<space>avaient<space>énergiquement<space>refusé<space>et<space>que<space>l'on<space>retrouve<space>dans<space>le<space>programme<space>commun<space>de<space>la<space>goupe

2026-01-28 17:07:04,463 | INFO | speech length: 14240
2026-01-28 17:07:04,502 | INFO | decoder input length: 21
2026-01-28 17:07:04,502 | INFO | max output length: 21
2026-01-28 17:07:04,502 | INFO | min output length: 2
2026-01-28 17:07:05,087 | INFO | end detected at 16
2026-01-28 17:07:05,090 | INFO |  -2.34 * 0.5 =  -1.17 for decoder
2026-01-28 17:07:05,090 | INFO |  -9.43 * 0.5 =  -4.72 for ctc
2026-01-28 17:07:05,091 | INFO | total log probability: -5.89
2026-01-28 17:07:05,091 | INFO | normalized log probability: -0.98
2026-01-28 17:07:05,091 | INFO | total number of ended hypotheses: 217
2026-01-28 17:07:05,091 | INFO | best hypo: bien

2026-01-28 17:07:05,093 | INFO | speech length: 83840
2026-01-28 17:07:05,139 | INFO | decoder input length: 130
2026-01-28 17:07:05,139 | INFO | max output length: 130
2026-01-28 17:07:05,139 | INFO | min output length: 13
2026-01-28 17:07:08,816 | INFO | end detected at 68
2026-01-28 17:07:08,818 | INFO |  -4.98 * 0.5 =  -2.49 for decoder
2026-01-28 17:07:08,818 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-28 17:07:08,818 | INFO | total log probability: -2.50
2026-01-28 17:07:08,818 | INFO | normalized log probability: -0.04
2026-01-28 17:07:08,818 | INFO | total number of ended hypotheses: 170
2026-01-28 17:07:08,820 | INFO | best hypo: lorsque<space>vous<space>semblez<space>mettre<space>en<space>doute<space>notre<space>amour<space>des<space>libertés

2026-01-28 17:07:08,822 | INFO | speech length: 56000
2026-01-28 17:07:08,875 | INFO | decoder input length: 87
2026-01-28 17:07:08,875 | INFO | max output length: 87
2026-01-28 17:07:08,875 | INFO | min output length: 8
2026-01-28 17:07:12,350 | INFO | end detected at 48
2026-01-28 17:07:12,353 | INFO |  -3.40 * 0.5 =  -1.70 for decoder
2026-01-28 17:07:12,353 | INFO |  -0.05 * 0.5 =  -0.02 for ctc
2026-01-28 17:07:12,353 | INFO | total log probability: -1.72
2026-01-28 17:07:12,353 | INFO | normalized log probability: -0.04
2026-01-28 17:07:12,353 | INFO | total number of ended hypotheses: 168
2026-01-28 17:07:12,354 | INFO | best hypo: c'est<space>un<space>outrage<space>que<space>nous<space>n'acceptons<space>pas

2026-01-28 17:07:12,358 | INFO | speech length: 107360
2026-01-28 17:07:12,404 | INFO | decoder input length: 167
2026-01-28 17:07:12,405 | INFO | max output length: 167
2026-01-28 17:07:12,405 | INFO | min output length: 16
2026-01-28 17:07:17,778 | INFO | end detected at 111
2026-01-28 17:07:17,780 | INFO |  -8.44 * 0.5 =  -4.22 for decoder
2026-01-28 17:07:17,780 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-28 17:07:17,780 | INFO | total log probability: -4.22
2026-01-28 17:07:17,780 | INFO | normalized log probability: -0.04
2026-01-28 17:07:17,780 | INFO | total number of ended hypotheses: 170
2026-01-28 17:07:17,781 | INFO | best hypo: nous<space>sommes<space>les<space>héritiers<space>de<space>la<space>tradition<space>qui<space>a<space>instauré<space>dans<space>ce<space>pays<space>la<space>démocratie<space>politique<space>et<space>sociale

2026-01-28 17:07:17,783 | INFO | speech length: 68481
2026-01-28 17:07:17,828 | INFO | decoder input length: 106
2026-01-28 17:07:17,828 | INFO | max output length: 106
2026-01-28 17:07:17,828 | INFO | min output length: 10
2026-01-28 17:07:20,109 | INFO | end detected at 53
2026-01-28 17:07:20,111 | INFO |  -4.21 * 0.5 =  -2.11 for decoder
2026-01-28 17:07:20,112 | INFO |  -4.74 * 0.5 =  -2.37 for ctc
2026-01-28 17:07:20,112 | INFO | total log probability: -4.48
2026-01-28 17:07:20,112 | INFO | normalized log probability: -0.10
2026-01-28 17:07:20,112 | INFO | total number of ended hypotheses: 199
2026-01-28 17:07:20,112 | INFO | best hypo: toujours<space>toujours<space>contre<space>la<space>droite<space>coalisée

2026-01-28 17:07:20,115 | INFO | speech length: 421120
2026-01-28 17:07:20,158 | INFO | decoder input length: 657
2026-01-28 17:07:20,158 | INFO | max output length: 657
2026-01-28 17:07:20,158 | INFO | min output length: 65
2026-01-28 17:07:47,343 | INFO | end detected at 250
2026-01-28 17:07:47,346 | INFO | -39.21 * 0.5 = -19.61 for decoder
2026-01-28 17:07:47,347 | INFO | -22.07 * 0.5 = -11.03 for ctc
2026-01-28 17:07:47,347 | INFO | total log probability: -30.64
2026-01-28 17:07:47,347 | INFO | normalized log probability: -0.13
2026-01-28 17:07:47,347 | INFO | total number of ended hypotheses: 219
2026-01-28 17:07:47,350 | INFO | best hypo: nos<space>combats<space>pour<space>la<space>conquête<space>du<space>droit<space>jalonnent<space>l'histoire<space>des<space>deux<space>derniers<space>siècles<space>c'est<space>à<space>ceux<space>de<space>votre<space>tradition<space>que<space>nous<space>avons<space>à<space>rasser<space>le<space>suffrage<space>universel<space>la<space>liberté<space>d'association<space>que<space>nous<space>avons<space>à<space>arracher<space>la<space>liberté<space>d'association

2026-01-28 17:07:47,353 | INFO | speech length: 53760
2026-01-28 17:07:47,396 | INFO | decoder input length: 83
2026-01-28 17:07:47,396 | INFO | max output length: 83
2026-01-28 17:07:47,396 | INFO | min output length: 8
2026-01-28 17:07:49,506 | INFO | end detected at 54
2026-01-28 17:07:49,508 | INFO |  -3.93 * 0.5 =  -1.97 for decoder
2026-01-28 17:07:49,508 | INFO |  -0.80 * 0.5 =  -0.40 for ctc
2026-01-28 17:07:49,508 | INFO | total log probability: -2.37
2026-01-28 17:07:49,508 | INFO | normalized log probability: -0.05
2026-01-28 17:07:49,508 | INFO | total number of ended hypotheses: 174
2026-01-28 17:07:49,509 | INFO | best hypo: que<space>nous<space>avons<space>arraché<space>la<space>liberté<space>de<space>la<space>presse

2026-01-28 17:07:49,510 | INFO | speech length: 177120
2026-01-28 17:07:49,552 | INFO | decoder input length: 276
2026-01-28 17:07:49,552 | INFO | max output length: 276
2026-01-28 17:07:49,552 | INFO | min output length: 27
2026-01-28 17:07:58,813 | INFO | end detected at 150
2026-01-28 17:07:58,815 | INFO | -31.13 * 0.5 = -15.57 for decoder
2026-01-28 17:07:58,815 | INFO | -19.77 * 0.5 =  -9.88 for ctc
2026-01-28 17:07:58,815 | INFO | total log probability: -25.45
2026-01-28 17:07:58,815 | INFO | normalized log probability: -0.18
2026-01-28 17:07:58,815 | INFO | total number of ended hypotheses: 181
2026-01-28 17:07:58,817 | INFO | best hypo: le<space>droit<space>de<space>grève<space>le<space>droit<space>à<space>l'instruction<space>segon<space>excellons<space>nous<space>serons<space>à<space>monsieur<space>mitterrand<space>nous<space>selons<space>monsieur<space>bitant<space>voulez<space>vous<space>continuer

2026-01-28 17:07:58,819 | INFO | speech length: 81440
2026-01-28 17:07:58,855 | INFO | decoder input length: 126
2026-01-28 17:07:58,855 | INFO | max output length: 126
2026-01-28 17:07:58,855 | INFO | min output length: 12
2026-01-28 17:08:02,783 | INFO | end detected at 92
2026-01-28 17:08:02,784 | INFO |  -6.90 * 0.5 =  -3.45 for decoder
2026-01-28 17:08:02,784 | INFO |  -0.05 * 0.5 =  -0.03 for ctc
2026-01-28 17:08:02,784 | INFO | total log probability: -3.47
2026-01-28 17:08:02,784 | INFO | normalized log probability: -0.04
2026-01-28 17:08:02,784 | INFO | total number of ended hypotheses: 179
2026-01-28 17:08:02,786 | INFO | best hypo: mais<space>monsieur<space>le<space>président<space>il<space>est<space>naturel<space>que<space>la<space>majorité<space>se<space>montre<space>telle<space>qu'elle<space>est

2026-01-28 17:08:02,788 | INFO | speech length: 55360
2026-01-28 17:08:02,825 | INFO | decoder input length: 86
2026-01-28 17:08:02,825 | INFO | max output length: 86
2026-01-28 17:08:02,825 | INFO | min output length: 8
2026-01-28 17:08:04,956 | INFO | end detected at 53
2026-01-28 17:08:04,960 | INFO |  -7.18 * 0.5 =  -3.59 for decoder
2026-01-28 17:08:04,960 | INFO | -16.35 * 0.5 =  -8.18 for ctc
2026-01-28 17:08:04,960 | INFO | total log probability: -11.77
2026-01-28 17:08:04,960 | INFO | normalized log probability: -0.36
2026-01-28 17:08:04,960 | INFO | total number of ended hypotheses: 217
2026-01-28 17:08:04,961 | INFO | best hypo: poursuivez<space>monsieur<space>coalisé<space>que

2026-01-28 17:08:04,967 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:08:04,968 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:08:04,968 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:08:04,969 | INFO | Chunk: 3 | WER=14.814815 | S=4 D=0 I=0
2026-01-28 17:08:04,971 | INFO | Chunk: 4 | WER=11.363636 | S=5 D=0 I=0
2026-01-28 17:08:04,971 | INFO | Chunk: 5 | WER=0.000000 | S=1 D=0 I=0
2026-01-28 17:08:04,971 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:08:04,971 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:08:04,972 | INFO | Chunk: 8 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:08:04,972 | INFO | Chunk: 9 | WER=50.000000 | S=3 D=0 I=0
2026-01-28 17:08:04,974 | INFO | Chunk: 10 | WER=10.000000 | S=2 D=0 I=2
2026-01-28 17:08:04,974 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:08:04,975 | INFO | Chunk: 12 | WER=34.782609 | S=8 D=0 I=0
2026-01-28 17:08:04,975 | INFO | Chunk: 13 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:08:04,975 | INFO | Chunk: 14 | WER=62.500000 | S=1 D=4 I=0
2026-01-28 17:08:05,006 | INFO | File: Rhap-D2006.wav | WER=12.605042 | S=23 D=4 I=3
2026-01-28 17:08:05,006 | INFO | ------------------------------
2026-01-28 17:08:05,006 | INFO | Conf ester Done!
2026-01-28 17:09:53,643 | INFO | Chunk: 0 | WER=16.666667 | S=0 D=0 I=1
2026-01-28 17:09:53,644 | INFO | Chunk: 1 | WER=5.263158 | S=1 D=0 I=0
2026-01-28 17:09:53,645 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:09:53,646 | INFO | Chunk: 3 | WER=14.814815 | S=3 D=1 I=0
2026-01-28 17:09:53,648 | INFO | Chunk: 4 | WER=13.636364 | S=5 D=1 I=0
2026-01-28 17:09:53,648 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:09:53,649 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:09:53,649 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:09:53,650 | INFO | Chunk: 8 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:09:53,650 | INFO | Chunk: 9 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:09:53,652 | INFO | Chunk: 10 | WER=22.500000 | S=7 D=1 I=1
2026-01-28 17:09:53,652 | INFO | Chunk: 11 | WER=33.333333 | S=1 D=1 I=1
2026-01-28 17:09:53,653 | INFO | Chunk: 12 | WER=39.130435 | S=6 D=3 I=0
2026-01-28 17:09:53,654 | INFO | Chunk: 13 | WER=6.250000 | S=0 D=1 I=0
2026-01-28 17:09:53,654 | INFO | Chunk: 14 | WER=75.000000 | S=6 D=0 I=0
2026-01-28 17:09:53,696 | INFO | File: Rhap-D2006.wav | WER=16.806723 | S=29 D=8 I=3
2026-01-28 17:09:53,696 | INFO | ------------------------------
2026-01-28 17:09:53,697 | INFO | hmm_tdnn Done!
2026-01-28 17:09:53,873 | INFO | ==================================Rhap-D2007.wav=========================================
2026-01-28 17:09:54,129 | INFO | Using rVAD model
2026-01-28 17:10:12,261 | INFO | Chunk: 0 | WER=43.333333 | S=16 D=36 I=0
2026-01-28 17:10:12,268 | INFO | Chunk: 1 | WER=41.406250 | S=19 D=34 I=0
2026-01-28 17:10:12,272 | INFO | Chunk: 2 | WER=28.571429 | S=6 D=15 I=1
2026-01-28 17:10:12,272 | INFO | Chunk: 3 | WER=50.000000 | S=4 D=4 I=0
2026-01-28 17:10:12,273 | INFO | Chunk: 4 | WER=20.000000 | S=3 D=3 I=2
2026-01-28 17:10:12,276 | INFO | Chunk: 5 | WER=50.000000 | S=14 D=25 I=1
2026-01-28 17:10:12,277 | INFO | Chunk: 6 | WER=7.142857 | S=0 D=1 I=0
2026-01-28 17:10:12,284 | INFO | Chunk: 7 | WER=69.753086 | S=20 D=93 I=0
2026-01-28 17:10:12,284 | INFO | Chunk: 8 | WER=42.857143 | S=3 D=2 I=1
2026-01-28 17:10:12,287 | INFO | Chunk: 9 | WER=46.575342 | S=10 D=24 I=0
2026-01-28 17:10:12,287 | INFO | Chunk: 10 | WER=56.521739 | S=5 D=8 I=0
2026-01-28 17:10:12,287 | INFO | Chunk: 11 | WER=28.571429 | S=2 D=2 I=0
2026-01-28 17:10:12,288 | INFO | Chunk: 12 | WER=30.000000 | S=6 D=2 I=1
2026-01-28 17:10:12,289 | INFO | Chunk: 13 | WER=27.272727 | S=2 D=0 I=1
2026-01-28 17:10:12,291 | INFO | Chunk: 14 | WER=45.762712 | S=17 D=8 I=2
2026-01-28 17:10:12,297 | INFO | Chunk: 15 | WER=38.983051 | S=13 D=31 I=2
2026-01-28 17:10:12,679 | INFO | File: Rhap-D2007.wav | WER=44.739530 | S=143 D=286 I=9
2026-01-28 17:10:12,679 | INFO | ------------------------------
2026-01-28 17:10:12,679 | INFO | w2vec vad chunk Done!
2026-01-28 17:10:28,678 | INFO | Chunk: 0 | WER=94.166667 | S=0 D=113 I=0
2026-01-28 17:10:28,681 | INFO | Chunk: 1 | WER=85.156250 | S=4 D=105 I=0
2026-01-28 17:10:28,682 | INFO | Chunk: 2 | WER=81.818182 | S=5 D=58 I=0
2026-01-28 17:10:28,683 | INFO | Chunk: 3 | WER=37.500000 | S=3 D=3 I=0
2026-01-28 17:10:28,684 | INFO | Chunk: 4 | WER=65.000000 | S=4 D=22 I=0
2026-01-28 17:10:28,685 | INFO | Chunk: 5 | WER=96.250000 | S=5 D=72 I=0
2026-01-28 17:10:28,685 | INFO | Chunk: 6 | WER=7.142857 | S=0 D=1 I=0
2026-01-28 17:10:28,688 | INFO | Chunk: 7 | WER=90.740741 | S=4 D=143 I=0
2026-01-28 17:10:28,688 | INFO | Chunk: 8 | WER=57.142857 | S=6 D=1 I=1
2026-01-28 17:10:28,689 | INFO | Chunk: 9 | WER=76.712329 | S=2 D=54 I=0
2026-01-28 17:10:28,690 | INFO | Chunk: 10 | WER=34.782609 | S=3 D=5 I=0
2026-01-28 17:10:28,690 | INFO | Chunk: 11 | WER=100.000000 | S=4 D=9 I=1
2026-01-28 17:10:28,691 | INFO | Chunk: 12 | WER=80.000000 | S=2 D=22 I=0
2026-01-28 17:10:28,691 | INFO | Chunk: 13 | WER=81.818182 | S=1 D=1 I=7
2026-01-28 17:10:28,692 | INFO | Chunk: 14 | WER=76.271186 | S=2 D=43 I=0
2026-01-28 17:10:28,695 | INFO | Chunk: 15 | WER=70.338983 | S=2 D=81 I=0
2026-01-28 17:10:28,830 | INFO | File: Rhap-D2007.wav | WER=79.570991 | S=55 D=724 I=0
2026-01-28 17:10:28,830 | INFO | ------------------------------
2026-01-28 17:10:28,831 | INFO | whisper med Done!
2026-01-28 17:10:56,122 | INFO | Chunk: 0 | WER=95.000000 | S=1 D=113 I=0
2026-01-28 17:10:56,125 | INFO | Chunk: 1 | WER=85.156250 | S=3 D=106 I=0
2026-01-28 17:10:56,127 | INFO | Chunk: 2 | WER=66.233766 | S=3 D=47 I=1
2026-01-28 17:10:56,127 | INFO | Chunk: 3 | WER=43.750000 | S=4 D=3 I=0
2026-01-28 17:10:56,128 | INFO | Chunk: 4 | WER=50.000000 | S=0 D=20 I=0
2026-01-28 17:10:56,131 | INFO | Chunk: 5 | WER=60.000000 | S=13 D=34 I=1
2026-01-28 17:10:56,131 | INFO | Chunk: 6 | WER=14.285714 | S=1 D=1 I=0
2026-01-28 17:10:56,135 | INFO | Chunk: 7 | WER=80.246914 | S=6 D=124 I=0
2026-01-28 17:10:56,136 | INFO | Chunk: 8 | WER=28.571429 | S=1 D=2 I=1
2026-01-28 17:10:56,137 | INFO | Chunk: 9 | WER=76.712329 | S=1 D=55 I=0
2026-01-28 17:10:56,137 | INFO | Chunk: 10 | WER=30.434783 | S=2 D=4 I=1
2026-01-28 17:10:56,138 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:10:56,138 | INFO | Chunk: 12 | WER=80.000000 | S=2 D=22 I=0
2026-01-28 17:10:56,139 | INFO | Chunk: 13 | WER=9.090909 | S=0 D=0 I=1
2026-01-28 17:10:56,140 | INFO | Chunk: 14 | WER=72.881356 | S=5 D=38 I=0
2026-01-28 17:10:56,143 | INFO | Chunk: 15 | WER=71.186441 | S=2 D=82 I=0
2026-01-28 17:10:56,321 | INFO | File: Rhap-D2007.wav | WER=71.195097 | S=43 D=650 I=4
2026-01-28 17:10:56,321 | INFO | ------------------------------
2026-01-28 17:10:56,321 | INFO | whisper large Done!
2026-01-28 17:10:56,455 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 17:10:56,493 | INFO | Vocabulary size: 350
2026-01-28 17:10:57,426 | INFO | Gradient checkpoint layers: []
2026-01-28 17:10:58,146 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 17:10:58,153 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 17:10:58,154 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 17:10:58,155 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 17:10:58,156 | INFO | speech length: 470400
2026-01-28 17:10:58,270 | INFO | decoder input length: 734
2026-01-28 17:10:58,270 | INFO | max output length: 734
2026-01-28 17:10:58,270 | INFO | min output length: 73
2026-01-28 17:11:28,346 | INFO | end detected at 222
2026-01-28 17:11:28,348 | INFO | -565.34 * 0.5 = -282.67 for decoder
2026-01-28 17:11:28,348 | INFO | -267.49 * 0.5 = -133.74 for ctc
2026-01-28 17:11:28,348 | INFO | total log probability: -416.41
2026-01-28 17:11:28,348 | INFO | normalized log probability: -1.95
2026-01-28 17:11:28,348 | INFO | total number of ended hypotheses: 130
2026-01-28 17:11:28,351 | INFO | best hypo: ▁ils▁étaient▁un▁peu▁choqués▁quand▁même▁s'ils▁ont▁été▁solidaires▁pour▁mon▁futur▁procès▁et▁te▁vaspont▁bien▁passé▁c'est▁à▁dire▁que▁j'ai▁vécu▁comme▁un▁choc▁quoi▁et▁un▁moi▁c'est▁pour▁ce▁que▁je▁vous▁appelle▁radiotinsique▁qu'i▁c'est▁pour▁mouiller▁perse▁romanai▁à▁l'antenne▁à▁moi▁j'ai▁dit▁un▁moi▁mais▁j'ai▁antenne▁vrai▁ment▁et▁donc▁on▁est▁partie▁et▁c'est▁brai▁avec▁jean▁françois▁allin▁alexandre▁demars▁et▁les▁supppers▁calis▁de▁notamment▁à▁la▁tes▁ensemble▁tes

2026-01-28 17:11:28,356 | INFO | speech length: 417600
2026-01-28 17:11:28,412 | INFO | decoder input length: 652
2026-01-28 17:11:28,413 | INFO | max output length: 652
2026-01-28 17:11:28,413 | INFO | min output length: 65
2026-01-28 17:11:59,622 | INFO | end detected at 256
2026-01-28 17:11:59,624 | INFO | -573.72 * 0.5 = -286.86 for decoder
2026-01-28 17:11:59,624 | INFO | -308.73 * 0.5 = -154.37 for ctc
2026-01-28 17:11:59,624 | INFO | total log probability: -441.23
2026-01-28 17:11:59,624 | INFO | normalized log probability: -1.76
2026-01-28 17:11:59,624 | INFO | total number of ended hypotheses: 169
2026-01-28 17:11:59,628 | INFO | best hypo: ▁oui▁oui▁en▁face▁c'▁c'est▁vrai▁que▁c'était▁le▁côté▁public▁direct▁et▁que▁ça▁c'est▁comme▁si▁je▁fais▁un▁spectacle▁en▁plus▁comme▁c'est▁l'époque▁où▁vous▁tourniez▁landrui▁à▁la▁barbe▁le▁bonnais▁s'inferne▁oui▁oui▁oui▁c'est▁contextez▁pas▁la▁radio▁cet▁derière▁moment▁et▁c'éper▁du▁chauge▁me▁planquait▁sous▁le▁buron▁de▁vous▁expliquez▁ce▁que▁je▁faisais▁c'ai▁j'ai▁sansez▁angels▁vous▁avez▁commencé▁à▁l'orssez▁réalisadteurs▁et▁suite▁scénariste▁en▁suite▁aniamateur▁de▁radio▁simpe▁à▁la▁carrière▁d'ors▁wai▁s'allembertte

2026-01-28 17:11:59,632 | INFO | speech length: 388160
2026-01-28 17:11:59,684 | INFO | decoder input length: 606
2026-01-28 17:11:59,684 | INFO | max output length: 606
2026-01-28 17:11:59,684 | INFO | min output length: 60
2026-01-28 17:12:16,859 | INFO | end detected at 145
2026-01-28 17:12:16,860 | INFO | -286.96 * 0.5 = -143.48 for decoder
2026-01-28 17:12:16,860 | INFO | -91.63 * 0.5 = -45.81 for ctc
2026-01-28 17:12:16,860 | INFO | total log probability: -189.30
2026-01-28 17:12:16,860 | INFO | normalized log probability: -1.35
2026-01-28 17:12:16,860 | INFO | total number of ended hypotheses: 147
2026-01-28 17:12:16,862 | INFO | best hypo: ▁on▁est▁tout▁à▁fait▁ça▁fait▁oui▁une▁autre▁question▁peut▁être▁j'ai▁plus▁de▁questions▁l'attends▁va▁faire▁une▁interview▁vaudeville▁dans▁ça▁marche▁et▁ça▁marche▁pour▁tous▁les▁deux▁he▁est▁ce▁qu'un▁après▁midi▁emmanuel▁emmanuel▁d'est▁un▁après▁midi▁en▁rentrant▁plutôt▁vous▁avez▁déjà▁surpris▁votre▁un▁petit▁ami▁au▁lit▁avec▁quelqu'un▁d'autre

2026-01-28 17:12:16,865 | INFO | speech length: 53280
2026-01-28 17:12:16,906 | INFO | decoder input length: 82
2026-01-28 17:12:16,906 | INFO | max output length: 82
2026-01-28 17:12:16,906 | INFO | min output length: 8
2026-01-28 17:12:18,578 | INFO | end detected at 33
2026-01-28 17:12:18,580 | INFO |  -5.90 * 0.5 =  -2.95 for decoder
2026-01-28 17:12:18,580 | INFO |  -9.33 * 0.5 =  -4.66 for ctc
2026-01-28 17:12:18,580 | INFO | total log probability: -7.61
2026-01-28 17:12:18,580 | INFO | normalized log probability: -0.27
2026-01-28 17:12:18,580 | INFO | total number of ended hypotheses: 166
2026-01-28 17:12:18,581 | INFO | best hypo: ▁sa▁soreille▁serait▁mort▁moi▁je▁serais▁en▁prison▁sa▁sereine

2026-01-28 17:12:18,583 | INFO | speech length: 187200
2026-01-28 17:12:18,645 | INFO | decoder input length: 292
2026-01-28 17:12:18,645 | INFO | max output length: 292
2026-01-28 17:12:18,645 | INFO | min output length: 29
2026-01-28 17:12:31,072 | INFO | end detected at 96
2026-01-28 17:12:31,076 | INFO | -55.10 * 0.5 = -27.55 for decoder
2026-01-28 17:12:31,076 | INFO | -19.92 * 0.5 =  -9.96 for ctc
2026-01-28 17:12:31,076 | INFO | total log probability: -37.51
2026-01-28 17:12:31,076 | INFO | normalized log probability: -0.43
2026-01-28 17:12:31,076 | INFO | total number of ended hypotheses: 197
2026-01-28 17:12:31,078 | INFO | best hypo: ▁moi▁je▁préviens▁vendre▁entrer▁et▁surtout▁plutôt▁quoi▁plus▁tard▁je▁préviens▁pas▁plutôt▁je▁préviens▁d'emmanuel▁est▁ce▁que▁vous▁avez▁déjà▁fait▁sortir▁amant▁ou▁une▁maîtresse▁par▁la▁fenêtre▁en▁catastrophe

2026-01-28 17:12:31,083 | INFO | speech length: 319840
2026-01-28 17:12:31,147 | INFO | decoder input length: 499
2026-01-28 17:12:31,147 | INFO | max output length: 499
2026-01-28 17:12:31,147 | INFO | min output length: 49
2026-01-28 17:12:45,442 | INFO | end detected at 149
2026-01-28 17:12:45,444 | INFO | -299.24 * 0.5 = -149.62 for decoder
2026-01-28 17:12:45,445 | INFO | -142.00 * 0.5 = -71.00 for ctc
2026-01-28 17:12:45,445 | INFO | total log probability: -220.62
2026-01-28 17:12:45,445 | INFO | normalized log probability: -1.55
2026-01-28 17:12:45,445 | INFO | total number of ended hypotheses: 200
2026-01-28 17:12:45,447 | INFO | best hypo: ▁on▁répéche▁jusqu'à▁noblesse▁que▁dans▁les▁voitures▁mais▁je▁ne▁suis▁pas▁une▁trompeuse▁mais▁j'aime▁pas▁je▁non▁j'aime▁pas▁le▁principe▁mais▁çai▁ma▁mère▁je▁le▁dét▁sans▁si▁on▁sans▁les▁parents▁s'en▁va▁mais▁c'est▁traits▁si▁désobisser▁mais▁voilà▁j'aime▁pers▁j▁d'tudeshabits▁successives▁en▁faite

2026-01-28 17:12:45,449 | INFO | speech length: 45760
2026-01-28 17:12:45,510 | INFO | decoder input length: 71
2026-01-28 17:12:45,510 | INFO | max output length: 71
2026-01-28 17:12:45,510 | INFO | min output length: 7
2026-01-28 17:12:47,744 | INFO | end detected at 37
2026-01-28 17:12:47,746 | INFO |  -7.16 * 0.5 =  -3.58 for decoder
2026-01-28 17:12:47,746 | INFO |  -7.38 * 0.5 =  -3.69 for ctc
2026-01-28 17:12:47,746 | INFO | total log probability: -7.27
2026-01-28 17:12:47,746 | INFO | normalized log probability: -0.24
2026-01-28 17:12:47,746 | INFO | total number of ended hypotheses: 183
2026-01-28 17:12:47,747 | INFO | best hypo: ▁moi▁je▁était▁fidèle▁quand▁j'ai▁aimé▁j'ai▁été▁fidèle

2026-01-28 17:12:47,749 | INFO | speech length: 460480
2026-01-28 17:12:47,797 | INFO | decoder input length: 719
2026-01-28 17:12:47,798 | INFO | max output length: 719
2026-01-28 17:12:47,798 | INFO | min output length: 71
2026-01-28 17:13:19,989 | INFO | end detected at 277
2026-01-28 17:13:19,991 | INFO | -806.22 * 0.5 = -403.11 for decoder
2026-01-28 17:13:19,991 | INFO | -402.80 * 0.5 = -201.40 for ctc
2026-01-28 17:13:19,991 | INFO | total log probability: -604.51
2026-01-28 17:13:19,991 | INFO | normalized log probability: -2.26
2026-01-28 17:13:19,991 | INFO | total number of ended hypotheses: 157
2026-01-28 17:13:19,994 | INFO | best hypo: ▁et▁toi▁c'était▁j'arrivais▁de▁faire▁sortir▁un▁amant▁ou▁une▁maîtresse▁par▁la▁fenêtre▁voyageuse▁qui▁va▁lire▁ce▁soir▁et▁au▁guinbreux▁tiens▁non▁est▁ce▁qu'on▁fier▁je▁prendrai▁faire▁entre▁madai▁un▁endroit▁d'avall▁et▁bons▁et▁déjà▁désormais▁à▁parce▁qu'ils▁confiéante▁et▁tupe▁jamais▁avant▁de▁la▁plaine▁pour▁eux▁quoi▁tubélié▁qu'au▁quoi▁il▁a▁été▁confié▁en▁télé▁puis▁cette▁hane▁royante▁che▁je▁pr▁rien▁d'être▁hé▁je▁ne▁savais▁pas▁oui▁je▁sac▁et▁terrible▁avec▁le▁faite▁t'habiller▁comme▁moi▁prendrait▁plus▁sincère▁c'en▁vrai▁ças▁jamais▁assez▁vrais▁qu'on▁d'abit▁mada

2026-01-28 17:13:19,997 | INFO | speech length: 185760
2026-01-28 17:13:20,052 | INFO | decoder input length: 289
2026-01-28 17:13:20,052 | INFO | max output length: 289
2026-01-28 17:13:20,052 | INFO | min output length: 28
2026-01-28 17:13:23,876 | INFO | end detected at 54
2026-01-28 17:13:23,878 | INFO | -25.05 * 0.5 = -12.53 for decoder
2026-01-28 17:13:23,878 | INFO | -45.80 * 0.5 = -22.90 for ctc
2026-01-28 17:13:23,878 | INFO | total log probability: -35.43
2026-01-28 17:13:23,878 | INFO | normalized log probability: -0.77
2026-01-28 17:13:23,878 | INFO | total number of ended hypotheses: 113
2026-01-28 17:13:23,879 | INFO | best hypo: ▁oui▁monsieur▁pourça▁je▁regarde▁à▁les▁charger▁de▁bientôt▁cher▁et▁grisonnant▁ma▁papa▁à▁partir▁de▁pas

2026-01-28 17:13:23,881 | INFO | speech length: 271680
2026-01-28 17:13:23,927 | INFO | decoder input length: 424
2026-01-28 17:13:23,927 | INFO | max output length: 424
2026-01-28 17:13:23,927 | INFO | min output length: 42
2026-01-28 17:13:37,057 | INFO | end detected at 157
2026-01-28 17:13:37,058 | INFO | -208.07 * 0.5 = -104.03 for decoder
2026-01-28 17:13:37,058 | INFO | -44.83 * 0.5 = -22.42 for ctc
2026-01-28 17:13:37,058 | INFO | total log probability: -126.45
2026-01-28 17:13:37,058 | INFO | normalized log probability: -0.84
2026-01-28 17:13:37,058 | INFO | total number of ended hypotheses: 158
2026-01-28 17:13:37,060 | INFO | best hypo: ▁emmanuelle▁est▁ce▁que▁vous▁êtes▁déjà▁écrié▁ciel▁mon▁mari▁en▁entendant▁la▁porte▁d'entrée▁s'ouvrir▁j'ai▁jamais▁si▁jamais▁je▁ne▁suis▁jamais▁marié▁vu▁déjà▁arrivée▁dire▁seulement▁marie▁tout▁en▁entendant▁la▁porte▁que▁ça▁vous▁voudrait▁dire▁que▁tu▁trop▁tome▁et▁non▁plus▁tu▁chez▁toi▁plein▁non▁je▁n'ai▁jamais▁trompé▁mon▁mari

2026-01-28 17:13:37,063 | INFO | speech length: 58080
2026-01-28 17:13:37,111 | INFO | decoder input length: 90
2026-01-28 17:13:37,111 | INFO | max output length: 90
2026-01-28 17:13:37,111 | INFO | min output length: 9
2026-01-28 17:13:39,151 | INFO | end detected at 39
2026-01-28 17:13:39,153 | INFO |  -6.29 * 0.5 =  -3.14 for decoder
2026-01-28 17:13:39,153 | INFO | -17.81 * 0.5 =  -8.90 for ctc
2026-01-28 17:13:39,153 | INFO | total log probability: -12.05
2026-01-28 17:13:39,153 | INFO | normalized log probability: -0.38
2026-01-28 17:13:39,153 | INFO | total number of ended hypotheses: 165
2026-01-28 17:13:39,154 | INFO | best hypo: ▁est▁ce▁que▁t'as▁dit▁à▁ta▁petite▁amie▁menon▁se▁parce▁que▁tu▁croises

2026-01-28 17:13:39,157 | INFO | speech length: 57600
2026-01-28 17:13:39,204 | INFO | decoder input length: 89
2026-01-28 17:13:39,204 | INFO | max output length: 89
2026-01-28 17:13:39,204 | INFO | min output length: 8
2026-01-28 17:13:41,095 | INFO | end detected at 36
2026-01-28 17:13:41,097 | INFO |  -4.86 * 0.5 =  -2.43 for decoder
2026-01-28 17:13:41,097 | INFO |  -2.32 * 0.5 =  -1.16 for ctc
2026-01-28 17:13:41,097 | INFO | total log probability: -3.59
2026-01-28 17:13:41,097 | INFO | normalized log probability: -0.12
2026-01-28 17:13:41,098 | INFO | total number of ended hypotheses: 167
2026-01-28 17:13:41,098 | INFO | best hypo: ▁j'arrivais▁ça▁parce▁que▁c'était▁la▁vérité▁à▁chaque▁fois

2026-01-28 17:13:41,101 | INFO | speech length: 132000
2026-01-28 17:13:41,158 | INFO | decoder input length: 205
2026-01-28 17:13:41,158 | INFO | max output length: 205
2026-01-28 17:13:41,158 | INFO | min output length: 20
2026-01-28 17:13:48,065 | INFO | end detected at 64
2026-01-28 17:13:48,067 | INFO | -15.47 * 0.5 =  -7.74 for decoder
2026-01-28 17:13:48,067 | INFO | -19.38 * 0.5 =  -9.69 for ctc
2026-01-28 17:13:48,067 | INFO | total log probability: -17.43
2026-01-28 17:13:48,068 | INFO | normalized log probability: -0.30
2026-01-28 17:13:48,068 | INFO | total number of ended hypotheses: 154
2026-01-28 17:13:48,069 | INFO | best hypo: ▁ce▁sont▁le▁césar▁un▁quel▁acteur▁est▁ce▁que▁dans▁l'intimité▁vous▁avez▁dû▁peuplé▁votre▁petite▁amie▁votre▁fiancée▁par▁un▁autre▁prénom▁lucien

2026-01-28 17:13:48,073 | INFO | speech length: 116000
2026-01-28 17:13:48,122 | INFO | decoder input length: 180
2026-01-28 17:13:48,122 | INFO | max output length: 180
2026-01-28 17:13:48,122 | INFO | min output length: 18
2026-01-28 17:13:50,366 | INFO | end detected at 33
2026-01-28 17:13:50,369 | INFO |  -3.42 * 0.5 =  -1.71 for decoder
2026-01-28 17:13:50,370 | INFO |  -5.76 * 0.5 =  -2.88 for ctc
2026-01-28 17:13:50,370 | INFO | total log probability: -4.59
2026-01-28 17:13:50,370 | INFO | normalized log probability: -0.16
2026-01-28 17:13:50,370 | INFO | total number of ended hypotheses: 173
2026-01-28 17:13:50,370 | INFO | best hypo: ▁chacun▁oui▁c'est▁arrivé▁parce▁que▁le▁début▁c'était▁là

2026-01-28 17:13:50,373 | INFO | speech length: 340640
2026-01-28 17:13:50,422 | INFO | decoder input length: 531
2026-01-28 17:13:50,422 | INFO | max output length: 531
2026-01-28 17:13:50,422 | INFO | min output length: 53
2026-01-28 17:14:04,987 | INFO | end detected at 124
2026-01-28 17:14:04,989 | INFO | -253.12 * 0.5 = -126.56 for decoder
2026-01-28 17:14:04,989 | INFO | -105.75 * 0.5 = -52.88 for ctc
2026-01-28 17:14:04,989 | INFO | total log probability: -179.44
2026-01-28 17:14:04,989 | INFO | normalized log probability: -1.56
2026-01-28 17:14:04,989 | INFO | total number of ended hypotheses: 167
2026-01-28 17:14:04,991 | INFO | best hypo: ▁ah▁voyez▁ce▁que▁je▁veux▁dire▁donc▁très▁bien▁voilà▁ray▁doit▁sauter▁d'arrivée▁euh▁bas▁en▁général▁je▁prends▁tout▁avec▁le▁même▁prénom▁jacques▁c'est▁bien▁à▁ginettes▁à▁la▁basse▁barbe▁rick▁est▁ce▁que▁téléphone▁et▁vous▁avaient▁déjà▁chauffé▁la▁mere▁en▁pensant▁que▁je▁vrai

2026-01-28 17:14:04,994 | INFO | speech length: 408320
2026-01-28 17:14:05,046 | INFO | decoder input length: 637
2026-01-28 17:14:05,046 | INFO | max output length: 637
2026-01-28 17:14:05,046 | INFO | min output length: 63
2026-01-28 17:14:34,750 | INFO | end detected at 234
2026-01-28 17:14:34,752 | INFO | -686.84 * 0.5 = -343.42 for decoder
2026-01-28 17:14:34,752 | INFO | -245.08 * 0.5 = -122.54 for ctc
2026-01-28 17:14:34,752 | INFO | total log probability: -465.96
2026-01-28 17:14:34,752 | INFO | normalized log probability: -2.05
2026-01-28 17:14:34,752 | INFO | total number of ended hypotheses: 172
2026-01-28 17:14:34,755 | INFO | best hypo: ▁euh▁non▁lance▁qui▁est▁arrivée▁c'est▁des▁fois▁de▁déconnu▁en▁appelant▁un▁pote▁et▁tomber▁sur▁son▁père▁mais▁ça▁s'était▁pas▁une▁relation▁amoureuse▁mais▁on▁ne▁s'est▁pas▁lieu▁mais▁eu▁ce▁se▁se▁gourret▁puis▁uffé▁que▁s'ins▁allait▁à▁draguouillers▁la▁mère▁ou▁draguée▁où▁on▁faire▁la▁malaration▁d'amourant▁à▁la▁mère▁non▁rendstous▁c'elle▁oui▁oui▁mais▁ie▁çans▁peur▁c'était▁la▁fille▁croyante▁c'est▁ma▁fille▁ah▁oui▁h▁non▁j'ai▁toujours▁fétté▁la▁mère▁d'aller▁elle▁répon▁d'abord▁elle▁compr

2026-01-28 17:14:34,769 | INFO | Chunk: 0 | WER=59.166667 | S=42 D=27 I=2
2026-01-28 17:14:34,777 | INFO | Chunk: 1 | WER=53.906250 | S=40 D=27 I=2
2026-01-28 17:14:34,780 | INFO | Chunk: 2 | WER=38.961039 | S=12 D=14 I=4
2026-01-28 17:14:34,781 | INFO | Chunk: 3 | WER=56.250000 | S=4 D=5 I=0
2026-01-28 17:14:34,782 | INFO | Chunk: 4 | WER=30.000000 | S=6 D=5 I=1
2026-01-28 17:14:34,785 | INFO | Chunk: 5 | WER=60.000000 | S=28 D=20 I=0
2026-01-28 17:14:34,786 | INFO | Chunk: 6 | WER=28.571429 | S=2 D=2 I=0
2026-01-28 17:14:34,796 | INFO | Chunk: 7 | WER=69.753086 | S=63 D=49 I=1
2026-01-28 17:14:34,796 | INFO | Chunk: 8 | WER=121.428571 | S=6 D=3 I=8
2026-01-28 17:14:34,800 | INFO | Chunk: 9 | WER=32.876712 | S=13 D=10 I=1
2026-01-28 17:14:34,800 | INFO | Chunk: 10 | WER=52.173913 | S=5 D=7 I=0
2026-01-28 17:14:34,800 | INFO | Chunk: 11 | WER=28.571429 | S=2 D=2 I=0
2026-01-28 17:14:34,801 | INFO | Chunk: 12 | WER=43.333333 | S=8 D=4 I=1
2026-01-28 17:14:34,802 | INFO | Chunk: 13 | WER=18.181818 | S=1 D=0 I=1
2026-01-28 17:14:34,804 | INFO | Chunk: 14 | WER=49.152542 | S=18 D=8 I=3
2026-01-28 17:14:34,811 | INFO | Chunk: 15 | WER=53.389831 | S=38 D=21 I=4
2026-01-28 17:14:35,250 | INFO | File: Rhap-D2007.wav | WER=52.808989 | S=301 D=196 I=20
2026-01-28 17:14:35,250 | INFO | ------------------------------
2026-01-28 17:14:35,250 | INFO | Conf cv Done!
2026-01-28 17:14:35,385 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 17:14:35,408 | INFO | Vocabulary size: 47
2026-01-28 17:14:36,295 | INFO | Gradient checkpoint layers: []
2026-01-28 17:14:37,036 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 17:14:37,041 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 17:14:37,042 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 17:14:37,042 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 17:14:37,046 | INFO | speech length: 470400
2026-01-28 17:14:37,092 | INFO | decoder input length: 734
2026-01-28 17:14:37,092 | INFO | max output length: 734
2026-01-28 17:14:37,092 | INFO | min output length: 73
2026-01-28 17:15:48,841 | INFO | end detected at 537
2026-01-28 17:15:48,843 | INFO | -488.09 * 0.5 = -244.04 for decoder
2026-01-28 17:15:48,843 | INFO | -193.77 * 0.5 = -96.89 for ctc
2026-01-28 17:15:48,843 | INFO | total log probability: -340.93
2026-01-28 17:15:48,843 | INFO | normalized log probability: -0.64
2026-01-28 17:15:48,843 | INFO | total number of ended hypotheses: 195
2026-01-28 17:15:48,850 | INFO | best hypo: alors<space>j'étais<space>un<space>peu<space>choqué<space>quand<space>même<space>mais<space>ça<space>sont<space>innocés<space>on<space>fait<space>pas<space>elle<space>était<space>solidaire<space>pour<space>mon<space>futur<space>procès<space>ouais<space>çat<space>vachement<space>bien<space>ben<space>c'est<space>c'est<space>à<space>dire<space>que<space>je<space>l'ai<space>vécu<space>comme<space>un<space>show<space>quoi<space>ça<space>fait<space>un<space>mois<space>c'est<space>pour<space>ça<space>que<space>ça<space>s'appelle<space>radio<space>timzide<space>quoi<space>c'est<space>pour<space>mouiller<space>personne<space>fin<space>on<space>m'a<space>dit<space>à<space>l'antenne<space>un<space>moi<space>j'ai<space>dit<space>un<space>moi<space>mais<space>j'ai<space>l'antenne<space>vraiment<space>et<space>donc<space>on<space>est<space>parti<space>eun<space>c'est<space>vrai<space>avect<space>jean<space>françois<space>lel<space>alexandre<space>de<space>bles<space>mais<space>les<space>esuer<space>complices<space>on<space>et<space>et<space>det<space>qun<space>ça<space>le<space>ralet<space>lesebl<space>et<space>el<space>et<space>aussil

2026-01-28 17:15:48,852 | INFO | speech length: 417600
2026-01-28 17:15:48,889 | INFO | decoder input length: 652
2026-01-28 17:15:48,890 | INFO | max output length: 652
2026-01-28 17:15:48,890 | INFO | min output length: 65
2026-01-28 17:16:38,807 | INFO | end detected at 574
2026-01-28 17:16:38,809 | INFO | -696.91 * 0.5 = -348.45 for decoder
2026-01-28 17:16:38,809 | INFO | -79.42 * 0.5 = -39.71 for ctc
2026-01-28 17:16:38,809 | INFO | total log probability: -388.16
2026-01-28 17:16:38,809 | INFO | normalized log probability: -0.69
2026-01-28 17:16:38,809 | INFO | total number of ended hypotheses: 208
2026-01-28 17:16:38,817 | INFO | best hypo: oui<space>oui<space>oui<space>enfin<space>c'est<space>c'est<space>vrai<space>que<space>c'était<space>le<space>côté<space>public<space>direct<space>et<space>que<space>et<space>que<space>ça<space>c'est<space>c'est<space>comme<space>si<space>je<space>faisais<space>un<space>spectacle<space>en<space>plus<space>comme<space>c'est<space>l'époque<space>où<space>vous<space>tourniez<space>l'endrue<space>à<space>la<space>barbe<space>le<space>bonet<space>c'est<space>un<space>ferme<space>oui<space>mais<space>oui<space>c'est<space>pour<space>ça<space>que<space>c'était<space>pas<space>d<space>la<space>radio<space>c'était<space>à<space>un<space>moment<space>c'est<space>c'était<space>du<space>chose<space>je<space>me<space>plaquais<space>sur<space>le<space>bureau<space>de<space>m'expliquer<space>ce<space>que<space>je<space>faisais<space>euh<space>c'est<space>c'est<space>or<space>c'est<space>marant<space>parce<space>que<space>le<space>vous<space>avez<space>commensé<space>cteurs<space>ensuite<space>réalisateur<space>ensuite<space>scénariste<space>ensuite<space>animateur<space>radio<space>c'est<space>un<space>peu<space>la<space>carrière<space>d'horson<space>waite<space>ça<space>l'emern

2026-01-28 17:16:38,819 | INFO | speech length: 388160
2026-01-28 17:16:38,858 | INFO | decoder input length: 606
2026-01-28 17:16:38,858 | INFO | max output length: 606
2026-01-28 17:16:38,858 | INFO | min output length: 60
2026-01-28 17:17:15,002 | INFO | end detected at 354
2026-01-28 17:17:15,004 | INFO | -102.78 * 0.5 = -51.39 for decoder
2026-01-28 17:17:15,004 | INFO | -27.07 * 0.5 = -13.53 for ctc
2026-01-28 17:17:15,004 | INFO | total log probability: -64.92
2026-01-28 17:17:15,004 | INFO | normalized log probability: -0.19
2026-01-28 17:17:15,004 | INFO | total number of ended hypotheses: 137
2026-01-28 17:17:15,009 | INFO | best hypo: de<space>toute<space>façon<space>oui<space>tout<space>à<space>fait<space>tout<space>à<space>fait<space>oui<space>une<space>autre<space>question<space>peut<space>être<space>j'ai<space>plus<space>de<space>question<space>maintenant<space>on<space>va<space>faire<space>une<space>interview<space>votre<space>ville<space>alors<space>ça<space>marche<space>ça<space>marche<space>pour<space>tous<space>les<space>deux<space>est<space>ce<space>qu'un<space>après<space>midi<space>emmanuel<space>emmanuel<space>est<space>ce<space>qu'un<space>après<space>midi<space>en<space>rentrant<space>plustôt<space>vous<space>avez<space>déjà<space>surpris<space>votre<space>petit<space>ami<space>au<space>lit<space>avec<space>quelqu'un<space>d'autre

2026-01-28 17:17:15,013 | INFO | speech length: 53280
2026-01-28 17:17:15,070 | INFO | decoder input length: 82
2026-01-28 17:17:15,070 | INFO | max output length: 82
2026-01-28 17:17:15,070 | INFO | min output length: 8
2026-01-28 17:17:19,857 | INFO | end detected at 71
2026-01-28 17:17:19,861 | INFO |  -7.65 * 0.5 =  -3.82 for decoder
2026-01-28 17:17:19,862 | INFO |  -6.31 * 0.5 =  -3.15 for ctc
2026-01-28 17:17:19,862 | INFO | total log probability: -6.98
2026-01-28 17:17:19,862 | INFO | normalized log probability: -0.11
2026-01-28 17:17:19,862 | INFO | total number of ended hypotheses: 204
2026-01-28 17:17:19,864 | INFO | best hypo: ça<space>serait<space>i<space>serait<space>mort<space>moi<space>je<space>serai<space>en<space>prison<space>ça<space>serait<space>donc

2026-01-28 17:17:19,868 | INFO | speech length: 187200
2026-01-28 17:17:19,917 | INFO | decoder input length: 292
2026-01-28 17:17:19,917 | INFO | max output length: 292
2026-01-28 17:17:19,917 | INFO | min output length: 29
2026-01-28 17:17:34,450 | INFO | end detected at 226
2026-01-28 17:17:34,452 | INFO | -20.49 * 0.5 = -10.25 for decoder
2026-01-28 17:17:34,452 | INFO |  -9.92 * 0.5 =  -4.96 for ctc
2026-01-28 17:17:34,452 | INFO | total log probability: -15.20
2026-01-28 17:17:34,452 | INFO | normalized log probability: -0.07
2026-01-28 17:17:34,453 | INFO | total number of ended hypotheses: 197
2026-01-28 17:17:34,456 | INFO | best hypo: euh<space>moi<space>je<space>je<space>je<space>préviens<space>avant<space>de<space>rentrer<space>et<space>surtout<space>plus<space>tôt<space>quoi<space>plus<space>tard<space>je<space>préviens<space>pas<space>plutôt<space>je<space>préviens<space>des<space>manuels<space>est<space>ce<space>que<space>vous<space>avez<space>déjà<space>fait<space>sortir<space>un<space>amant<space>ou<space>une<space>maîtresse<space>par<space>la<space>fenêtre<space>en<space>catastrophe

2026-01-28 17:17:34,458 | INFO | speech length: 319840
2026-01-28 17:17:34,504 | INFO | decoder input length: 499
2026-01-28 17:17:34,504 | INFO | max output length: 499
2026-01-28 17:17:34,504 | INFO | min output length: 49
2026-01-28 17:18:01,386 | INFO | end detected at 327
2026-01-28 17:18:01,388 | INFO | -68.84 * 0.5 = -34.42 for decoder
2026-01-28 17:18:01,388 | INFO | -73.96 * 0.5 = -36.98 for ctc
2026-01-28 17:18:01,388 | INFO | total log probability: -71.40
2026-01-28 17:18:01,389 | INFO | normalized log probability: -0.23
2026-01-28 17:18:01,389 | INFO | total number of ended hypotheses: 189
2026-01-28 17:18:01,393 | INFO | best hypo: dans<space>la<space>pêcheuse<space>qu'elle<space>le<space>baisse<space>que<space>dans<space>les<space>voitures<space>alors<space>je<space>suis<space>pas<space>une<space>trompeuse<space>oui<space>j'aime<space>pas<space>je<space>non<space>j'aime<space>pas<space>le<space>principe<space>ça<space>m'emmerde<space>je<space>vais<space>des<space>fons<space>s'en<space>va<space>si<space>on<space>s'en<space>est<space>pas<space>euro<space>on<space>s'en<space>va<space>mais<space>c'est<space>très<space>c'est<space>désolé<space>c'est<space>mais<space>mais<space>voilà<space>ça<space>j'aime<space>pas<space>ça<space>m<space>j'ai<space>des<space>fidélités<space>successives<space>en<space>fait

2026-01-28 17:18:01,395 | INFO | speech length: 45760
2026-01-28 17:18:01,432 | INFO | decoder input length: 71
2026-01-28 17:18:01,432 | INFO | max output length: 71
2026-01-28 17:18:01,432 | INFO | min output length: 7
2026-01-28 17:18:03,858 | INFO | end detected at 61
2026-01-28 17:18:03,859 | INFO |  -4.40 * 0.5 =  -2.20 for decoder
2026-01-28 17:18:03,859 | INFO | -10.30 * 0.5 =  -5.15 for ctc
2026-01-28 17:18:03,859 | INFO | total log probability: -7.35
2026-01-28 17:18:03,859 | INFO | normalized log probability: -0.13
2026-01-28 17:18:03,859 | INFO | total number of ended hypotheses: 181
2026-01-28 17:18:03,860 | INFO | best hypo: moi<space>j'ai<space>été<space>fidèle<space>au<space>quand<space>j'ai<space>aimé<space>j'ai<space>été<space>fidèle

2026-01-28 17:18:03,862 | INFO | speech length: 460480
2026-01-28 17:18:03,897 | INFO | decoder input length: 719
2026-01-28 17:18:03,897 | INFO | max output length: 719
2026-01-28 17:18:03,897 | INFO | min output length: 71
2026-01-28 17:19:03,902 | INFO | end detected at 635
2026-01-28 17:19:03,904 | INFO | -620.25 * 0.5 = -310.13 for decoder
2026-01-28 17:19:03,904 | INFO | -354.66 * 0.5 = -177.33 for ctc
2026-01-28 17:19:03,904 | INFO | total log probability: -487.46
2026-01-28 17:19:03,904 | INFO | normalized log probability: -0.78
2026-01-28 17:19:03,904 | INFO | total number of ended hypotheses: 210
2026-01-28 17:19:03,911 | INFO | best hypo: les<space>toies<space>c'était<space>déjà<space>arrivé<space>de<space>faire<space>sortir<space>un<space>aman<space>ou<space>une<space>maîtresse<space>par<space>la<space>fenêtre<space>bien<space>sûr<space>il<space>va<space>le<space>dire<space>ce<space>soir<space>y<space>a<space>aucun<space>problème<space>si<space>un<space>endroit<space>pour<space>se<space>confier<space>je<space>crois<space>que<space>c'était<space>là<space>entre<space>toi<space>c'est<space>un<space>endroit<space>c'est<space>parent<space>y<space>a<space>des<space>gens<space>ils<space>ont<space>des<space>emmerde<space>parce<space>qu'ils<space>sont<space>confiés<space>en<space>télé<space>tu<space>peux<space>jamais<space>avoir<space>de<space>la<space>peine<space>pour<space>eux<space>qoi<space>tu<space>dis<space>mais<space>les<space>cours<space>quoi<space>il<space>a<space>été<space>se<space>confiant<space>en<space>télé<space>et<space>puns<space>c'es<space>te<space>jeuns<space>coies<space>con<space>le<space>çache<space>je<space>pes<space>tries<space>taies<space>téles<space>mais<space>je<space>savais<space>pas<space>oui<space>c'est<space>ça<space>quies<space>trtraies<space>ve<space>c<space>le<space>fat<space>de<space>pabies<space>coie<space>moie<space>pendrat<space>pus<space>sincères<space>c'est<space>vrai<space>çaies<space>jaie<space>bies<space>c'est<space>vrai<space>qu'on<space>habie<space>tra<space>bien

2026-01-28 17:19:03,914 | INFO | speech length: 185760
2026-01-28 17:19:03,958 | INFO | decoder input length: 289
2026-01-28 17:19:03,958 | INFO | max output length: 289
2026-01-28 17:19:03,959 | INFO | min output length: 28
2026-01-28 17:19:09,857 | INFO | end detected at 89
2026-01-28 17:19:09,861 | INFO | -18.19 * 0.5 =  -9.09 for decoder
2026-01-28 17:19:09,861 | INFO | -25.96 * 0.5 = -12.98 for ctc
2026-01-28 17:19:09,861 | INFO | total log probability: -22.08
2026-01-28 17:19:09,861 | INFO | normalized log probability: -0.29
2026-01-28 17:19:09,861 | INFO | total number of ended hypotheses: 30
2026-01-28 17:19:09,862 | INFO | best hypo: oui<space>mais<space>c'est<space>pour<space>ça<space>je<space>regardais<space>les<space>choses<space>de<space>bientôt<space>jerais<space>grisonnant

2026-01-28 17:19:09,864 | INFO | speech length: 271680
2026-01-28 17:19:09,912 | INFO | decoder input length: 424
2026-01-28 17:19:09,912 | INFO | max output length: 424
2026-01-28 17:19:09,912 | INFO | min output length: 42
2026-01-28 17:19:32,755 | INFO | end detected at 331
2026-01-28 17:19:32,757 | INFO | -62.64 * 0.5 = -31.32 for decoder
2026-01-28 17:19:32,757 | INFO | -52.65 * 0.5 = -26.32 for ctc
2026-01-28 17:19:32,757 | INFO | total log probability: -57.65
2026-01-28 17:19:32,757 | INFO | normalized log probability: -0.18
2026-01-28 17:19:32,757 | INFO | total number of ended hypotheses: 201
2026-01-28 17:19:32,762 | INFO | best hypo: emmanuel<space>est<space>ce<space>que<space>vous<space>êtes<space>déjà<space>écrit<space>ciel<space>mon<space>mari<space>en<space>entendant<space>la<space>porte<space>d'entrer<space>s'ouvrir<space>j'ai<space>jamais<space>si<space>jamais<space>je<space>ne<space>suis<space>jamais<space>na<space>rien<space>je<space>vais<space>j<space>arriver<space>de<space>dire<space>seul<space>mon<space>mari<space>tu<space>en<space>entendant<space>la<space>porte<space>que<space>ça<space>voudrait<space>dire<space>que<space>tu<space>trompes<space>on<space>me<space>et<space>v<space>en<space>plus<space>tu<space>es<space>chez<space>toi<space>hum<space>ta<space>non<space>je<space>n'ai<space>jamais<space>trompé<space>mon<space>mari

2026-01-28 17:19:32,765 | INFO | speech length: 58080
2026-01-28 17:19:32,807 | INFO | decoder input length: 90
2026-01-28 17:19:32,807 | INFO | max output length: 90
2026-01-28 17:19:32,807 | INFO | min output length: 9
2026-01-28 17:19:36,041 | INFO | end detected at 84
2026-01-28 17:19:36,044 | INFO | -20.15 * 0.5 = -10.07 for decoder
2026-01-28 17:19:36,044 | INFO | -25.16 * 0.5 = -12.58 for ctc
2026-01-28 17:19:36,044 | INFO | total log probability: -22.65
2026-01-28 17:19:36,044 | INFO | normalized log probability: -0.31
2026-01-28 17:19:36,044 | INFO | total number of ended hypotheses: 234
2026-01-28 17:19:36,045 | INFO | best hypo: est<space>ce<space>que<space>est<space>à<space>diani<space>à<space>ta<space>petit<space>anime<space>non<space>on<space>se<space>passe<space>que<space>tu<space>croises

2026-01-28 17:19:36,048 | INFO | speech length: 57600
2026-01-28 17:19:36,084 | INFO | decoder input length: 89
2026-01-28 17:19:36,084 | INFO | max output length: 89
2026-01-28 17:19:36,084 | INFO | min output length: 8
2026-01-28 17:19:38,777 | INFO | end detected at 68
2026-01-28 17:19:38,778 | INFO |  -5.15 * 0.5 =  -2.58 for decoder
2026-01-28 17:19:38,778 | INFO |  -1.66 * 0.5 =  -0.83 for ctc
2026-01-28 17:19:38,778 | INFO | total log probability: -3.41
2026-01-28 17:19:38,778 | INFO | normalized log probability: -0.05
2026-01-28 17:19:38,778 | INFO | total number of ended hypotheses: 172
2026-01-28 17:19:38,779 | INFO | best hypo: j'y<space>arrivais<space>ça<space>euh<space>parce<space>que<space>c'était<space>la<space>vérité<space>à<space>chaque<space>fois

2026-01-28 17:19:38,781 | INFO | speech length: 132000
2026-01-28 17:19:38,836 | INFO | decoder input length: 205
2026-01-28 17:19:38,836 | INFO | max output length: 205
2026-01-28 17:19:38,836 | INFO | min output length: 20
2026-01-28 17:19:53,344 | INFO | end detected at 155
2026-01-28 17:19:53,347 | INFO | -17.28 * 0.5 =  -8.64 for decoder
2026-01-28 17:19:53,347 | INFO |  -9.90 * 0.5 =  -4.95 for ctc
2026-01-28 17:19:53,347 | INFO | total log probability: -13.59
2026-01-28 17:19:53,347 | INFO | normalized log probability: -0.09
2026-01-28 17:19:53,347 | INFO | total number of ended hypotheses: 179
2026-01-28 17:19:53,350 | INFO | best hypo: ça<space>sent<space>le<space>césar<space>hein<space>quel<space>acteur<space>et<space>est<space>ce<space>que<space>son<space>intimité<space>vous<space>avez<space>déjà<space>appelé<space>votre<space>petit<space>ami<space>vous<space>être<space>financé<space>par<space>un<space>autre<space>prénom<space>que<space>le<space>sien

2026-01-28 17:19:53,355 | INFO | speech length: 116000
2026-01-28 17:19:53,400 | INFO | decoder input length: 180
2026-01-28 17:19:53,400 | INFO | max output length: 180
2026-01-28 17:19:53,400 | INFO | min output length: 18
2026-01-28 17:19:57,128 | INFO | end detected at 71
2026-01-28 17:19:57,131 | INFO | -10.00 * 0.5 =  -5.00 for decoder
2026-01-28 17:19:57,131 | INFO |  -8.91 * 0.5 =  -4.46 for ctc
2026-01-28 17:19:57,131 | INFO | total log probability: -9.46
2026-01-28 17:19:57,131 | INFO | normalized log probability: -0.16
2026-01-28 17:19:57,131 | INFO | total number of ended hypotheses: 261
2026-01-28 17:19:57,132 | INFO | best hypo: ça<space>tient<space>oui<space>c'est<space>arrivé<space>parce<space>que<space>le<space>début<space>c'était<space>dat

2026-01-28 17:19:57,134 | INFO | speech length: 340640
2026-01-28 17:19:57,179 | INFO | decoder input length: 531
2026-01-28 17:19:57,179 | INFO | max output length: 531
2026-01-28 17:19:57,179 | INFO | min output length: 53
2026-01-28 17:20:22,684 | INFO | end detected at 286
2026-01-28 17:20:22,686 | INFO | -71.19 * 0.5 = -35.60 for decoder
2026-01-28 17:20:22,686 | INFO | -63.63 * 0.5 = -31.81 for ctc
2026-01-28 17:20:22,686 | INFO | total log probability: -67.41
2026-01-28 17:20:22,686 | INFO | normalized log probability: -0.24
2026-01-28 17:20:22,687 | INFO | total number of ended hypotheses: 216
2026-01-28 17:20:22,690 | INFO | best hypo: ah<space>voyez<space>ce<space>que<space>je<space>vais<space>dire<space>donc<space>très<space>bien<space>voilà<space>toi<space>c'était<space>arrivé<space>euh<space>ben<space>en<space>général<space>j'ai<space>prends<space>tout<space>avec<space>le<space>même<space>prénom<space>d'accord<space>c'est<space>bien<space>gênette<space>ça<space>passe<space>pas<space>un<space>peu<space>henri<space>patrick<space>est<space>ce<space>que<space>au<space>téléphone<space>vous<space>avez<space>déjà<space>chauffé<space>la<space>mer<space>en<space>pensant<space>que<space>vous<space>abrez<space>s'y<space>à<space>la

2026-01-28 17:20:22,693 | INFO | speech length: 408320
2026-01-28 17:20:22,750 | INFO | decoder input length: 637
2026-01-28 17:20:22,750 | INFO | max output length: 637
2026-01-28 17:20:22,750 | INFO | min output length: 63
2026-01-28 17:21:08,737 | INFO | end detected at 529
2026-01-28 17:21:08,738 | INFO | -417.85 * 0.5 = -208.93 for decoder
2026-01-28 17:21:08,738 | INFO | -136.10 * 0.5 = -68.05 for ctc
2026-01-28 17:21:08,738 | INFO | total log probability: -276.98
2026-01-28 17:21:08,738 | INFO | normalized log probability: -0.53
2026-01-28 17:21:08,738 | INFO | total number of ended hypotheses: 162
2026-01-28 17:21:08,744 | INFO | best hypo: euh<space>non<space>alors<space>non<space>ce<space>qui<space>m'est<space>arrivé<space>c'est<space>des<space>fois<space>de<space>déconner<space>en<space>appelant<space>un<space>pote<space>et<space>de<space>tomber<space>sur<space>son<space>père<space>mais<space>ça<space>c'était<space>pas<space>une<space>relation<space>amoureuse<space>on<space>ne<space>sait<space>pas<space>du<space>mais<space>euh<space>euh<space>ce<space>ce<space>ce<space>bourret<space>et<space>puis<space>chauffer<space>comme<space>ça<space>c'est<space>à<space>dire<space>dragouiller<space>la<space>mère<space>au<space>dragué<space>ou<space>faire<space>une<space>déclaration<space>d'amont<space>à<space>la<space>mère<space>bon<space>enfin<space>surtout<space>s'il<space>le<space>dit<space>oui<space>mais<space>j'y<space>pense<space>enfin<space>on<space>c'était<space>la<space>fille<space>on<space>croyain<space>que<space>c'étail<space>la<space>fille<space>a<space>oi<space>nononon<space>j'ai<space>toujours<space>fait<space>qui<space>c'état<space>la<space>mère<space>d'alavez<space>l<space>réponde<space>d'abord<space>et<space>l<space>comprendra<space>pas

2026-01-28 17:21:08,761 | INFO | Chunk: 0 | WER=39.166667 | S=33 D=9 I=5
2026-01-28 17:21:08,770 | INFO | Chunk: 1 | WER=27.343750 | S=24 D=7 I=4
2026-01-28 17:21:08,774 | INFO | Chunk: 2 | WER=24.675325 | S=7 D=10 I=2
2026-01-28 17:21:08,774 | INFO | Chunk: 3 | WER=50.000000 | S=5 D=3 I=0
2026-01-28 17:21:08,776 | INFO | Chunk: 4 | WER=15.000000 | S=2 D=1 I=3
2026-01-28 17:21:08,780 | INFO | Chunk: 5 | WER=37.500000 | S=16 D=10 I=4
2026-01-28 17:21:08,780 | INFO | Chunk: 6 | WER=7.142857 | S=1 D=0 I=0
2026-01-28 17:21:08,792 | INFO | Chunk: 7 | WER=45.679012 | S=46 D=26 I=2
2026-01-28 17:21:08,793 | INFO | Chunk: 8 | WER=57.142857 | S=4 D=2 I=2
2026-01-28 17:21:08,796 | INFO | Chunk: 9 | WER=28.767123 | S=17 D=3 I=1
2026-01-28 17:21:08,796 | INFO | Chunk: 10 | WER=65.217391 | S=9 D=6 I=0
2026-01-28 17:21:08,797 | INFO | Chunk: 11 | WER=35.714286 | S=3 D=1 I=1
2026-01-28 17:21:08,798 | INFO | Chunk: 12 | WER=20.000000 | S=4 D=1 I=1
2026-01-28 17:21:08,798 | INFO | Chunk: 13 | WER=27.272727 | S=1 D=0 I=2
2026-01-28 17:21:08,801 | INFO | Chunk: 14 | WER=37.288136 | S=15 D=3 I=4
2026-01-28 17:21:08,808 | INFO | Chunk: 15 | WER=38.983051 | S=35 D=7 I=4
2026-01-28 17:21:09,310 | INFO | File: Rhap-D2007.wav | WER=34.831461 | S=227 D=84 I=30
2026-01-28 17:21:09,310 | INFO | ------------------------------
2026-01-28 17:21:09,311 | INFO | Conf ester Done!
2026-01-28 17:25:39,979 | INFO | Chunk: 0 | WER=43.333333 | S=27 D=18 I=7
2026-01-28 17:25:39,998 | INFO | Chunk: 1 | WER=22.656250 | S=15 D=12 I=2
2026-01-28 17:25:40,006 | INFO | Chunk: 2 | WER=35.064935 | S=16 D=10 I=1
2026-01-28 17:25:40,007 | INFO | Chunk: 3 | WER=50.000000 | S=3 D=5 I=0
2026-01-28 17:25:40,010 | INFO | Chunk: 4 | WER=30.000000 | S=10 D=2 I=0
2026-01-28 17:25:40,017 | INFO | Chunk: 5 | WER=50.000000 | S=24 D=16 I=0
2026-01-28 17:25:40,017 | INFO | Chunk: 6 | WER=14.285714 | S=2 D=0 I=0
2026-01-28 17:25:40,041 | INFO | Chunk: 7 | WER=58.641975 | S=49 D=45 I=1
2026-01-28 17:25:40,042 | INFO | Chunk: 8 | WER=78.571429 | S=4 D=1 I=6
2026-01-28 17:25:40,049 | INFO | Chunk: 9 | WER=30.136986 | S=13 D=8 I=1
2026-01-28 17:25:40,050 | INFO | Chunk: 10 | WER=56.521739 | S=5 D=8 I=0
2026-01-28 17:25:40,050 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 17:25:40,052 | INFO | Chunk: 12 | WER=23.333333 | S=5 D=2 I=0
2026-01-28 17:25:40,053 | INFO | Chunk: 13 | WER=18.181818 | S=1 D=0 I=1
2026-01-28 17:25:40,059 | INFO | Chunk: 14 | WER=50.847458 | S=16 D=12 I=2
2026-01-28 17:25:40,074 | INFO | Chunk: 15 | WER=36.440678 | S=25 D=17 I=1
2026-01-28 17:25:41,063 | INFO | File: Rhap-D2007.wav | WER=39.836568 | S=218 D=153 I=19
2026-01-28 17:25:41,064 | INFO | ------------------------------
2026-01-28 17:25:41,064 | INFO | hmm_tdnn Done!
2026-01-28 17:25:41,312 | INFO | ==================================Rhap-D2008.wav=========================================
2026-01-28 17:25:41,806 | INFO | Using rVAD model
2026-01-28 17:26:21,899 | INFO | Chunk: 0 | WER=53.947368 | S=3 D=4 I=34
2026-01-28 17:26:21,899 | INFO | Chunk: 1 | WER=100.000000 | S=5 D=2 I=0
2026-01-28 17:26:21,906 | INFO | Chunk: 2 | WER=72.380952 | S=7 D=36 I=33
2026-01-28 17:26:21,912 | INFO | Chunk: 3 | WER=73.076923 | S=10 D=37 I=29
2026-01-28 17:26:21,912 | INFO | Chunk: 4 | WER=200.000000 | S=4 D=0 I=4
2026-01-28 17:26:21,917 | INFO | Chunk: 5 | WER=88.172043 | S=10 D=47 I=25
2026-01-28 17:26:21,922 | INFO | Chunk: 6 | WER=67.010309 | S=6 D=27 I=32
2026-01-28 17:26:21,931 | INFO | Chunk: 7 | WER=66.406250 | S=5 D=42 I=38
2026-01-28 17:26:21,937 | INFO | Chunk: 8 | WER=88.073394 | S=28 D=43 I=25
2026-01-28 17:26:21,939 | INFO | Chunk: 9 | WER=88.888889 | S=37 D=17 I=2
2026-01-28 17:26:21,945 | INFO | Chunk: 10 | WER=89.690722 | S=5 D=42 I=40
2026-01-28 17:26:21,949 | INFO | Chunk: 11 | WER=93.670886 | S=65 D=6 I=3
2026-01-28 17:26:21,952 | INFO | Chunk: 12 | WER=91.891892 | S=41 D=26 I=1
2026-01-28 17:26:21,952 | INFO | Chunk: 13 | WER=95.652174 | S=9 D=13 I=0
2026-01-28 17:26:21,954 | INFO | Chunk: 14 | WER=102.631579 | S=32 D=3 I=4
2026-01-28 17:26:21,962 | INFO | Chunk: 15 | WER=77.868852 | S=8 D=46 I=41
2026-01-28 17:26:21,970 | INFO | Chunk: 16 | WER=70.000000 | S=5 D=56 I=30
2026-01-28 17:26:21,976 | INFO | Chunk: 17 | WER=61.467890 | S=1 D=37 I=29
2026-01-28 17:26:21,981 | INFO | Chunk: 18 | WER=73.863636 | S=3 D=31 I=31
2026-01-28 17:26:21,989 | INFO | Chunk: 19 | WER=83.064516 | S=13 D=49 I=41
2026-01-28 17:26:21,999 | INFO | Chunk: 20 | WER=79.710145 | S=10 D=57 I=43
2026-01-28 17:26:22,007 | INFO | Chunk: 21 | WER=69.531250 | S=2 D=56 I=31
2026-01-28 17:26:22,008 | INFO | Chunk: 22 | WER=94.594595 | S=32 D=2 I=1
2026-01-28 17:26:24,027 | INFO | File: Rhap-D2008.wav | WER=23.416118 | S=180 D=222 I=60
2026-01-28 17:26:24,028 | INFO | ------------------------------
2026-01-28 17:26:24,028 | INFO | w2vec vad chunk Done!
2026-01-28 17:27:06,714 | INFO | Chunk: 0 | WER=73.684211 | S=29 D=27 I=0
2026-01-28 17:27:06,715 | INFO | Chunk: 1 | WER=100.000000 | S=5 D=2 I=0
2026-01-28 17:27:06,717 | INFO | Chunk: 2 | WER=75.238095 | S=4 D=75 I=0
2026-01-28 17:27:06,721 | INFO | Chunk: 3 | WER=71.153846 | S=16 D=58 I=0
2026-01-28 17:27:06,721 | INFO | Chunk: 4 | WER=300.000000 | S=4 D=0 I=8
2026-01-28 17:27:06,722 | INFO | Chunk: 5 | WER=89.247312 | S=0 D=83 I=0
2026-01-28 17:27:06,725 | INFO | Chunk: 6 | WER=63.917526 | S=0 D=62 I=0
2026-01-28 17:27:06,729 | INFO | Chunk: 7 | WER=69.531250 | S=3 D=86 I=0
2026-01-28 17:27:06,731 | INFO | Chunk: 8 | WER=80.733945 | S=0 D=88 I=0
2026-01-28 17:27:06,732 | INFO | Chunk: 9 | WER=85.714286 | S=27 D=27 I=0
2026-01-28 17:27:06,734 | INFO | Chunk: 10 | WER=80.412371 | S=2 D=76 I=0
2026-01-28 17:27:06,736 | INFO | Chunk: 11 | WER=74.683544 | S=2 D=57 I=0
2026-01-28 17:27:06,737 | INFO | Chunk: 12 | WER=79.729730 | S=1 D=58 I=0
2026-01-28 17:27:06,738 | INFO | Chunk: 13 | WER=91.304348 | S=14 D=7 I=0
2026-01-28 17:27:06,739 | INFO | Chunk: 14 | WER=100.000000 | S=31 D=1 I=6
2026-01-28 17:27:06,743 | INFO | Chunk: 15 | WER=73.770492 | S=13 D=73 I=4
2026-01-28 17:27:06,746 | INFO | Chunk: 16 | WER=84.615385 | S=0 D=110 I=0
2026-01-28 17:27:06,747 | INFO | Chunk: 17 | WER=88.073394 | S=0 D=96 I=0
2026-01-28 17:27:06,750 | INFO | Chunk: 18 | WER=59.090909 | S=5 D=47 I=0
2026-01-28 17:27:06,753 | INFO | Chunk: 19 | WER=78.225806 | S=0 D=97 I=0
2026-01-28 17:27:06,755 | INFO | Chunk: 20 | WER=90.579710 | S=2 D=123 I=0
2026-01-28 17:27:06,758 | INFO | Chunk: 21 | WER=69.531250 | S=1 D=88 I=0
2026-01-28 17:27:06,759 | INFO | Chunk: 22 | WER=94.594595 | S=31 D=3 I=1
2026-01-28 17:27:07,477 | INFO | File: Rhap-D2008.wav | WER=72.934617 | S=100 D=1332 I=7
2026-01-28 17:27:07,477 | INFO | ------------------------------
2026-01-28 17:27:07,477 | INFO | whisper med Done!
2026-01-28 17:28:13,548 | INFO | Chunk: 0 | WER=46.052632 | S=10 D=25 I=0
2026-01-28 17:28:13,549 | INFO | Chunk: 1 | WER=100.000000 | S=6 D=1 I=0
2026-01-28 17:28:13,552 | INFO | Chunk: 2 | WER=67.619048 | S=2 D=69 I=0
2026-01-28 17:28:13,555 | INFO | Chunk: 3 | WER=69.230769 | S=6 D=65 I=1
2026-01-28 17:28:13,555 | INFO | Chunk: 4 | WER=225.000000 | S=4 D=0 I=5
2026-01-28 17:28:13,558 | INFO | Chunk: 5 | WER=73.118280 | S=1 D=54 I=13
2026-01-28 17:28:13,561 | INFO | Chunk: 6 | WER=62.886598 | S=0 D=61 I=0
2026-01-28 17:28:13,565 | INFO | Chunk: 7 | WER=67.968750 | S=2 D=85 I=0
2026-01-28 17:28:13,568 | INFO | Chunk: 8 | WER=65.137615 | S=1 D=69 I=1
2026-01-28 17:28:13,570 | INFO | Chunk: 9 | WER=85.714286 | S=27 D=27 I=0
2026-01-28 17:28:13,571 | INFO | Chunk: 10 | WER=93.814433 | S=0 D=91 I=0
2026-01-28 17:28:13,573 | INFO | Chunk: 11 | WER=84.810127 | S=3 D=49 I=15
2026-01-28 17:28:13,576 | INFO | Chunk: 12 | WER=93.243243 | S=37 D=32 I=0
2026-01-28 17:28:13,576 | INFO | Chunk: 13 | WER=95.652174 | S=10 D=12 I=0
2026-01-28 17:28:13,577 | INFO | Chunk: 14 | WER=100.000000 | S=32 D=1 I=5
2026-01-28 17:28:13,580 | INFO | Chunk: 15 | WER=80.327869 | S=9 D=89 I=0
2026-01-28 17:28:13,583 | INFO | Chunk: 16 | WER=84.615385 | S=1 D=109 I=0
2026-01-28 17:28:13,587 | INFO | Chunk: 17 | WER=55.045872 | S=8 D=49 I=3
2026-01-28 17:28:13,590 | INFO | Chunk: 18 | WER=63.636364 | S=2 D=45 I=9
2026-01-28 17:28:13,593 | INFO | Chunk: 19 | WER=75.806452 | S=1 D=93 I=0
2026-01-28 17:28:13,595 | INFO | Chunk: 20 | WER=89.130435 | S=1 D=122 I=0
2026-01-28 17:28:13,599 | INFO | Chunk: 21 | WER=68.750000 | S=2 D=86 I=0
2026-01-28 17:28:13,600 | INFO | Chunk: 22 | WER=94.594595 | S=31 D=3 I=1
2026-01-28 17:28:14,483 | INFO | File: Rhap-D2008.wav | WER=66.447035 | S=103 D=1196 I=12
2026-01-28 17:28:14,484 | INFO | ------------------------------
2026-01-28 17:28:14,484 | INFO | whisper large Done!
2026-01-28 17:28:14,660 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 17:28:14,698 | INFO | Vocabulary size: 350
2026-01-28 17:28:16,327 | INFO | Gradient checkpoint layers: []
2026-01-28 17:28:17,362 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 17:28:17,370 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 17:28:17,371 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 17:28:17,372 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 17:28:17,373 | INFO | speech length: 467680
2026-01-28 17:28:17,444 | INFO | decoder input length: 730
2026-01-28 17:28:17,444 | INFO | max output length: 730
2026-01-28 17:28:17,444 | INFO | min output length: 73
2026-01-28 17:28:51,901 | INFO | end detected at 286
2026-01-28 17:28:51,902 | INFO | -855.56 * 0.5 = -427.78 for decoder
2026-01-28 17:28:51,902 | INFO | -300.64 * 0.5 = -150.32 for ctc
2026-01-28 17:28:51,902 | INFO | total log probability: -578.10
2026-01-28 17:28:51,902 | INFO | normalized log probability: -2.07
2026-01-28 17:28:51,902 | INFO | total number of ended hypotheses: 176
2026-01-28 17:28:51,906 | INFO | best hypo: ▁bonsoir▁à▁tous▁nous▁consacrerons▁la▁dernière▁partie▁de▁cette▁émission▁à▁vos▁questions▁sesmes▁alors▁ou▁en▁est▁on▁vraiment▁du▁côté▁de▁la▁grippe▁a▁h▁n▁la▁tendance▁dans▁les▁médias▁et▁plutôt▁de▁l'az▁vous▁vue▁du▁côté▁d'uns▁léger▁aassaoupaspement▁mais▁les▁déclarations▁de▁spesalistes▁et▁de▁loémès▁ont▁effectativement▁de▁quoi▁'armée▁c'est▁comme▁un▁très▁mauvais▁ou▁renindez▁vous▁a▁venir▁rappelons▁à▁ququ'il▁y▁a▁moins▁'une▁seuine▁le▁numéro▁de▁l'organistation▁mondiale▁de▁la▁santés▁estimait▁qu'un▁tiers▁de▁la▁population▁mondiale▁pourrait▁être▁contaminés▁nous▁avant▁intitulé▁cette▁émis▁ion▁gripp▁à▁léchi

2026-01-28 17:28:51,910 | INFO | speech length: 16319
2026-01-28 17:28:51,969 | INFO | decoder input length: 24
2026-01-28 17:28:51,969 | INFO | max output length: 24
2026-01-28 17:28:51,969 | INFO | min output length: 2
2026-01-28 17:28:53,143 | INFO | end detected at 18
2026-01-28 17:28:53,146 | INFO |  -2.90 * 0.5 =  -1.45 for decoder
2026-01-28 17:28:53,146 | INFO |  -7.76 * 0.5 =  -3.88 for ctc
2026-01-28 17:28:53,146 | INFO | total log probability: -5.33
2026-01-28 17:28:53,146 | INFO | normalized log probability: -0.48
2026-01-28 17:28:53,146 | INFO | total number of ended hypotheses: 183
2026-01-28 17:28:53,147 | INFO | best hypo: ▁pour▁accepter▁dix▁parties

2026-01-28 17:28:53,150 | INFO | speech length: 466720
2026-01-28 17:28:53,203 | INFO | decoder input length: 728
2026-01-28 17:28:53,203 | INFO | max output length: 728
2026-01-28 17:28:53,203 | INFO | min output length: 72
2026-01-28 17:29:52,308 | INFO | end detected at 305
2026-01-28 17:29:52,310 | INFO | -732.25 * 0.5 = -366.13 for decoder
2026-01-28 17:29:52,311 | INFO | -285.24 * 0.5 = -142.62 for ctc
2026-01-28 17:29:52,311 | INFO | total log probability: -508.75
2026-01-28 17:29:52,311 | INFO | normalized log probability: -1.70
2026-01-28 17:29:52,311 | INFO | total number of ended hypotheses: 143
2026-01-28 17:29:52,315 | INFO | best hypo: ▁comme▁chaque▁soir▁antoine▁flaho▁est▁épidémiologiste▁spécialiste▁de▁la▁gribe▁des▁êtres▁directeurs▁de▁l'école▁des▁hautes▁études▁et▁en▁santanté▁publique▁à▁rennes▁nous▁avons▁tous▁entendu▁par▁les▁deux▁votres▁scénarios▁envisageant▁trente▁mille▁morts▁dans▁notre▁pays▁l'▁prochains▁pour▁cause▁de▁grés▁ah▁h▁helin▁je▁rappelles▁que▁votres▁livre▁des▁émeidémies▁et▁les▁hommes▁viendnt▁parître▁aux▁éditions▁de▁la▁martins▁jean▁fançois▁saluzo▁et▁le▁virologues▁experts▁auprès▁de▁l'hëmès▁et▁anciens▁chercheurs▁à▁l'institut▁pasteurs▁à▁vous▁être▁par▁ailleurs▁ex▁resptonsaires▁et▁du▁développement▁des▁vacins▁de▁contre▁des▁maladies▁virales▁émertents▁dans▁un▁labatoire

2026-01-28 17:29:52,319 | INFO | speech length: 457760
2026-01-28 17:29:52,370 | INFO | decoder input length: 714
2026-01-28 17:29:52,370 | INFO | max output length: 714
2026-01-28 17:29:52,370 | INFO | min output length: 71
2026-01-28 17:30:33,288 | INFO | end detected at 303
2026-01-28 17:30:33,291 | INFO | -719.46 * 0.5 = -359.73 for decoder
2026-01-28 17:30:33,291 | INFO | -360.44 * 0.5 = -180.22 for ctc
2026-01-28 17:30:33,291 | INFO | total log probability: -539.95
2026-01-28 17:30:33,291 | INFO | normalized log probability: -1.82
2026-01-28 17:30:33,291 | INFO | total number of ended hypotheses: 155
2026-01-28 17:30:33,295 | INFO | best hypo: ▁vo'autre▁dernier▁livre▁à▁la▁conquête▁des▁virus▁vient▁lui▁de▁paraître▁chez▁belin▁christian▁péronne▁est▁infectiologue▁chef▁du▁service▁des▁maladies▁infectienteuses▁et▁tropicales▁à▁l'hôpital▁raymond▁poincaré▁de▁gages▁dans▁les▁hauts▁de▁seine▁ou▁préséidés▁par▁ailleurs▁commissisions▁aux▁malandie▁et▁transtsible▁du▁haut▁consfeils▁de▁la▁santé▁pumlique▁en▁saint▁michel▁de▁segumons▁et▁sociologues▁spéraliste▁des▁cris▁sanitaires▁aux▁êtres▁directeurs▁de▁recherches▁ocns▁et▁responsaires▁du▁centre▁de▁recherches▁sur▁le▁risisques▁et▁sa▁régulation▁à▁l'é▁école▁des▁hautes▁études▁santés▁pubiques▁et▁antine▁flargombains▁commens▁par▁voc

2026-01-28 17:30:33,298 | INFO | speech length: 22880
2026-01-28 17:30:33,346 | INFO | decoder input length: 35
2026-01-28 17:30:33,347 | INFO | max output length: 35
2026-01-28 17:30:33,347 | INFO | min output length: 3
2026-01-28 17:30:34,327 | INFO | end detected at 22
2026-01-28 17:30:34,330 | INFO |  -1.24 * 0.5 =  -0.62 for decoder
2026-01-28 17:30:34,330 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-28 17:30:34,330 | INFO | total log probability: -0.63
2026-01-28 17:30:34,330 | INFO | normalized log probability: -0.04
2026-01-28 17:30:34,330 | INFO | total number of ended hypotheses: 144
2026-01-28 17:30:34,330 | INFO | best hypo: ▁est▁ce▁que▁vous▁maintenez▁vos▁prévisions

2026-01-28 17:30:34,332 | INFO | speech length: 388800
2026-01-28 17:30:34,396 | INFO | decoder input length: 607
2026-01-28 17:30:34,396 | INFO | max output length: 607
2026-01-28 17:30:34,396 | INFO | min output length: 60
2026-01-28 17:30:59,095 | INFO | end detected at 199
2026-01-28 17:30:59,097 | INFO | -577.37 * 0.5 = -288.68 for decoder
2026-01-28 17:30:59,097 | INFO | -201.38 * 0.5 = -100.69 for ctc
2026-01-28 17:30:59,097 | INFO | total log probability: -389.37
2026-01-28 17:30:59,097 | INFO | normalized log probability: -2.01
2026-01-28 17:30:59,097 | INFO | total number of ended hypotheses: 156
2026-01-28 17:30:59,100 | INFO | best hypo: ▁enfin▁sauf▁que▁l'on▁n'est▁pas▁dans▁la▁météo▁oui▁non▁que▁ce▁sont▁les▁prévisions▁si▁vous▁l'avez▁peut▁être▁mieux▁expliciter▁audios▁c'est▁un▁scénario▁voilà▁un▁des▁scénarios▁possibles▁c'est▁à▁dire▁que▁si▁vraiment▁l'activité▁de▁l'on▁onstate▁aujourd'hui▁amérique▁du▁nord▁et▁devient▁l'activité▁d'une▁pendémiet▁alors▁on▁n'a▁sa▁jamais▁et▁vraiment▁sueuis'à▁prés▁arrêté▁les▁éponémie▁grippe▁saisonnières

2026-01-28 17:30:59,102 | INFO | speech length: 420640
2026-01-28 17:30:59,147 | INFO | decoder input length: 656
2026-01-28 17:30:59,147 | INFO | max output length: 656
2026-01-28 17:30:59,148 | INFO | min output length: 65
2026-01-28 17:31:28,574 | INFO | end detected at 242
2026-01-28 17:31:28,578 | INFO | -540.82 * 0.5 = -270.41 for decoder
2026-01-28 17:31:28,578 | INFO | -215.41 * 0.5 = -107.71 for ctc
2026-01-28 17:31:28,578 | INFO | total log probability: -378.12
2026-01-28 17:31:28,578 | INFO | normalized log probability: -1.61
2026-01-28 17:31:28,578 | INFO | total number of ended hypotheses: 196
2026-01-28 17:31:28,582 | INFO | best hypo: ▁et▁on▁n'a▁pas▁beaucoup▁d'expériences▁pendant▁quand▁même▁l'expérience▁de▁trois▁pondémies▁passées▁et▁aucun▁des▁pays▁n'a▁su▁pour▁l'instant▁arrêter▁la▁vague▁pandémique▁dans▁son▁pays▁alors▁il▁est▁posible▁dans▁le▁vingt▁et▁huitième▁siècle▁dans▁certains▁pays▁mieux▁douteter▁que▁d'autres▁on▁phuisse▁de▁freiner▁ou▁arrêter▁atténuer▁la▁vague▁mais▁si▁elle▁se▁déroule▁de▁fathons▁je▁dirai▁à▁un▁peu▁ou▁comme▁les▁fois▁passés▁'est▁les▁scé▁nario▁que▁l'on▁peut▁imaginer▁sauis▁très▁bien▁vouloirs▁passer▁les▁choses▁comme▁çà▁ce▁chiffre▁de▁trente▁mille

2026-01-28 17:31:28,586 | INFO | speech length: 460320
2026-01-28 17:31:28,642 | INFO | decoder input length: 718
2026-01-28 17:31:28,642 | INFO | max output length: 718
2026-01-28 17:31:28,642 | INFO | min output length: 71
2026-01-28 17:32:03,361 | INFO | end detected at 295
2026-01-28 17:32:03,363 | INFO | -877.06 * 0.5 = -438.53 for decoder
2026-01-28 17:32:03,363 | INFO | -315.75 * 0.5 = -157.87 for ctc
2026-01-28 17:32:03,363 | INFO | total log probability: -596.41
2026-01-28 17:32:03,363 | INFO | normalized log probability: -2.07
2026-01-28 17:32:03,363 | INFO | total number of ended hypotheses: 177
2026-01-28 17:32:03,367 | INFO | best hypo: ▁c'est▁finalement▁une▁extrapolation▁que▁vous▁faites▁à▁partir▁de▁scénarios▁passés▁entre▁guillemets▁raisonnables▁et▁en▁tenant▁compte▁de▁ce▁qu'on▁a▁pu▁faire▁ou▁surtout▁ne▁ce▁qu'on▁n'a▁pas▁pu▁faire▁je▁vous▁ai▁bien▁comprise▁oui▁exactement▁c'est▁exactement▁cela▁qui▁peut▁vous▁parliez▁de▁la▁météo▁mais▁dans▁le▁fonns▁quand▁on▁veut▁pas▁prédire▁un▁cyclone▁aujourn▁d'▁aujourd'hui▁on▁se▁rapporte'▁au▁catahlogues▁▁cyclonnsés▁et▁on▁en▁regarde▁le▁plus▁proche▁cyclone▁ou▁e▁mbryant▁de▁cyclone▁de▁ce▁que▁l'on▁v▁voit▁peu▁actuellement▁et▁bien▁'on▁peut▁dre▁un▁peut▁de▁t▁la▁même▁çon▁d'aujourd'hui▁ce▁que▁l'on▁ne▁voit▁'est▁un▁prémice▁de▁démarrage'▁pandémie

2026-01-28 17:32:03,370 | INFO | speech length: 393440
2026-01-28 17:32:03,420 | INFO | decoder input length: 614
2026-01-28 17:32:03,420 | INFO | max output length: 614
2026-01-28 17:32:03,420 | INFO | min output length: 61
2026-01-28 17:32:33,764 | INFO | end detected at 247
2026-01-28 17:32:33,766 | INFO | -517.42 * 0.5 = -258.71 for decoder
2026-01-28 17:32:33,766 | INFO | -157.86 * 0.5 = -78.93 for ctc
2026-01-28 17:32:33,766 | INFO | total log probability: -337.64
2026-01-28 17:32:33,766 | INFO | normalized log probability: -1.41
2026-01-28 17:32:33,766 | INFO | total number of ended hypotheses: 164
2026-01-28 17:32:33,770 | INFO | best hypo: ▁on▁n'est▁pas▁des▁devins▁on▁ne▁fait▁pas▁de▁la▁prédiction▁ou▁de▁la▁prévision▁on▁imagine▁des▁scénarios▁et▁il▁y▁a▁plusieurs▁scénarios▁possibles▁mais▁celui▁qui▁est▁au▁plus▁grave▁le▁plus▁pour▁l'anorantan▁pas▁le▁plus▁grave▁pas▁masca▁trente▁mille▁vous▁êtes▁encore▁dans▁le▁d'en▁réseau▁afindre▁à▁trente▁milles▁on▁née▁dans▁quelque▁chose▁où▁les▁mortalité▁est▁très▁raisine▁de▁la▁mortalité▁de▁la▁grippes▁saisonnière▁carand▁la▁grippes▁saisonnière▁est▁meurtrière▁la▁grptsonnières▁entre▁cinq▁mille▁mets▁par▁an▁et▁trente▁mille▁je▁voilà▁et▁à▁six▁fois▁de▁ca▁vous▁avait▁raison

2026-01-28 17:32:33,773 | INFO | speech length: 215200
2026-01-28 17:32:33,842 | INFO | decoder input length: 335
2026-01-28 17:32:33,843 | INFO | max output length: 335
2026-01-28 17:32:33,843 | INFO | min output length: 33
2026-01-28 17:32:43,718 | INFO | end detected at 119
2026-01-28 17:32:43,721 | INFO | -95.79 * 0.5 = -47.90 for decoder
2026-01-28 17:32:43,721 | INFO | -19.43 * 0.5 =  -9.72 for ctc
2026-01-28 17:32:43,721 | INFO | total log probability: -57.61
2026-01-28 17:32:43,721 | INFO | normalized log probability: -0.51
2026-01-28 17:32:43,721 | INFO | total number of ended hypotheses: 188
2026-01-28 17:32:43,723 | INFO | best hypo: ▁si▁a▁six▁fois▁plus▁de▁cas▁simplement▁de▁cas▁de▁gens▁affectés▁par▁le▁virus▁alors▁mécaniquement▁il▁y▁a▁une▁chance▁un▁risque▁qu'il▁y▁ait▁six▁fois▁plus▁de▁ce▁fut▁pas▁perverse▁afin▁de▁voir▁le▁paradrime▁si▁ça▁c'était▁on▁va▁dire▁une▁des▁estimation▁à▁potentiel▁raisonnable

2026-01-28 17:32:43,725 | INFO | speech length: 377440
2026-01-28 17:32:43,768 | INFO | decoder input length: 589
2026-01-28 17:32:43,768 | INFO | max output length: 589
2026-01-28 17:32:43,768 | INFO | min output length: 58
2026-01-28 17:33:07,823 | INFO | end detected at 231
2026-01-28 17:33:07,826 | INFO | -404.95 * 0.5 = -202.48 for decoder
2026-01-28 17:33:07,826 | INFO | -142.67 * 0.5 = -71.34 for ctc
2026-01-28 17:33:07,826 | INFO | total log probability: -273.81
2026-01-28 17:33:07,826 | INFO | normalized log probability: -1.22
2026-01-28 17:33:07,826 | INFO | total number of ended hypotheses: 176
2026-01-28 17:33:07,829 | INFO | best hypo: ▁l'estimation▁inquiétante▁c'est▁quoi▁les▁estimations▁inquiétantes▁allées▁jusqu'à▁combien▁concernant▁les▁estlas▁france▁et▁les▁français▁les▁estimations▁inquiétantes▁elles▁ne▁me▁semblent▁pas▁beaucoup▁pas▁très▁probables▁une▁des▁estimations▁inquiétantes▁c'est▁c'elles▁de▁mille▁neuf▁cent▁dix▁huit▁il▁eurent▁mille▁neuf▁cent▁dix▁huit▁sur▁l'ensemble▁de▁la▁planète▁ontant▁quarante▁millions▁de▁décès▁c'est▁à▁dire▁une▁ou▁mortalité▁eitroyables▁bien▁pourquoi▁mille▁neuf▁cent▁dix▁huit▁à▁renne▁par▁exemple▁soixante▁dix▁huit▁mille▁abitant▁mille▁neuf▁cent▁dix▁huit▁avait▁trenteignoires

2026-01-28 17:33:07,832 | INFO | speech length: 294720
2026-01-28 17:33:07,876 | INFO | decoder input length: 460
2026-01-28 17:33:07,876 | INFO | max output length: 460
2026-01-28 17:33:07,876 | INFO | min output length: 46
2026-01-28 17:33:21,478 | INFO | end detected at 151
2026-01-28 17:33:21,480 | INFO | -171.63 * 0.5 = -85.82 for decoder
2026-01-28 17:33:21,480 | INFO | -41.14 * 0.5 = -20.57 for ctc
2026-01-28 17:33:21,481 | INFO | total log probability: -106.39
2026-01-28 17:33:21,481 | INFO | normalized log probability: -0.73
2026-01-28 17:33:21,481 | INFO | total number of ended hypotheses: 169
2026-01-28 17:33:21,483 | INFO | best hypo: ▁en▁mille▁neuf▁cent▁dix▁huit▁il▁avait▁pas▁de▁de▁la▁moitié▁des▁domiciles▁parisiens▁n'avaient▁pas▁leur▁accord▁à▁l'égout▁vous▁aviez▁donc▁d'antibiotique▁bien▁entendu▁on▁ne▁savait▁pas▁ce▁queté▁un▁virus▁en▁mille▁neuf▁cent▁dix▁huit▁on▁est▁au▁moyen▁âge▁de▁la▁santé▁▁mille▁neuf▁cent▁dix▁huit▁kapad▁d'antivirou▁y▁a▁pas▁de▁vaccin▁je▁pense▁que▁aujourd'hui

2026-01-28 17:33:21,485 | INFO | speech length: 240641
2026-01-28 17:33:21,530 | INFO | decoder input length: 375
2026-01-28 17:33:21,530 | INFO | max output length: 375
2026-01-28 17:33:21,530 | INFO | min output length: 37
2026-01-28 17:33:31,394 | INFO | end detected at 120
2026-01-28 17:33:31,396 | INFO | -101.83 * 0.5 = -50.92 for decoder
2026-01-28 17:33:31,396 | INFO | -37.02 * 0.5 = -18.51 for ctc
2026-01-28 17:33:31,396 | INFO | total log probability: -69.43
2026-01-28 17:33:31,396 | INFO | normalized log probability: -0.61
2026-01-28 17:33:31,397 | INFO | total number of ended hypotheses: 183
2026-01-28 17:33:31,398 | INFO | best hypo: ▁même▁une▁grippe▁aussi▁virulente▁que▁ce▁mille▁neuf▁cent▁dix▁huit▁n'entraînerait▁pas▁une▁mortalité▁aussi▁effroyable▁donc▁que▁les▁scénarios▁catastrophes▁qu'on▁a▁pu▁voir▁éclore▁ci▁de▁ci▁de▁là▁ne▁me▁paraissent▁pas▁mais▁encore▁et▁fois▁je▁suis▁devin▁ne▁me▁paraissent▁pas▁très▁probables

2026-01-28 17:33:31,400 | INFO | speech length: 76640
2026-01-28 17:33:31,456 | INFO | decoder input length: 119
2026-01-28 17:33:31,456 | INFO | max output length: 119
2026-01-28 17:33:31,457 | INFO | min output length: 11
2026-01-28 17:33:33,145 | INFO | end detected at 33
2026-01-28 17:33:33,147 | INFO |  -6.37 * 0.5 =  -3.19 for decoder
2026-01-28 17:33:33,147 | INFO |  -5.87 * 0.5 =  -2.93 for ctc
2026-01-28 17:33:33,147 | INFO | total log probability: -6.12
2026-01-28 17:33:33,147 | INFO | normalized log probability: -0.23
2026-01-28 17:33:33,147 | INFO | total number of ended hypotheses: 181
2026-01-28 17:33:33,147 | INFO | best hypo: ▁y▁enfin▁dans▁cette▁projection▁est▁ce▁qu'est▁une▁limite▁de▁date

2026-01-28 17:33:33,149 | INFO | speech length: 148960
2026-01-28 17:33:33,198 | INFO | decoder input length: 232
2026-01-28 17:33:33,199 | INFO | max output length: 232
2026-01-28 17:33:33,199 | INFO | min output length: 23
2026-01-28 17:33:38,576 | INFO | end detected at 84
2026-01-28 17:33:38,577 | INFO | -18.50 * 0.5 =  -9.25 for decoder
2026-01-28 17:33:38,578 | INFO | -19.67 * 0.5 =  -9.84 for ctc
2026-01-28 17:33:38,578 | INFO | total log probability: -19.09
2026-01-28 17:33:38,578 | INFO | normalized log probability: -0.24
2026-01-28 17:33:38,578 | INFO | total number of ended hypotheses: 197
2026-01-28 17:33:38,579 | INFO | best hypo: ▁nous▁dites▁voilà▁si▁ça▁tourne▁mal▁c'est▁dans▁l'année▁à▁venir▁voilà▁est▁ce▁que▁vous▁êtes▁fixé▁aussi▁un▁cadre▁circulaire▁dans▁la▁durée▁du▁scénario▁et▁donc▁du▁film▁en▁quelque▁sorte

2026-01-28 17:33:38,581 | INFO | speech length: 479840
2026-01-28 17:33:38,636 | INFO | decoder input length: 749
2026-01-28 17:33:38,636 | INFO | max output length: 749
2026-01-28 17:33:38,637 | INFO | min output length: 74
2026-01-28 17:34:15,377 | INFO | end detected at 289
2026-01-28 17:34:15,379 | INFO | -778.57 * 0.5 = -389.29 for decoder
2026-01-28 17:34:15,380 | INFO | -358.70 * 0.5 = -179.35 for ctc
2026-01-28 17:34:15,380 | INFO | total log probability: -568.64
2026-01-28 17:34:15,380 | INFO | normalized log probability: -2.01
2026-01-28 17:34:15,380 | INFO | total number of ended hypotheses: 159
2026-01-28 17:34:15,384 | INFO | best hypo: ▁alors▁oui▁c'est▁il▁serveur▁à▁tous▁été▁frais▁très▁frappé▁par▁exemple▁on▁notemise▁de▁la▁santé▁roseline▁bachelot▁elle▁agérait▁cette▁crise▁en▁hier▁au▁minimum▁en▁trois▁temps▁en▁termes▁de▁communications▁per▁çue▁par▁nonts▁et▁elle▁a▁commencé▁par▁nous▁dire▁on▁ne▁aux▁affaires▁après▁l'un▁dit▁on▁ne▁aux▁affaires▁perette▁graves▁elle▁adie▁gémito▁montan▁fluts▁de▁côtés▁et▁on▁travailles▁là▁tu▁mais▁on▁ne▁peut▁pas▁exclure▁et▁il▁y▁a▁été▁quelque▁chose▁de▁graves▁oui▁et▁don▁ayeu▁dégradations▁je▁le▁voilà▁oui▁surtout▁que▁nous▁ai▁dits▁fauts▁me▁méfer▁du▁retours▁de▁balnciers▁on▁gros▁de▁pass▁et▁l'été▁qu'est▁ce▁qui▁va▁se▁paser▁l'hiver▁prochain

2026-01-28 17:34:15,387 | INFO | speech length: 425760
2026-01-28 17:34:15,435 | INFO | decoder input length: 664
2026-01-28 17:34:15,435 | INFO | max output length: 664
2026-01-28 17:34:15,435 | INFO | min output length: 66
2026-01-28 17:34:43,104 | INFO | end detected at 242
2026-01-28 17:34:43,106 | INFO | -596.67 * 0.5 = -298.33 for decoder
2026-01-28 17:34:43,106 | INFO | -252.42 * 0.5 = -126.21 for ctc
2026-01-28 17:34:43,106 | INFO | total log probability: -424.54
2026-01-28 17:34:43,106 | INFO | normalized log probability: -1.79
2026-01-28 17:34:43,106 | INFO | total number of ended hypotheses: 155
2026-01-28 17:34:43,109 | INFO | best hypo: ▁dans▁le▁cas▁déjà▁de▁la▁grippe▁traditionnelle▁est▁ce▁que▁ce▁chiffre▁de▁tranquille▁il▁s'applique▁à▁ça▁oui▁c'est▁dire▁que▁la▁grippe▁se▁comprenait▁de▁la▁grippe▁parce▁que▁le▁grippe▁est▁une▁maladie▁l'on▁connaît▁bien▁oui▁est▁chaque▁année▁elle▁revient▁▁dans▁notre▁payse▁et▁chaque▁année▁et▁c'est▁en▁virus▁qui▁a▁fait▁le▁groupe▁tour▁du▁monde▁qui▁nous▁revient▁et▁le▁viruse▁et▁il▁émerge▁part▁d▁dans▁le▁monde▁qui▁va▁dans▁l'hétémisphère▁du▁sud▁pendant▁les▁mois▁de▁g▁juillet▁et▁d'août▁qui▁est▁un▁l'or▁hiver▁ausral▁il▁circule▁'on▁ne▁sait▁pas▁très▁bien▁ou▁comment▁cars▁patresse▁surveillé

2026-01-28 17:34:43,112 | INFO | speech length: 475680
2026-01-28 17:34:43,158 | INFO | decoder input length: 742
2026-01-28 17:34:43,159 | INFO | max output length: 742
2026-01-28 17:34:43,159 | INFO | min output length: 74
2026-01-28 17:35:37,572 | INFO | end detected at 246
2026-01-28 17:35:37,575 | INFO | -561.67 * 0.5 = -280.83 for decoder
2026-01-28 17:35:37,576 | INFO | -160.29 * 0.5 = -80.15 for ctc
2026-01-28 17:35:37,576 | INFO | total log probability: -360.98
2026-01-28 17:35:37,576 | INFO | normalized log probability: -1.50
2026-01-28 17:35:37,576 | INFO | total number of ended hypotheses: 165
2026-01-28 17:35:37,581 | INFO | best hypo: ▁dans▁la▁zone▁inter▁tropicale▁et▁remonte▁chez▁nous▁pendant▁leurs▁river▁à▁nous▁donc▁le▁scénario▁auquel▁on▁peut▁s'attendre▁encore▁une▁fois▁avec▁toutes▁les▁précautions▁qu'il▁faut▁d'usage▁s'il▁faut▁mètres▁c'est▁que▁cette▁che▁que▁l'on▁voit▁l'▁en▁amérique▁du▁nordre▁ne▁va▁pas▁se▁plaire▁b▁eaucoup▁avec▁l'été▁chez▁nous▁dans▁l'hémisphère▁nord▁mais▁elle▁pourrait▁tout▁au▁fait▁entraîner▁un▁certain▁nombre▁d'éroidémies▁de▁plus▁forte▁ampleur▁que▁les▁épémies▁saisonnière▁habituelles▁dans▁l'hémisphère▁sud▁on▁va▁donc▁avoir▁nos▁regard▁braqués▁sur▁l'hémisphère▁sud

2026-01-28 17:35:37,586 | INFO | speech length: 418400
2026-01-28 17:35:37,644 | INFO | decoder input length: 653
2026-01-28 17:35:37,644 | INFO | max output length: 653
2026-01-28 17:35:37,644 | INFO | min output length: 65
2026-01-28 17:36:07,247 | INFO | end detected at 229
2026-01-28 17:36:07,248 | INFO | -576.01 * 0.5 = -288.01 for decoder
2026-01-28 17:36:07,248 | INFO | -217.44 * 0.5 = -108.72 for ctc
2026-01-28 17:36:07,248 | INFO | total log probability: -396.73
2026-01-28 17:36:07,248 | INFO | normalized log probability: -1.77
2026-01-28 17:36:07,248 | INFO | total number of ended hypotheses: 163
2026-01-28 17:36:07,252 | INFO | best hypo: ▁et▁notamment▁sur▁les▁pays▁de▁la▁zone▁tempérée▁d'hémisphère▁sud▁dans▁les▁mois▁à▁venir▁et▁puis▁après▁mais▁si▁jamais▁ces▁épidémies▁surviennent▁dans▁l'hémisphère▁sud▁on▁aura▁été▁chance'▁encore▁plus▁renforcé▁ce▁qui▁peutrendre▁le▁discours▁de▁nos▁autorités▁de▁santé▁qui▁y▁vont▁se▁voir▁l'▁n'on▁pas▁récontortée▁un▁mais▁plutôt▁▁viagilant▁avec▁une▁virigilence▁encore▁recrue▁sur▁l'hi▁ver▁qui▁est▁ce▁qui▁arrive▁ou▁une▁derannière▁petit▁quesestion▁manque▁d'une▁première▁portée▁si▁ça▁se▁face▁comme▁ça▁c'est▁à▁dire▁de▁faire

2026-01-28 17:36:07,255 | INFO | speech length: 460000
2026-01-28 17:36:07,304 | INFO | decoder input length: 718
2026-01-28 17:36:07,304 | INFO | max output length: 718
2026-01-28 17:36:07,304 | INFO | min output length: 71
2026-01-28 17:36:43,945 | INFO | end detected at 265
2026-01-28 17:36:43,947 | INFO | -706.87 * 0.5 = -353.44 for decoder
2026-01-28 17:36:43,948 | INFO | -262.01 * 0.5 = -131.01 for ctc
2026-01-28 17:36:43,948 | INFO | total log probability: -484.44
2026-01-28 17:36:43,948 | INFO | normalized log probability: -1.87
2026-01-28 17:36:43,948 | INFO | total number of ended hypotheses: 161
2026-01-28 17:36:43,952 | INFO | best hypo: ▁inquiétante▁on▁le▁sait▁quand▁on▁le▁sait▁le▁premier▁octobre▁on▁c'est▁le▁premier▁décembre▁on▁ne▁sait▁premier▁oui▁ce▁que▁je▁dis▁à▁quel▁moment▁n'en▁pas▁la▁journée▁presse▁à▁férir▁on▁je▁direvollaque▁à▁quel▁moment▁dans▁l'année▁l'automne▁l'hiver▁on▁sait▁qu'on▁est▁de▁tempse▁et▁çavse▁ça▁sera▁pas▁la▁gripte▁traditionnelle▁mais▁haute▁chose▁la▁probilité▁de▁ne▁fait▁qu'augmenter▁avec▁le▁temps▁de▁déjà▁la▁probilité▁qu'on▁ne▁me▁concomne▁dans▁par▁exemple▁ne▁fait▁qu'au▁de▁bourourait▁pas▁en▁vacurs▁de▁bonne▁humeure▁j'enre▁viens▁et▁au▁milieute▁l'é▁thermes▁dans▁un▁qu'est▁ce▁spatial▁à▁l'automne

2026-01-28 17:36:43,955 | INFO | speech length: 475040
2026-01-28 17:36:44,017 | INFO | decoder input length: 741
2026-01-28 17:36:44,017 | INFO | max output length: 741
2026-01-28 17:36:44,017 | INFO | min output length: 74
2026-01-28 17:37:23,079 | INFO | end detected at 288
2026-01-28 17:37:23,080 | INFO | -776.80 * 0.5 = -388.40 for decoder
2026-01-28 17:37:23,080 | INFO | -331.72 * 0.5 = -165.86 for ctc
2026-01-28 17:37:23,080 | INFO | total log probability: -554.26
2026-01-28 17:37:23,080 | INFO | normalized log probability: -1.96
2026-01-28 17:37:23,080 | INFO | total number of ended hypotheses: 154
2026-01-28 17:37:23,084 | INFO | best hypo: ▁et▁quand▁je▁reçois▁le▁bulletin▁trimestriel▁le▁point▁de▁bulletin▁trimestriel▁de▁malade▁ma▁fille▁je▁j'appelle▁les▁pompes▁funèbres▁lorsque▁je▁dire▁c'est▁que▁c'écumulatif▁jourd'hui▁on▁sait▁que▁la▁petite▁che▁s'a▁émerger▁au▁mexique▁est▁quand▁même▁un▁peu▁plus▁contagieuse▁que▁on▁n'aurait▁pu▁l'espprer▁elle▁est▁plus▁contagieuse▁elle▁s'est▁répandu▁assez▁rappement▁comme▁un▁au▁aux▁etats▁ou▁uni▁au▁canada▁ou▁en▁grande▁breptagne▁ou▁au▁royau▁uni▁même▁dans▁tous▁ces▁hendroits▁là▁elle▁y▁ame▁des▁chaînes▁de▁munsions▁locales▁et▁c'a▁dire▁que▁le▁virus▁sait▁fait▁faire▁de▁sa▁propre▁au▁boine▁de▁lui▁même▁sur▁plasse▁ce▁que▁je▁crois▁qu'à▁la▁fin▁de▁l'été

2026-01-28 17:37:23,086 | INFO | speech length: 424960
2026-01-28 17:37:23,143 | INFO | decoder input length: 663
2026-01-28 17:37:23,143 | INFO | max output length: 663
2026-01-28 17:37:23,143 | INFO | min output length: 66
2026-01-28 17:37:55,272 | INFO | end detected at 247
2026-01-28 17:37:55,274 | INFO | -585.98 * 0.5 = -292.99 for decoder
2026-01-28 17:37:55,274 | INFO | -132.43 * 0.5 = -66.22 for ctc
2026-01-28 17:37:55,274 | INFO | total log probability: -359.21
2026-01-28 17:37:55,274 | INFO | normalized log probability: -1.49
2026-01-28 17:37:55,274 | INFO | total number of ended hypotheses: 133
2026-01-28 17:37:55,278 | INFO | best hypo: ▁si▁on▁sait▁qu'elle▁eut▁des▁très▁fortes▁épidémies▁dans▁l'hémisphère▁sud▁la▁probabilité▁sera▁très▁renforcée▁que▁ça▁la▁loi▁nous▁reviennent▁pendant▁notre▁hiver▁à▁nous▁et▁il▁peut▁être▁même▁pendant▁l'autône▁sa▁dire▁peut▁être▁assez▁précocement▁et▁donc▁la▁vigilencera▁crue▁ce▁moment▁je▁rappelle▁que▁depuis▁le▁début▁de▁la▁crise▁l'organisation▁mondiale▁de▁la▁santé▁elle▁contribue▁largement▁à▁l'angoise▁soit▁et▁l'iniquiétude▁par▁s'est▁communiquer▁et▁on▁seractait▁jus▁qu'auen▁niveau▁v▁d'une▁échelle▁qui▁compte▁sans▁qui▁concerne▁bien▁entendus▁les▁niveaux▁d'alerte

2026-01-28 17:37:55,281 | INFO | speech length: 187520
2026-01-28 17:37:55,328 | INFO | decoder input length: 292
2026-01-28 17:37:55,328 | INFO | max output length: 292
2026-01-28 17:37:55,328 | INFO | min output length: 29
2026-01-28 17:38:03,281 | INFO | end detected at 99
2026-01-28 17:38:03,283 | INFO | -20.85 * 0.5 = -10.43 for decoder
2026-01-28 17:38:03,283 | INFO | -12.53 * 0.5 =  -6.27 for ctc
2026-01-28 17:38:03,283 | INFO | total log probability: -16.69
2026-01-28 17:38:03,283 | INFO | normalized log probability: -0.18
2026-01-28 17:38:03,283 | INFO | total number of ended hypotheses: 168
2026-01-28 17:38:03,285 | INFO | best hypo: ▁loëmès▁voit▁d'ailleurs▁grand▁puisqu'elle▁a▁même▁évoqué▁la▁possibilité▁d'une▁pandémie▁touchant▁le▁tiers▁de▁l'humanité▁visite▁à▁l'épicentre▁de▁la▁peur▁avec▁sophie▁pagesse▁et▁dominique▁lemarché

2026-01-28 17:38:03,307 | INFO | Chunk: 0 | WER=69.736842 | S=14 D=3 I=36
2026-01-28 17:38:03,308 | INFO | Chunk: 1 | WER=100.000000 | S=4 D=3 I=0
2026-01-28 17:38:03,317 | INFO | Chunk: 2 | WER=92.380952 | S=23 D=36 I=38
2026-01-28 17:38:03,325 | INFO | Chunk: 3 | WER=87.500000 | S=24 D=36 I=31
2026-01-28 17:38:03,326 | INFO | Chunk: 4 | WER=175.000000 | S=4 D=0 I=3
2026-01-28 17:38:03,332 | INFO | Chunk: 5 | WER=80.645161 | S=11 D=40 I=24
2026-01-28 17:38:03,340 | INFO | Chunk: 6 | WER=74.226804 | S=8 D=30 I=34
2026-01-28 17:38:03,352 | INFO | Chunk: 7 | WER=75.781250 | S=9 D=45 I=43
2026-01-28 17:38:03,362 | INFO | Chunk: 8 | WER=86.238532 | S=11 D=43 I=40
2026-01-28 17:38:03,365 | INFO | Chunk: 9 | WER=90.476190 | S=47 D=9 I=1
2026-01-28 17:38:03,373 | INFO | Chunk: 10 | WER=88.659794 | S=10 D=39 I=37
2026-01-28 17:38:03,378 | INFO | Chunk: 11 | WER=87.341772 | S=61 D=8 I=0
2026-01-28 17:38:03,381 | INFO | Chunk: 12 | WER=87.837838 | S=43 D=22 I=0
2026-01-28 17:38:03,382 | INFO | Chunk: 13 | WER=95.652174 | S=12 D=10 I=0
2026-01-28 17:38:03,384 | INFO | Chunk: 14 | WER=97.368421 | S=32 D=3 I=2
2026-01-28 17:38:03,395 | INFO | Chunk: 15 | WER=99.180328 | S=31 D=43 I=47
2026-01-28 17:38:03,407 | INFO | Chunk: 16 | WER=79.230769 | S=9 D=53 I=41
2026-01-28 17:38:03,416 | INFO | Chunk: 17 | WER=72.477064 | S=10 D=37 I=32
2026-01-28 17:38:03,423 | INFO | Chunk: 18 | WER=92.045455 | S=6 D=33 I=42
2026-01-28 17:38:03,435 | INFO | Chunk: 19 | WER=94.354839 | S=111 D=5 I=1
2026-01-28 17:38:03,448 | INFO | Chunk: 20 | WER=91.304348 | S=19 D=58 I=49
2026-01-28 17:38:03,459 | INFO | Chunk: 21 | WER=78.906250 | S=11 D=58 I=32
2026-01-28 17:38:03,460 | INFO | Chunk: 22 | WER=94.594595 | S=32 D=3 I=0
2026-01-28 17:38:06,314 | INFO | File: Rhap-D2008.wav | WER=37.202230 | S=380 D=219 I=135
2026-01-28 17:38:06,314 | INFO | ------------------------------
2026-01-28 17:38:06,315 | INFO | Conf cv Done!
2026-01-28 17:38:06,521 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 17:38:06,550 | INFO | Vocabulary size: 47
2026-01-28 17:38:07,431 | INFO | Gradient checkpoint layers: []
2026-01-28 17:38:08,150 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 17:38:08,154 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 17:38:08,155 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 17:38:08,155 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 17:38:08,158 | INFO | speech length: 467680
2026-01-28 17:38:08,199 | INFO | decoder input length: 730
2026-01-28 17:38:08,199 | INFO | max output length: 730
2026-01-28 17:38:08,199 | INFO | min output length: 73
2026-01-28 17:39:05,096 | INFO | end detected at 602
2026-01-28 17:39:05,098 | INFO | -639.11 * 0.5 = -319.56 for decoder
2026-01-28 17:39:05,099 | INFO | -45.62 * 0.5 = -22.81 for ctc
2026-01-28 17:39:05,099 | INFO | total log probability: -342.37
2026-01-28 17:39:05,099 | INFO | normalized log probability: -0.58
2026-01-28 17:39:05,099 | INFO | total number of ended hypotheses: 187
2026-01-28 17:39:05,106 | INFO | best hypo: bonsoir<space>à<space>tous<space>nous<space>consacrerons<space>la<space>dernière<space>partie<space>de<space>cette<space>émission<space>à<space>vos<space>questions<space>sms<space>alors<space>où<space>en<space>est<space>on<space>vraiment<space>du<space>côté<space>de<space>la<space>grippe<space>a<space>h<space>n<space>a<space>la<space>tendance<space>dans<space>les<space>médias<space>et<space>plutôt<space>vous<space>l'avez<space>vu<space>du<space>côté<space>d'un<space>léger<space>assouplissement<space>mais<space>les<space>déclarations<space>des<space>spécialistes<space>et<space>de<space>l'oms<space>euh<space>ont<space>effectivement<space>de<space>quoi<space>à<space>l'armée<space>c'est<space>comme<space>un<space>très<space>mauvais<space>rendez<space>vous<space>à<space>venir<space>rappelons<space>qu'il<space>y<space>a<space>moins<space>d'une<space>semaine<space>le<space>numéro<space>de<space>x<space>de<space>l'organisation<space>mondiale<space>de<space>la<space>santé<space>estimie<space>qun<space>tiers<space>de<space>la<space>population<space>mondiale<space>pourrait<space>être<space>contaminée<space>nous<space>avons<space>intitulé<space>cette<space>émission<space>gripe<space>a<space>les<space>chiffre

2026-01-28 17:39:05,109 | INFO | speech length: 16319
2026-01-28 17:39:05,146 | INFO | decoder input length: 24
2026-01-28 17:39:05,146 | INFO | max output length: 24
2026-01-28 17:39:05,146 | INFO | min output length: 2
2026-01-28 17:39:05,880 | INFO | adding <eos> in the last position in the loop
2026-01-28 17:39:05,886 | INFO | no hypothesis. Finish decoding.
2026-01-28 17:39:05,887 | INFO |  -8.63 * 0.5 =  -4.31 for decoder
2026-01-28 17:39:05,887 | INFO | -25.03 * 0.5 = -12.51 for ctc
2026-01-28 17:39:05,887 | INFO | total log probability: -16.83
2026-01-28 17:39:05,887 | INFO | normalized log probability: -0.65
2026-01-28 17:39:05,887 | INFO | total number of ended hypotheses: 43
2026-01-28 17:39:05,887 | INFO | best hypo: qui<space>ont<space>acepté<space>dix<space>parti

2026-01-28 17:39:05,887 | WARNING | best hypo length: 24 == max output length: 24
2026-01-28 17:39:05,887 | WARNING | decoding may be stopped by the max output length limitation, please consider to increase the maxlenratio.
2026-01-28 17:39:05,888 | INFO | speech length: 466720
2026-01-28 17:39:05,923 | INFO | decoder input length: 728
2026-01-28 17:39:05,924 | INFO | max output length: 728
2026-01-28 17:39:05,924 | INFO | min output length: 72
2026-01-28 17:40:06,218 | INFO | end detected at 632
2026-01-28 17:40:06,220 | INFO | -806.15 * 0.5 = -403.07 for decoder
2026-01-28 17:40:06,220 | INFO | -93.04 * 0.5 = -46.52 for ctc
2026-01-28 17:40:06,220 | INFO | total log probability: -449.60
2026-01-28 17:40:06,220 | INFO | normalized log probability: -0.72
2026-01-28 17:40:06,220 | INFO | total number of ended hypotheses: 192
2026-01-28 17:40:06,227 | INFO | best hypo: comme<space>chaque<space>soir<space>antoine<space>flaou<space>est<space>épidémiologiste<space>spécialiste<space>de<space>la<space>grippe<space>vous<space>êtes<space>directeur<space>de<space>l'école<space>des<space>hautes<space>études<space>en<space>santé<space>publique<space>à<space>rennes<space>nous<space>avons<space>tous<space>entendu<space>parler<space>de<space>votre<space>scénario<space>envisageant<space>trente<space>mille<space>morts<space>dans<space>notre<space>pays<space>l'an<space>prochain<space>pour<space>cause<space>de<space>gré<space>h<space>un<space>l<space>un<space>euh<space>je<space>rappelle<space>que<space>votre<space>livre<space>des<space>épidémies<space>et<space>des<space>hommes<space>vient<space>de<space>paraître<space>aux<space>éditions<space>de<space>la<space>martinière<space>jean<space>françois<space>saluzaute<space>t<space>l<space>h<space>hirologe<space>vous<space>êtes<space>expere<space>auprès<space>de<space>l'oms<space>e<space>ancien<space>chercheur<space>à<space>l'institut<space>pasteur<space>vous<space>être<space>par<space>ailleurs<space>reponsbe<space>du<space>développement<space>de<space>vacis<space>contre<space>les<space>maladis<space>virale<space>émerente<space>dans<space>un<space>laboratoire

2026-01-28 17:40:06,230 | INFO | speech length: 457760
2026-01-28 17:40:06,267 | INFO | decoder input length: 714
2026-01-28 17:40:06,268 | INFO | max output length: 714
2026-01-28 17:40:06,268 | INFO | min output length: 71
2026-01-28 17:41:05,480 | INFO | end detected at 624
2026-01-28 17:41:05,482 | INFO | -683.83 * 0.5 = -341.91 for decoder
2026-01-28 17:41:05,482 | INFO | -80.00 * 0.5 = -40.00 for ctc
2026-01-28 17:41:05,482 | INFO | total log probability: -381.91
2026-01-28 17:41:05,482 | INFO | normalized log probability: -0.62
2026-01-28 17:41:05,482 | INFO | total number of ended hypotheses: 171
2026-01-28 17:41:05,492 | INFO | best hypo: votre<space>dernier<space>livre<space>à<space>la<space>conquête<space>des<space>virus<space>vient<space>lui<space>de<space>paraître<space>chez<space>belin<space>christian<space>perron<space>est<space>un<space>sectiologue<space>chef<space>du<space>service<space>des<space>maladies<space>insectieux<space>et<space>tropicales<space>à<space>l'hôpital<space>raymond<space>point<space>carré<space>de<space>gage<space>dans<space>les<space>haut<space>de<space>seine<space>vous<space>présidez<space>par<space>ailleurs<space>la<space>commission<space>maladie<space>transmissible<space>euh<space>du<space>haut<space>conseil<space>de<space>la<space>santé<space>publique<space>enfin<space>michel<space>sédebon<space>est<space>sociologue<space>spécialiste<space>des<space>crises<space>sanitaires<space>vous<space>êtres<space>dire<space>teur<space>de<space>recherche<space>au<space>cnrs<space>st<space>responsable<space>du<space>centre<space>de<space>reche<space>che<space>sur<space>le<space>risque<space>et<space>sa<space>régulation<space>à<space>l'école<space>des<space>hautes<space>études<space>en<space>santé<space>publique<space>euh<space>antone<space>saio<space>bon<space>ban<space>on<space>commence<space>par<space>vous<space>hein<space>cest

2026-01-28 17:41:05,495 | INFO | speech length: 22880
2026-01-28 17:41:05,537 | INFO | decoder input length: 35
2026-01-28 17:41:05,537 | INFO | max output length: 35
2026-01-28 17:41:05,537 | INFO | min output length: 3
2026-01-28 17:41:06,638 | INFO | adding <eos> in the last position in the loop
2026-01-28 17:41:06,644 | INFO | no hypothesis. Finish decoding.
2026-01-28 17:41:06,645 | INFO | -21.57 * 0.5 = -10.79 for decoder
2026-01-28 17:41:06,645 | INFO | -24.37 * 0.5 = -12.18 for ctc
2026-01-28 17:41:06,645 | INFO | total log probability: -22.97
2026-01-28 17:41:06,645 | INFO | normalized log probability: -0.62
2026-01-28 17:41:06,645 | INFO | total number of ended hypotheses: 40
2026-01-28 17:41:06,645 | INFO | best hypo: est<space>ceque<space>vous<space>maintez<space>vos<space>pvisions

2026-01-28 17:41:06,645 | WARNING | best hypo length: 35 == max output length: 35
2026-01-28 17:41:06,645 | WARNING | decoding may be stopped by the max output length limitation, please consider to increase the maxlenratio.
2026-01-28 17:41:06,646 | INFO | speech length: 388800
2026-01-28 17:41:06,704 | INFO | decoder input length: 607
2026-01-28 17:41:06,705 | INFO | max output length: 607
2026-01-28 17:41:06,705 | INFO | min output length: 60
2026-01-28 17:41:46,870 | INFO | end detected at 442
2026-01-28 17:41:46,872 | INFO | -158.05 * 0.5 = -79.02 for decoder
2026-01-28 17:41:46,872 | INFO | -31.88 * 0.5 = -15.94 for ctc
2026-01-28 17:41:46,872 | INFO | total log probability: -94.96
2026-01-28 17:41:46,872 | INFO | normalized log probability: -0.22
2026-01-28 17:41:46,872 | INFO | total number of ended hypotheses: 197
2026-01-28 17:41:46,877 | INFO | best hypo: enfin<space>sauf<space>que<space>là<space>on<space>n'est<space>pas<space>dans<space>la<space>météo<space>oui<space>donc<space>euh<space>s<space>ce<space>sont<space>pas<space>des<space>prévisions<space>si<space>vous<space>l'avez<space>peut<space>être<space>mieux<space>euh<space>explicité<space>en<space>disant<space>que<space>c'est<space>un<space>scénario<space>voilà<space>un<space>des<space>scénarios<space>possibles<space>euh<space>c'est<space>à<space>dire<space>que<space>si<space>vraiment<space>euh<space>l'activité<space>que<space>l'on<space>on<space>constate<space>aujourd'hui<space>en<space>en<space>amérique<space>du<space>nord<space>euh<space>devient<space>l'activité<space>d'une<space>pandémie<space>alors<space>euh<space>on<space>n'a<space>jamais<space>vraiment<space>su<space>jusqu'à<space>présent<space>arrête<space>les<space>épidémies<space>de<space>gripes<space>saisonnières

2026-01-28 17:41:46,880 | INFO | speech length: 420640
2026-01-28 17:41:46,924 | INFO | decoder input length: 656
2026-01-28 17:41:46,924 | INFO | max output length: 656
2026-01-28 17:41:46,924 | INFO | min output length: 65
2026-01-28 17:42:35,718 | INFO | end detected at 565
2026-01-28 17:42:35,719 | INFO | -573.36 * 0.5 = -286.68 for decoder
2026-01-28 17:42:35,719 | INFO | -46.07 * 0.5 = -23.03 for ctc
2026-01-28 17:42:35,719 | INFO | total log probability: -309.71
2026-01-28 17:42:35,719 | INFO | normalized log probability: -0.55
2026-01-28 17:42:35,719 | INFO | total number of ended hypotheses: 156
2026-01-28 17:42:35,726 | INFO | best hypo: et<space>euh<space>on<space>n'a<space>pas<space>beaucoup<space>d'expériences<space>mais<space>on<space>a<space>quand<space>même<space>une<space>expérience<space>de<space>trois<space>en<space>emi<space>passés<space>et<space>aucun<space>des<space>pays<space>n'a<space>su<space>pour<space>l'instant<space>arrêter<space>la<space>vague<space>pandémique<space>dans<space>son<space>pays<space>alors<space>il<space>est<space>possible<space>que<space>dans<space>le<space>vingt<space>et<space>unième<space>siècle<space>dans<space>certains<space>pays<space>mieux<space>doter<space>que<space>d'autres<space>on<space>puisse<space>freiner<space>arrêter<space>atténuer<space>la<space>vague<space>mais<space>si<space>elles<space>se<space>déroulent<space>de<space>façon<space>je<space>dirais<space>euh<space>un<space>peu<space>comme<space>dans<space>les<space>les<space>fois<space>passés<space>c'est<space>les<space>scénarios<space>que<space>l'on<space>pe<space>t<space>imagine<space>alors<space>je<space>s<space>de<space>c'est<space>très<space>bie<space>que<space>ous<space>nous<space>précisez<space>des<space>chos<space>comme<space>ça<space>on<space>s<space>ce<space>chiffre<space>de<space>trente<space>mille

2026-01-28 17:42:35,729 | INFO | speech length: 460320
2026-01-28 17:42:35,770 | INFO | decoder input length: 718
2026-01-28 17:42:35,770 | INFO | max output length: 718
2026-01-28 17:42:35,770 | INFO | min output length: 71
2026-01-28 17:43:33,974 | INFO | end detected at 645
2026-01-28 17:43:33,976 | INFO | -841.38 * 0.5 = -420.69 for decoder
2026-01-28 17:43:33,976 | INFO | -65.39 * 0.5 = -32.70 for ctc
2026-01-28 17:43:33,976 | INFO | total log probability: -453.39
2026-01-28 17:43:33,976 | INFO | normalized log probability: -0.71
2026-01-28 17:43:33,976 | INFO | total number of ended hypotheses: 172
2026-01-28 17:43:33,984 | INFO | best hypo: c'est<space>finalement<space>une<space>extrapolation<space>que<space>vous<space>faites<space>mais<space>à<space>partir<space>de<space>scénarios<space>passé<space>entre<space>guillemets<space>raisonables<space>et<space>en<space>teant<space>compte<space>de<space>ce<space>qu'on<space>a<space>pu<space>faire<space>ou<space>surtout<space>de<space>ce<space>qu'on<space>a<space>pas<space>pu<space>faire<space>je<space>vous<space>ai<space>bien<space>compris<space>oui<space>exactement<space>c'est<space>exactement<space>cela<space>un<space>petit<space>peu<space>vous<space>parliez<space>de<space>la<space>météo<space>mais<space>dans<space>le<space>fond<space>quand<space>on<space>veut<space>prédire<space>un<space>cyclone<space>au<space>jour<space>d'aujourd'hui<space>on<space>se<space>rapporte<space>au<space>catalogue<space>des<space>cyclones<space>passés<space>et<space>on<space>regarde<space>s<space>le<space>plus<space>proche<space>cyclone<space>euh<space>ou<space>embrant<space>de<space>cyclone<space>de<space>ce<space>que<space>l'on<space>voit<space>euh<space>actuellement<space>eh<space>bien<space>on<space>peut<space>dire<space>un<space>peu<space>de<space>la<space>même<space>façon<space>qu'aujourd'hui<space>ce<space>que<space>l'on<space>voit<space>c'est<space>un<space>prémis<space>de<space>démarrage<space>de<space>pandémie<space>euh

2026-01-28 17:43:33,986 | INFO | speech length: 393440
2026-01-28 17:43:34,028 | INFO | decoder input length: 614
2026-01-28 17:43:34,028 | INFO | max output length: 614
2026-01-28 17:43:34,029 | INFO | min output length: 61
2026-01-28 17:44:22,075 | INFO | end detected at 576
2026-01-28 17:44:22,076 | INFO | -789.79 * 0.5 = -394.89 for decoder
2026-01-28 17:44:22,076 | INFO | -157.24 * 0.5 = -78.62 for ctc
2026-01-28 17:44:22,076 | INFO | total log probability: -473.52
2026-01-28 17:44:22,076 | INFO | normalized log probability: -0.83
2026-01-28 17:44:22,076 | INFO | total number of ended hypotheses: 163
2026-01-28 17:44:22,083 | INFO | best hypo: on<space>n'est<space>pas<space>des<space>devins<space>on<space>ne<space>fait<space>pas<space>de<space>la<space>prédiction<space>ou<space>la<space>prévision<space>euh<space>on<space>imagine<space>des<space>scénarios<space>et<space>y<space>a<space>plusieurs<space>scénarios<space>possible<space>mais<space>celui<space>qui<space>est<space>au<space>plus<space>grave<space>le<space>plus<space>pour<space>la<space>mrand<space>pas<space>le<space>plus<space>grave<space>pas<space>parce<space>qu'à<space>trente<space>mille<space>vous<space>êtes<space>dans<space>dans<space>le<space>encore<space>dans<space>le<space>dans<space>le<space>rés<space>enfin<space>dans<space>le<space>à<space>trente<space>mille<space>on<space>est<space>dans<space>quelque<space>chose<space>où<space>la<space>mortalité<space>est<space>très<space>voisine<space>de<space>la<space>mortalité<space>de<space>la<space>griple<space>paisonière<space>ra<space>ra<space>raippe<space>raisonnère<space>et<space>meutrière<space>la<space>raipe<space>sonsonie<space>enfin<space>ntre<space>cinq<space>mille<space>monts<space>par<space>an<space>et<space>trente<space>mile<space>je<space>voilà<space>y<space>a<space>six<space>fis<space>plus<space>de<space>ca<space>vos<space>avez<space>raison

2026-01-28 17:44:22,086 | INFO | speech length: 215200
2026-01-28 17:44:22,127 | INFO | decoder input length: 335
2026-01-28 17:44:22,127 | INFO | max output length: 335
2026-01-28 17:44:22,127 | INFO | min output length: 33
2026-01-28 17:44:39,852 | INFO | end detected at 293
2026-01-28 17:44:39,855 | INFO | -27.21 * 0.5 = -13.60 for decoder
2026-01-28 17:44:39,855 | INFO | -13.50 * 0.5 =  -6.75 for ctc
2026-01-28 17:44:39,855 | INFO | total log probability: -20.35
2026-01-28 17:44:39,855 | INFO | normalized log probability: -0.07
2026-01-28 17:44:39,855 | INFO | total number of ended hypotheses: 242
2026-01-28 17:44:39,859 | INFO | best hypo: si<space>y<space>a<space>six<space>fois<space>plus<space>de<space>cas<space>simplement<space>de<space>cas<space>de<space>gens<space>affectés<space>par<space>le<space>virus<space>alors<space>mécaniquement<space>y<space>a<space>une<space>chance<space>un<space>risque<space>qu'il<space>y<space>ait<space>six<space>fois<space>plus<space>de<space>de<space>ch<space>ça<space>je<space>suis<space>pas<space>pervers<space>mais<space>enfin<space>alors<space>pardonnez<space>moi<space>si<space>ça<space>c'était<space>on<space>va<space>dire<space>une<space>des<space>estimations<space>à<space>potentielle<space>raisonnable

2026-01-28 17:44:39,863 | INFO | speech length: 377440
2026-01-28 17:44:39,905 | INFO | decoder input length: 589
2026-01-28 17:44:39,906 | INFO | max output length: 589
2026-01-28 17:44:39,906 | INFO | min output length: 58
2026-01-28 17:45:22,606 | INFO | end detected at 543
2026-01-28 17:45:22,608 | INFO | -435.87 * 0.5 = -217.94 for decoder
2026-01-28 17:45:22,608 | INFO | -316.15 * 0.5 = -158.08 for ctc
2026-01-28 17:45:22,608 | INFO | total log probability: -376.01
2026-01-28 17:45:22,608 | INFO | normalized log probability: -0.70
2026-01-28 17:45:22,608 | INFO | total number of ended hypotheses: 175
2026-01-28 17:45:22,614 | INFO | best hypo: l'estimation<space>inquiétante<space>c'est<space>quoi<space>les<space>estimations<space>inquiétantes<space>euh<space>a<space>allées<space>jusqu'à<space>combien<space>concernant<space>les<space>les<space>la<space>france<space>et<space>les<space>français<space>les<space>estimations<space>inquiétantes<space>euh<space>elle<space>ne<space>me<space>semble<space>pas<space>beau<space>pas<space>très<space>probable<space>une<space>des<space>estimation<space>inquiétante<space>c'est<space>celle<space>de<space>milleneuf<space>cent<space>dix<space>huit<space>il<space>a<space>eu<space>en<space>mille<space>neuf<space>cent<space>dix<space>huit<space>sur<space>l'ensemble<space>de<space>la<space>planète<space>on<space>dit<space>quarante<space>millions<space>de<space>décès<space>c'est<space>à<space>dire<space>une<space>entait<space>tre<space>baie<space>bais<space>urquie<space>ie<space>ille<space>ne<space>ne<space>nt<space>de<space>huit<space>à<space>re<space>e<space>pare<space>empe<space>point<space>di<space>mille<space>bait<space>t<space>e<space>mille<space>e<space>e<space>di<space>huit<space>y<space>ait<space>tre<space>bagnoires

2026-01-28 17:45:22,617 | INFO | speech length: 294720
2026-01-28 17:45:22,658 | INFO | decoder input length: 460
2026-01-28 17:45:22,658 | INFO | max output length: 460
2026-01-28 17:45:22,658 | INFO | min output length: 46
2026-01-28 17:45:50,269 | INFO | end detected at 393
2026-01-28 17:45:50,271 | INFO | -59.78 * 0.5 = -29.89 for decoder
2026-01-28 17:45:50,271 | INFO | -14.30 * 0.5 =  -7.15 for ctc
2026-01-28 17:45:50,271 | INFO | total log probability: -37.04
2026-01-28 17:45:50,271 | INFO | normalized log probability: -0.10
2026-01-28 17:45:50,271 | INFO | total number of ended hypotheses: 173
2026-01-28 17:45:50,276 | INFO | best hypo: en<space>mille<space>neuf<space>cent<space>dix<space>huit<space>y<space>avait<space>pas<space>de<space>de<space>de<space>la<space>moitié<space>des<space>des<space>des<space>domiciles<space>parisiens<space>n'avaient<space>pas<space>le<space>le<space>raccord<space>à<space>l'égoût<space>euh<space>vous<space>aviez<space>donc<space>euh<space>pas<space>d'antibiotique<space>bien<space>entendu<space>on<space>ne<space>savait<space>pas<space>ce<space>qu'était<space>un<space>virus<space>en<space>mille<space>neuf<space>cent<space>dix<space>huit<space>on<space>est<space>au<space>moyen<space>âge<space>de<space>la<space>santé<space>en<space>mille<space>neuf<space>cent<space>dix<space>huit<space>y<space>a<space>pas<space>de<space>d'anti<space>virous<space>y<space>a<space>pas<space>de<space>vaccin<space>je<space>pense<space>que<space>aujourd'hui<space>euh

2026-01-28 17:45:50,278 | INFO | speech length: 240641
2026-01-28 17:45:50,317 | INFO | decoder input length: 375
2026-01-28 17:45:50,318 | INFO | max output length: 375
2026-01-28 17:45:50,318 | INFO | min output length: 37
2026-01-28 17:46:13,175 | INFO | end detected at 333
2026-01-28 17:46:13,179 | INFO | -40.41 * 0.5 = -20.20 for decoder
2026-01-28 17:46:13,179 | INFO | -25.76 * 0.5 = -12.88 for ctc
2026-01-28 17:46:13,179 | INFO | total log probability: -33.08
2026-01-28 17:46:13,179 | INFO | normalized log probability: -0.10
2026-01-28 17:46:13,179 | INFO | total number of ended hypotheses: 202
2026-01-28 17:46:13,184 | INFO | best hypo: même<space>une<space>grippe<space>aussi<space>virulente<space>que<space>celle<space>de<space>mille<space>neuf<space>cent<space>dix<space>huit<space>n'entraînerait<space>pas<space>une<space>mortalité<space>aussi<space>effroyable<space>donc<space>euh<space>les<space>scénarios<space>euh<space>catastrophes<space>qu'on<space>a<space>pu<space>voir<space>et<space>que<space>leurs<space>ici<space>euh<space>de<space>si<space>de<space>là<space>ne<space>me<space>paraissent<space>pas<space>mais<space>encore<space>une<space>fois<space>je<space>suis<space>pas<space>un<space>un<space>indevin<space>mais<space>ne<space>me<space>paraissent<space>pas<space>très<space>probable<space>euh

2026-01-28 17:46:13,188 | INFO | speech length: 76640
2026-01-28 17:46:13,225 | INFO | decoder input length: 119
2026-01-28 17:46:13,225 | INFO | max output length: 119
2026-01-28 17:46:13,225 | INFO | min output length: 11
2026-01-28 17:46:16,720 | INFO | end detected at 79
2026-01-28 17:46:16,722 | INFO |  -5.92 * 0.5 =  -2.96 for decoder
2026-01-28 17:46:16,722 | INFO |  -0.66 * 0.5 =  -0.33 for ctc
2026-01-28 17:46:16,723 | INFO | total log probability: -3.29
2026-01-28 17:46:16,723 | INFO | normalized log probability: -0.05
2026-01-28 17:46:16,723 | INFO | total number of ended hypotheses: 189
2026-01-28 17:46:16,724 | INFO | best hypo: y<space>a<space>enfin<space>dans<space>cette<space>projection<space>euh<space>est<space>ce<space>qu'il<space>y<space>a<space>une<space>limite<space>de<space>date

2026-01-28 17:46:16,726 | INFO | speech length: 148960
2026-01-28 17:46:16,763 | INFO | decoder input length: 232
2026-01-28 17:46:16,763 | INFO | max output length: 232
2026-01-28 17:46:16,763 | INFO | min output length: 23
2026-01-28 17:46:27,310 | INFO | end detected at 212
2026-01-28 17:46:27,313 | INFO | -17.40 * 0.5 =  -8.70 for decoder
2026-01-28 17:46:27,313 | INFO | -10.32 * 0.5 =  -5.16 for ctc
2026-01-28 17:46:27,313 | INFO | total log probability: -13.86
2026-01-28 17:46:27,313 | INFO | normalized log probability: -0.07
2026-01-28 17:46:27,313 | INFO | total number of ended hypotheses: 248
2026-01-28 17:46:27,316 | INFO | best hypo: nous<space>dites<space>voilà<space>si<space>ça<space>tourne<space>mal<space>c'est<space>dans<space>l'année<space>à<space>venir<space>euh<space>voilà<space>est<space>ce<space>que<space>vous<space>vous<space>êtes<space>fixé<space>aussi<space>un<space>cadre<space>si<space>je<space>puis<space>dire<space>dans<space>la<space>durée<space>du<space>du<space>du<space>scénario<space>et<space>donc<space>du<space>film<space>en<space>quelque<space>sorte

2026-01-28 17:46:27,319 | INFO | speech length: 479840
2026-01-28 17:46:27,360 | INFO | decoder input length: 749
2026-01-28 17:46:27,360 | INFO | max output length: 749
2026-01-28 17:46:27,360 | INFO | min output length: 74
2026-01-28 17:47:33,022 | INFO | end detected at 668
2026-01-28 17:47:33,023 | INFO | -1041.43 * 0.5 = -520.72 for decoder
2026-01-28 17:47:33,023 | INFO | -135.60 * 0.5 = -67.80 for ctc
2026-01-28 17:47:33,023 | INFO | total log probability: -588.51
2026-01-28 17:47:33,023 | INFO | normalized log probability: -0.89
2026-01-28 17:47:33,023 | INFO | total number of ended hypotheses: 148
2026-01-28 17:47:33,031 | INFO | best hypo: alors<space>euh<space>oui<space>c'est<space>une<space>ch<space>c'est<space>vrai<space>on<space>c'était<space>frai<space>très<space>frappé<space>par<space>exemple<space>no<space>notre<space>ministre<space>de<space>la<space>santé<space>roselyne<space>bachelot<space>elle<space>a<space>géré<space>cette<space>crise<space>on<space>va<space>dire<space>au<space>minimum<space>en<space>trois<space>ans<space>en<space>termes<space>de<space>communication<space>perçue<space>par<space>non<space>de<space>ces<space>elle<space>a<space>commencé<space>par<space>nous<space>le<space>dire<space>euh<space>on<space>est<space>aux<space>affaires<space>après<space>il<space>a<space>dit<space>on<space>et<space>aux<space>affaires<space>ça<space>pourait<space>être<space>grave<space>ensuite<space>elle<space>a<space>dit<space>j'ai<space>mis<space>tou<space>montagny<space>fus<space>de<space>côté<space>ontravaille<space>là<space>dessus<space>mais<space>on<space>ne<space>peut<space>pas<space>exclure<space>que<space>y<space>a<space>t<space>quelque<space>chose<space>de<space>grave<space>voyez<space>donc<space>y<space>a<space>y<space>a<space>eun<space>dégravation<space>je<space>voilà<space>eun<space>mais<space>surtout<space>in<space>nous<space>a<space>dit<space>faut<space>se<space>méfier<space>du<space>retour<space>de<space>balancier<space>en<space>gros<space>de<space>paser<space>l'été<space>qu'st<space>ce<space>qui<space>va<space>se<space>passer<space>l'hiver<space>prochain

2026-01-28 17:47:33,033 | INFO | speech length: 425760
2026-01-28 17:47:33,078 | INFO | decoder input length: 664
2026-01-28 17:47:33,078 | INFO | max output length: 664
2026-01-28 17:47:33,078 | INFO | min output length: 66
2026-01-28 17:48:25,142 | INFO | end detected at 574
2026-01-28 17:48:25,144 | INFO | -585.68 * 0.5 = -292.84 for decoder
2026-01-28 17:48:25,144 | INFO | -61.07 * 0.5 = -30.53 for ctc
2026-01-28 17:48:25,144 | INFO | total log probability: -323.37
2026-01-28 17:48:25,144 | INFO | normalized log probability: -0.57
2026-01-28 17:48:25,144 | INFO | total number of ended hypotheses: 177
2026-01-28 17:48:25,153 | INFO | best hypo: dans<space>le<space>cas<space>déjà<space>de<space>la<space>grippe<space>traditionnelle<space>est<space>ce<space>que<space>ce<space>chiffre<space>de<space>trente<space>mille<space>et<space>s'applique<space>à<space>ça<space>oui<space>c'est<space>à<space>dire<space>que<space>la<space>grippe<space>ce<space>qu'on<space>connaît<space>de<space>la<space>grppe<space>parce<space>que<space>la<space>grippe<space>est<space>une<space>maladie<space>qu'on<space>connaît<space>bien<space>oui<space>euh<space>chaque<space>année<space>elle<space>revient<space>dans<space>notre<space>pays<space>et<space>chaque<space>année<space>c'est<space>un<space>virus<space>qui<space>a<space>fait<space>le<space>tour<space>du<space>monde<space>qui<space>nous<space>revient<space>euh<space>le<space>virus<space>il<space>émerge<space>quelque<space>part<space>dans<space>le<space>monde<space>il<space>va<space>dans<space>l'hémisphère<space>sud<space>pendant<space>les<space>mois<space>de<space>juillet<space>et<space>cout<space>e<space>h<space>p<space>qui<space>est<space>l'or<space>rivers<space>astrale<space>il<space>circule<space>t<space>on<space>ne<space>sa<space>t<space>pas<space>très<space>bien<space>comment<space>car<space>c'est<space>pas<space>très<space>surveillé

2026-01-28 17:48:25,157 | INFO | speech length: 475680
2026-01-28 17:48:25,203 | INFO | decoder input length: 742
2026-01-28 17:48:25,204 | INFO | max output length: 742
2026-01-28 17:48:25,204 | INFO | min output length: 74
2026-01-28 17:49:27,361 | INFO | end detected at 573
2026-01-28 17:49:27,362 | INFO | -603.94 * 0.5 = -301.97 for decoder
2026-01-28 17:49:27,362 | INFO | -29.33 * 0.5 = -14.67 for ctc
2026-01-28 17:49:27,362 | INFO | total log probability: -316.64
2026-01-28 17:49:27,362 | INFO | normalized log probability: -0.56
2026-01-28 17:49:27,362 | INFO | total number of ended hypotheses: 161
2026-01-28 17:49:27,369 | INFO | best hypo: dans<space>la<space>zone<space>intertropicale<space>et<space>il<space>remonte<space>chez<space>nous<space>pendant<space>notre<space>rivere<space>à<space>nous<space>donc<space>euh<space>le<space>scénario<space>auquel<space>on<space>peut<space>s'attendre<space>encore<space>une<space>fois<space>avec<space>toutes<space>les<space>précautions<space>qu'il<space>faut<space>d'usage<space>qu'il<space>faut<space>mettre<space>c'est<space>que<space>cette<space>souche<space>que<space>l'on<space>voit<space>en<space>amérique<space>du<space>nord<space>ne<space>va<space>pas<space>se<space>plaire<space>beaucoup<space>avec<space>l'été<space>euh<space>chez<space>nous<space>dans<space>l'hémisphère<space>nord<space>mais<space>elle<space>pourrait<space>tout<space>à<space>fait<space>f<space>euh<space>entraîner<space>un<space>certain<space>nombre<space>d'épidémies<space>euh<space>de<space>plus<space>forte<space>ampleur<space>que<space>les<space>épidémies<space>saisonnières<space>haituelles<space>dans<space>l'hémisphère<space>sud<space>on<space>va<space>donc<space>avoir<space>nos<space>regads<space>braqués<space>sur<space>l'hémisphère<space>sud

2026-01-28 17:49:27,372 | INFO | speech length: 418400
2026-01-28 17:49:27,410 | INFO | decoder input length: 653
2026-01-28 17:49:27,410 | INFO | max output length: 653
2026-01-28 17:49:27,410 | INFO | min output length: 65
2026-01-28 17:50:18,943 | INFO | end detected at 544
2026-01-28 17:50:18,946 | INFO | -641.39 * 0.5 = -320.69 for decoder
2026-01-28 17:50:18,946 | INFO | -39.72 * 0.5 = -19.86 for ctc
2026-01-28 17:50:18,946 | INFO | total log probability: -340.55
2026-01-28 17:50:18,946 | INFO | normalized log probability: -0.64
2026-01-28 17:50:18,946 | INFO | total number of ended hypotheses: 210
2026-01-28 17:50:18,954 | INFO | best hypo: et<space>notamment<space>sur<space>les<space>pays<space>de<space>la<space>zone<space>tempérée<space>de<space>l'hémisphère<space>sud<space>dans<space>les<space>mois<space>à<space>venir<space>et<space>puis<space>après<space>euh<space>si<space>jamais<space>ces<space>épidémies<space>euh<space>surviennent<space>dans<space>l'hémisphère<space>sud<space>on<space>aura<space>une<space>chance<space>encore<space>plus<space>renforcée<space>ce<space>qui<space>peut<space>comprendre<space>le<space>le<space>discours<space>de<space>nos<space>autorités<space>de<space>santé<space>qui<space>vont<space>se<space>voir<space>euh<space>n'ont<space>pas<space>réconfortéer<space>hein<space>mais<space>euh<space>plutôt<space>euh<space>vigilant<space>avec<space>une<space>vigilance<space>encore<space>accrue<space>sur<space>euh<space>l'hiver<space>qui<space>qui<space>arrive<space>une<space>dernière<space>petite<space>question<space>avant<space>qu'on<space>de<space>con<space>notre<space>première<space>porage<space>c'est<space>euh<space>s<space>ça<space>se<space>passe<space>comme<space>ça<space>c'est<space>à<space>dire<space>de

2026-01-28 17:50:18,957 | INFO | speech length: 460000
2026-01-28 17:50:18,996 | INFO | decoder input length: 718
2026-01-28 17:50:18,997 | INFO | max output length: 718
2026-01-28 17:50:18,997 | INFO | min output length: 71
2026-01-28 17:51:21,650 | INFO | end detected at 621
2026-01-28 17:51:21,651 | INFO | -663.90 * 0.5 = -331.95 for decoder
2026-01-28 17:51:21,651 | INFO | -105.21 * 0.5 = -52.61 for ctc
2026-01-28 17:51:21,651 | INFO | total log probability: -384.56
2026-01-28 17:51:21,651 | INFO | normalized log probability: -0.63
2026-01-28 17:51:21,652 | INFO | total number of ended hypotheses: 177
2026-01-28 17:51:21,660 | INFO | best hypo: inquiétante<space>on<space>le<space>sait<space>quand<space>on<space>le<space>sait<space>le<space>premier<space>octobre<space>on<space>c'est<space>le<space>premier<space>décembre<space>euh<space>on<space>le<space>sait<space>le<space>premier<space>voyez<space>ce<space>que<space>je<space>dis<space>à<space>euh<space>à<space>quel<space>moment<space>euh<space>fin<space>la<space>journée<space>presse<space>à<space>serrir<space>cette<space>bon<space>euh<space>mais<space>je<space>veux<space>dire<space>voilà<space>à<space>quel<space>moment<space>dans<space>l'année<space>euh<space>à<space>l'automne<space>à<space>l'hiver<space>on<space>sait<space>qu'on<space>est<space>dedans<space>et<space>ça<space>va<space>s<space>s<space>ça<space>sera<space>pas<space>la<space>grippe<space>traditionnelle<space>mais<space>autre<space>chose<space>alors<space>l<space>la<space>la<space>promité<space>ne<space>fait<space>qu'augmener<space>avec<space>le<space>temps<space>déjà<space>la<space>promiet<space>qu'on<space>qunon<space>cone<space>de<space>tans<space>par<space>exemple<space>enfin<space>ne<space>fait<space>que<space>bon<space>deuie<space>de<space>pat<space>n<space>vacances<space>de<space>bonne<space>humeur<space>je<space>reiens<space>a<space>mileu<space>de<space>l'été<space>e<space>disant<space>quest<space>ce<space>qui<space>va<space>se<space>passe<space>l'autone

2026-01-28 17:51:21,662 | INFO | speech length: 475040
2026-01-28 17:51:21,704 | INFO | decoder input length: 741
2026-01-28 17:51:21,704 | INFO | max output length: 741
2026-01-28 17:51:21,704 | INFO | min output length: 74
2026-01-28 17:52:22,425 | INFO | end detected at 640
2026-01-28 17:52:22,426 | INFO | -760.36 * 0.5 = -380.18 for decoder
2026-01-28 17:52:22,426 | INFO | -92.00 * 0.5 = -46.00 for ctc
2026-01-28 17:52:22,426 | INFO | total log probability: -426.18
2026-01-28 17:52:22,426 | INFO | normalized log probability: -0.67
2026-01-28 17:52:22,426 | INFO | total number of ended hypotheses: 161
2026-01-28 17:52:22,434 | INFO | best hypo: et<space>quad<space>je<space>reçois<space>le<space>butin<space>trimestriel<space>le<space>premier<space>bulletin<space>trimestriel<space>de<space>ma<space>de<space>ma<space>file<space>je<space>j'appelle<space>les<space>pompes<space>funèbres<space>non<space>ce<space>que<space>je<space>eux<space>dire<space>c'est<space>que<space>c'estculatif<space>aujourd'hui<space>on<space>sait<space>que<space>la<space>petite<space>souche<space>qui<space>a<space>émergé<space>au<space>mexique<space>est<space>quand<space>même<space>un<space>peu<space>plus<space>contagieuse<space>que<space>euh<space>on<space>aurait<space>pu<space>l'espérer<space>elle<space>est<space>plus<space>contagieuse<space>elle<space>s'est<space>re<space>répondue<space>assez<space>rapidement<space>quand<space>même<space>aux<space>états<space>unis<space>au<space>canada<space>en<space>en<space>grande<space>bre<space>agne<space>au<space>au<space>royaume<space>uni<space>dans<space>tous<space>ces<space>endroits<space>là<space>il<space>y<space>des<space>chaînes<space>de<space>transmission<space>locale<space>c'est<space>à<space>dire<space>que<space>le<space>le<space>virus<space>fait<space>faire<space>sa<space>propre<space>besoin<space>de<space>lui<space>même<space>sur<space>place<space>ce<space>que<space>je<space>crois<space>c'est<space>qu'à<space>la<space>fin<space>de<space>l'été

2026-01-28 17:52:22,437 | INFO | speech length: 424960
2026-01-28 17:52:22,486 | INFO | decoder input length: 663
2026-01-28 17:52:22,486 | INFO | max output length: 663
2026-01-28 17:52:22,486 | INFO | min output length: 66
2026-01-28 17:53:12,439 | INFO | end detected at 575
2026-01-28 17:53:12,440 | INFO | -652.20 * 0.5 = -326.10 for decoder
2026-01-28 17:53:12,440 | INFO | -173.15 * 0.5 = -86.57 for ctc
2026-01-28 17:53:12,440 | INFO | total log probability: -412.67
2026-01-28 17:53:12,440 | INFO | normalized log probability: -0.73
2026-01-28 17:53:12,440 | INFO | total number of ended hypotheses: 160
2026-01-28 17:53:12,447 | INFO | best hypo: si<space>on<space>sait<space>qu'<space>y<space>a<space>eu<space>de<space>très<space>fortes<space>épidémies<space>dans<space>l'hémisphère<space>sud<space>la<space>promité<space>sera<space>très<space>renforcée<space>que<space>ça<space>nous<space>n<space>nous<space>revienne<space>pendant<space>notre<space>hiver<space>à<space>nous<space>et<space>peut<space>être<space>même<space>pendant<space>l'automne<space>c'est<space>à<space>dire<space>peut<space>être<space>assez<space>précocement<space>alrs<space>donc<space>la<space>vigilance<space>a<space>accru<space>à<space>ce<space>moment<space>là<space>je<space>vous<space>rappelle<space>que<space>depuis<space>ledébut<space>de<space>la<space>crise<space>l'organisation<space>mondiae<space>de<space>la<space>santé<space>euh<space>contribue<space>largement<space>euh<space>à<space>l'angoisse<space>oi<space>à<space>l'inquiétude<space>paur<space>ce<space>communiqués<space>euh<space>on<space>cera<space>montée<space>jusqu'au<space>niveau<space>cinq<space>euh<space>d'une<space>écelle<space>qui<space>en<space>compte<space>six<space>en<space>ce<space>qui<space>concerne<space>bien<space>entendu<space>les<space>niveaux<space>d'alerte

2026-01-28 17:53:12,449 | INFO | speech length: 187520
2026-01-28 17:53:12,486 | INFO | decoder input length: 292
2026-01-28 17:53:12,486 | INFO | max output length: 292
2026-01-28 17:53:12,486 | INFO | min output length: 29
2026-01-28 17:53:34,598 | INFO | end detected at 204
2026-01-28 17:53:34,601 | INFO | -16.47 * 0.5 =  -8.23 for decoder
2026-01-28 17:53:34,601 | INFO |  -1.36 * 0.5 =  -0.68 for ctc
2026-01-28 17:53:34,602 | INFO | total log probability: -8.92
2026-01-28 17:53:34,602 | INFO | normalized log probability: -0.04
2026-01-28 17:53:34,602 | INFO | total number of ended hypotheses: 176
2026-01-28 17:53:34,606 | INFO | best hypo: euh<space>l'oms<space>voit<space>d'ailleurs<space>grand<space>puisqu'elle<space>a<space>même<space>évoqué<space>la<space>possibilité<space>d'une<space>pandémie<space>touchant<space>le<space>tiers<space>de<space>l'humanité<space>euh<space>visite<space>à<space>l'épicentre<space>de<space>la<space>peur<space>avec<space>sophie<space>pagesse<space>et<space>dominique<space>lemarché

2026-01-28 17:53:34,640 | INFO | Chunk: 0 | WER=56.578947 | S=4 D=1 I=38
2026-01-28 17:53:34,641 | INFO | Chunk: 1 | WER=100.000000 | S=5 D=2 I=0
2026-01-28 17:53:34,655 | INFO | Chunk: 2 | WER=69.523810 | S=4 D=33 I=36
2026-01-28 17:53:34,669 | INFO | Chunk: 3 | WER=83.653846 | S=11 D=37 I=39
2026-01-28 17:53:34,670 | INFO | Chunk: 4 | WER=150.000000 | S=4 D=0 I=2
2026-01-28 17:53:34,680 | INFO | Chunk: 5 | WER=78.494624 | S=2 D=38 I=33
2026-01-28 17:53:34,694 | INFO | Chunk: 6 | WER=81.443299 | S=9 D=26 I=44
2026-01-28 17:53:34,713 | INFO | Chunk: 7 | WER=67.187500 | S=5 D=40 I=41
2026-01-28 17:53:34,728 | INFO | Chunk: 8 | WER=83.486239 | S=7 D=39 I=45
2026-01-28 17:53:34,733 | INFO | Chunk: 9 | WER=90.476190 | S=52 D=5 I=0
2026-01-28 17:53:34,746 | INFO | Chunk: 10 | WER=96.907216 | S=9 D=38 I=47
2026-01-28 17:53:34,755 | INFO | Chunk: 11 | WER=94.936709 | S=67 D=1 I=7
2026-01-28 17:53:34,761 | INFO | Chunk: 12 | WER=87.837838 | S=53 D=12 I=0
2026-01-28 17:53:34,762 | INFO | Chunk: 13 | WER=91.304348 | S=15 D=6 I=0
2026-01-28 17:53:34,765 | INFO | Chunk: 14 | WER=102.631579 | S=29 D=2 I=8
2026-01-28 17:53:34,785 | INFO | Chunk: 15 | WER=83.606557 | S=21 D=31 I=50
2026-01-28 17:53:34,803 | INFO | Chunk: 16 | WER=70.769231 | S=4 D=48 I=40
2026-01-28 17:53:34,817 | INFO | Chunk: 17 | WER=65.137615 | S=1 D=36 I=34
2026-01-28 17:53:34,829 | INFO | Chunk: 18 | WER=88.636364 | S=2 D=30 I=46
2026-01-28 17:53:34,848 | INFO | Chunk: 19 | WER=84.677419 | S=11 D=42 I=52
2026-01-28 17:53:34,869 | INFO | Chunk: 20 | WER=77.536232 | S=9 D=53 I=45
2026-01-28 17:53:34,885 | INFO | Chunk: 21 | WER=74.218750 | S=8 D=52 I=35
2026-01-28 17:53:34,888 | INFO | Chunk: 22 | WER=97.297297 | S=32 D=2 I=2
2026-01-28 17:53:39,732 | INFO | File: Rhap-D2008.wav | WER=24.277750 | S=247 D=81 I=151
2026-01-28 17:53:39,733 | INFO | ------------------------------
2026-01-28 17:53:39,733 | INFO | Conf ester Done!
2026-01-28 17:58:37,613 | INFO | Chunk: 0 | WER=51.315789 | S=3 D=1 I=35
2026-01-28 17:58:37,614 | INFO | Chunk: 1 | WER=100.000000 | S=5 D=2 I=0
2026-01-28 17:58:37,624 | INFO | Chunk: 2 | WER=76.190476 | S=12 D=32 I=36
2026-01-28 17:58:37,633 | INFO | Chunk: 3 | WER=77.884615 | S=9 D=37 I=35
2026-01-28 17:58:37,634 | INFO | Chunk: 4 | WER=175.000000 | S=4 D=0 I=3
2026-01-28 17:58:37,640 | INFO | Chunk: 5 | WER=77.419355 | S=6 D=40 I=26
2026-01-28 17:58:37,648 | INFO | Chunk: 6 | WER=78.350515 | S=7 D=31 I=38
2026-01-28 17:58:37,660 | INFO | Chunk: 7 | WER=68.750000 | S=8 D=43 I=37
2026-01-28 17:58:37,670 | INFO | Chunk: 8 | WER=83.486239 | S=6 D=41 I=44
2026-01-28 17:58:37,673 | INFO | Chunk: 9 | WER=92.063492 | S=47 D=10 I=1
2026-01-28 17:58:37,682 | INFO | Chunk: 10 | WER=90.721649 | S=80 D=2 I=6
2026-01-28 17:58:37,688 | INFO | Chunk: 11 | WER=91.139241 | S=67 D=1 I=4
2026-01-28 17:58:37,692 | INFO | Chunk: 12 | WER=87.837838 | S=47 D=18 I=0
2026-01-28 17:58:37,693 | INFO | Chunk: 13 | WER=91.304348 | S=12 D=9 I=0
2026-01-28 17:58:37,694 | INFO | Chunk: 14 | WER=110.526316 | S=33 D=1 I=8
2026-01-28 17:58:37,706 | INFO | Chunk: 15 | WER=76.229508 | S=9 D=39 I=45
2026-01-28 17:58:37,718 | INFO | Chunk: 16 | WER=66.923077 | S=4 D=50 I=33
2026-01-28 17:58:37,727 | INFO | Chunk: 17 | WER=65.137615 | S=4 D=37 I=30
2026-01-28 17:58:37,734 | INFO | Chunk: 18 | WER=80.681818 | S=2 D=31 I=38
2026-01-28 17:58:37,746 | INFO | Chunk: 19 | WER=79.838710 | S=8 D=46 I=45
2026-01-28 17:58:37,759 | INFO | Chunk: 20 | WER=81.159420 | S=10 D=57 I=45
2026-01-28 17:58:37,770 | INFO | Chunk: 21 | WER=67.968750 | S=3 D=52 I=32
2026-01-28 17:58:37,771 | INFO | Chunk: 22 | WER=94.594595 | S=32 D=2 I=1
2026-01-28 17:58:40,708 | INFO | File: Rhap-D2008.wav | WER=19.158642 | S=174 D=122 I=82
2026-01-28 17:58:40,709 | INFO | ------------------------------
2026-01-28 17:58:40,710 | INFO | hmm_tdnn Done!
2026-01-28 17:58:40,936 | INFO | ==================================Rhap-D2009.wav=========================================
2026-01-28 17:58:41,181 | INFO | Using rVAD model
2026-01-28 17:59:07,629 | INFO | Chunk: 0 | WER=6.593407 | S=1 D=2 I=3
2026-01-28 17:59:07,632 | INFO | Chunk: 1 | WER=22.580645 | S=6 D=3 I=5
2026-01-28 17:59:07,639 | INFO | Chunk: 2 | WER=17.045455 | S=5 D=6 I=4
2026-01-28 17:59:07,645 | INFO | Chunk: 3 | WER=22.352941 | S=6 D=9 I=4
2026-01-28 17:59:07,648 | INFO | Chunk: 4 | WER=12.727273 | S=1 D=3 I=3
2026-01-28 17:59:07,653 | INFO | Chunk: 5 | WER=20.000000 | S=8 D=9 I=0
2026-01-28 17:59:07,660 | INFO | Chunk: 6 | WER=22.826087 | S=10 D=7 I=4
2026-01-28 17:59:07,669 | INFO | Chunk: 7 | WER=17.757009 | S=6 D=10 I=3
2026-01-28 17:59:07,676 | INFO | Chunk: 8 | WER=22.471910 | S=6 D=8 I=6
2026-01-28 17:59:07,682 | INFO | Chunk: 9 | WER=26.666667 | S=10 D=12 I=2
2026-01-28 17:59:07,689 | INFO | Chunk: 10 | WER=19.318182 | S=6 D=9 I=2
2026-01-28 17:59:07,692 | INFO | Chunk: 11 | WER=21.666667 | S=5 D=5 I=3
2026-01-28 17:59:08,402 | INFO | File: Rhap-D2009.wav | WER=14.314516 | S=78 D=54 I=10
2026-01-28 17:59:08,402 | INFO | ------------------------------
2026-01-28 17:59:08,402 | INFO | w2vec vad chunk Done!
2026-01-28 17:59:33,883 | INFO | Chunk: 0 | WER=76.923077 | S=0 D=70 I=0
2026-01-28 17:59:33,886 | INFO | Chunk: 1 | WER=33.870968 | S=0 D=21 I=0
2026-01-28 17:59:33,889 | INFO | Chunk: 2 | WER=54.545455 | S=16 D=32 I=0
2026-01-28 17:59:33,892 | INFO | Chunk: 3 | WER=61.176471 | S=19 D=33 I=0
2026-01-28 17:59:33,894 | INFO | Chunk: 4 | WER=27.272727 | S=1 D=14 I=0
2026-01-28 17:59:33,895 | INFO | Chunk: 5 | WER=80.000000 | S=0 D=68 I=0
2026-01-28 17:59:33,897 | INFO | Chunk: 6 | WER=84.782609 | S=0 D=78 I=0
2026-01-28 17:59:33,898 | INFO | Chunk: 7 | WER=94.392523 | S=2 D=99 I=0
2026-01-28 17:59:33,900 | INFO | Chunk: 8 | WER=67.415730 | S=5 D=55 I=0
2026-01-28 17:59:33,902 | INFO | Chunk: 9 | WER=80.000000 | S=0 D=72 I=0
2026-01-28 17:59:33,904 | INFO | Chunk: 10 | WER=71.590909 | S=3 D=60 I=0
2026-01-28 17:59:33,906 | INFO | Chunk: 11 | WER=26.666667 | S=4 D=12 I=0
2026-01-28 17:59:34,113 | INFO | File: Rhap-D2009.wav | WER=66.532258 | S=46 D=614 I=0
2026-01-28 17:59:34,113 | INFO | ------------------------------
2026-01-28 17:59:34,113 | INFO | whisper med Done!
2026-01-28 18:00:23,641 | INFO | Chunk: 0 | WER=52.747253 | S=18 D=30 I=0
2026-01-28 18:00:23,644 | INFO | Chunk: 1 | WER=32.258065 | S=0 D=20 I=0
2026-01-28 18:00:23,646 | INFO | Chunk: 2 | WER=67.045455 | S=3 D=56 I=0
2026-01-28 18:00:23,648 | INFO | Chunk: 3 | WER=55.294118 | S=6 D=41 I=0
2026-01-28 18:00:23,650 | INFO | Chunk: 4 | WER=21.818182 | S=1 D=11 I=0
2026-01-28 18:00:23,654 | INFO | Chunk: 5 | WER=40.000000 | S=10 D=24 I=0
2026-01-28 18:00:23,657 | INFO | Chunk: 6 | WER=42.391304 | S=5 D=33 I=1
2026-01-28 18:00:23,661 | INFO | Chunk: 7 | WER=53.271028 | S=0 D=57 I=0
2026-01-28 18:00:23,664 | INFO | Chunk: 8 | WER=59.550562 | S=7 D=45 I=1
2026-01-28 18:00:23,666 | INFO | Chunk: 9 | WER=80.000000 | S=0 D=72 I=0
2026-01-28 18:00:23,668 | INFO | Chunk: 10 | WER=70.454545 | S=2 D=60 I=0
2026-01-28 18:00:23,670 | INFO | Chunk: 11 | WER=38.333333 | S=6 D=8 I=9
2026-01-28 18:00:23,968 | INFO | File: Rhap-D2009.wav | WER=52.923387 | S=57 D=457 I=11
2026-01-28 18:00:23,969 | INFO | ------------------------------
2026-01-28 18:00:23,969 | INFO | whisper large Done!
2026-01-28 18:00:24,146 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 18:00:24,185 | INFO | Vocabulary size: 350
2026-01-28 18:00:25,150 | INFO | Gradient checkpoint layers: []
2026-01-28 18:00:25,810 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 18:00:25,815 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 18:00:25,815 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 18:00:25,815 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 18:00:25,816 | INFO | speech length: 464480
2026-01-28 18:00:25,871 | INFO | decoder input length: 725
2026-01-28 18:00:25,871 | INFO | max output length: 725
2026-01-28 18:00:25,871 | INFO | min output length: 72
2026-01-28 18:00:54,524 | INFO | end detected at 226
2026-01-28 18:00:54,525 | INFO | -446.22 * 0.5 = -223.11 for decoder
2026-01-28 18:00:54,525 | INFO | -100.25 * 0.5 = -50.12 for ctc
2026-01-28 18:00:54,525 | INFO | total log probability: -273.24
2026-01-28 18:00:54,525 | INFO | normalized log probability: -1.24
2026-01-28 18:00:54,525 | INFO | total number of ended hypotheses: 142
2026-01-28 18:00:54,528 | INFO | best hypo: ▁roj'accorde▁une▁puissance▁énorme▁à▁l'acte▁d'écrire▁mais▁comme▁toujours▁l'acte▁d'écrire▁peut▁prendre▁différents▁masques▁différentes▁valeurs▁il▁y▁a▁des▁moments▁où▁on▁écrits▁parce▁qu'on▁pense▁participer▁à▁un▁combats▁ça▁été▁le▁cas▁dision▁dans▁les▁débuts▁de▁sa▁carentrière▁d'écrivain▁ou▁d'écrivains▁et▁puis▁peu▁à▁peu▁dégage▁finalement▁la▁vérité▁une▁vérité▁plus▁nue▁si▁je▁puise▁dire▁'est▁que▁on▁écrit▁parce▁que▁au▁fond▁on▁aime▁cela▁et▁parce▁que▁ça▁fait▁plaisir

2026-01-28 18:00:54,532 | INFO | speech length: 317120
2026-01-28 18:00:54,583 | INFO | decoder input length: 495
2026-01-28 18:00:54,583 | INFO | max output length: 495
2026-01-28 18:00:54,583 | INFO | min output length: 49
2026-01-28 18:01:08,938 | INFO | end detected at 147
2026-01-28 18:01:08,940 | INFO | -171.87 * 0.5 = -85.94 for decoder
2026-01-28 18:01:08,940 | INFO | -29.26 * 0.5 = -14.63 for ctc
2026-01-28 18:01:08,940 | INFO | total log probability: -100.57
2026-01-28 18:01:08,941 | INFO | normalized log probability: -0.71
2026-01-28 18:01:08,941 | INFO | total number of ended hypotheses: 176
2026-01-28 18:01:08,943 | INFO | best hypo: ▁c'est▁donc▁finalement▁pour▁un▁motif▁de▁jouissance▁qu'on▁écrit▁ce▁qui▁ne▁veut▁pas▁dire▁que▁dans▁cet▁acte▁de▁jouissance▁on▁ne▁rencontre▁pas▁d'autres▁motivations▁et▁disons▁qu'on▁ne▁rencontre▁pas▁d'autres▁débats▁et▁qu'on▁ne▁rencontre▁pas▁tout▁simplement▁des▁autres▁mais▁l'acte▁d'écrire▁est▁il▁le▁prolondement▁de▁l'acte▁de▁pensée

2026-01-28 18:01:08,946 | INFO | speech length: 411680
2026-01-28 18:01:08,997 | INFO | decoder input length: 642
2026-01-28 18:01:08,997 | INFO | max output length: 642
2026-01-28 18:01:08,997 | INFO | min output length: 64
2026-01-28 18:01:29,260 | INFO | end detected at 179
2026-01-28 18:01:29,261 | INFO | -383.76 * 0.5 = -191.88 for decoder
2026-01-28 18:01:29,262 | INFO | -76.46 * 0.5 = -38.23 for ctc
2026-01-28 18:01:29,262 | INFO | total log probability: -230.11
2026-01-28 18:01:29,262 | INFO | normalized log probability: -1.32
2026-01-28 18:01:29,262 | INFO | total number of ended hypotheses: 160
2026-01-28 18:01:29,264 | INFO | best hypo: ▁or▁énonce▁non▁seulement▁le▁prolongement▁mais▁peut▁être▁même▁il▁n'en▁est▁pas▁séparable▁peut▁être▁que▁la▁question▁vous▁posez▁très▁très▁importante▁passerait▁assez▁compliquée▁d'y▁répondre▁mais▁mais▁d'abord▁nous▁savons▁bien▁qu'en▁fait▁on▁pense▁toujoujour▁avec▁du▁langage▁que▁on▁pense▁et▁en▁parlant▁qu'on▁parle▁pensant▁qu'il▁n'y▁a▁pas▁de▁pensait▁préalable▁au▁langage▁qui▁est▁en▁nous▁finalement▁et▁par▁conséquent▁et▁en▁plus▁dans▁certains▁cas▁comme▁le▁mien

2026-01-28 18:01:29,266 | INFO | speech length: 430560
2026-01-28 18:01:29,333 | INFO | decoder input length: 672
2026-01-28 18:01:29,333 | INFO | max output length: 672
2026-01-28 18:01:29,333 | INFO | min output length: 67
2026-01-28 18:01:50,298 | INFO | end detected at 178
2026-01-28 18:01:50,300 | INFO | -497.04 * 0.5 = -248.52 for decoder
2026-01-28 18:01:50,300 | INFO | -114.22 * 0.5 = -57.11 for ctc
2026-01-28 18:01:50,301 | INFO | total log probability: -305.63
2026-01-28 18:01:50,301 | INFO | normalized log probability: -1.78
2026-01-28 18:01:50,301 | INFO | total number of ended hypotheses: 149
2026-01-28 18:01:50,304 | INFO | best hypo: ▁le▁penser▁est▁immédiatement▁lié▁à▁une▁forme▁que▁jeu▁si▁je▁puis▁dire▁je▁je▁visualise▁déjà▁comme▁une▁forme▁écrite▁voyez▁vous▁c'est▁à▁ophone▁j'ai▁tends▁et▁c'est▁peut▁être▁un▁peu▁ambigu▁mais▁j'ai▁tendance▁à▁penser▁une▁phrase▁ont▁mais▁non▁pas▁à▁penser▁pas▁pensée▁et▁c'est▁tant▁cela▁que▁peut▁être▁ma▁situation'▁est▁peu▁difficile▁à▁repérer▁je▁ne▁suis▁pas▁un▁phosophe▁je▁ne▁suis▁pas▁un▁penseur

2026-01-28 18:01:50,307 | INFO | speech length: 234240
2026-01-28 18:01:50,431 | INFO | decoder input length: 365
2026-01-28 18:01:50,432 | INFO | max output length: 365
2026-01-28 18:01:50,432 | INFO | min output length: 36
2026-01-28 18:01:59,992 | INFO | end detected at 118
2026-01-28 18:01:59,994 | INFO | -138.21 * 0.5 = -69.11 for decoder
2026-01-28 18:01:59,994 | INFO | -43.78 * 0.5 = -21.89 for ctc
2026-01-28 18:01:59,994 | INFO | total log probability: -91.00
2026-01-28 18:01:59,994 | INFO | normalized log probability: -0.81
2026-01-28 18:01:59,994 | INFO | total number of ended hypotheses: 178
2026-01-28 18:01:59,996 | INFO | best hypo: ▁je▁suis▁pas▁non▁plus▁peut'être▁tout▁à▁fait▁un▁écrivain▁comme▁on▁l'entendait▁il▁y▁a▁cinquante▁ans▁je▁vais▁de▁la▁penser▁à▁la▁phrase▁et▁réciproquement▁et▁c'est▁c'est▁cela▁qui▁est▁un▁peu▁difficile▁à▁situer▁peut▁être▁et▁pourquoi▁à▁ce▁joint▁avoir▁voulu▁réhabiliter▁lesques

2026-01-28 18:01:59,998 | INFO | speech length: 444800
2026-01-28 18:02:00,048 | INFO | decoder input length: 694
2026-01-28 18:02:00,048 | INFO | max output length: 694
2026-01-28 18:02:00,048 | INFO | min output length: 69
2026-01-28 18:02:24,605 | INFO | end detected at 197
2026-01-28 18:02:24,607 | INFO | -471.13 * 0.5 = -235.57 for decoder
2026-01-28 18:02:24,607 | INFO | -146.00 * 0.5 = -73.00 for ctc
2026-01-28 18:02:24,607 | INFO | total log probability: -308.57
2026-01-28 18:02:24,607 | INFO | normalized log probability: -1.62
2026-01-28 18:02:24,607 | INFO | total number of ended hypotheses: 173
2026-01-28 18:02:24,610 | INFO | best hypo: ▁ah▁mes▁ça▁sait▁parce▁que▁d'aboriya▁il▁y▁avait▁une▁tâche▁historique▁à▁accomplir▁et▁vous▁savez▁que▁notre▁histoire▁de▁la▁littérature▁la▁façon▁d'en▁parler▁de▁la▁littérature▁depuis▁huit▁au▁moins▁cent▁cinq▁ans▁ou▁cent▁cinquante▁ans▁é▁une▁façon▁privilégiait▁beaucoup▁l'▁écrivain▁à▁toutes▁les▁'histoire▁la▁littérature▁sont▁l'histoire▁des▁écrivainsent▁des▁écoles▁et▁'on▁ne▁pensait▁jamaist▁écture▁la▁le'cture▁n'était▁pas

2026-01-28 18:02:24,613 | INFO | speech length: 443520
2026-01-28 18:02:24,678 | INFO | decoder input length: 692
2026-01-28 18:02:24,679 | INFO | max output length: 692
2026-01-28 18:02:24,679 | INFO | min output length: 69
2026-01-28 18:02:52,978 | INFO | end detected at 240
2026-01-28 18:02:52,980 | INFO | -536.82 * 0.5 = -268.41 for decoder
2026-01-28 18:02:52,980 | INFO | -248.85 * 0.5 = -124.43 for ctc
2026-01-28 18:02:52,980 | INFO | total log probability: -392.84
2026-01-28 18:02:52,980 | INFO | normalized log probability: -1.69
2026-01-28 18:02:52,980 | INFO | total number of ended hypotheses: 193
2026-01-28 18:02:52,983 | INFO | best hypo: ▁un▁niveau▁auquel▁on▁s'intéressait▁quand▁on▁ait▁une▁théorie▁de▁la▁littérature▁alors▁il▁est▁nécessaire▁maintenant▁et▁d'associer▁le▁le▁lecteur▁à▁la▁jouissance▁d'écrire▁il▁n'y▁a▁pas▁de▁jouissance▁d'écrire▁sans▁ja▁jouissance▁de▁lire▁les▁deux▁se▁compénètres▁en▁quelque▁sorte▁et▁par▁cons▁séquent▁il▁y▁a▁pas▁une▁sorte▁de▁nouvelles▁thésories▁et▁de▁la▁littérature▁qui▁s'élébauche▁elle▁doit▁tenir▁compte▁d'écteur▁ce▁motement▁liz▁vous▁roll'mbarte▁qui▁est▁rapppez▁ce▁mots▁est▁bachlarle▁et▁les▁écrivains▁sont▁seulement▁lus▁à▁alors▁au▁pourrait

2026-01-28 18:02:52,987 | INFO | speech length: 475360
2026-01-28 18:02:53,036 | INFO | decoder input length: 742
2026-01-28 18:02:53,036 | INFO | max output length: 742
2026-01-28 18:02:53,036 | INFO | min output length: 74
2026-01-28 18:03:25,618 | INFO | end detected at 240
2026-01-28 18:03:25,620 | INFO | -503.38 * 0.5 = -251.69 for decoder
2026-01-28 18:03:25,620 | INFO | -136.49 * 0.5 = -68.24 for ctc
2026-01-28 18:03:25,620 | INFO | total log probability: -319.93
2026-01-28 18:03:25,620 | INFO | normalized log probability: -1.37
2026-01-28 18:03:25,620 | INFO | total number of ended hypotheses: 170
2026-01-28 18:03:25,624 | INFO | best hypo: ▁posé▁la▁question▁les▁écrivains▁ont▁ils▁existé▁deux▁écrivains▁qu'ils▁existaient▁oui▁parce▁que▁tout▁de▁même▁bont▁finalement▁les▁lit▁encore▁maintenant▁les▁ly▁à▁travers▁des▁éclairages▁historiques▁très▁différents▁ils▁n'ontaient▁jamais▁toujours▁le▁même▁sens▁et▁c'est▁comme▁ça▁que▁l'histoire▁de▁march▁en▁quelque▁sorte▁je▁vous▁vous▁avez▁dit▁un▁jour▁que▁la▁littérature▁qui▁allait▁à▁sa▁perte▁et▁c'était▁un▁mot▁ou▁bien▁vous▁pensez▁vraoiment▁ou▁peut▁être▁il▁y▁a▁t▁il▁une▁nouvelle▁une▁anti▁littérature▁avec▁une▁contre▁littérature▁pas▁c'est▁dire▁la▁littérature▁avec▁un▁grand▁elle

2026-01-28 18:03:25,628 | INFO | speech length: 377760
2026-01-28 18:03:25,680 | INFO | decoder input length: 589
2026-01-28 18:03:25,680 | INFO | max output length: 589
2026-01-28 18:03:25,680 | INFO | min output length: 58
2026-01-28 18:03:50,026 | INFO | end detected at 207
2026-01-28 18:03:50,029 | INFO | -302.46 * 0.5 = -151.23 for decoder
2026-01-28 18:03:50,029 | INFO | -86.46 * 0.5 = -43.23 for ctc
2026-01-28 18:03:50,030 | INFO | total log probability: -194.46
2026-01-28 18:03:50,030 | INFO | normalized log probability: -0.98
2026-01-28 18:03:50,030 | INFO | total number of ended hypotheses: 185
2026-01-28 18:03:50,033 | INFO | best hypo: ▁l'espace▁a▁été▁pendant▁très▁longtemps▁une▁véritable▁institution▁ça▁faisait▁partie▁des▁institutions▁de▁la▁société▁alors▁je▁crois▁que▁cet▁aspect▁institutionnel▁de▁la▁littérature▁est▁tout▁à▁même▁en▁train▁sin▁de▁disparaître▁tout▁au▁à▁moins▁de▁se▁modifier▁profondément▁la▁preuve▁c'est▁que▁un▁actuellement▁voyez▁vous▁en▁soixante▁quinze▁lorsquerait▁d'accord▁avec▁moi▁il▁n'y▁a▁plus▁de▁dis▁qu'on▁appait▁autrefois▁de▁grands▁écrivains▁bien▁plus▁de▁grands▁écrivains

2026-01-28 18:03:50,036 | INFO | speech length: 411680
2026-01-28 18:03:50,078 | INFO | decoder input length: 642
2026-01-28 18:03:50,078 | INFO | max output length: 642
2026-01-28 18:03:50,078 | INFO | min output length: 64
2026-01-28 18:04:17,456 | INFO | end detected at 222
2026-01-28 18:04:17,458 | INFO | -550.31 * 0.5 = -275.16 for decoder
2026-01-28 18:04:17,458 | INFO | -175.10 * 0.5 = -87.55 for ctc
2026-01-28 18:04:17,458 | INFO | total log probability: -362.70
2026-01-28 18:04:17,458 | INFO | normalized log probability: -1.68
2026-01-28 18:04:17,458 | INFO | total number of ended hypotheses: 146
2026-01-28 18:04:17,461 | INFO | best hypo: ▁entre▁les▁deux▁guerres▁moi▁quand▁j'étais▁adolescent▁il▁y▁avait▁des▁grands▁leaders▁de▁la▁littérale▁normaux▁il▁y▁avait▁prostrit▁y▁avait▁poussent▁encore▁que▁il▁était▁mort▁disant▁un▁moment▁où▁je▁suis▁de▁venu▁adholescent▁mais▁il▁y▁avait▁de▁gens▁comme▁gilles▁claudels▁valérie▁et▁malrots▁c'étaient▁de▁très▁grand▁noms▁qui▁polarisait▁toute▁une▁activité▁et▁toute▁une▁séduction▁culturelle▁ors▁lintant▁il▁faut▁et▁bien▁reconnaître▁de▁ça▁n'éxiste▁plus▁il▁y▁a▁des▁intellectuels▁il▁y▁a▁des▁profeseurs

2026-01-28 18:04:17,463 | INFO | speech length: 467040
2026-01-28 18:04:17,509 | INFO | decoder input length: 729
2026-01-28 18:04:17,509 | INFO | max output length: 729
2026-01-28 18:04:17,510 | INFO | min output length: 72
2026-01-28 18:04:43,120 | INFO | end detected at 206
2026-01-28 18:04:43,121 | INFO | -534.59 * 0.5 = -267.29 for decoder
2026-01-28 18:04:43,121 | INFO | -164.54 * 0.5 = -82.27 for ctc
2026-01-28 18:04:43,121 | INFO | total log probability: -349.56
2026-01-28 18:04:43,121 | INFO | normalized log probability: -1.74
2026-01-28 18:04:43,121 | INFO | total number of ended hypotheses: 149
2026-01-28 18:04:43,124 | INFO | best hypo: ▁il▁y▁a▁des▁gens▁qui▁écrivent▁mais▁selon▁un▁statut▁souvent▁un▁peu▁honteux▁et▁cela▁désigne▁l'espèce▁de▁force▁qui▁mine▁l'institution▁littéraire▁certainement▁le▁françois▁doriac▁était▁il▁le▁de▁larnier▁alors▁ou▁leurgruille▁en▁un▁sens▁oui▁c'était▁l'un▁des▁derniers▁de▁ces▁l'ader▁dont▁de▁je▁parlai▁il▁y▁a▁homme▁qui▁fait▁la▁charnière▁siz▁vous▁voulez▁et▁qu'ili▁se▁situait▁jusement▁au▁point▁de▁désagrégation▁de▁la▁litaire▁de▁désagression▁historique▁de▁la▁littérature▁sartre

2026-01-28 18:04:43,126 | INFO | speech length: 375840
2026-01-28 18:04:43,178 | INFO | decoder input length: 586
2026-01-28 18:04:43,178 | INFO | max output length: 586
2026-01-28 18:04:43,178 | INFO | min output length: 58
2026-01-28 18:04:58,990 | INFO | end detected at 147
2026-01-28 18:04:58,991 | INFO | -290.85 * 0.5 = -145.43 for decoder
2026-01-28 18:04:58,991 | INFO | -77.07 * 0.5 = -38.53 for ctc
2026-01-28 18:04:58,991 | INFO | total log probability: -183.96
2026-01-28 18:04:58,991 | INFO | normalized log probability: -1.29
2026-01-28 18:04:58,991 | INFO | total number of ended hypotheses: 143
2026-01-28 18:04:58,993 | INFO | best hypo: ▁parce▁que▁au▁fond▁il▁a▁là▁il▁a▁tenu▁et▁il▁vient▁encrin▁encore▁cette▁sorte▁de▁leadership▁de▁la▁culture▁et▁de▁la▁littérature▁même▁mais▁comme▁précisér▁son▁oeuvre▁elle▁même▁se▁défini▁comme▁une▁destruction▁du▁memblant▁littéraire▁de▁la▁pouse▁littéraire▁par▁la▁même▁et▁il▁y▁contribue▁il▁a▁contribuer▁puissamment▁à▁la▁destruction▁du▁mythe▁littéraire

2026-01-28 18:04:59,006 | INFO | Chunk: 0 | WER=17.582418 | S=8 D=5 I=3
2026-01-28 18:04:59,009 | INFO | Chunk: 1 | WER=14.516129 | S=2 D=2 I=5
2026-01-28 18:04:59,013 | INFO | Chunk: 2 | WER=22.727273 | S=6 D=8 I=6
2026-01-28 18:04:59,018 | INFO | Chunk: 3 | WER=24.705882 | S=12 D=6 I=3
2026-01-28 18:04:59,020 | INFO | Chunk: 4 | WER=14.545455 | S=2 D=3 I=3
2026-01-28 18:04:59,024 | INFO | Chunk: 5 | WER=36.470588 | S=16 D=12 I=3
2026-01-28 18:04:59,029 | INFO | Chunk: 6 | WER=35.869565 | S=19 D=3 I=11
2026-01-28 18:04:59,036 | INFO | Chunk: 7 | WER=28.971963 | S=9 D=11 I=11
2026-01-28 18:04:59,041 | INFO | Chunk: 8 | WER=29.213483 | S=10 D=12 I=4
2026-01-28 18:04:59,046 | INFO | Chunk: 9 | WER=34.444444 | S=19 D=6 I=6
2026-01-28 18:04:59,050 | INFO | Chunk: 10 | WER=31.818182 | S=14 D=7 I=7
2026-01-28 18:04:59,053 | INFO | Chunk: 11 | WER=31.666667 | S=8 D=4 I=7
2026-01-28 18:04:59,587 | INFO | File: Rhap-D2009.wav | WER=22.983871 | S=128 D=55 I=45
2026-01-28 18:04:59,587 | INFO | ------------------------------
2026-01-28 18:04:59,587 | INFO | Conf cv Done!
2026-01-28 18:04:59,765 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 18:04:59,788 | INFO | Vocabulary size: 47
2026-01-28 18:05:00,721 | INFO | Gradient checkpoint layers: []
2026-01-28 18:05:01,386 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 18:05:01,390 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 18:05:01,391 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 18:05:01,391 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 18:05:01,394 | INFO | speech length: 464480
2026-01-28 18:05:01,441 | INFO | decoder input length: 725
2026-01-28 18:05:01,441 | INFO | max output length: 725
2026-01-28 18:05:01,441 | INFO | min output length: 72
2026-01-28 18:05:51,248 | INFO | end detected at 470
2026-01-28 18:05:51,250 | INFO | -281.57 * 0.5 = -140.79 for decoder
2026-01-28 18:05:51,251 | INFO | -16.62 * 0.5 =  -8.31 for ctc
2026-01-28 18:05:51,251 | INFO | total log probability: -149.10
2026-01-28 18:05:51,251 | INFO | normalized log probability: -0.32
2026-01-28 18:05:51,251 | INFO | total number of ended hypotheses: 166
2026-01-28 18:05:51,256 | INFO | best hypo: oh<space>j'accorde<space>une<space>puissance<space>énorme<space>à<space>l'acte<space>d'écrire<space>mais<space>comme<space>toujours<space>l'acte<space>d'écrire<space>peut<space>prendre<space>différents<space>masques<space>différentes<space>valeurs<space>y<space>a<space>des<space>moments<space>où<space>on<space>écrit<space>parce<space>qu'on<space>pense<space>participer<space>à<space>un<space>combat<space>ça<space>été<space>le<space>cas<space>disons<space>dans<space>les<space>débuts<space>de<space>ma<space>carrière<space>d'écrivain<space>ou<space>d'écrivain<space>et<space>puis<space>peu<space>à<space>peu<space>se<space>dégage<space>finalement<space>euh<space>la<space>vérité<space>une<space>vérité<space>plus<space>plus<space>nue<space>si<space>je<space>puis<space>dire<space>c'est<space>que<space>on<space>écrit<space>parce<space>que<space>a<space>fond<space>euh<space>on<space>aime<space>cela<space>et<space>parce<space>que<space>ça<space>fait<space>plaisir

2026-01-28 18:05:51,259 | INFO | speech length: 317120
2026-01-28 18:05:51,303 | INFO | decoder input length: 495
2026-01-28 18:05:51,303 | INFO | max output length: 495
2026-01-28 18:05:51,303 | INFO | min output length: 49
2026-01-28 18:06:18,740 | INFO | end detected at 341
2026-01-28 18:06:18,743 | INFO | -50.29 * 0.5 = -25.14 for decoder
2026-01-28 18:06:18,743 | INFO |  -5.81 * 0.5 =  -2.90 for ctc
2026-01-28 18:06:18,743 | INFO | total log probability: -28.05
2026-01-28 18:06:18,743 | INFO | normalized log probability: -0.08
2026-01-28 18:06:18,743 | INFO | total number of ended hypotheses: 234
2026-01-28 18:06:18,747 | INFO | best hypo: c'est<space>donc<space>finalement<space>pour<space>un<space>motif<space>de<space>jouissance<space>qu'on<space>écrit<space>euh<space>ce<space>qui<space>ne<space>veut<space>pas<space>dire<space>que<space>dans<space>cet<space>acte<space>de<space>jouissance<space>euh<space>on<space>ne<space>rencontre<space>pas<space>d'autres<space>motivations<space>et<space>disons<space>qu'on<space>ne<space>rencontre<space>pas<space>d'autres<space>débats<space>et<space>qu'on<space>ne<space>rencontre<space>pas<space>tout<space>simplement<space>les<space>autres<space>mais<space>l'acte<space>d'écrire<space>est<space>il<space>le<space>prolongement<space>de<space>l'acte<space>de<space>pensée

2026-01-28 18:06:18,751 | INFO | speech length: 411680
2026-01-28 18:06:18,797 | INFO | decoder input length: 642
2026-01-28 18:06:18,797 | INFO | max output length: 642
2026-01-28 18:06:18,797 | INFO | min output length: 64
2026-01-28 18:07:01,762 | INFO | end detected at 469
2026-01-28 18:07:01,764 | INFO | -485.85 * 0.5 = -242.92 for decoder
2026-01-28 18:07:01,764 | INFO |  -2.84 * 0.5 =  -1.42 for ctc
2026-01-28 18:07:01,765 | INFO | total log probability: -244.34
2026-01-28 18:07:01,765 | INFO | normalized log probability: -0.53
2026-01-28 18:07:01,765 | INFO | total number of ended hypotheses: 176
2026-01-28 18:07:01,770 | INFO | best hypo: ah<space>non<space>s<space>non<space>seulement<space>le<space>prolongement<space>mais<space>peut<space>être<space>même<space>il<space>n'en<space>est<space>pas<space>séparable<space>peut<space>être<space>que<space>euh<space>s<space>la<space>question<space>que<space>vous<space>posez<space>est<space>très<space>très<space>importante<space>ça<space>serait<space>assez<space>compliqué<space>d'y<space>répondre<space>mais<space>d'abord<space>nous<space>savons<space>bien<space>qu'en<space>fait<space>on<space>pense<space>toujours<space>avec<space>du<space>langage<space>que<space>on<space>pense<space>en<space>parlant<space>qu'on<space>parle<space>en<space>pensant<space>qu'il<space>n'y<space>a<space>pas<space>de<space>pensée<space>préalable<space>au<space>langage<space>qui<space>est<space>en<space>nous<space>finalement<space>euh<space>et<space>par<space>conséquent<space>euh<space>et<space>en<space>plus<space>dans<space>certains<space>cas<space>comme<space>le<space>mien

2026-01-28 18:07:01,773 | INFO | speech length: 430560
2026-01-28 18:07:01,816 | INFO | decoder input length: 672
2026-01-28 18:07:01,816 | INFO | max output length: 672
2026-01-28 18:07:01,816 | INFO | min output length: 67
2026-01-28 18:07:43,294 | INFO | end detected at 415
2026-01-28 18:07:43,296 | INFO | -453.08 * 0.5 = -226.54 for decoder
2026-01-28 18:07:43,296 | INFO |  -6.04 * 0.5 =  -3.02 for ctc
2026-01-28 18:07:43,296 | INFO | total log probability: -229.56
2026-01-28 18:07:43,296 | INFO | normalized log probability: -0.56
2026-01-28 18:07:43,296 | INFO | total number of ended hypotheses: 194
2026-01-28 18:07:43,301 | INFO | best hypo: euh<space>penser<space>est<space>immédiatement<space>lié<space>à<space>une<space>forme<space>que<space>je<space>si<space>je<space>puis<space>dire<space>je<space>je<space>visualise<space>déjà<space>comme<space>une<space>forme<space>écrite<space>voyez<space>vous<space>c'est<space>euh<space>au<space>fond<space>euh<space>j'ai<space>tendance<space>et<space>c'est<space>peut<space>être<space>un<space>peu<space>ambigu<space>mais<space>j'ai<space>tendance<space>à<space>à<space>penser<space>par<space>phrase<space>disons<space>et<space>non<space>pas<space>à<space>penser<space>par<space>penser<space>et<space>c'est<space>en<space>cela<space>que<space>peut<space>être<space>ma<space>situation<space>est<space>un<space>peu<space>difficile<space>à<space>repérer<space>je<space>ne<space>suis<space>pas<space>un<space>philosophe<space>euh<space>je<space>ne<space>suis<space>pas<space>un<space>penseur<space>euh

2026-01-28 18:07:43,304 | INFO | speech length: 234240
2026-01-28 18:07:43,344 | INFO | decoder input length: 365
2026-01-28 18:07:43,344 | INFO | max output length: 365
2026-01-28 18:07:43,344 | INFO | min output length: 36
2026-01-28 18:08:01,218 | INFO | end detected at 274
2026-01-28 18:08:01,220 | INFO | -23.26 * 0.5 = -11.63 for decoder
2026-01-28 18:08:01,220 | INFO |  -1.93 * 0.5 =  -0.97 for ctc
2026-01-28 18:08:01,220 | INFO | total log probability: -12.59
2026-01-28 18:08:01,220 | INFO | normalized log probability: -0.05
2026-01-28 18:08:01,220 | INFO | total number of ended hypotheses: 202
2026-01-28 18:08:01,224 | INFO | best hypo: je<space>suis<space>pas<space>non<space>plus<space>peut<space>être<space>tout<space>à<space>fait<space>un<space>écrivain<space>comme<space>on<space>l'entendait<space>il<space>y<space>a<space>cinquante<space>ans<space>euh<space>je<space>vais<space>de<space>la<space>penser<space>à<space>la<space>phrase<space>et<space>réciproquement<space>et<space>c'est<space>c'est<space>cela<space>qui<space>est<space>un<space>peu<space>difficile<space>à<space>situer<space>peut<space>être<space>pourquoi<space>à<space>ce<space>point<space>avoir<space>voulu<space>réhabiliter<space>le<space>lec

2026-01-28 18:08:01,226 | INFO | speech length: 444800
2026-01-28 18:08:01,267 | INFO | decoder input length: 694
2026-01-28 18:08:01,267 | INFO | max output length: 694
2026-01-28 18:08:01,267 | INFO | min output length: 69
2026-01-28 18:08:48,975 | INFO | end detected at 458
2026-01-28 18:08:48,977 | INFO | -330.14 * 0.5 = -165.07 for decoder
2026-01-28 18:08:48,977 | INFO | -25.97 * 0.5 = -12.99 for ctc
2026-01-28 18:08:48,977 | INFO | total log probability: -178.06
2026-01-28 18:08:48,977 | INFO | normalized log probability: -0.40
2026-01-28 18:08:48,977 | INFO | total number of ended hypotheses: 168
2026-01-28 18:08:48,983 | INFO | best hypo: ah<space>mais<space>ça<space>c'est<space>parce<space>que<space>d'abord<space>y<space>a<space>il<space>y<space>avait<space>une<space>tâche<space>historique<space>à<space>accomplir<space>euh<space>vous<space>savez<space>que<space>notre<space>histoire<space>de<space>la<space>littérature<space>la<space>façon<space>dont<space>on<space>parlait<space>de<space>la<space>littérature<space>depuis<space>euh<space>euh<space>au<space>moins<space>cent<space>cent<space>ans<space>ou<space>cent<space>cinquante<space>ans<space>euh<space>était<space>une<space>façon<space>qui<space>privilégiait<space>beaucoup<space>l'écrivain<space>toutes<space>les<space>histoires<space>de<space>la<space>de<space>de<space>la<space>littérature<space>son<space>histoire<space>des<space>écrivains<space>des<space>écoles<space>et<space>on<space>ne<space>pensait<space>jamais<space>eux<space>lecteur<space>la<space>la<space>lecture<space>n'était<space>pas<space>euh

2026-01-28 18:08:48,986 | INFO | speech length: 443520
2026-01-28 18:08:49,032 | INFO | decoder input length: 692
2026-01-28 18:08:49,032 | INFO | max output length: 692
2026-01-28 18:08:49,032 | INFO | min output length: 69
2026-01-28 18:09:38,572 | INFO | end detected at 523
2026-01-28 18:09:38,573 | INFO | -439.27 * 0.5 = -219.63 for decoder
2026-01-28 18:09:38,574 | INFO | -19.86 * 0.5 =  -9.93 for ctc
2026-01-28 18:09:38,574 | INFO | total log probability: -229.56
2026-01-28 18:09:38,574 | INFO | normalized log probability: -0.44
2026-01-28 18:09:38,574 | INFO | total number of ended hypotheses: 158
2026-01-28 18:09:38,580 | INFO | best hypo: un<space>niveau<space>auquel<space>on<space>s'intéressait<space>quand<space>on<space>faisait<space>une<space>théorie<space>de<space>la<space>littérature<space>alors<space>il<space>est<space>nécessaire<space>maintenant<space>euh<space>d'associer<space>le<space>lecteur<space>à<space>la<space>jouissance<space>d'écrire<space>il<space>n'y<space>a<space>pas<space>de<space>jouissance<space>d'écrire<space>sans<space>jouissance<space>de<space>lire<space>les<space>deux<space>se<space>qu'on<space>pénètre<space>en<space>quelque<space>sorte<space>et<space>par<space>conséquent<space>si<space>y<space>a<space>une<space>sorte<space>de<space>nouvelle<space>théorie<space>de<space>la<space>littérature<space>qui<space>s'ébauche<space>elle<space>doit<space>tenir<space>compte<space>du<space>lecteur<space>certainement<space>si<space>vous<space>roland<space>barthes<space>qui<space>e<space>h<space>rappler<space>ce<space>mot<space>de<space>bachelar<space>les<space>les<space>écrivains<space>sont<space>seulement<space>lus<space>alors<space>on<space>pourrait

2026-01-28 18:09:38,583 | INFO | speech length: 475360
2026-01-28 18:09:38,640 | INFO | decoder input length: 742
2026-01-28 18:09:38,640 | INFO | max output length: 742
2026-01-28 18:09:38,640 | INFO | min output length: 74
2026-01-28 18:10:38,082 | INFO | end detected at 566
2026-01-28 18:10:38,084 | INFO | -684.09 * 0.5 = -342.04 for decoder
2026-01-28 18:10:38,084 | INFO | -37.17 * 0.5 = -18.59 for ctc
2026-01-28 18:10:38,084 | INFO | total log probability: -360.63
2026-01-28 18:10:38,084 | INFO | normalized log probability: -0.65
2026-01-28 18:10:38,084 | INFO | total number of ended hypotheses: 207
2026-01-28 18:10:38,091 | INFO | best hypo: poser<space>la<space>question<space>les<space>écrivains<space>ont<space>il<space>existé<space>des<space>écrivains<space>qu'ils<space>existaient<space>oui<space>parce<space>que<space>tout<space>de<space>même<space>euh<space>bon<space>on<space>l<space>finalement<space>on<space>les<space>lie<space>encore<space>maintenant<space>on<space>les<space>lit<space>à<space>travers<space>des<space>éclairages<space>historiques<space>très<space>différents<space>ils<space>n'ont<space>jamais<space>toujours<space>le<space>même<space>sens<space>et<space>c'est<space>comme<space>ça<space>que<space>l'histoire<space>marche<space>en<space>quelque<space>sorte<space>vous<space>avez<space>dit<space>un<space>jour<space>que<space>la<space>littérature<space>elle<space>st<space>à<space>sa<space>perte<space>c'était<space>un<space>mot<space>ou<space>bien<space>vous<space>le<space>pensez<space>vraiment<space>ou<space>peut<space>être<space>y<space>t<space>il<space>une<space>nouvelle<space>une<space>anti<space>littérature<space>une<space>contre<space>littérature<space>bah<space>c'est<space>à<space>dire<space>la<space>littérature<space>euh<space>avec<space>un<space>grand<space>el

2026-01-28 18:10:38,094 | INFO | speech length: 377760
2026-01-28 18:10:38,131 | INFO | decoder input length: 589
2026-01-28 18:10:38,131 | INFO | max output length: 589
2026-01-28 18:10:38,131 | INFO | min output length: 58
2026-01-28 18:11:18,820 | INFO | end detected at 497
2026-01-28 18:11:18,822 | INFO | -362.78 * 0.5 = -181.39 for decoder
2026-01-28 18:11:18,822 | INFO | -24.90 * 0.5 = -12.45 for ctc
2026-01-28 18:11:18,822 | INFO | total log probability: -193.84
2026-01-28 18:11:18,822 | INFO | normalized log probability: -0.39
2026-01-28 18:11:18,822 | INFO | total number of ended hypotheses: 168
2026-01-28 18:11:18,829 | INFO | best hypo: n'est<space>ce<space>pas<space>ça<space>a<space>été<space>pendant<space>très<space>longtemps<space>une<space>véritable<space>institution<space>ça<space>faisait<space>partie<space>des<space>institutions<space>de<space>la<space>société<space>alors<space>je<space>crois<space>que<space>cet<space>aspect<space>institutionnel<space>de<space>la<space>littérature<space>est<space>tout<space>de<space>même<space>en<space>train<space>euh<space>sinon<space>de<space>disparaître<space>tout<space>au<space>moins<space>de<space>se<space>modifier<space>profondément<space>la<space>preuve<space>c'est<space>que<space>actuellement<space>voyez<space>vous<space>en<space>soixante<space>quinze<space>je<space>pense<space>que<space>vous<space>serez<space>d'accord<space>avec<space>moi<space>il<space>n'y<space>a<space>plus<space>disans<space>e<space>qu'on<space>appelait<space>autrefois<space>d<space>de<space>grands<space>écrivains<space>bain<space>il<space>n'y<space>a<space>plus<space>de<space>grands<space>écrivains

2026-01-28 18:11:18,831 | INFO | speech length: 411680
2026-01-28 18:11:18,875 | INFO | decoder input length: 642
2026-01-28 18:11:18,875 | INFO | max output length: 642
2026-01-28 18:11:18,875 | INFO | min output length: 64
2026-01-28 18:12:01,477 | INFO | end detected at 472
2026-01-28 18:12:01,478 | INFO | -269.62 * 0.5 = -134.81 for decoder
2026-01-28 18:12:01,479 | INFO | -19.48 * 0.5 =  -9.74 for ctc
2026-01-28 18:12:01,479 | INFO | total log probability: -144.55
2026-01-28 18:12:01,479 | INFO | normalized log probability: -0.31
2026-01-28 18:12:01,479 | INFO | total number of ended hypotheses: 161
2026-01-28 18:12:01,484 | INFO | best hypo: entre<space>les<space>deux<space>guerres<space>moi<space>quand<space>j'étais<space>adolescent<space>y<space>avait<space>des<space>grands<space>leaders<space>de<space>la<space>littérature<space>nous<space>il<space>y<space>avait<space>prost<space>oui<space>y<space>avait<space>pousse<space>encore<space>que<space>il<space>était<space>mort<space>disons<space>un<space>moment<space>où<space>je<space>suis<space>devenu<space>adolescent<space>mais<space>y<space>avait<space>des<space>gens<space>comme<space>gilles<space>claudel<space>valéry<space>malrot<space>c'était<space>de<space>très<space>grand<space>nom<space>qui<space>polarisait<space>toute<space>une<space>activité<space>toute<space>une<space>séduction<space>culturelle<space>or<space>maintenant<space>il<space>faut<space>bien<space>reconnaître<space>que<space>ça<space>n'existe<space>plus<space>il<space>y<space>a<space>des<space>intellectuels<space>il<space>y<space>a<space>des<space>professeurs

2026-01-28 18:12:01,487 | INFO | speech length: 467040
2026-01-28 18:12:01,544 | INFO | decoder input length: 729
2026-01-28 18:12:01,544 | INFO | max output length: 729
2026-01-28 18:12:01,544 | INFO | min output length: 72
2026-01-28 18:12:55,515 | INFO | end detected at 489
2026-01-28 18:12:55,516 | INFO | -326.83 * 0.5 = -163.42 for decoder
2026-01-28 18:12:55,516 | INFO | -40.90 * 0.5 = -20.45 for ctc
2026-01-28 18:12:55,517 | INFO | total log probability: -183.87
2026-01-28 18:12:55,517 | INFO | normalized log probability: -0.38
2026-01-28 18:12:55,517 | INFO | total number of ended hypotheses: 180
2026-01-28 18:12:55,523 | INFO | best hypo: il<space>y<space>a<space>des<space>gens<space>qui<space>écrivent<space>mais<space>selon<space>un<space>statut<space>souvent<space>un<space>peu<space>honteux<space>et<space>euh<space>cela<space>désigne<space>l'espèce<space>de<space>de<space>force<space>qui<space>mine<space>l'institution<space>littéraire<space>certainement<space>françois<space>nauriat<space>qui<space>était<space>il<space>le<space>dernier<space>à<space>la<space>ou<space>le<space>raconter<space>en<space>un<space>sens<space>oui<space>c'était<space>le<space>l'un<space>des<space>derniers<space>de<space>ces<space>grands<space>leaders<space>dont<space>je<space>parlais<space>euh<space>il<space>y<space>a<space>un<space>homme<space>qui<space>fait<space>la<space>charnière<space>si<space>vous<space>voulez<space>et<space>qui<space>se<space>situe<space>justement<space>au<space>point<space>de<space>désagrégation<space>de<space>le<space>litare<space>de<space>désagréssion<space>historique<space>de<space>la<space>littérature<space>cés<space>sartre

2026-01-28 18:12:55,525 | INFO | speech length: 375840
2026-01-28 18:12:55,563 | INFO | decoder input length: 586
2026-01-28 18:12:55,563 | INFO | max output length: 586
2026-01-28 18:12:55,563 | INFO | min output length: 58
2026-01-28 18:13:26,770 | INFO | end detected at 348
2026-01-28 18:13:26,772 | INFO | -122.97 * 0.5 = -61.49 for decoder
2026-01-28 18:13:26,772 | INFO |  -9.69 * 0.5 =  -4.84 for ctc
2026-01-28 18:13:26,772 | INFO | total log probability: -66.33
2026-01-28 18:13:26,772 | INFO | normalized log probability: -0.20
2026-01-28 18:13:26,772 | INFO | total number of ended hypotheses: 182
2026-01-28 18:13:26,777 | INFO | best hypo: parce<space>que<space>au<space>fond<space>il<space>a<space>il<space>a<space>tenu<space>et<space>il<space>tient<space>encre<space>encore<space>cette<space>sorte<space>de<space>leadership<space>euh<space>de<space>la<space>culture<space>et<space>de<space>la<space>littérature<space>mais<space>comme<space>précisément<space>son<space>oeuvre<space>même<space>se<space>définit<space>comme<space>une<space>destruction<space>du<space>semblant<space>littéraire<space>de<space>la<space>pose<space>littéraire<space>euh<space>par<space>là<space>même<space>eu<space>il<space>il<space>contribue<space>il<space>a<space>contribué<space>puissamment<space>à<space>la<space>destruction<space>du<space>mythe<space>littéraire

2026-01-28 18:13:26,790 | INFO | Chunk: 0 | WER=9.890110 | S=2 D=2 I=5
2026-01-28 18:13:26,793 | INFO | Chunk: 1 | WER=14.516129 | S=0 D=2 I=7
2026-01-28 18:13:26,798 | INFO | Chunk: 2 | WER=11.363636 | S=1 D=2 I=7
2026-01-28 18:13:26,803 | INFO | Chunk: 3 | WER=15.294118 | S=4 D=3 I=6
2026-01-28 18:13:26,805 | INFO | Chunk: 4 | WER=14.545455 | S=1 D=3 I=4
2026-01-28 18:13:26,809 | INFO | Chunk: 5 | WER=16.470588 | S=5 D=4 I=5
2026-01-28 18:13:26,815 | INFO | Chunk: 6 | WER=19.565217 | S=4 D=5 I=9
2026-01-28 18:13:26,821 | INFO | Chunk: 7 | WER=17.757009 | S=6 D=6 I=7
2026-01-28 18:13:26,826 | INFO | Chunk: 8 | WER=14.606742 | S=2 D=4 I=7
2026-01-28 18:13:26,831 | INFO | Chunk: 9 | WER=20.000000 | S=9 D=7 I=2
2026-01-28 18:13:26,836 | INFO | Chunk: 10 | WER=20.454545 | S=9 D=2 I=7
2026-01-28 18:13:26,839 | INFO | Chunk: 11 | WER=18.333333 | S=1 D=4 I=6
2026-01-28 18:13:27,390 | INFO | File: Rhap-D2009.wav | WER=10.786290 | S=49 D=15 I=43
2026-01-28 18:13:27,391 | INFO | ------------------------------
2026-01-28 18:13:27,391 | INFO | Conf ester Done!
2026-01-28 18:15:58,033 | INFO | Chunk: 0 | WER=8.791209 | S=3 D=2 I=3
2026-01-28 18:15:58,037 | INFO | Chunk: 1 | WER=14.516129 | S=2 D=2 I=5
2026-01-28 18:15:58,043 | INFO | Chunk: 2 | WER=14.772727 | S=3 D=6 I=4
2026-01-28 18:15:58,049 | INFO | Chunk: 3 | WER=20.000000 | S=6 D=7 I=4
2026-01-28 18:15:58,052 | INFO | Chunk: 4 | WER=14.545455 | S=0 D=4 I=4
2026-01-28 18:15:58,058 | INFO | Chunk: 5 | WER=12.941176 | S=5 D=4 I=2
2026-01-28 18:15:58,066 | INFO | Chunk: 6 | WER=23.913043 | S=9 D=5 I=8
2026-01-28 18:15:58,075 | INFO | Chunk: 7 | WER=19.626168 | S=7 D=8 I=6
2026-01-28 18:15:58,081 | INFO | Chunk: 8 | WER=22.471910 | S=7 D=9 I=4
2026-01-28 18:15:58,088 | INFO | Chunk: 9 | WER=17.777778 | S=5 D=7 I=4
2026-01-28 18:15:58,095 | INFO | Chunk: 10 | WER=19.318182 | S=7 D=3 I=7
2026-01-28 18:15:58,099 | INFO | Chunk: 11 | WER=26.666667 | S=7 D=4 I=5
2026-01-28 18:15:58,827 | INFO | File: Rhap-D2009.wav | WER=12.802419 | S=66 D=33 I=28
2026-01-28 18:15:58,827 | INFO | ------------------------------
2026-01-28 18:15:58,827 | INFO | hmm_tdnn Done!
2026-01-28 18:15:59,122 | INFO | ==================================Rhap-D2010.wav=========================================
2026-01-28 18:15:59,451 | INFO | Using rVAD model
2026-01-28 18:16:21,711 | INFO | Chunk: 0 | WER=10.714286 | S=4 D=2 I=0
2026-01-28 18:16:21,712 | INFO | Chunk: 1 | WER=50.000000 | S=0 D=0 I=1
2026-01-28 18:16:21,712 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:16:21,712 | INFO | Chunk: 3 | WER=10.000000 | S=0 D=1 I=0
2026-01-28 18:16:21,712 | INFO | Chunk: 4 | WER=33.333333 | S=0 D=1 I=1
2026-01-28 18:16:21,713 | INFO | Chunk: 5 | WER=60.000000 | S=3 D=0 I=0
2026-01-28 18:16:21,713 | INFO | Chunk: 6 | WER=22.222222 | S=1 D=1 I=0
2026-01-28 18:16:21,713 | INFO | Chunk: 7 | WER=33.333333 | S=1 D=0 I=1
2026-01-28 18:16:21,719 | INFO | Chunk: 8 | WER=44.166667 | S=18 D=34 I=1
2026-01-28 18:16:21,720 | INFO | Chunk: 9 | WER=23.809524 | S=3 D=2 I=0
2026-01-28 18:16:21,720 | INFO | Chunk: 10 | WER=50.000000 | S=2 D=2 I=0
2026-01-28 18:16:21,720 | INFO | Chunk: 11 | WER=16.666667 | S=0 D=1 I=0
2026-01-28 18:16:21,721 | INFO | Chunk: 12 | WER=45.454545 | S=0 D=4 I=1
2026-01-28 18:16:21,721 | INFO | Chunk: 13 | WER=33.333333 | S=1 D=0 I=0
2026-01-28 18:16:21,721 | INFO | Chunk: 14 | WER=33.333333 | S=1 D=2 I=0
2026-01-28 18:16:21,721 | INFO | Chunk: 15 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:16:21,722 | INFO | Chunk: 16 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:16:21,722 | INFO | Chunk: 17 | WER=40.000000 | S=2 D=4 I=0
2026-01-28 18:16:21,722 | INFO | Chunk: 18 | WER=10.000000 | S=1 D=0 I=0
2026-01-28 18:16:21,722 | INFO | Chunk: 19 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:16:21,723 | INFO | Chunk: 20 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:16:21,727 | INFO | Chunk: 21 | WER=34.042553 | S=17 D=15 I=0
2026-01-28 18:16:21,730 | INFO | Chunk: 22 | WER=60.000000 | S=23 D=20 I=2
2026-01-28 18:16:21,730 | INFO | Chunk: 23 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:16:21,731 | INFO | Chunk: 24 | WER=40.000000 | S=2 D=0 I=0
2026-01-28 18:16:21,731 | INFO | Chunk: 25 | WER=125.000000 | S=3 D=0 I=2
2026-01-28 18:16:21,731 | INFO | Chunk: 26 | WER=16.666667 | S=1 D=0 I=1
2026-01-28 18:16:21,732 | INFO | Chunk: 27 | WER=22.222222 | S=3 D=1 I=0
2026-01-28 18:16:21,732 | INFO | Chunk: 28 | WER=75.000000 | S=1 D=1 I=1
2026-01-28 18:16:21,732 | INFO | Chunk: 29 | WER=13.043478 | S=1 D=2 I=0
2026-01-28 18:16:21,733 | INFO | Chunk: 30 | WER=18.181818 | S=3 D=1 I=0
2026-01-28 18:16:21,733 | INFO | Chunk: 31 | WER=40.000000 | S=3 D=1 I=0
2026-01-28 18:16:21,734 | INFO | Chunk: 32 | WER=33.333333 | S=3 D=0 I=1
2026-01-28 18:16:21,734 | INFO | Chunk: 33 | WER=26.086957 | S=1 D=5 I=0
2026-01-28 18:16:21,734 | INFO | Chunk: 34 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 18:16:21,735 | INFO | Chunk: 35 | WER=66.666667 | S=4 D=6 I=0
2026-01-28 18:16:21,740 | INFO | Chunk: 36 | WER=27.368421 | S=10 D=14 I=2
2026-01-28 18:16:21,740 | INFO | Chunk: 37 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:16:21,740 | INFO | Chunk: 38 | WER=18.750000 | S=2 D=1 I=0
2026-01-28 18:16:21,741 | INFO | Chunk: 39 | WER=12.500000 | S=0 D=0 I=1
2026-01-28 18:16:21,741 | INFO | Chunk: 40 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:16:21,741 | INFO | Chunk: 41 | WER=66.666667 | S=2 D=2 I=0
2026-01-28 18:16:21,741 | INFO | Chunk: 42 | WER=31.250000 | S=3 D=1 I=1
2026-01-28 18:16:21,742 | INFO | Chunk: 43 | WER=33.333333 | S=2 D=0 I=0
2026-01-28 18:16:21,743 | INFO | Chunk: 44 | WER=60.000000 | S=12 D=27 I=0
2026-01-28 18:16:21,744 | INFO | Chunk: 45 | WER=29.032258 | S=4 D=5 I=0
2026-01-28 18:16:21,744 | INFO | Chunk: 46 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:16:21,745 | INFO | Chunk: 47 | WER=50.000000 | S=2 D=0 I=0
2026-01-28 18:16:21,745 | INFO | Chunk: 48 | WER=100.000000 | S=0 D=0 I=1
2026-01-28 18:16:21,745 | INFO | Chunk: 49 | WER=55.000000 | S=4 D=7 I=0
2026-01-28 18:16:21,745 | INFO | Chunk: 50 | WER=75.000000 | S=2 D=1 I=0
2026-01-28 18:16:21,746 | INFO | Chunk: 51 | WER=100.000000 | S=2 D=0 I=0
2026-01-28 18:16:21,746 | INFO | Chunk: 52 | WER=25.000000 | S=1 D=1 I=0
2026-01-28 18:16:21,746 | INFO | Chunk: 53 | WER=22.222222 | S=1 D=1 I=0
2026-01-28 18:16:21,747 | INFO | Chunk: 54 | WER=16.666667 | S=1 D=1 I=1
2026-01-28 18:16:21,747 | INFO | Chunk: 55 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:16:21,747 | INFO | Chunk: 56 | WER=46.666667 | S=3 D=3 I=1
2026-01-28 18:16:21,747 | INFO | Chunk: 57 | WER=33.333333 | S=1 D=0 I=0
2026-01-28 18:16:21,748 | INFO | Chunk: 58 | WER=7.142857 | S=0 D=0 I=1
2026-01-28 18:16:21,748 | INFO | Chunk: 59 | WER=200.000000 | S=2 D=0 I=2
2026-01-28 18:16:21,748 | INFO | Chunk: 60 | WER=100.000000 | S=1 D=1 I=0
2026-01-28 18:16:21,748 | INFO | Chunk: 61 | WER=100.000000 | S=2 D=0 I=0
2026-01-28 18:16:21,748 | INFO | Chunk: 62 | WER=62.500000 | S=5 D=0 I=0
2026-01-28 18:16:21,748 | INFO | Chunk: 63 | WER=200.000000 | S=2 D=0 I=2
2026-01-28 18:16:22,236 | INFO | File: Rhap-D2010.wav | WER=35.137795 | S=171 D=166 I=20
2026-01-28 18:16:22,236 | INFO | ------------------------------
2026-01-28 18:16:22,236 | INFO | w2vec vad chunk Done!
2026-01-28 18:17:02,923 | INFO | Chunk: 0 | WER=30.357143 | S=5 D=12 I=0
2026-01-28 18:17:02,923 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:17:02,924 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:17:02,924 | INFO | Chunk: 3 | WER=10.000000 | S=0 D=1 I=0
2026-01-28 18:17:02,924 | INFO | Chunk: 4 | WER=33.333333 | S=0 D=1 I=1
2026-01-28 18:17:02,924 | INFO | Chunk: 5 | WER=20.000000 | S=1 D=0 I=0
2026-01-28 18:17:02,924 | INFO | Chunk: 6 | WER=22.222222 | S=2 D=0 I=0
2026-01-28 18:17:02,925 | INFO | Chunk: 7 | WER=66.666667 | S=1 D=0 I=3
2026-01-28 18:17:02,926 | INFO | Chunk: 8 | WER=93.333333 | S=2 D=110 I=0
2026-01-28 18:17:02,927 | INFO | Chunk: 9 | WER=33.333333 | S=4 D=3 I=0
2026-01-28 18:17:02,927 | INFO | Chunk: 10 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:17:02,927 | INFO | Chunk: 11 | WER=16.666667 | S=0 D=1 I=0
2026-01-28 18:17:02,928 | INFO | Chunk: 12 | WER=63.636364 | S=1 D=5 I=1
2026-01-28 18:17:02,928 | INFO | Chunk: 13 | WER=100.000000 | S=3 D=0 I=0
2026-01-28 18:17:02,928 | INFO | Chunk: 14 | WER=22.222222 | S=1 D=1 I=0
2026-01-28 18:17:02,928 | INFO | Chunk: 15 | WER=22.222222 | S=2 D=0 I=0
2026-01-28 18:17:02,928 | INFO | Chunk: 16 | WER=100.000000 | S=1 D=3 I=0
2026-01-28 18:17:02,929 | INFO | Chunk: 17 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:17:02,929 | INFO | Chunk: 18 | WER=30.000000 | S=0 D=3 I=0
2026-01-28 18:17:02,929 | INFO | Chunk: 19 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:17:02,929 | INFO | Chunk: 20 | WER=100.000000 | S=2 D=2 I=0
2026-01-28 18:17:02,932 | INFO | Chunk: 21 | WER=71.276596 | S=5 D=62 I=0
2026-01-28 18:17:02,933 | INFO | Chunk: 22 | WER=74.666667 | S=6 D=50 I=0
2026-01-28 18:17:02,934 | INFO | Chunk: 23 | WER=100.000000 | S=4 D=0 I=0
2026-01-28 18:17:02,934 | INFO | Chunk: 24 | WER=20.000000 | S=1 D=0 I=0
2026-01-28 18:17:02,934 | INFO | Chunk: 25 | WER=150.000000 | S=4 D=0 I=2
2026-01-28 18:17:02,934 | INFO | Chunk: 26 | WER=25.000000 | S=2 D=1 I=0
2026-01-28 18:17:02,935 | INFO | Chunk: 27 | WER=33.333333 | S=2 D=2 I=2
2026-01-28 18:17:02,935 | INFO | Chunk: 28 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 18:17:02,935 | INFO | Chunk: 29 | WER=47.826087 | S=4 D=7 I=0
2026-01-28 18:17:02,936 | INFO | Chunk: 30 | WER=13.636364 | S=2 D=1 I=0
2026-01-28 18:17:02,936 | INFO | Chunk: 31 | WER=40.000000 | S=1 D=2 I=1
2026-01-28 18:17:02,937 | INFO | Chunk: 32 | WER=58.333333 | S=1 D=6 I=0
2026-01-28 18:17:02,937 | INFO | Chunk: 33 | WER=52.173913 | S=3 D=9 I=0
2026-01-28 18:17:02,937 | INFO | Chunk: 34 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 18:17:02,938 | INFO | Chunk: 35 | WER=60.000000 | S=3 D=6 I=0
2026-01-28 18:17:02,940 | INFO | Chunk: 36 | WER=74.736842 | S=6 D=65 I=0
2026-01-28 18:17:02,940 | INFO | Chunk: 37 | WER=100.000000 | S=0 D=0 I=1
2026-01-28 18:17:02,940 | INFO | Chunk: 38 | WER=12.500000 | S=0 D=2 I=0
2026-01-28 18:17:02,941 | INFO | Chunk: 39 | WER=12.500000 | S=0 D=0 I=1
2026-01-28 18:17:02,941 | INFO | Chunk: 40 | WER=100.000000 | S=1 D=3 I=0
2026-01-28 18:17:02,941 | INFO | Chunk: 41 | WER=66.666667 | S=2 D=2 I=0
2026-01-28 18:17:02,941 | INFO | Chunk: 42 | WER=18.750000 | S=1 D=2 I=0
2026-01-28 18:17:02,942 | INFO | Chunk: 43 | WER=66.666667 | S=4 D=0 I=0
2026-01-28 18:17:02,943 | INFO | Chunk: 44 | WER=80.000000 | S=0 D=52 I=0
2026-01-28 18:17:02,943 | INFO | Chunk: 45 | WER=22.580645 | S=1 D=6 I=0
2026-01-28 18:17:02,944 | INFO | Chunk: 46 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:17:02,944 | INFO | Chunk: 47 | WER=75.000000 | S=2 D=1 I=0
2026-01-28 18:17:02,944 | INFO | Chunk: 48 | WER=100.000000 | S=0 D=0 I=1
2026-01-28 18:17:02,944 | INFO | Chunk: 49 | WER=25.000000 | S=2 D=2 I=1
2026-01-28 18:17:02,945 | INFO | Chunk: 50 | WER=75.000000 | S=1 D=2 I=0
2026-01-28 18:17:02,945 | INFO | Chunk: 51 | WER=200.000000 | S=2 D=0 I=2
2026-01-28 18:17:02,945 | INFO | Chunk: 52 | WER=12.500000 | S=0 D=1 I=0
2026-01-28 18:17:02,945 | INFO | Chunk: 53 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:17:02,946 | INFO | Chunk: 54 | WER=44.444444 | S=0 D=7 I=1
2026-01-28 18:17:02,946 | INFO | Chunk: 55 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:17:02,946 | INFO | Chunk: 56 | WER=66.666667 | S=7 D=3 I=0
2026-01-28 18:17:02,946 | INFO | Chunk: 57 | WER=100.000000 | S=3 D=0 I=0
2026-01-28 18:17:02,947 | INFO | Chunk: 58 | WER=7.142857 | S=1 D=0 I=0
2026-01-28 18:17:02,947 | INFO | Chunk: 59 | WER=250.000000 | S=2 D=0 I=3
2026-01-28 18:17:02,947 | INFO | Chunk: 60 | WER=100.000000 | S=1 D=1 I=0
2026-01-28 18:17:02,947 | INFO | Chunk: 61 | WER=100.000000 | S=2 D=0 I=0
2026-01-28 18:17:02,947 | INFO | Chunk: 62 | WER=62.500000 | S=4 D=0 I=1
2026-01-28 18:17:02,948 | INFO | Chunk: 63 | WER=200.000000 | S=2 D=0 I=2
2026-01-28 18:17:03,288 | INFO | File: Rhap-D2010.wav | WER=55.019685 | S=113 D=429 I=17
2026-01-28 18:17:03,288 | INFO | ------------------------------
2026-01-28 18:17:03,288 | INFO | whisper med Done!
2026-01-28 18:18:00,238 | INFO | Chunk: 0 | WER=25.000000 | S=3 D=9 I=2
2026-01-28 18:18:00,239 | INFO | Chunk: 1 | WER=50.000000 | S=0 D=0 I=1
2026-01-28 18:18:00,239 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:18:00,239 | INFO | Chunk: 3 | WER=10.000000 | S=0 D=1 I=0
2026-01-28 18:18:00,240 | INFO | Chunk: 4 | WER=33.333333 | S=0 D=1 I=1
2026-01-28 18:18:00,240 | INFO | Chunk: 5 | WER=60.000000 | S=2 D=0 I=1
2026-01-28 18:18:00,240 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:18:00,240 | INFO | Chunk: 7 | WER=66.666667 | S=1 D=0 I=3
2026-01-28 18:18:00,243 | INFO | Chunk: 8 | WER=87.500000 | S=2 D=103 I=0
2026-01-28 18:18:00,244 | INFO | Chunk: 9 | WER=23.809524 | S=2 D=3 I=0
2026-01-28 18:18:00,244 | INFO | Chunk: 10 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:18:00,244 | INFO | Chunk: 11 | WER=16.666667 | S=0 D=1 I=0
2026-01-28 18:18:00,245 | INFO | Chunk: 12 | WER=54.545455 | S=1 D=4 I=1
2026-01-28 18:18:00,245 | INFO | Chunk: 13 | WER=133.333333 | S=2 D=0 I=2
2026-01-28 18:18:00,245 | INFO | Chunk: 14 | WER=11.111111 | S=0 D=1 I=0
2026-01-28 18:18:00,246 | INFO | Chunk: 15 | WER=22.222222 | S=1 D=1 I=0
2026-01-28 18:18:00,246 | INFO | Chunk: 16 | WER=100.000000 | S=1 D=3 I=0
2026-01-28 18:18:00,246 | INFO | Chunk: 17 | WER=73.333333 | S=1 D=10 I=0
2026-01-28 18:18:00,247 | INFO | Chunk: 18 | WER=20.000000 | S=2 D=0 I=0
2026-01-28 18:18:00,247 | INFO | Chunk: 19 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:18:00,247 | INFO | Chunk: 20 | WER=75.000000 | S=1 D=2 I=0
2026-01-28 18:18:00,250 | INFO | Chunk: 21 | WER=74.468085 | S=1 D=69 I=0
2026-01-28 18:18:00,253 | INFO | Chunk: 22 | WER=58.666667 | S=8 D=36 I=0
2026-01-28 18:18:00,253 | INFO | Chunk: 23 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:18:00,253 | INFO | Chunk: 24 | WER=20.000000 | S=1 D=0 I=0
2026-01-28 18:18:00,254 | INFO | Chunk: 25 | WER=125.000000 | S=3 D=0 I=2
2026-01-28 18:18:00,254 | INFO | Chunk: 26 | WER=16.666667 | S=1 D=1 I=0
2026-01-28 18:18:00,255 | INFO | Chunk: 27 | WER=33.333333 | S=2 D=2 I=2
2026-01-28 18:18:00,255 | INFO | Chunk: 28 | WER=50.000000 | S=0 D=1 I=1
2026-01-28 18:18:00,256 | INFO | Chunk: 29 | WER=34.782609 | S=2 D=6 I=0
2026-01-28 18:18:00,256 | INFO | Chunk: 30 | WER=4.545455 | S=1 D=0 I=0
2026-01-28 18:18:00,257 | INFO | Chunk: 31 | WER=30.000000 | S=1 D=0 I=2
2026-01-28 18:18:00,257 | INFO | Chunk: 32 | WER=25.000000 | S=2 D=0 I=1
2026-01-28 18:18:00,258 | INFO | Chunk: 33 | WER=39.130435 | S=4 D=4 I=1
2026-01-28 18:18:00,258 | INFO | Chunk: 34 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 18:18:00,259 | INFO | Chunk: 35 | WER=53.333333 | S=4 D=4 I=0
2026-01-28 18:18:00,262 | INFO | Chunk: 36 | WER=72.631579 | S=2 D=67 I=0
2026-01-28 18:18:00,262 | INFO | Chunk: 37 | WER=100.000000 | S=0 D=0 I=1
2026-01-28 18:18:00,262 | INFO | Chunk: 38 | WER=6.250000 | S=1 D=0 I=0
2026-01-28 18:18:00,263 | INFO | Chunk: 39 | WER=12.500000 | S=0 D=0 I=1
2026-01-28 18:18:00,263 | INFO | Chunk: 40 | WER=75.000000 | S=3 D=0 I=0
2026-01-28 18:18:00,263 | INFO | Chunk: 41 | WER=50.000000 | S=2 D=1 I=0
2026-01-28 18:18:00,264 | INFO | Chunk: 42 | WER=12.500000 | S=1 D=1 I=0
2026-01-28 18:18:00,264 | INFO | Chunk: 43 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:18:00,265 | INFO | Chunk: 44 | WER=81.538462 | S=1 D=52 I=0
2026-01-28 18:18:00,266 | INFO | Chunk: 45 | WER=29.032258 | S=1 D=8 I=0
2026-01-28 18:18:00,267 | INFO | Chunk: 46 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:18:00,267 | INFO | Chunk: 47 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:18:00,267 | INFO | Chunk: 48 | WER=100.000000 | S=0 D=0 I=1
2026-01-28 18:18:00,268 | INFO | Chunk: 49 | WER=10.000000 | S=1 D=1 I=0
2026-01-28 18:18:00,268 | INFO | Chunk: 50 | WER=100.000000 | S=3 D=0 I=1
2026-01-28 18:18:00,268 | INFO | Chunk: 51 | WER=150.000000 | S=2 D=0 I=1
2026-01-28 18:18:00,268 | INFO | Chunk: 52 | WER=12.500000 | S=0 D=1 I=0
2026-01-28 18:18:00,269 | INFO | Chunk: 53 | WER=11.111111 | S=0 D=0 I=1
2026-01-28 18:18:00,269 | INFO | Chunk: 54 | WER=5.555556 | S=0 D=0 I=1
2026-01-28 18:18:00,270 | INFO | Chunk: 55 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:18:00,270 | INFO | Chunk: 56 | WER=53.333333 | S=4 D=3 I=1
2026-01-28 18:18:00,270 | INFO | Chunk: 57 | WER=100.000000 | S=1 D=2 I=0
2026-01-28 18:18:00,271 | INFO | Chunk: 58 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:18:00,271 | INFO | Chunk: 59 | WER=250.000000 | S=2 D=0 I=3
2026-01-28 18:18:00,271 | INFO | Chunk: 60 | WER=100.000000 | S=1 D=1 I=0
2026-01-28 18:18:00,271 | INFO | Chunk: 61 | WER=100.000000 | S=2 D=0 I=0
2026-01-28 18:18:00,272 | INFO | Chunk: 62 | WER=75.000000 | S=5 D=0 I=1
2026-01-28 18:18:00,272 | INFO | Chunk: 63 | WER=200.000000 | S=2 D=0 I=2
2026-01-28 18:18:00,978 | INFO | File: Rhap-D2010.wav | WER=50.295276 | S=85 D=395 I=31
2026-01-28 18:18:00,978 | INFO | ------------------------------
2026-01-28 18:18:00,979 | INFO | whisper large Done!
2026-01-28 18:18:01,153 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 18:18:01,191 | INFO | Vocabulary size: 350
2026-01-28 18:18:02,157 | INFO | Gradient checkpoint layers: []
2026-01-28 18:18:02,957 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 18:18:02,961 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 18:18:02,962 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 18:18:02,962 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 18:18:02,963 | INFO | speech length: 220320
2026-01-28 18:18:03,024 | INFO | decoder input length: 343
2026-01-28 18:18:03,024 | INFO | max output length: 343
2026-01-28 18:18:03,024 | INFO | min output length: 34
2026-01-28 18:18:13,368 | INFO | end detected at 140
2026-01-28 18:18:13,369 | INFO | -159.88 * 0.5 = -79.94 for decoder
2026-01-28 18:18:13,369 | INFO | -75.09 * 0.5 = -37.54 for ctc
2026-01-28 18:18:13,369 | INFO | total log probability: -117.48
2026-01-28 18:18:13,369 | INFO | normalized log probability: -0.88
2026-01-28 18:18:13,370 | INFO | total number of ended hypotheses: 176
2026-01-28 18:18:13,371 | INFO | best hypo: ▁lors▁je▁sais▁bien▁marie▁duras▁que▁vous▁c'est▁vrai▁vous▁avez▁obtenu▁d'autres▁succès▁j'en▁énumérai▁d'ailleurs▁avance▁générique▁mais▁quand▁même▁celui▁là▁cent▁mille▁exemplaires▁quatre▁semaines▁subsument▁fabuleux▁est▁ce▁que▁votrecette▁sensibilité▁qui▁est▁très▁aigule▁et▁pointue▁avait▁un▁peu▁pressenti

2026-01-28 18:18:13,375 | INFO | speech length: 16320
2026-01-28 18:18:13,413 | INFO | decoder input length: 25
2026-01-28 18:18:13,413 | INFO | max output length: 25
2026-01-28 18:18:13,413 | INFO | min output length: 2
2026-01-28 18:18:13,927 | INFO | end detected at 13
2026-01-28 18:18:13,929 | INFO |  -3.46 * 0.5 =  -1.73 for decoder
2026-01-28 18:18:13,929 | INFO |  -4.29 * 0.5 =  -2.15 for ctc
2026-01-28 18:18:13,929 | INFO | total log probability: -3.88
2026-01-28 18:18:13,929 | INFO | normalized log probability: -0.48
2026-01-28 18:18:13,929 | INFO | total number of ended hypotheses: 185
2026-01-28 18:18:13,929 | INFO | best hypo: ▁non▁parlons

2026-01-28 18:18:13,931 | INFO | speech length: 20640
2026-01-28 18:18:13,969 | INFO | decoder input length: 31
2026-01-28 18:18:13,969 | INFO | max output length: 31
2026-01-28 18:18:13,969 | INFO | min output length: 3
2026-01-28 18:18:14,565 | INFO | end detected at 15
2026-01-28 18:18:14,566 | INFO |  -0.73 * 0.5 =  -0.36 for decoder
2026-01-28 18:18:14,566 | INFO |  -0.89 * 0.5 =  -0.45 for ctc
2026-01-28 18:18:14,566 | INFO | total log probability: -0.81
2026-01-28 18:18:14,566 | INFO | normalized log probability: -0.08
2026-01-28 18:18:14,566 | INFO | total number of ended hypotheses: 158
2026-01-28 18:18:14,566 | INFO | best hypo: ▁non▁tout▁au▁contraire

2026-01-28 18:18:14,568 | INFO | speech length: 44000
2026-01-28 18:18:14,603 | INFO | decoder input length: 68
2026-01-28 18:18:14,604 | INFO | max output length: 68
2026-01-28 18:18:14,604 | INFO | min output length: 6
2026-01-28 18:18:15,697 | INFO | end detected at 25
2026-01-28 18:18:15,698 | INFO |  -1.61 * 0.5 =  -0.81 for decoder
2026-01-28 18:18:15,698 | INFO |  -0.42 * 0.5 =  -0.21 for ctc
2026-01-28 18:18:15,698 | INFO | total log probability: -1.02
2026-01-28 18:18:15,699 | INFO | normalized log probability: -0.05
2026-01-28 18:18:15,699 | INFO | total number of ended hypotheses: 161
2026-01-28 18:18:15,699 | INFO | best hypo: ▁après▁le▁succès▁de▁la▁maladie▁de▁la▁mort

2026-01-28 18:18:15,701 | INFO | speech length: 21120
2026-01-28 18:18:15,741 | INFO | decoder input length: 32
2026-01-28 18:18:15,741 | INFO | max output length: 32
2026-01-28 18:18:15,741 | INFO | min output length: 3
2026-01-28 18:18:16,483 | INFO | end detected at 19
2026-01-28 18:18:16,484 | INFO |  -1.09 * 0.5 =  -0.55 for decoder
2026-01-28 18:18:16,484 | INFO |  -0.10 * 0.5 =  -0.05 for ctc
2026-01-28 18:18:16,484 | INFO | total log probability: -0.60
2026-01-28 18:18:16,484 | INFO | normalized log probability: -0.04
2026-01-28 18:18:16,484 | INFO | total number of ended hypotheses: 151
2026-01-28 18:18:16,484 | INFO | best hypo: ▁j'avais▁peur▁pour▁ce▁livre

2026-01-28 18:18:16,486 | INFO | speech length: 31040
2026-01-28 18:18:16,540 | INFO | decoder input length: 48
2026-01-28 18:18:16,540 | INFO | max output length: 48
2026-01-28 18:18:16,540 | INFO | min output length: 4
2026-01-28 18:18:17,867 | INFO | end detected at 23
2026-01-28 18:18:17,869 | INFO |  -5.08 * 0.5 =  -2.54 for decoder
2026-01-28 18:18:17,869 | INFO | -12.19 * 0.5 =  -6.10 for ctc
2026-01-28 18:18:17,869 | INFO | total log probability: -8.64
2026-01-28 18:18:17,869 | INFO | normalized log probability: -0.51
2026-01-28 18:18:17,869 | INFO | total number of ended hypotheses: 171
2026-01-28 18:18:17,869 | INFO | best hypo: ▁une▁montagne▁peur▁relative▁au▁marqué

2026-01-28 18:18:17,871 | INFO | speech length: 25760
2026-01-28 18:18:17,907 | INFO | decoder input length: 39
2026-01-28 18:18:17,907 | INFO | max output length: 39
2026-01-28 18:18:17,907 | INFO | min output length: 3
2026-01-28 18:18:18,856 | INFO | end detected at 24
2026-01-28 18:18:18,858 | INFO |  -1.54 * 0.5 =  -0.77 for decoder
2026-01-28 18:18:18,858 | INFO |  -1.72 * 0.5 =  -0.86 for ctc
2026-01-28 18:18:18,858 | INFO | total log probability: -1.63
2026-01-28 18:18:18,858 | INFO | normalized log probability: -0.09
2026-01-28 18:18:18,859 | INFO | total number of ended hypotheses: 165
2026-01-28 18:18:18,859 | INFO | best hypo: ▁mais▁j'avais▁peur▁qu'il▁ne▁soit▁pas

2026-01-28 18:18:18,860 | INFO | speech length: 23680
2026-01-28 18:18:18,901 | INFO | decoder input length: 36
2026-01-28 18:18:18,902 | INFO | max output length: 36
2026-01-28 18:18:18,902 | INFO | min output length: 3
2026-01-28 18:18:19,725 | INFO | end detected at 21
2026-01-28 18:18:19,726 | INFO |  -1.25 * 0.5 =  -0.63 for decoder
2026-01-28 18:18:19,726 | INFO |  -1.31 * 0.5 =  -0.66 for ctc
2026-01-28 18:18:19,726 | INFO | total log probability: -1.28
2026-01-28 18:18:19,726 | INFO | normalized log probability: -0.08
2026-01-28 18:18:19,726 | INFO | total number of ended hypotheses: 148
2026-01-28 18:18:19,726 | INFO | best hypo: ▁celui▁que▁les▁gens▁attendaient▁de▁moi

2026-01-28 18:18:19,728 | INFO | speech length: 455200
2026-01-28 18:18:19,768 | INFO | decoder input length: 710
2026-01-28 18:18:19,768 | INFO | max output length: 710
2026-01-28 18:18:19,768 | INFO | min output length: 71
2026-01-28 18:18:53,509 | INFO | end detected at 213
2026-01-28 18:18:53,511 | INFO | -613.09 * 0.5 = -306.54 for decoder
2026-01-28 18:18:53,511 | INFO | -185.56 * 0.5 = -92.78 for ctc
2026-01-28 18:18:53,511 | INFO | total log probability: -399.32
2026-01-28 18:18:53,511 | INFO | normalized log probability: -1.94
2026-01-28 18:18:53,511 | INFO | total number of ended hypotheses: 178
2026-01-28 18:18:53,514 | INFO | best hypo: ▁la▁critique▁déferlante▁c'est▁de▁nous▁croire▁quelqu'un▁dit▁du▁mal▁dira▁bon▁oui▁il▁en▁faut▁bien▁c'est▁dire▁qu'il▁l'ai▁dit▁mais▁quelqu'un▁dit▁on▁ne▁comprend▁pas▁qu'un▁éditeur▁laissez▁passer▁des▁photogrammaire▁mais▁on▁viendra▁d'ailleurs▁soleils▁le▁styleurs▁c'est▁vraixe▁les▁critiques▁il▁vous▁aiboudé▁d'ailleurs▁un▁peu▁depuis▁dans▁quelques▁nées▁plutôt▁d'un▁couple▁avec▁leursout▁on▁voyez▁vous▁et▁c'est▁vous'un▁couvre▁fleurs▁ça▁vous▁a▁fait▁plaisir

2026-01-28 18:18:53,517 | INFO | speech length: 83200
2026-01-28 18:18:53,561 | INFO | decoder input length: 129
2026-01-28 18:18:53,561 | INFO | max output length: 129
2026-01-28 18:18:53,561 | INFO | min output length: 12
2026-01-28 18:18:55,830 | INFO | end detected at 46
2026-01-28 18:18:55,831 | INFO | -10.57 * 0.5 =  -5.28 for decoder
2026-01-28 18:18:55,831 | INFO | -27.92 * 0.5 = -13.96 for ctc
2026-01-28 18:18:55,831 | INFO | total log probability: -19.25
2026-01-28 18:18:55,831 | INFO | normalized log probability: -0.46
2026-01-28 18:18:55,831 | INFO | total number of ended hypotheses: 145
2026-01-28 18:18:55,832 | INFO | best hypo: ▁je▁t'as▁peu▁gênanti▁cher▁déjeunant▁sais▁je▁pendant▁dix▁ans▁ça▁durait▁dix▁ans▁de▁silence

2026-01-28 18:18:55,833 | INFO | speech length: 35360
2026-01-28 18:18:55,870 | INFO | decoder input length: 54
2026-01-28 18:18:55,870 | INFO | max output length: 54
2026-01-28 18:18:55,870 | INFO | min output length: 5
2026-01-28 18:18:56,893 | INFO | end detected at 24
2026-01-28 18:18:56,894 | INFO |  -4.23 * 0.5 =  -2.11 for decoder
2026-01-28 18:18:56,894 | INFO |  -6.77 * 0.5 =  -3.39 for ctc
2026-01-28 18:18:56,895 | INFO | total log probability: -5.50
2026-01-28 18:18:56,895 | INFO | normalized log probability: -0.29
2026-01-28 18:18:56,895 | INFO | total number of ended hypotheses: 172
2026-01-28 18:18:56,895 | INFO | best hypo: ▁eh▁lah▁évidemment▁c'est▁un▁peu▁dure

2026-01-28 18:18:56,898 | INFO | speech length: 19680
2026-01-28 18:18:57,014 | INFO | decoder input length: 30
2026-01-28 18:18:57,014 | INFO | max output length: 30
2026-01-28 18:18:57,014 | INFO | min output length: 3
2026-01-28 18:18:57,764 | INFO | end detected at 19
2026-01-28 18:18:57,765 | INFO |  -1.73 * 0.5 =  -0.87 for decoder
2026-01-28 18:18:57,765 | INFO |  -1.57 * 0.5 =  -0.79 for ctc
2026-01-28 18:18:57,765 | INFO | total log probability: -1.65
2026-01-28 18:18:57,765 | INFO | normalized log probability: -0.11
2026-01-28 18:18:57,765 | INFO | total number of ended hypotheses: 139
2026-01-28 18:18:57,765 | INFO | best hypo: ▁je▁peux▁pas▁ouvrir▁un▁journal

2026-01-28 18:18:57,767 | INFO | speech length: 38080
2026-01-28 18:18:57,808 | INFO | decoder input length: 59
2026-01-28 18:18:57,808 | INFO | max output length: 59
2026-01-28 18:18:57,808 | INFO | min output length: 5
2026-01-28 18:18:58,952 | INFO | end detected at 27
2026-01-28 18:18:58,953 | INFO |  -4.17 * 0.5 =  -2.08 for decoder
2026-01-28 18:18:58,953 | INFO | -17.61 * 0.5 =  -8.81 for ctc
2026-01-28 18:18:58,953 | INFO | total log probability: -10.89
2026-01-28 18:18:58,953 | INFO | normalized log probability: -0.49
2026-01-28 18:18:58,953 | INFO | total number of ended hypotheses: 190
2026-01-28 18:18:58,954 | INFO | best hypo: ▁on▁peut▁y▁a▁un▁réflexe▁de▁pudeur▁quoi

2026-01-28 18:18:58,956 | INFO | speech length: 12800
2026-01-28 18:18:58,991 | INFO | decoder input length: 19
2026-01-28 18:18:58,991 | INFO | max output length: 19
2026-01-28 18:18:58,991 | INFO | min output length: 1
2026-01-28 18:18:59,531 | INFO | end detected at 14
2026-01-28 18:18:59,532 | INFO |  -1.79 * 0.5 =  -0.89 for decoder
2026-01-28 18:18:59,532 | INFO |  -7.09 * 0.5 =  -3.54 for ctc
2026-01-28 18:18:59,532 | INFO | total log probability: -4.44
2026-01-28 18:18:59,532 | INFO | normalized log probability: -0.63
2026-01-28 18:18:59,532 | INFO | total number of ended hypotheses: 163
2026-01-28 18:18:59,533 | INFO | best hypo: ▁le▁fut▁exposé

2026-01-28 18:18:59,534 | INFO | speech length: 29920
2026-01-28 18:18:59,575 | INFO | decoder input length: 46
2026-01-28 18:18:59,575 | INFO | max output length: 46
2026-01-28 18:18:59,576 | INFO | min output length: 4
2026-01-28 18:19:00,663 | INFO | end detected at 27
2026-01-28 18:19:00,664 | INFO |  -1.77 * 0.5 =  -0.89 for decoder
2026-01-28 18:19:00,664 | INFO |  -3.46 * 0.5 =  -1.73 for ctc
2026-01-28 18:19:00,664 | INFO | total log probability: -2.62
2026-01-28 18:19:00,665 | INFO | normalized log probability: -0.12
2026-01-28 18:19:00,665 | INFO | total number of ended hypotheses: 172
2026-01-28 18:19:00,665 | INFO | best hypo: ▁c'est▁toujours▁un▁temps▁très▁difficile▁que

2026-01-28 18:19:00,667 | INFO | speech length: 29280
2026-01-28 18:19:00,711 | INFO | decoder input length: 45
2026-01-28 18:19:00,711 | INFO | max output length: 45
2026-01-28 18:19:00,711 | INFO | min output length: 4
2026-01-28 18:19:01,604 | INFO | end detected at 22
2026-01-28 18:19:01,605 | INFO |  -1.80 * 0.5 =  -0.90 for decoder
2026-01-28 18:19:01,605 | INFO |  -0.18 * 0.5 =  -0.09 for ctc
2026-01-28 18:19:01,605 | INFO | total log probability: -0.99
2026-01-28 18:19:01,605 | INFO | normalized log probability: -0.06
2026-01-28 18:19:01,605 | INFO | total number of ended hypotheses: 158
2026-01-28 18:19:01,605 | INFO | best hypo: ▁que▁le▁temps▁de▁la▁parution▁d'un▁livre

2026-01-28 18:19:01,607 | INFO | speech length: 14880
2026-01-28 18:19:01,650 | INFO | decoder input length: 22
2026-01-28 18:19:01,650 | INFO | max output length: 22
2026-01-28 18:19:01,650 | INFO | min output length: 2
2026-01-28 18:19:02,169 | INFO | end detected at 13
2026-01-28 18:19:02,171 | INFO |  -2.20 * 0.5 =  -1.10 for decoder
2026-01-28 18:19:02,171 | INFO |  -2.49 * 0.5 =  -1.24 for ctc
2026-01-28 18:19:02,171 | INFO | total log probability: -2.35
2026-01-28 18:19:02,171 | INFO | normalized log probability: -0.39
2026-01-28 18:19:02,171 | INFO | total number of ended hypotheses: 197
2026-01-28 18:19:02,171 | INFO | best hypo: ▁c'est▁peu

2026-01-28 18:19:02,173 | INFO | speech length: 67520
2026-01-28 18:19:02,217 | INFO | decoder input length: 105
2026-01-28 18:19:02,217 | INFO | max output length: 105
2026-01-28 18:19:02,217 | INFO | min output length: 10
2026-01-28 18:19:03,854 | INFO | end detected at 30
2026-01-28 18:19:03,856 | INFO |  -2.71 * 0.5 =  -1.35 for decoder
2026-01-28 18:19:03,856 | INFO | -10.90 * 0.5 =  -5.45 for ctc
2026-01-28 18:19:03,856 | INFO | total log probability: -6.81
2026-01-28 18:19:03,856 | INFO | normalized log probability: -0.26
2026-01-28 18:19:03,856 | INFO | total number of ended hypotheses: 168
2026-01-28 18:19:03,856 | INFO | best hypo: ▁trop▁c'est▁trop▁peut▁être▁des▁habitudes▁de▁la▁critique

2026-01-28 18:19:03,858 | INFO | speech length: 45440
2026-01-28 18:19:03,895 | INFO | decoder input length: 70
2026-01-28 18:19:03,895 | INFO | max output length: 70
2026-01-28 18:19:03,895 | INFO | min output length: 7
2026-01-28 18:19:04,951 | INFO | end detected at 24
2026-01-28 18:19:04,953 | INFO |  -3.17 * 0.5 =  -1.58 for decoder
2026-01-28 18:19:04,953 | INFO |  -1.43 * 0.5 =  -0.72 for ctc
2026-01-28 18:19:04,953 | INFO | total log probability: -2.30
2026-01-28 18:19:04,953 | INFO | normalized log probability: -0.11
2026-01-28 18:19:04,953 | INFO | total number of ended hypotheses: 143
2026-01-28 18:19:04,953 | INFO | best hypo: ▁fait▁que▁un▁livre▁a▁toujours▁pris▁un▁peu

2026-01-28 18:19:04,955 | INFO | speech length: 31520
2026-01-28 18:19:05,017 | INFO | decoder input length: 48
2026-01-28 18:19:05,017 | INFO | max output length: 48
2026-01-28 18:19:05,017 | INFO | min output length: 4
2026-01-28 18:19:06,215 | INFO | end detected at 16
2026-01-28 18:19:06,217 | INFO |  -0.88 * 0.5 =  -0.44 for decoder
2026-01-28 18:19:06,217 | INFO |  -0.15 * 0.5 =  -0.07 for ctc
2026-01-28 18:19:06,217 | INFO | total log probability: -0.51
2026-01-28 18:19:06,217 | INFO | normalized log probability: -0.04
2026-01-28 18:19:06,217 | INFO | total number of ended hypotheses: 139
2026-01-28 18:19:06,217 | INFO | best hypo: ▁comme▁une▁certaine▁culpabilité

2026-01-28 18:19:06,221 | INFO | speech length: 10240
2026-01-28 18:19:06,275 | INFO | decoder input length: 15
2026-01-28 18:19:06,275 | INFO | max output length: 15
2026-01-28 18:19:06,275 | INFO | min output length: 1
2026-01-28 18:19:07,203 | INFO | adding <eos> in the last position in the loop
2026-01-28 18:19:07,219 | INFO | no hypothesis. Finish decoding.
2026-01-28 18:19:07,221 | INFO |  -1.73 * 0.5 =  -0.87 for decoder
2026-01-28 18:19:07,222 | INFO |  -7.90 * 0.5 =  -3.95 for ctc
2026-01-28 18:19:07,222 | INFO | total log probability: -4.81
2026-01-28 18:19:07,222 | INFO | normalized log probability: -0.40
2026-01-28 18:19:07,222 | INFO | total number of ended hypotheses: 148
2026-01-28 18:19:07,222 | INFO | best hypo: ▁j'ai▁d'accord

2026-01-28 18:19:07,225 | INFO | speech length: 296640
2026-01-28 18:19:07,295 | INFO | decoder input length: 463
2026-01-28 18:19:07,295 | INFO | max output length: 463
2026-01-28 18:19:07,295 | INFO | min output length: 46
2026-01-28 18:19:37,747 | INFO | end detected at 206
2026-01-28 18:19:37,748 | INFO | -360.76 * 0.5 = -180.38 for decoder
2026-01-28 18:19:37,748 | INFO | -152.46 * 0.5 = -76.23 for ctc
2026-01-28 18:19:37,748 | INFO | total log probability: -256.61
2026-01-28 18:19:37,748 | INFO | normalized log probability: -1.28
2026-01-28 18:19:37,748 | INFO | total number of ended hypotheses: 166
2026-01-28 18:19:37,751 | INFO | best hypo: ▁c'est▁certaines▁orgueils▁pour▁les▁auteurs▁mais▁dits▁et▁pierre▁lorges▁et▁une▁rumeur▁qui▁courtparait▁certains▁membres▁de▁l'académie▁goncourt▁aurait▁dit▁mais▁pourquoi▁ne▁lui▁donnerait▁ont▁pas▁le▁pai▁oncours▁l'engir▁garde▁un▁petit▁peu▁dans▁les▁statistiquess▁en▁mille▁cent▁cinquante▁qu'offe▁et▁fait▁paraîtres▁ce▁chef▁d'offe▁pour▁moi▁cet▁d'un▁'offe▁s'appelle▁un▁barrage▁dans▁le▁pacifique▁où▁l'avait▁raté▁de▁peu▁dans▁l'académie▁goncourse▁est▁complètement

2026-01-28 18:19:37,754 | INFO | speech length: 240640
2026-01-28 18:19:37,811 | INFO | decoder input length: 375
2026-01-28 18:19:37,811 | INFO | max output length: 375
2026-01-28 18:19:37,811 | INFO | min output length: 37
2026-01-28 18:19:49,416 | INFO | end detected at 152
2026-01-28 18:19:49,419 | INFO | -217.19 * 0.5 = -108.60 for decoder
2026-01-28 18:19:49,419 | INFO | -112.40 * 0.5 = -56.20 for ctc
2026-01-28 18:19:49,419 | INFO | total log probability: -164.80
2026-01-28 18:19:49,419 | INFO | normalized log probability: -1.14
2026-01-28 18:19:49,419 | INFO | total number of ended hypotheses: 179
2026-01-28 18:19:49,421 | INFO | best hypo: ▁ces▁bacs▁isez▁à▁côté▁ne▁sont▁pas▁les▁mêmes▁mais▁c'était▁des▁d'autres▁n'étant▁cinquante▁donc▁aucun▁de▁jeux▁aujourd'hui▁ce▁qu'ils▁ont▁été▁couronnés▁les▁jeux▁sauvages▁de▁polcolats▁et▁les▁barrages▁dans▁le▁pacifiques▁ils▁l'ont▁laisé▁de▁tomber▁et▁d'avoir▁du▁regret▁elle▁ne▁voulait▁pas▁vous▁ne▁parez▁pas▁donner▁au▁barrage

2026-01-28 18:19:49,424 | INFO | speech length: 24000
2026-01-28 18:19:49,469 | INFO | decoder input length: 37
2026-01-28 18:19:49,469 | INFO | max output length: 37
2026-01-28 18:19:49,469 | INFO | min output length: 3
2026-01-28 18:19:50,072 | INFO | end detected at 15
2026-01-28 18:19:50,073 | INFO |  -0.73 * 0.5 =  -0.37 for decoder
2026-01-28 18:19:50,073 | INFO |  -0.44 * 0.5 =  -0.22 for ctc
2026-01-28 18:19:50,073 | INFO | total log probability: -0.59
2026-01-28 18:19:50,073 | INFO | normalized log probability: -0.05
2026-01-28 18:19:50,073 | INFO | total number of ended hypotheses: 152
2026-01-28 18:19:50,073 | INFO | best hypo: ▁parce▁que▁j'étais

2026-01-28 18:19:50,075 | INFO | speech length: 26720
2026-01-28 18:19:50,137 | INFO | decoder input length: 41
2026-01-28 18:19:50,137 | INFO | max output length: 41
2026-01-28 18:19:50,137 | INFO | min output length: 4
2026-01-28 18:19:51,410 | INFO | end detected at 18
2026-01-28 18:19:51,412 | INFO |  -1.11 * 0.5 =  -0.56 for decoder
2026-01-28 18:19:51,412 | INFO |  -2.49 * 0.5 =  -1.25 for ctc
2026-01-28 18:19:51,412 | INFO | total log probability: -1.80
2026-01-28 18:19:51,413 | INFO | normalized log probability: -0.14
2026-01-28 18:19:51,413 | INFO | total number of ended hypotheses: 159
2026-01-28 18:19:51,413 | INFO | best hypo: ▁de▁gauche▁je▁te▁communiste

2026-01-28 18:19:51,416 | INFO | speech length: 15360
2026-01-28 18:19:51,464 | INFO | decoder input length: 23
2026-01-28 18:19:51,464 | INFO | max output length: 23
2026-01-28 18:19:51,465 | INFO | min output length: 2
2026-01-28 18:19:52,338 | INFO | end detected at 20
2026-01-28 18:19:52,339 | INFO | -12.71 * 0.5 =  -6.35 for decoder
2026-01-28 18:19:52,340 | INFO | -17.90 * 0.5 =  -8.95 for ctc
2026-01-28 18:19:52,340 | INFO | total log probability: -15.30
2026-01-28 18:19:52,340 | INFO | normalized log probability: -1.28
2026-01-28 18:19:52,340 | INFO | total number of ended hypotheses: 181
2026-01-28 18:19:52,340 | INFO | best hypo: ▁un▁jetais▁membre▁du▁père

2026-01-28 18:19:52,343 | INFO | speech length: 51840
2026-01-28 18:19:52,385 | INFO | decoder input length: 80
2026-01-28 18:19:52,386 | INFO | max output length: 80
2026-01-28 18:19:52,386 | INFO | min output length: 8
2026-01-28 18:19:54,104 | INFO | end detected at 33
2026-01-28 18:19:54,106 | INFO |  -6.88 * 0.5 =  -3.44 for decoder
2026-01-28 18:19:54,106 | INFO |  -8.30 * 0.5 =  -4.15 for ctc
2026-01-28 18:19:54,106 | INFO | total log probability: -7.59
2026-01-28 18:19:54,106 | INFO | normalized log probability: -0.30
2026-01-28 18:19:54,106 | INFO | total number of ended hypotheses: 185
2026-01-28 18:19:54,107 | INFO | best hypo: ▁ce▁tu▁fais▁que▁le▁barrage▁a▁une▁expérience▁épouvantable

2026-01-28 18:19:54,109 | INFO | speech length: 68640
2026-01-28 18:19:54,153 | INFO | decoder input length: 106
2026-01-28 18:19:54,153 | INFO | max output length: 106
2026-01-28 18:19:54,153 | INFO | min output length: 10
2026-01-28 18:19:56,516 | INFO | end detected at 43
2026-01-28 18:19:56,518 | INFO |  -5.36 * 0.5 =  -2.68 for decoder
2026-01-28 18:19:56,519 | INFO |  -8.79 * 0.5 =  -4.40 for ctc
2026-01-28 18:19:56,519 | INFO | total log probability: -7.08
2026-01-28 18:19:56,519 | INFO | normalized log probability: -0.20
2026-01-28 18:19:56,519 | INFO | total number of ended hypotheses: 191
2026-01-28 18:19:56,519 | INFO | best hypo: ▁pour▁la▁famille▁pour▁ma▁mère▁et▁j'en▁étais▁puni▁par▁ailleurs▁quand▁j'en▁ai▁pas

2026-01-28 18:19:56,522 | INFO | speech length: 12160
2026-01-28 18:19:56,567 | INFO | decoder input length: 18
2026-01-28 18:19:56,567 | INFO | max output length: 18
2026-01-28 18:19:56,567 | INFO | min output length: 1
2026-01-28 18:19:57,299 | INFO | adding <eos> in the last position in the loop
2026-01-28 18:19:57,310 | INFO | end detected at 17
2026-01-28 18:19:57,312 | INFO |  -3.30 * 0.5 =  -1.65 for decoder
2026-01-28 18:19:57,312 | INFO |  -4.68 * 0.5 =  -2.34 for ctc
2026-01-28 18:19:57,312 | INFO | total log probability: -3.99
2026-01-28 18:19:57,312 | INFO | normalized log probability: -0.33
2026-01-28 18:19:57,312 | INFO | total number of ended hypotheses: 170
2026-01-28 18:19:57,312 | INFO | best hypo: ▁j'en▁ai▁écrit

2026-01-28 18:19:57,314 | INFO | speech length: 117280
2026-01-28 18:19:57,381 | INFO | decoder input length: 182
2026-01-28 18:19:57,381 | INFO | max output length: 182
2026-01-28 18:19:57,381 | INFO | min output length: 18
2026-01-28 18:20:02,599 | INFO | end detected at 49
2026-01-28 18:20:02,601 | INFO | -15.41 * 0.5 =  -7.70 for decoder
2026-01-28 18:20:02,601 | INFO |  -8.27 * 0.5 =  -4.13 for ctc
2026-01-28 18:20:02,601 | INFO | total log probability: -11.84
2026-01-28 18:20:02,601 | INFO | normalized log probability: -0.28
2026-01-28 18:20:02,602 | INFO | total number of ended hypotheses: 174
2026-01-28 18:20:02,603 | INFO | best hypo: ▁revenons▁à▁la▁main▁la▁main▁nous▁sommes▁à▁nous▁sommes▁à▁saïgon▁entre▁les▁deux▁guerres▁dans▁les▁années▁vingt

2026-01-28 18:20:02,607 | INFO | speech length: 94560
2026-01-28 18:20:02,655 | INFO | decoder input length: 147
2026-01-28 18:20:02,656 | INFO | max output length: 147
2026-01-28 18:20:02,656 | INFO | min output length: 14
2026-01-28 18:20:06,850 | INFO | end detected at 49
2026-01-28 18:20:06,851 | INFO |  -4.89 * 0.5 =  -2.44 for decoder
2026-01-28 18:20:06,851 | INFO |  -0.73 * 0.5 =  -0.37 for ctc
2026-01-28 18:20:06,851 | INFO | total log probability: -2.81
2026-01-28 18:20:06,851 | INFO | normalized log probability: -0.06
2026-01-28 18:20:06,852 | INFO | total number of ended hypotheses: 153
2026-01-28 18:20:06,852 | INFO | best hypo: ▁vous▁avez▁quinze▁ans▁et▁demi▁et▁vous▁êtes▁sur▁un▁bac▁qui▁traverse▁le▁fleuve▁c'est▁dire▁le▁méconte

2026-01-28 18:20:06,855 | INFO | speech length: 42400
2026-01-28 18:20:06,896 | INFO | decoder input length: 65
2026-01-28 18:20:06,896 | INFO | max output length: 65
2026-01-28 18:20:06,897 | INFO | min output length: 6
2026-01-28 18:20:08,215 | INFO | end detected at 31
2026-01-28 18:20:08,216 | INFO |  -5.07 * 0.5 =  -2.53 for decoder
2026-01-28 18:20:08,216 | INFO | -11.49 * 0.5 =  -5.74 for ctc
2026-01-28 18:20:08,216 | INFO | total log probability: -8.28
2026-01-28 18:20:08,217 | INFO | normalized log probability: -0.34
2026-01-28 18:20:08,217 | INFO | total number of ended hypotheses: 197
2026-01-28 18:20:08,217 | INFO | best hypo: ▁et▁vous▁êtes▁drôlement▁attifé▁vous▁voulez▁pas

2026-01-28 18:20:08,219 | INFO | speech length: 51520
2026-01-28 18:20:08,280 | INFO | decoder input length: 80
2026-01-28 18:20:08,280 | INFO | max output length: 80
2026-01-28 18:20:08,280 | INFO | min output length: 8
2026-01-28 18:20:11,307 | INFO | end detected at 40
2026-01-28 18:20:11,311 | INFO |  -5.70 * 0.5 =  -2.85 for decoder
2026-01-28 18:20:11,311 | INFO | -17.80 * 0.5 =  -8.90 for ctc
2026-01-28 18:20:11,312 | INFO | total log probability: -11.75
2026-01-28 18:20:11,312 | INFO | normalized log probability: -0.33
2026-01-28 18:20:11,312 | INFO | total number of ended hypotheses: 169
2026-01-28 18:20:11,313 | INFO | best hypo: ▁vous▁êtes▁satisfait▁comment▁abordez▁nous▁j'ai▁un▁chapeau▁d'hommes

2026-01-28 18:20:11,316 | INFO | speech length: 119360
2026-01-28 18:20:11,382 | INFO | decoder input length: 186
2026-01-28 18:20:11,382 | INFO | max output length: 186
2026-01-28 18:20:11,382 | INFO | min output length: 18
2026-01-28 18:20:15,255 | INFO | end detected at 59
2026-01-28 18:20:15,256 | INFO | -14.34 * 0.5 =  -7.17 for decoder
2026-01-28 18:20:15,256 | INFO | -18.43 * 0.5 =  -9.22 for ctc
2026-01-28 18:20:15,256 | INFO | total log probability: -16.39
2026-01-28 18:20:15,256 | INFO | normalized log probability: -0.31
2026-01-28 18:20:15,256 | INFO | total number of ended hypotheses: 187
2026-01-28 18:20:15,257 | INFO | best hypo: ▁en▁fait▁oui▁un▁feutre▁rose▁couleur▁bois▁de▁rose▁au▁large▁rubans▁noirs▁et▁au▁pieds▁j'aillus▁son▁chapeau

2026-01-28 18:20:15,259 | INFO | speech length: 11360
2026-01-28 18:20:15,305 | INFO | decoder input length: 17
2026-01-28 18:20:15,306 | INFO | max output length: 17
2026-01-28 18:20:15,306 | INFO | min output length: 1
2026-01-28 18:20:15,903 | INFO | adding <eos> in the last position in the loop
2026-01-28 18:20:15,912 | INFO | end detected at 16
2026-01-28 18:20:15,913 | INFO |  -1.03 * 0.5 =  -0.52 for decoder
2026-01-28 18:20:15,913 | INFO |  -3.88 * 0.5 =  -1.94 for ctc
2026-01-28 18:20:15,913 | INFO | total log probability: -2.46
2026-01-28 18:20:15,913 | INFO | normalized log probability: -0.22
2026-01-28 18:20:15,913 | INFO | total number of ended hypotheses: 181
2026-01-28 18:20:15,914 | INFO | best hypo: ▁comment▁je▁l'ai▁vu

2026-01-28 18:20:15,915 | INFO | speech length: 77760
2026-01-28 18:20:15,952 | INFO | decoder input length: 121
2026-01-28 18:20:15,952 | INFO | max output length: 121
2026-01-28 18:20:15,952 | INFO | min output length: 12
2026-01-28 18:20:17,650 | INFO | end detected at 30
2026-01-28 18:20:17,652 | INFO |  -8.40 * 0.5 =  -4.20 for decoder
2026-01-28 18:20:17,652 | INFO |  -7.87 * 0.5 =  -3.93 for ctc
2026-01-28 18:20:17,652 | INFO | total log probability: -8.14
2026-01-28 18:20:17,652 | INFO | normalized log probability: -0.37
2026-01-28 18:20:17,652 | INFO | total number of ended hypotheses: 192
2026-01-28 18:20:17,653 | INFO | best hypo: ▁au▁piège▁des▁de▁lameor▁des▁sujets▁de▁pour▁danser

2026-01-28 18:20:17,655 | INFO | speech length: 309120
2026-01-28 18:20:17,704 | INFO | decoder input length: 482
2026-01-28 18:20:17,704 | INFO | max output length: 482
2026-01-28 18:20:17,704 | INFO | min output length: 48
2026-01-28 18:20:36,656 | INFO | end detected at 215
2026-01-28 18:20:36,657 | INFO | -412.86 * 0.5 = -206.43 for decoder
2026-01-28 18:20:36,657 | INFO | -150.04 * 0.5 = -75.02 for ctc
2026-01-28 18:20:36,657 | INFO | total log probability: -281.45
2026-01-28 18:20:36,657 | INFO | normalized log probability: -1.35
2026-01-28 18:20:36,657 | INFO | total number of ended hypotheses: 165
2026-01-28 18:20:36,660 | INFO | best hypo: ▁car▁je▁sais▁vraiment▁sur▁le▁bac▁évidemment▁vous▁devez▁faire▁on▁doit▁vous▁remarquer▁aujourd'hui▁on▁vous▁remarquer▁à▁peine▁c'est▁une▁mode▁qui▁se▁porte▁couramment▁chez▁jeune▁fille▁oui▁mais▁c'est▁vraie▁aujourd'hui▁vous▁remarquerez▁permer▁à▁cette▁époque▁on▁vous▁remarquer▁et▁d'ailleurs▁quelqu'un▁vous▁remarquer▁qu'▁est▁dans▁une▁grande▁limousine▁noire▁mais▁gréez▁et▁vos▁regards▁qui▁secroise▁et▁on▁peut▁à▁dire▁et▁ce▁qu'on▁peut▁à▁dire▁que▁cet▁échange▁de▁regard▁va▁changer▁votre

2026-01-28 18:20:36,663 | INFO | speech length: 9920
2026-01-28 18:20:36,696 | INFO | decoder input length: 15
2026-01-28 18:20:36,696 | INFO | max output length: 15
2026-01-28 18:20:36,696 | INFO | min output length: 1
2026-01-28 18:20:37,010 | INFO | end detected at 8
2026-01-28 18:20:37,011 | INFO |  -0.23 * 0.5 =  -0.12 for decoder
2026-01-28 18:20:37,011 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-28 18:20:37,011 | INFO | total log probability: -0.12
2026-01-28 18:20:37,011 | INFO | normalized log probability: -0.03
2026-01-28 18:20:37,011 | INFO | total number of ended hypotheses: 149
2026-01-28 18:20:37,012 | INFO | best hypo: ▁oui

2026-01-28 18:20:37,013 | INFO | speech length: 94720
2026-01-28 18:20:37,060 | INFO | decoder input length: 147
2026-01-28 18:20:37,061 | INFO | max output length: 147
2026-01-28 18:20:37,061 | INFO | min output length: 14
2026-01-28 18:20:39,837 | INFO | end detected at 53
2026-01-28 18:20:39,839 | INFO |  -6.62 * 0.5 =  -3.31 for decoder
2026-01-28 18:20:39,839 | INFO | -10.87 * 0.5 =  -5.44 for ctc
2026-01-28 18:20:39,839 | INFO | total log probability: -8.75
2026-01-28 18:20:39,839 | INFO | normalized log probability: -0.18
2026-01-28 18:20:39,839 | INFO | total number of ended hypotheses: 172
2026-01-28 18:20:39,840 | INFO | best hypo: ▁le▁livre▁s'applaît▁avant▁la▁photographie▁absolue▁il▁m'a▁été▁commandé▁ce▁livre▁intensif

2026-01-28 18:20:39,842 | INFO | speech length: 38080
2026-01-28 18:20:39,879 | INFO | decoder input length: 59
2026-01-28 18:20:39,879 | INFO | max output length: 59
2026-01-28 18:20:39,879 | INFO | min output length: 5
2026-01-28 18:20:41,283 | INFO | end detected at 34
2026-01-28 18:20:41,285 | INFO |  -2.24 * 0.5 =  -1.12 for decoder
2026-01-28 18:20:41,285 | INFO |  -0.19 * 0.5 =  -0.09 for ctc
2026-01-28 18:20:41,285 | INFO | total log probability: -1.22
2026-01-28 18:20:41,285 | INFO | normalized log probability: -0.04
2026-01-28 18:20:41,285 | INFO | total number of ended hypotheses: 169
2026-01-28 18:20:41,286 | INFO | best hypo: ▁et▁cette▁photographie▁absolue▁n'avait▁pas▁été▁prise

2026-01-28 18:20:41,287 | INFO | speech length: 13440
2026-01-28 18:20:41,331 | INFO | decoder input length: 20
2026-01-28 18:20:41,331 | INFO | max output length: 20
2026-01-28 18:20:41,331 | INFO | min output length: 2
2026-01-28 18:20:41,945 | INFO | end detected at 15
2026-01-28 18:20:41,947 | INFO |  -0.93 * 0.5 =  -0.47 for decoder
2026-01-28 18:20:41,947 | INFO |  -1.71 * 0.5 =  -0.86 for ctc
2026-01-28 18:20:41,947 | INFO | total log probability: -1.32
2026-01-28 18:20:41,947 | INFO | normalized log probability: -0.12
2026-01-28 18:20:41,947 | INFO | total number of ended hypotheses: 154
2026-01-28 18:20:41,947 | INFO | best hypo: ▁c'était▁celle▁là

2026-01-28 18:20:41,949 | INFO | speech length: 20800
2026-01-28 18:20:41,990 | INFO | decoder input length: 32
2026-01-28 18:20:41,990 | INFO | max output length: 32
2026-01-28 18:20:41,990 | INFO | min output length: 3
2026-01-28 18:20:42,658 | INFO | end detected at 17
2026-01-28 18:20:42,659 | INFO |  -0.92 * 0.5 =  -0.46 for decoder
2026-01-28 18:20:42,659 | INFO |  -0.29 * 0.5 =  -0.14 for ctc
2026-01-28 18:20:42,659 | INFO | total log probability: -0.60
2026-01-28 18:20:42,659 | INFO | normalized log probability: -0.05
2026-01-28 18:20:42,659 | INFO | total number of ended hypotheses: 145
2026-01-28 18:20:42,659 | INFO | best hypo: ▁cet▁instant▁là▁du▁bac

2026-01-28 18:20:42,661 | INFO | speech length: 51200
2026-01-28 18:20:42,697 | INFO | decoder input length: 79
2026-01-28 18:20:42,697 | INFO | max output length: 79
2026-01-28 18:20:42,697 | INFO | min output length: 7
2026-01-28 18:20:44,357 | INFO | end detected at 38
2026-01-28 18:20:44,358 | INFO |  -3.54 * 0.5 =  -1.77 for decoder
2026-01-28 18:20:44,359 | INFO |  -4.80 * 0.5 =  -2.40 for ctc
2026-01-28 18:20:44,359 | INFO | total log probability: -4.17
2026-01-28 18:20:44,359 | INFO | normalized log probability: -0.13
2026-01-28 18:20:44,359 | INFO | total number of ended hypotheses: 167
2026-01-28 18:20:44,359 | INFO | best hypo: ▁on▁n'avait▁rien▁vu▁qu'un▁homme▁une▁auto▁noir▁et▁une▁jeune▁fille

2026-01-28 18:20:44,361 | INFO | speech length: 24640
2026-01-28 18:20:44,402 | INFO | decoder input length: 38
2026-01-28 18:20:44,402 | INFO | max output length: 38
2026-01-28 18:20:44,402 | INFO | min output length: 3
2026-01-28 18:20:45,241 | INFO | end detected at 21
2026-01-28 18:20:45,244 | INFO |  -2.54 * 0.5 =  -1.27 for decoder
2026-01-28 18:20:45,244 | INFO |  -5.46 * 0.5 =  -2.73 for ctc
2026-01-28 18:20:45,244 | INFO | total log probability: -4.00
2026-01-28 18:20:45,244 | INFO | normalized log probability: -0.27
2026-01-28 18:20:45,244 | INFO | total number of ended hypotheses: 195
2026-01-28 18:20:45,245 | INFO | best hypo: ▁il▁est▁déclarent▁pour▁indigène▁mais

2026-01-28 18:20:45,247 | INFO | speech length: 206560
2026-01-28 18:20:45,291 | INFO | decoder input length: 322
2026-01-28 18:20:45,291 | INFO | max output length: 322
2026-01-28 18:20:45,291 | INFO | min output length: 32
2026-01-28 18:20:52,936 | INFO | end detected at 107
2026-01-28 18:20:52,938 | INFO | -142.18 * 0.5 = -71.09 for decoder
2026-01-28 18:20:52,938 | INFO | -86.41 * 0.5 = -43.20 for ctc
2026-01-28 18:20:52,938 | INFO | total log probability: -114.29
2026-01-28 18:20:52,938 | INFO | normalized log probability: -1.18
2026-01-28 18:20:52,938 | INFO | total number of ended hypotheses: 140
2026-01-28 18:20:52,940 | INFO | best hypo: ▁c'est▁le▁lac▁tout▁est▁parqué▁le▁fleuve▁une▁fois▁traversé▁qui▁yadan▁cria▁t▁il▁dans▁la▁voiture▁noire▁alors▁vous▁ordonnez▁c'est▁une▁maurice▁léon▁brûlée▁eh▁mais▁il▁n'en▁va▁plus▁de▁ces▁autour▁là▁huien▁de▁façon▁les▁musées

2026-01-28 18:20:52,942 | INFO | speech length: 116001
2026-01-28 18:20:52,978 | INFO | decoder input length: 180
2026-01-28 18:20:52,978 | INFO | max output length: 180
2026-01-28 18:20:52,978 | INFO | min output length: 18
2026-01-28 18:20:56,824 | INFO | end detected at 67
2026-01-28 18:20:56,825 | INFO | -16.69 * 0.5 =  -8.34 for decoder
2026-01-28 18:20:56,825 | INFO | -16.42 * 0.5 =  -8.21 for ctc
2026-01-28 18:20:56,825 | INFO | total log probability: -16.55
2026-01-28 18:20:56,825 | INFO | normalized log probability: -0.27
2026-01-28 18:20:56,826 | INFO | total number of ended hypotheses: 161
2026-01-28 18:20:56,826 | INFO | best hypo: ▁je▁pense▁que▁si▁on▁s'était▁acis▁l'un▁devant▁l'autre▁on▁aurait▁été▁à▁cette▁distance▁là▁avec▁un▁doto▁dans▁cet▁auto▁si▁gigantesque

2026-01-28 18:20:56,829 | INFO | speech length: 16000
2026-01-28 18:20:56,867 | INFO | decoder input length: 24
2026-01-28 18:20:56,867 | INFO | max output length: 24
2026-01-28 18:20:56,867 | INFO | min output length: 2
2026-01-28 18:20:57,378 | INFO | end detected at 13
2026-01-28 18:20:57,379 | INFO |  -1.08 * 0.5 =  -0.54 for decoder
2026-01-28 18:20:57,379 | INFO |  -1.76 * 0.5 =  -0.88 for ctc
2026-01-28 18:20:57,379 | INFO | total log probability: -1.42
2026-01-28 18:20:57,379 | INFO | normalized log probability: -0.16
2026-01-28 18:20:57,379 | INFO | total number of ended hypotheses: 149
2026-01-28 18:20:57,379 | INFO | best hypo: ▁dans▁qui▁vous▁regarde

2026-01-28 18:20:57,381 | INFO | speech length: 14560
2026-01-28 18:20:57,446 | INFO | decoder input length: 22
2026-01-28 18:20:57,446 | INFO | max output length: 22
2026-01-28 18:20:57,447 | INFO | min output length: 2
2026-01-28 18:20:58,312 | INFO | end detected at 13
2026-01-28 18:20:58,313 | INFO |  -0.62 * 0.5 =  -0.31 for decoder
2026-01-28 18:20:58,313 | INFO |  -0.05 * 0.5 =  -0.02 for ctc
2026-01-28 18:20:58,314 | INFO | total log probability: -0.34
2026-01-28 18:20:58,314 | INFO | normalized log probability: -0.04
2026-01-28 18:20:58,314 | INFO | total number of ended hypotheses: 148
2026-01-28 18:20:58,314 | INFO | best hypo: ▁c'est▁un▁chinois

2026-01-28 18:20:58,317 | INFO | speech length: 46880
2026-01-28 18:20:58,363 | INFO | decoder input length: 72
2026-01-28 18:20:58,363 | INFO | max output length: 72
2026-01-28 18:20:58,363 | INFO | min output length: 7
2026-01-28 18:21:00,204 | INFO | end detected at 37
2026-01-28 18:21:00,206 | INFO |  -7.24 * 0.5 =  -3.62 for decoder
2026-01-28 18:21:00,207 | INFO |  -9.75 * 0.5 =  -4.87 for ctc
2026-01-28 18:21:00,207 | INFO | total log probability: -8.49
2026-01-28 18:21:00,207 | INFO | normalized log probability: -0.27
2026-01-28 18:21:00,207 | INFO | total number of ended hypotheses: 171
2026-01-28 18:21:00,207 | INFO | best hypo: ▁mais▁vous▁savez▁pas▁qui▁dirige▁à▁ce▁moment▁si▁je▁le▁sais▁à▁l'automobile

2026-01-28 18:21:00,210 | INFO | speech length: 11360
2026-01-28 18:21:00,304 | INFO | decoder input length: 17
2026-01-28 18:21:00,305 | INFO | max output length: 17
2026-01-28 18:21:00,305 | INFO | min output length: 1
2026-01-28 18:21:01,162 | INFO | end detected at 15
2026-01-28 18:21:01,163 | INFO |  -7.38 * 0.5 =  -3.69 for decoder
2026-01-28 18:21:01,163 | INFO |  -5.77 * 0.5 =  -2.88 for ctc
2026-01-28 18:21:01,163 | INFO | total log probability: -6.58
2026-01-28 18:21:01,163 | INFO | normalized log probability: -0.73
2026-01-28 18:21:01,164 | INFO | total number of ended hypotheses: 173
2026-01-28 18:21:01,164 | INFO | best hypo: ▁les▁purchoffer

2026-01-28 18:21:01,166 | INFO | speech length: 21120
2026-01-28 18:21:01,201 | INFO | decoder input length: 32
2026-01-28 18:21:01,201 | INFO | max output length: 32
2026-01-28 18:21:01,201 | INFO | min output length: 3
2026-01-28 18:21:02,092 | INFO | end detected at 20
2026-01-28 18:21:02,093 | INFO |  -1.30 * 0.5 =  -0.65 for decoder
2026-01-28 18:21:02,094 | INFO |  -0.14 * 0.5 =  -0.07 for ctc
2026-01-28 18:21:02,094 | INFO | total log probability: -0.72
2026-01-28 18:21:02,094 | INFO | normalized log probability: -0.05
2026-01-28 18:21:02,094 | INFO | total number of ended hypotheses: 165
2026-01-28 18:21:02,094 | INFO | best hypo: ▁il▁a▁douze▁ans▁puisque▁vous

2026-01-28 18:21:02,096 | INFO | speech length: 31200
2026-01-28 18:21:02,132 | INFO | decoder input length: 48
2026-01-28 18:21:02,132 | INFO | max output length: 48
2026-01-28 18:21:02,132 | INFO | min output length: 4
2026-01-28 18:21:03,231 | INFO | end detected at 27
2026-01-28 18:21:03,232 | INFO |  -1.92 * 0.5 =  -0.96 for decoder
2026-01-28 18:21:03,232 | INFO |  -9.90 * 0.5 =  -4.95 for ctc
2026-01-28 18:21:03,232 | INFO | total log probability: -5.91
2026-01-28 18:21:03,232 | INFO | normalized log probability: -0.26
2026-01-28 18:21:03,232 | INFO | total number of ended hypotheses: 170
2026-01-28 18:21:03,233 | INFO | best hypo: ▁j'ai▁quinze▁ans▁il▁a▁vingt▁sept▁ans▁environ

2026-01-28 18:21:03,235 | INFO | speech length: 77920
2026-01-28 18:21:03,278 | INFO | decoder input length: 121
2026-01-28 18:21:03,279 | INFO | max output length: 121
2026-01-28 18:21:03,279 | INFO | min output length: 12
2026-01-28 18:21:05,376 | INFO | end detected at 42
2026-01-28 18:21:05,377 | INFO |  -5.55 * 0.5 =  -2.78 for decoder
2026-01-28 18:21:05,377 | INFO |  -2.96 * 0.5 =  -1.48 for ctc
2026-01-28 18:21:05,378 | INFO | total log probability: -4.26
2026-01-28 18:21:05,378 | INFO | normalized log probability: -0.11
2026-01-28 18:21:05,378 | INFO | total number of ended hypotheses: 169
2026-01-28 18:21:05,378 | INFO | best hypo: ▁et▁vous▁savez▁on▁impression▁que▁vous▁savez▁qu'il▁va▁devenir▁votre▁amant▁votre▁première▁amant

2026-01-28 18:21:05,380 | INFO | speech length: 9280
2026-01-28 18:21:05,410 | INFO | decoder input length: 14
2026-01-28 18:21:05,410 | INFO | max output length: 14
2026-01-28 18:21:05,410 | INFO | min output length: 1
2026-01-28 18:21:05,803 | INFO | end detected at 10
2026-01-28 18:21:05,804 | INFO |  -0.37 * 0.5 =  -0.18 for decoder
2026-01-28 18:21:05,804 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-28 18:21:05,804 | INFO | total log probability: -0.19
2026-01-28 18:21:05,804 | INFO | normalized log probability: -0.03
2026-01-28 18:21:05,804 | INFO | total number of ended hypotheses: 140
2026-01-28 18:21:05,805 | INFO | best hypo: ▁tout▁de▁suite

2026-01-28 18:21:05,806 | INFO | speech length: 51360
2026-01-28 18:21:05,859 | INFO | decoder input length: 79
2026-01-28 18:21:05,859 | INFO | max output length: 79
2026-01-28 18:21:05,859 | INFO | min output length: 7
2026-01-28 18:21:07,285 | INFO | end detected at 32
2026-01-28 18:21:07,286 | INFO |  -6.57 * 0.5 =  -3.28 for decoder
2026-01-28 18:21:07,286 | INFO | -10.20 * 0.5 =  -5.10 for ctc
2026-01-28 18:21:07,287 | INFO | total log probability: -8.38
2026-01-28 18:21:07,287 | INFO | normalized log probability: -0.32
2026-01-28 18:21:07,287 | INFO | total number of ended hypotheses: 185
2026-01-28 18:21:07,287 | INFO | best hypo: ▁d'elle▁que▁je▁suis▁à▁prêt▁qu'il▁n'avait▁pas▁déplu

2026-01-28 18:21:07,289 | INFO | speech length: 11360
2026-01-28 18:21:07,332 | INFO | decoder input length: 17
2026-01-28 18:21:07,332 | INFO | max output length: 17
2026-01-28 18:21:07,332 | INFO | min output length: 1
2026-01-28 18:21:07,801 | INFO | end detected at 12
2026-01-28 18:21:07,802 | INFO |  -0.77 * 0.5 =  -0.39 for decoder
2026-01-28 18:21:07,802 | INFO |  -1.74 * 0.5 =  -0.87 for ctc
2026-01-28 18:21:07,802 | INFO | total log probability: -1.26
2026-01-28 18:21:07,802 | INFO | normalized log probability: -0.18
2026-01-28 18:21:07,803 | INFO | total number of ended hypotheses: 167
2026-01-28 18:21:07,803 | INFO | best hypo: ▁jean▁le▁bac

2026-01-28 18:21:07,804 | INFO | speech length: 40000
2026-01-28 18:21:07,852 | INFO | decoder input length: 62
2026-01-28 18:21:07,853 | INFO | max output length: 62
2026-01-28 18:21:07,853 | INFO | min output length: 6
2026-01-28 18:21:09,233 | INFO | end detected at 33
2026-01-28 18:21:09,234 | INFO |  -3.10 * 0.5 =  -1.55 for decoder
2026-01-28 18:21:09,234 | INFO |  -1.48 * 0.5 =  -0.74 for ctc
2026-01-28 18:21:09,234 | INFO | total log probability: -2.29
2026-01-28 18:21:09,234 | INFO | normalized log probability: -0.08
2026-01-28 18:21:09,234 | INFO | total number of ended hypotheses: 146
2026-01-28 18:21:09,235 | INFO | best hypo: ▁et▁sur▁ce▁moment▁je▁n'avais▁pas▁d'avis▁sauf▁que▁j'étais

2026-01-28 18:21:09,236 | INFO | speech length: 16640
2026-01-28 18:21:09,296 | INFO | decoder input length: 25
2026-01-28 18:21:09,297 | INFO | max output length: 25
2026-01-28 18:21:09,297 | INFO | min output length: 2
2026-01-28 18:21:10,447 | INFO | end detected at 17
2026-01-28 18:21:10,450 | INFO |  -5.09 * 0.5 =  -2.55 for decoder
2026-01-28 18:21:10,450 | INFO |  -9.33 * 0.5 =  -4.66 for ctc
2026-01-28 18:21:10,450 | INFO | total log probability: -7.21
2026-01-28 18:21:10,450 | INFO | normalized log probability: -0.66
2026-01-28 18:21:10,451 | INFO | total number of ended hypotheses: 204
2026-01-28 18:21:10,451 | INFO | best hypo: ▁on▁s'est▁donné▁par

2026-01-28 18:21:10,455 | INFO | speech length: 9760
2026-01-28 18:21:10,500 | INFO | decoder input length: 14
2026-01-28 18:21:10,501 | INFO | max output length: 14
2026-01-28 18:21:10,501 | INFO | min output length: 1
2026-01-28 18:21:11,349 | INFO | adding <eos> in the last position in the loop
2026-01-28 18:21:11,366 | INFO | end detected at 13
2026-01-28 18:21:11,370 | INFO |  -0.83 * 0.5 =  -0.41 for decoder
2026-01-28 18:21:11,370 | INFO |  -8.16 * 0.5 =  -4.08 for ctc
2026-01-28 18:21:11,371 | INFO | total log probability: -4.49
2026-01-28 18:21:11,371 | INFO | normalized log probability: -0.56
2026-01-28 18:21:11,371 | INFO | total number of ended hypotheses: 213
2026-01-28 18:21:11,371 | INFO | best hypo: ▁j'irai

2026-01-28 18:21:11,375 | INFO | speech length: 15840
2026-01-28 18:21:11,436 | INFO | decoder input length: 24
2026-01-28 18:21:11,436 | INFO | max output length: 24
2026-01-28 18:21:11,436 | INFO | min output length: 2
2026-01-28 18:21:12,327 | INFO | end detected at 13
2026-01-28 18:21:12,330 | INFO |  -0.57 * 0.5 =  -0.28 for decoder
2026-01-28 18:21:12,330 | INFO |  -0.11 * 0.5 =  -0.05 for ctc
2026-01-28 18:21:12,330 | INFO | total log probability: -0.34
2026-01-28 18:21:12,330 | INFO | normalized log probability: -0.04
2026-01-28 18:21:12,330 | INFO | total number of ended hypotheses: 141
2026-01-28 18:21:12,330 | INFO | best hypo: ▁deux▁milliardaires

2026-01-28 18:21:12,333 | INFO | speech length: 22560
2026-01-28 18:21:12,386 | INFO | decoder input length: 34
2026-01-28 18:21:12,386 | INFO | max output length: 34
2026-01-28 18:21:12,386 | INFO | min output length: 3
2026-01-28 18:21:13,467 | INFO | end detected at 19
2026-01-28 18:21:13,469 | INFO |  -5.36 * 0.5 =  -2.68 for decoder
2026-01-28 18:21:13,469 | INFO |  -7.36 * 0.5 =  -3.68 for ctc
2026-01-28 18:21:13,469 | INFO | total log probability: -6.36
2026-01-28 18:21:13,469 | INFO | normalized log probability: -0.45
2026-01-28 18:21:13,469 | INFO | total number of ended hypotheses: 160
2026-01-28 18:21:13,469 | INFO | best hypo: ▁un▁péton▁est▁un▁peu▁ppâté

2026-01-28 18:21:13,472 | INFO | speech length: 14240
2026-01-28 18:21:13,517 | INFO | decoder input length: 21
2026-01-28 18:21:13,518 | INFO | max output length: 21
2026-01-28 18:21:13,518 | INFO | min output length: 2
2026-01-28 18:21:14,099 | INFO | end detected at 13
2026-01-28 18:21:14,101 | INFO |  -1.69 * 0.5 =  -0.84 for decoder
2026-01-28 18:21:14,101 | INFO |  -3.16 * 0.5 =  -1.58 for ctc
2026-01-28 18:21:14,101 | INFO | total log probability: -2.42
2026-01-28 18:21:14,101 | INFO | normalized log probability: -0.30
2026-01-28 18:21:14,101 | INFO | total number of ended hypotheses: 160
2026-01-28 18:21:14,101 | INFO | best hypo: ▁il▁y▁a▁tiré

2026-01-28 18:21:14,117 | INFO | Chunk: 0 | WER=25.000000 | S=9 D=5 I=0
2026-01-28 18:21:14,117 | INFO | Chunk: 1 | WER=50.000000 | S=1 D=0 I=0
2026-01-28 18:21:14,117 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:21:14,118 | INFO | Chunk: 3 | WER=10.000000 | S=0 D=1 I=0
2026-01-28 18:21:14,118 | INFO | Chunk: 4 | WER=33.333333 | S=0 D=1 I=1
2026-01-28 18:21:14,118 | INFO | Chunk: 5 | WER=80.000000 | S=1 D=1 I=2
2026-01-28 18:21:14,119 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:21:14,119 | INFO | Chunk: 7 | WER=16.666667 | S=0 D=0 I=1
2026-01-28 18:21:14,127 | INFO | Chunk: 8 | WER=50.833333 | S=27 D=33 I=1
2026-01-28 18:21:14,128 | INFO | Chunk: 9 | WER=57.142857 | S=9 D=3 I=0
2026-01-28 18:21:14,129 | INFO | Chunk: 10 | WER=37.500000 | S=3 D=0 I=0
2026-01-28 18:21:14,129 | INFO | Chunk: 11 | WER=16.666667 | S=1 D=0 I=0
2026-01-28 18:21:14,129 | INFO | Chunk: 12 | WER=54.545455 | S=2 D=3 I=1
2026-01-28 18:21:14,129 | INFO | Chunk: 13 | WER=100.000000 | S=3 D=0 I=0
2026-01-28 18:21:14,130 | INFO | Chunk: 14 | WER=11.111111 | S=0 D=1 I=0
2026-01-28 18:21:14,130 | INFO | Chunk: 15 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:21:14,130 | INFO | Chunk: 16 | WER=25.000000 | S=0 D=1 I=0
2026-01-28 18:21:14,131 | INFO | Chunk: 17 | WER=40.000000 | S=2 D=4 I=0
2026-01-28 18:21:14,131 | INFO | Chunk: 18 | WER=20.000000 | S=1 D=1 I=0
2026-01-28 18:21:14,131 | INFO | Chunk: 19 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:21:14,132 | INFO | Chunk: 20 | WER=50.000000 | S=2 D=0 I=0
2026-01-28 18:21:14,138 | INFO | Chunk: 21 | WER=42.553191 | S=28 D=12 I=0
2026-01-28 18:21:14,142 | INFO | Chunk: 22 | WER=56.000000 | S=25 D=14 I=3
2026-01-28 18:21:14,143 | INFO | Chunk: 23 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:21:14,143 | INFO | Chunk: 24 | WER=40.000000 | S=2 D=0 I=0
2026-01-28 18:21:14,143 | INFO | Chunk: 25 | WER=125.000000 | S=2 D=1 I=2
2026-01-28 18:21:14,144 | INFO | Chunk: 26 | WER=33.333333 | S=2 D=2 I=0
2026-01-28 18:21:14,144 | INFO | Chunk: 27 | WER=33.333333 | S=2 D=2 I=2
2026-01-28 18:21:14,145 | INFO | Chunk: 28 | WER=50.000000 | S=0 D=1 I=1
2026-01-28 18:21:14,145 | INFO | Chunk: 29 | WER=30.434783 | S=5 D=2 I=0
2026-01-28 18:21:14,146 | INFO | Chunk: 30 | WER=13.636364 | S=2 D=1 I=0
2026-01-28 18:21:14,147 | INFO | Chunk: 31 | WER=30.000000 | S=1 D=2 I=0
2026-01-28 18:21:14,147 | INFO | Chunk: 32 | WER=25.000000 | S=3 D=0 I=0
2026-01-28 18:21:14,148 | INFO | Chunk: 33 | WER=39.130435 | S=7 D=2 I=0
2026-01-28 18:21:14,148 | INFO | Chunk: 34 | WER=50.000000 | S=1 D=0 I=1
2026-01-28 18:21:14,148 | INFO | Chunk: 35 | WER=66.666667 | S=5 D=5 I=0
2026-01-28 18:21:14,156 | INFO | Chunk: 36 | WER=31.578947 | S=19 D=8 I=3
2026-01-28 18:21:14,156 | INFO | Chunk: 37 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:21:14,156 | INFO | Chunk: 38 | WER=12.500000 | S=2 D=0 I=0
2026-01-28 18:21:14,157 | INFO | Chunk: 39 | WER=12.500000 | S=0 D=0 I=1
2026-01-28 18:21:14,157 | INFO | Chunk: 40 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:21:14,157 | INFO | Chunk: 41 | WER=33.333333 | S=1 D=1 I=0
2026-01-28 18:21:14,158 | INFO | Chunk: 42 | WER=12.500000 | S=1 D=1 I=0
2026-01-28 18:21:14,158 | INFO | Chunk: 43 | WER=66.666667 | S=4 D=0 I=0
2026-01-28 18:21:14,161 | INFO | Chunk: 44 | WER=49.230769 | S=13 D=19 I=0
2026-01-28 18:21:14,162 | INFO | Chunk: 45 | WER=29.032258 | S=6 D=3 I=0
2026-01-28 18:21:14,162 | INFO | Chunk: 46 | WER=25.000000 | S=1 D=0 I=0
2026-01-28 18:21:14,163 | INFO | Chunk: 47 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:21:14,163 | INFO | Chunk: 48 | WER=30.000000 | S=2 D=4 I=0
2026-01-28 18:21:14,163 | INFO | Chunk: 49 | WER=100.000000 | S=2 D=2 I=0
2026-01-28 18:21:14,164 | INFO | Chunk: 50 | WER=37.500000 | S=1 D=2 I=0
2026-01-28 18:21:14,164 | INFO | Chunk: 51 | WER=11.111111 | S=0 D=0 I=1
2026-01-28 18:21:14,165 | INFO | Chunk: 52 | WER=22.222222 | S=1 D=2 I=1
2026-01-28 18:21:14,165 | INFO | Chunk: 53 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:21:14,165 | INFO | Chunk: 54 | WER=60.000000 | S=7 D=2 I=0
2026-01-28 18:21:14,166 | INFO | Chunk: 55 | WER=33.333333 | S=1 D=0 I=0
2026-01-28 18:21:14,166 | INFO | Chunk: 56 | WER=14.285714 | S=2 D=0 I=0
2026-01-28 18:21:14,166 | INFO | Chunk: 57 | WER=250.000000 | S=2 D=0 I=3
2026-01-28 18:21:14,166 | INFO | Chunk: 58 | WER=100.000000 | S=2 D=0 I=0
2026-01-28 18:21:14,167 | INFO | Chunk: 59 | WER=100.000000 | S=2 D=0 I=0
2026-01-28 18:21:14,167 | INFO | Chunk: 60 | WER=62.500000 | S=3 D=2 I=0
2026-01-28 18:21:14,167 | INFO | Chunk: 61 | WER=200.000000 | S=2 D=0 I=2
2026-01-28 18:21:14,852 | INFO | File: Rhap-D2010.wav | WER=37.808490 | S=225 D=138 I=20
2026-01-28 18:21:14,853 | INFO | ------------------------------
2026-01-28 18:21:14,853 | INFO | Conf cv Done!
2026-01-28 18:21:15,011 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 18:21:15,040 | INFO | Vocabulary size: 47
2026-01-28 18:21:16,224 | INFO | Gradient checkpoint layers: []
2026-01-28 18:21:16,855 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 18:21:16,860 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 18:21:16,860 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 18:21:16,860 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 18:21:16,863 | INFO | speech length: 220320
2026-01-28 18:21:16,903 | INFO | decoder input length: 343
2026-01-28 18:21:16,903 | INFO | max output length: 343
2026-01-28 18:21:16,903 | INFO | min output length: 34
2026-01-28 18:21:35,770 | INFO | end detected at 310
2026-01-28 18:21:35,773 | INFO | -33.07 * 0.5 = -16.53 for decoder
2026-01-28 18:21:35,773 | INFO | -32.04 * 0.5 = -16.02 for ctc
2026-01-28 18:21:35,773 | INFO | total log probability: -32.55
2026-01-28 18:21:35,773 | INFO | normalized log probability: -0.11
2026-01-28 18:21:35,773 | INFO | total number of ended hypotheses: 201
2026-01-28 18:21:35,777 | INFO | best hypo: alors<space>je<space>sais<space>bien<space>marie<space>duhase<space>que<space>c'est<space>vrai<space>vous<space>avez<space>obtenu<space>d'autres<space>succès<space>j'en<space>énuméré<space>d'ailleurs<space>avance<space>générique<space>mais<space>quand<space>même<space>celui<space>là<space>cent<space>mille<space>exemplaires<space>en<space>quatre<space>semaines<space>c'est<space>absolument<space>fabuleux<space>est<space>ce<space>que<space>votre<space>sensibilité<space>qui<space>est<space>très<space>aiguë<space>très<space>pointue<space>avait<space>un<space>peu<space>pressentie

2026-01-28 18:21:35,780 | INFO | speech length: 16320
2026-01-28 18:21:35,817 | INFO | decoder input length: 25
2026-01-28 18:21:35,817 | INFO | max output length: 25
2026-01-28 18:21:35,817 | INFO | min output length: 2
2026-01-28 18:21:36,392 | INFO | end detected at 16
2026-01-28 18:21:36,394 | INFO |  -1.27 * 0.5 =  -0.63 for decoder
2026-01-28 18:21:36,394 | INFO |  -0.83 * 0.5 =  -0.42 for ctc
2026-01-28 18:21:36,394 | INFO | total log probability: -1.05
2026-01-28 18:21:36,394 | INFO | normalized log probability: -0.10
2026-01-28 18:21:36,394 | INFO | total number of ended hypotheses: 158
2026-01-28 18:21:36,394 | INFO | best hypo: non<space>voilà

2026-01-28 18:21:36,396 | INFO | speech length: 20640
2026-01-28 18:21:36,437 | INFO | decoder input length: 31
2026-01-28 18:21:36,438 | INFO | max output length: 31
2026-01-28 18:21:36,438 | INFO | min output length: 3
2026-01-28 18:21:37,416 | INFO | end detected at 27
2026-01-28 18:21:37,417 | INFO |  -1.76 * 0.5 =  -0.88 for decoder
2026-01-28 18:21:37,417 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-28 18:21:37,417 | INFO | total log probability: -0.88
2026-01-28 18:21:37,417 | INFO | normalized log probability: -0.04
2026-01-28 18:21:37,417 | INFO | total number of ended hypotheses: 147
2026-01-28 18:21:37,417 | INFO | best hypo: non<space>tout<space>au<space>contraire

2026-01-28 18:21:37,419 | INFO | speech length: 44000
2026-01-28 18:21:37,457 | INFO | decoder input length: 68
2026-01-28 18:21:37,457 | INFO | max output length: 68
2026-01-28 18:21:37,457 | INFO | min output length: 6
2026-01-28 18:21:39,342 | INFO | end detected at 49
2026-01-28 18:21:39,343 | INFO |  -3.58 * 0.5 =  -1.79 for decoder
2026-01-28 18:21:39,343 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-28 18:21:39,343 | INFO | total log probability: -1.80
2026-01-28 18:21:39,343 | INFO | normalized log probability: -0.04
2026-01-28 18:21:39,343 | INFO | total number of ended hypotheses: 146
2026-01-28 18:21:39,344 | INFO | best hypo: après<space>le<space>le<space>succès<space>de<space>la<space>maladie<space>de<space>la<space>mort

2026-01-28 18:21:39,346 | INFO | speech length: 21120
2026-01-28 18:21:39,384 | INFO | decoder input length: 32
2026-01-28 18:21:39,384 | INFO | max output length: 32
2026-01-28 18:21:39,384 | INFO | min output length: 3
2026-01-28 18:21:40,446 | INFO | adding <eos> in the last position in the loop
2026-01-28 18:21:40,455 | INFO | no hypothesis. Finish decoding.
2026-01-28 18:21:40,456 | INFO |  -4.71 * 0.5 =  -2.35 for decoder
2026-01-28 18:21:40,456 | INFO |  -4.63 * 0.5 =  -2.32 for ctc
2026-01-28 18:21:40,456 | INFO | total log probability: -4.67
2026-01-28 18:21:40,456 | INFO | normalized log probability: -0.15
2026-01-28 18:21:40,456 | INFO | total number of ended hypotheses: 99
2026-01-28 18:21:40,457 | INFO | best hypo: j'a<space>j'avais<space>peur<space>pour<space>ce<space>livre

2026-01-28 18:21:40,458 | INFO | speech length: 31040
2026-01-28 18:21:40,501 | INFO | decoder input length: 48
2026-01-28 18:21:40,501 | INFO | max output length: 48
2026-01-28 18:21:40,501 | INFO | min output length: 4
2026-01-28 18:21:42,164 | INFO | adding <eos> in the last position in the loop
2026-01-28 18:21:42,175 | INFO | end detected at 47
2026-01-28 18:21:42,177 | INFO |  -7.22 * 0.5 =  -3.61 for decoder
2026-01-28 18:21:42,177 | INFO | -21.71 * 0.5 = -10.85 for ctc
2026-01-28 18:21:42,177 | INFO | total log probability: -14.46
2026-01-28 18:21:42,177 | INFO | normalized log probability: -0.40
2026-01-28 18:21:42,177 | INFO | total number of ended hypotheses: 212
2026-01-28 18:21:42,178 | INFO | best hypo: euh<space>ça<space>a<space>une<space>peur<space>relative<space>marquée

2026-01-28 18:21:42,180 | INFO | speech length: 25760
2026-01-28 18:21:42,233 | INFO | decoder input length: 39
2026-01-28 18:21:42,233 | INFO | max output length: 39
2026-01-28 18:21:42,234 | INFO | min output length: 3
2026-01-28 18:21:43,694 | INFO | adding <eos> in the last position in the loop
2026-01-28 18:21:43,703 | INFO | no hypothesis. Finish decoding.
2026-01-28 18:21:43,704 | INFO |  -3.33 * 0.5 =  -1.66 for decoder
2026-01-28 18:21:43,705 | INFO |  -0.11 * 0.5 =  -0.05 for ctc
2026-01-28 18:21:43,705 | INFO | total log probability: -1.72
2026-01-28 18:21:43,705 | INFO | normalized log probability: -0.05
2026-01-28 18:21:43,705 | INFO | total number of ended hypotheses: 122
2026-01-28 18:21:43,705 | INFO | best hypo: mais<space>j'avais<space>peur<space>qu'il<space>ne<space>soit<space>pas

2026-01-28 18:21:43,707 | INFO | speech length: 23680
2026-01-28 18:21:43,751 | INFO | decoder input length: 36
2026-01-28 18:21:43,751 | INFO | max output length: 36
2026-01-28 18:21:43,751 | INFO | min output length: 3
2026-01-28 18:21:44,942 | INFO | adding <eos> in the last position in the loop
2026-01-28 18:21:44,949 | INFO | no hypothesis. Finish decoding.
2026-01-28 18:21:44,950 | INFO | -13.95 * 0.5 =  -6.97 for decoder
2026-01-28 18:21:44,950 | INFO | -12.31 * 0.5 =  -6.16 for ctc
2026-01-28 18:21:44,950 | INFO | total log probability: -13.13
2026-01-28 18:21:44,950 | INFO | normalized log probability: -0.36
2026-01-28 18:21:44,950 | INFO | total number of ended hypotheses: 67
2026-01-28 18:21:44,951 | INFO | best hypo: celui<space>que<space>ls<space>gens<space>attendent<space>de<space>moi

2026-01-28 18:21:44,952 | INFO | speech length: 455200
2026-01-28 18:21:44,991 | INFO | decoder input length: 710
2026-01-28 18:21:44,991 | INFO | max output length: 710
2026-01-28 18:21:44,991 | INFO | min output length: 71
2026-01-28 18:22:38,268 | INFO | end detected at 557
2026-01-28 18:22:38,269 | INFO | -612.20 * 0.5 = -306.10 for decoder
2026-01-28 18:22:38,269 | INFO | -153.29 * 0.5 = -76.64 for ctc
2026-01-28 18:22:38,269 | INFO | total log probability: -382.74
2026-01-28 18:22:38,269 | INFO | normalized log probability: -0.69
2026-01-28 18:22:38,269 | INFO | total number of ended hypotheses: 161
2026-01-28 18:22:38,276 | INFO | best hypo: et<space>puis<space>voilà<space>et<space>alors<space>la<space>critique<space>déferlande<space>c'est<space>une<space>joie<space>croyez<space>à<space>quelqu'un<space>dit<space>du<space>mal<space>dirah<space>bon<space>oui<space>alors<space>bon<space>il<space>a<space>fin<space>il<space>en<space>faut<space>bien<space>non<space>c'est<space>et<space>des<space>terres<space>qui<space>me<space>l'a<space>dit<space>oui<space>bon<space>quelqu'un<space>a<space>dit<space>euh<space>on<space>ne<space>comprend<space>pas<space>qu'un<space>éditeur<space>laisse<space>passer<space>des<space>photos<space>de<space>grammaire<space>fin<space>ben<space>euh<space>on<space>y<space>viendra<space>d'ailleurs<space>c'est<space>sur<space>le<space>style<space>alors<space>c'est<space>vraique<space>alors<space>les<space>critiques<space>qui<space>vous<space>est<space>boudéeux<space>d'ailleurs<space>un<space>peun<space>depuis<space>quelques<space>anée<space>et<space>puistout<space>d'un<space>coup<space>alon<space>il<space>vous<space>retreuvet<space>il<space>vous<space>t<space>c'est<space>il<space>vous<space>couvret<space>de<space>fleur<space>heun<space>ça<space>vous<space>ça<space>fait<space>plaisir

2026-01-28 18:22:38,278 | INFO | speech length: 83200
2026-01-28 18:22:38,316 | INFO | decoder input length: 129
2026-01-28 18:22:38,316 | INFO | max output length: 129
2026-01-28 18:22:38,316 | INFO | min output length: 12
2026-01-28 18:22:42,777 | INFO | end detected at 107
2026-01-28 18:22:42,779 | INFO | -13.76 * 0.5 =  -6.88 for decoder
2026-01-28 18:22:42,779 | INFO | -19.89 * 0.5 =  -9.94 for ctc
2026-01-28 18:22:42,779 | INFO | total log probability: -16.82
2026-01-28 18:22:42,779 | INFO | normalized log probability: -0.17
2026-01-28 18:22:42,779 | INFO | total number of ended hypotheses: 197
2026-01-28 18:22:42,781 | INFO | best hypo: c'est<space>un<space>peu<space>gênant<space>aussi<space>ça<space>vous<space>mena<space>vous<space>savez<space>je<space>pendant<space>dix<space>ans<space>ça<space>a<space>duré<space>dix<space>ans<space>le<space>silence

2026-01-28 18:22:42,783 | INFO | speech length: 35360
2026-01-28 18:22:42,828 | INFO | decoder input length: 54
2026-01-28 18:22:42,828 | INFO | max output length: 54
2026-01-28 18:22:42,828 | INFO | min output length: 5
2026-01-28 18:22:44,637 | INFO | end detected at 42
2026-01-28 18:22:44,639 | INFO |  -2.78 * 0.5 =  -1.39 for decoder
2026-01-28 18:22:44,640 | INFO |  -0.08 * 0.5 =  -0.04 for ctc
2026-01-28 18:22:44,640 | INFO | total log probability: -1.43
2026-01-28 18:22:44,640 | INFO | normalized log probability: -0.04
2026-01-28 18:22:44,640 | INFO | total number of ended hypotheses: 204
2026-01-28 18:22:44,640 | INFO | best hypo: et<space>là<space>évidemment<space>c'est<space>un<space>peu<space>dur

2026-01-28 18:22:44,644 | INFO | speech length: 19680
2026-01-28 18:22:44,686 | INFO | decoder input length: 30
2026-01-28 18:22:44,686 | INFO | max output length: 30
2026-01-28 18:22:44,686 | INFO | min output length: 3
2026-01-28 18:22:45,809 | INFO | adding <eos> in the last position in the loop
2026-01-28 18:22:45,818 | INFO | no hypothesis. Finish decoding.
2026-01-28 18:22:45,819 | INFO |  -3.20 * 0.5 =  -1.60 for decoder
2026-01-28 18:22:45,819 | INFO |  -2.54 * 0.5 =  -1.27 for ctc
2026-01-28 18:22:45,819 | INFO | total log probability: -2.87
2026-01-28 18:22:45,819 | INFO | normalized log probability: -0.09
2026-01-28 18:22:45,819 | INFO | total number of ended hypotheses: 63
2026-01-28 18:22:45,819 | INFO | best hypo: je<space>peux<space>pas<space>ouvrir<space>le<space>journal<sos/eos>

2026-01-28 18:22:45,820 | WARNING | best hypo length: 30 == max output length: 30
2026-01-28 18:22:45,820 | WARNING | decoding may be stopped by the max output length limitation, please consider to increase the maxlenratio.
2026-01-28 18:22:45,821 | INFO | speech length: 38080
2026-01-28 18:22:45,860 | INFO | decoder input length: 59
2026-01-28 18:22:45,861 | INFO | max output length: 59
2026-01-28 18:22:45,861 | INFO | min output length: 5
2026-01-28 18:22:48,043 | INFO | end detected at 51
2026-01-28 18:22:48,046 | INFO |  -3.82 * 0.5 =  -1.91 for decoder
2026-01-28 18:22:48,046 | INFO |  -3.27 * 0.5 =  -1.63 for ctc
2026-01-28 18:22:48,046 | INFO | total log probability: -3.54
2026-01-28 18:22:48,046 | INFO | normalized log probability: -0.09
2026-01-28 18:22:48,046 | INFO | total number of ended hypotheses: 215
2026-01-28 18:22:48,047 | INFO | best hypo: un<space>peu<space>il<space>y<space>a<space>un<space>réflexe<space>de<space>pudeur<space>quoi

2026-01-28 18:22:48,050 | INFO | speech length: 12800
2026-01-28 18:22:48,095 | INFO | decoder input length: 19
2026-01-28 18:22:48,096 | INFO | max output length: 19
2026-01-28 18:22:48,096 | INFO | min output length: 1
2026-01-28 18:22:48,790 | INFO | adding <eos> in the last position in the loop
2026-01-28 18:22:48,798 | INFO | no hypothesis. Finish decoding.
2026-01-28 18:22:48,799 | INFO |  -4.21 * 0.5 =  -2.10 for decoder
2026-01-28 18:22:48,799 | INFO | -17.17 * 0.5 =  -8.59 for ctc
2026-01-28 18:22:48,799 | INFO | total log probability: -10.69
2026-01-28 18:22:48,799 | INFO | normalized log probability: -0.51
2026-01-28 18:22:48,799 | INFO | total number of ended hypotheses: 46
2026-01-28 18:22:48,799 | INFO | best hypo: le<space>fute<space>qui<space>se<space>pose

2026-01-28 18:22:48,799 | WARNING | best hypo length: 19 == max output length: 19
2026-01-28 18:22:48,799 | WARNING | decoding may be stopped by the max output length limitation, please consider to increase the maxlenratio.
2026-01-28 18:22:48,801 | INFO | speech length: 29920
2026-01-28 18:22:48,843 | INFO | decoder input length: 46
2026-01-28 18:22:48,843 | INFO | max output length: 46
2026-01-28 18:22:48,843 | INFO | min output length: 4
2026-01-28 18:22:50,656 | INFO | adding <eos> in the last position in the loop
2026-01-28 18:22:50,665 | INFO | no hypothesis. Finish decoding.
2026-01-28 18:22:50,666 | INFO |  -3.75 * 0.5 =  -1.87 for decoder
2026-01-28 18:22:50,666 | INFO |  -0.12 * 0.5 =  -0.06 for ctc
2026-01-28 18:22:50,666 | INFO | total log probability: -1.93
2026-01-28 18:22:50,666 | INFO | normalized log probability: -0.04
2026-01-28 18:22:50,666 | INFO | total number of ended hypotheses: 73
2026-01-28 18:22:50,667 | INFO | best hypo: c'est<space>toujours<space>un<space>un<space>temps<space>très<space>difficile<space>que<sos/eos>

2026-01-28 18:22:50,667 | WARNING | best hypo length: 46 == max output length: 46
2026-01-28 18:22:50,667 | WARNING | decoding may be stopped by the max output length limitation, please consider to increase the maxlenratio.
2026-01-28 18:22:50,669 | INFO | speech length: 29280
2026-01-28 18:22:50,724 | INFO | decoder input length: 45
2026-01-28 18:22:50,724 | INFO | max output length: 45
2026-01-28 18:22:50,724 | INFO | min output length: 4
2026-01-28 18:22:53,513 | INFO | adding <eos> in the last position in the loop
2026-01-28 18:22:53,523 | INFO | end detected at 44
2026-01-28 18:22:53,524 | INFO |  -3.13 * 0.5 =  -1.56 for decoder
2026-01-28 18:22:53,524 | INFO |  -0.03 * 0.5 =  -0.01 for ctc
2026-01-28 18:22:53,524 | INFO | total log probability: -1.58
2026-01-28 18:22:53,524 | INFO | normalized log probability: -0.04
2026-01-28 18:22:53,525 | INFO | total number of ended hypotheses: 178
2026-01-28 18:22:53,525 | INFO | best hypo: que<space>le<space>temps<space>de<space>la<space>parution<space>d'un<space>livre

2026-01-28 18:22:53,527 | INFO | speech length: 14880
2026-01-28 18:22:53,569 | INFO | decoder input length: 22
2026-01-28 18:22:53,570 | INFO | max output length: 22
2026-01-28 18:22:53,570 | INFO | min output length: 2
2026-01-28 18:22:54,193 | INFO | end detected at 18
2026-01-28 18:22:54,194 | INFO |  -1.06 * 0.5 =  -0.53 for decoder
2026-01-28 18:22:54,194 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 18:22:54,194 | INFO | total log probability: -0.53
2026-01-28 18:22:54,194 | INFO | normalized log probability: -0.04
2026-01-28 18:22:54,194 | INFO | total number of ended hypotheses: 139
2026-01-28 18:22:54,194 | INFO | best hypo: c'est<space>un<space>peu

2026-01-28 18:22:54,196 | INFO | speech length: 67520
2026-01-28 18:22:54,242 | INFO | decoder input length: 105
2026-01-28 18:22:54,242 | INFO | max output length: 105
2026-01-28 18:22:54,242 | INFO | min output length: 10
2026-01-28 18:22:58,022 | INFO | end detected at 79
2026-01-28 18:22:58,025 | INFO |  -7.82 * 0.5 =  -3.91 for decoder
2026-01-28 18:22:58,025 | INFO |  -6.17 * 0.5 =  -3.09 for ctc
2026-01-28 18:22:58,025 | INFO | total log probability: -6.99
2026-01-28 18:22:58,025 | INFO | normalized log probability: -0.11
2026-01-28 18:22:58,025 | INFO | total number of ended hypotheses: 232
2026-01-28 18:22:58,026 | INFO | best hypo: trop<space>c'est<space>trop<space>peut<space>être<space>là<space>non<space>mais<space>l'habitude<space>de<space>la<space>critique

2026-01-28 18:22:58,029 | INFO | speech length: 45440
2026-01-28 18:22:58,076 | INFO | decoder input length: 70
2026-01-28 18:22:58,077 | INFO | max output length: 70
2026-01-28 18:22:58,077 | INFO | min output length: 7
2026-01-28 18:23:00,296 | INFO | end detected at 50
2026-01-28 18:23:00,298 | INFO |  -3.58 * 0.5 =  -1.79 for decoder
2026-01-28 18:23:00,298 | INFO |  -1.52 * 0.5 =  -0.76 for ctc
2026-01-28 18:23:00,298 | INFO | total log probability: -2.55
2026-01-28 18:23:00,298 | INFO | normalized log probability: -0.06
2026-01-28 18:23:00,298 | INFO | total number of ended hypotheses: 189
2026-01-28 18:23:00,299 | INFO | best hypo: fait<space>que<space>un<space>livre<space>ait<space>toujours<space>pris<space>un<space>peu

2026-01-28 18:23:00,302 | INFO | speech length: 31520
2026-01-28 18:23:00,349 | INFO | decoder input length: 48
2026-01-28 18:23:00,349 | INFO | max output length: 48
2026-01-28 18:23:00,349 | INFO | min output length: 4
2026-01-28 18:23:02,058 | INFO | end detected at 41
2026-01-28 18:23:02,060 | INFO |  -2.84 * 0.5 =  -1.42 for decoder
2026-01-28 18:23:02,060 | INFO |  -0.06 * 0.5 =  -0.03 for ctc
2026-01-28 18:23:02,060 | INFO | total log probability: -1.45
2026-01-28 18:23:02,060 | INFO | normalized log probability: -0.04
2026-01-28 18:23:02,060 | INFO | total number of ended hypotheses: 178
2026-01-28 18:23:02,061 | INFO | best hypo: comme<space>euh<space>une<space>certaine<space>culpabilité

2026-01-28 18:23:02,063 | INFO | speech length: 10240
2026-01-28 18:23:02,116 | INFO | decoder input length: 15
2026-01-28 18:23:02,116 | INFO | max output length: 15
2026-01-28 18:23:02,116 | INFO | min output length: 1
2026-01-28 18:23:02,915 | INFO | adding <eos> in the last position in the loop
2026-01-28 18:23:02,927 | INFO | no hypothesis. Finish decoding.
2026-01-28 18:23:02,927 | INFO | -13.80 * 0.5 =  -6.90 for decoder
2026-01-28 18:23:02,928 | INFO | -10.26 * 0.5 =  -5.13 for ctc
2026-01-28 18:23:02,928 | INFO | total log probability: -12.03
2026-01-28 18:23:02,928 | INFO | normalized log probability: -0.80
2026-01-28 18:23:02,928 | INFO | total number of ended hypotheses: 53
2026-01-28 18:23:02,928 | INFO | best hypo: vêtes<space>d'accod

2026-01-28 18:23:02,930 | INFO | speech length: 296640
2026-01-28 18:23:02,986 | INFO | decoder input length: 463
2026-01-28 18:23:02,986 | INFO | max output length: 463
2026-01-28 18:23:02,986 | INFO | min output length: 46
2026-01-28 18:23:40,220 | INFO | end detected at 451
2026-01-28 18:23:40,221 | INFO | -147.74 * 0.5 = -73.87 for decoder
2026-01-28 18:23:40,221 | INFO | -479.38 * 0.5 = -239.69 for ctc
2026-01-28 18:23:40,222 | INFO | total log probability: -313.56
2026-01-28 18:23:40,222 | INFO | normalized log probability: -0.70
2026-01-28 18:23:40,222 | INFO | total number of ended hypotheses: 183
2026-01-28 18:23:40,227 | INFO | best hypo: c'est<space>certainement<space>rôle<space>pour<space>les<space>auteurs<space>mais<space>euh<space>dit<space>euh<space>et<space>puis<space>alors<space>je<space>y<space>a<space>une<space>rumeur<space>qui<space>court<space>paris<space>euh<space>certainement<space>de<space>l'académie<space>goncourt<space>auit<space>dit<space>mais<space>pourquoi<space>ne<space>devrait<space>on<space>pas<space>le<space>pays<space>beaucoup<space>alors<space>je<space>regard<space>un<space>petit<space>peu<space>le<space>statistique<space>en<space>mille<space>neuf<space>cent<space>cinquante<space>que<space>vous<space>a<space>fait<space>paraître<space>ce<space>ce<space>chef<space>d'oeuf<space>pour<space>moi<space>c'est<space>un<space>chdo<space>qui<space>s'appel<space>un<space>barrage<space>dans<space>pacifique<space>vous<space>l'avez<space>raté<space>de<space>peu<space>alor<space>l'acdémie<space>e<space>cour<space>cest<space>complètement

2026-01-28 18:23:40,230 | INFO | speech length: 240640
2026-01-28 18:23:40,268 | INFO | decoder input length: 375
2026-01-28 18:23:40,269 | INFO | max output length: 375
2026-01-28 18:23:40,269 | INFO | min output length: 37
2026-01-28 18:24:03,938 | INFO | end detected at 344
2026-01-28 18:24:03,940 | INFO | -74.07 * 0.5 = -37.03 for decoder
2026-01-28 18:24:03,941 | INFO | -141.28 * 0.5 = -70.64 for ctc
2026-01-28 18:24:03,941 | INFO | total log probability: -107.67
2026-01-28 18:24:03,941 | INFO | normalized log probability: -0.32
2026-01-28 18:24:03,941 | INFO | total number of ended hypotheses: 219
2026-01-28 18:24:03,946 | INFO | best hypo: c'est<space>bien<space>isé<space>à<space>côté<space>enfin<space>ce<space>sont<space>pas<space>les<space>mêmes<space>hein<space>c'était<space>les<space>autres<space>c'était<space>en<space>cinq<space>ans<space>donc<space>euh<space>aucun<space>de<space>ceux<space>aujourd'hui<space>est<space>ce<space>qu'ils<space>ont<space>couronné<space>les<space>jeux<space>sauvages<space>paul<space>colin<space>et<space>et<space>et<space>le<space>bage<space>on<space>le<space>pacifie<space>qu'ils<space>l'ont<space>laissé<space>tomber<space>et<space>d'avoir<space>du<space>regret<space>ils<space>voulaient<space>pas<space>vous<space>le<space>s<space>ils<space>voulait<space>pas<space>le<space>donner<space>un<space>au<space>barrage

2026-01-28 18:24:03,950 | INFO | speech length: 24000
2026-01-28 18:24:04,000 | INFO | decoder input length: 37
2026-01-28 18:24:04,000 | INFO | max output length: 37
2026-01-28 18:24:04,000 | INFO | min output length: 3
2026-01-28 18:24:04,953 | INFO | end detected at 23
2026-01-28 18:24:04,954 | INFO |  -1.45 * 0.5 =  -0.72 for decoder
2026-01-28 18:24:04,954 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-28 18:24:04,954 | INFO | total log probability: -0.73
2026-01-28 18:24:04,954 | INFO | normalized log probability: -0.04
2026-01-28 18:24:04,954 | INFO | total number of ended hypotheses: 138
2026-01-28 18:24:04,955 | INFO | best hypo: parce<space>que<space>j'étais

2026-01-28 18:24:04,957 | INFO | speech length: 26720
2026-01-28 18:24:05,004 | INFO | decoder input length: 41
2026-01-28 18:24:05,004 | INFO | max output length: 41
2026-01-28 18:24:05,004 | INFO | min output length: 4
2026-01-28 18:24:06,428 | INFO | end detected at 35
2026-01-28 18:24:06,430 | INFO |  -2.35 * 0.5 =  -1.18 for decoder
2026-01-28 18:24:06,430 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-28 18:24:06,430 | INFO | total log probability: -1.19
2026-01-28 18:24:06,430 | INFO | normalized log probability: -0.04
2026-01-28 18:24:06,430 | INFO | total number of ended hypotheses: 171
2026-01-28 18:24:06,431 | INFO | best hypo: de<space>gauche<space>j'étais<space>communiste

2026-01-28 18:24:06,433 | INFO | speech length: 15360
2026-01-28 18:24:06,480 | INFO | decoder input length: 23
2026-01-28 18:24:06,480 | INFO | max output length: 23
2026-01-28 18:24:06,480 | INFO | min output length: 2
2026-01-28 18:24:07,323 | INFO | adding <eos> in the last position in the loop
2026-01-28 18:24:07,331 | INFO | no hypothesis. Finish decoding.
2026-01-28 18:24:07,331 | INFO |  -8.93 * 0.5 =  -4.47 for decoder
2026-01-28 18:24:07,331 | INFO | -26.04 * 0.5 = -13.02 for ctc
2026-01-28 18:24:07,332 | INFO | total log probability: -17.48
2026-01-28 18:24:07,332 | INFO | normalized log probability: -0.79
2026-01-28 18:24:07,332 | INFO | total number of ended hypotheses: 44
2026-01-28 18:24:07,332 | INFO | best hypo: fin<space>justement<space>moi<space>du

2026-01-28 18:24:07,333 | INFO | speech length: 51840
2026-01-28 18:24:07,381 | INFO | decoder input length: 80
2026-01-28 18:24:07,381 | INFO | max output length: 80
2026-01-28 18:24:07,381 | INFO | min output length: 8
2026-01-28 18:24:10,582 | INFO | end detected at 68
2026-01-28 18:24:10,584 | INFO |  -5.98 * 0.5 =  -2.99 for decoder
2026-01-28 18:24:10,584 | INFO |  -3.45 * 0.5 =  -1.73 for ctc
2026-01-28 18:24:10,584 | INFO | total log probability: -4.72
2026-01-28 18:24:10,584 | INFO | normalized log probability: -0.08
2026-01-28 18:24:10,584 | INFO | total number of ended hypotheses: 171
2026-01-28 18:24:10,585 | INFO | best hypo: ce<space>qui<space>fait<space>que<space>le<space>barrage<space>a<space>été<space>une<space>expérience<space>épouvantable

2026-01-28 18:24:10,588 | INFO | speech length: 68640
2026-01-28 18:24:10,634 | INFO | decoder input length: 106
2026-01-28 18:24:10,635 | INFO | max output length: 106
2026-01-28 18:24:10,635 | INFO | min output length: 10
2026-01-28 18:24:14,772 | INFO | end detected at 90
2026-01-28 18:24:14,774 | INFO |  -7.67 * 0.5 =  -3.84 for decoder
2026-01-28 18:24:14,774 | INFO |  -2.87 * 0.5 =  -1.44 for ctc
2026-01-28 18:24:14,774 | INFO | total log probability: -5.27
2026-01-28 18:24:14,774 | INFO | normalized log probability: -0.06
2026-01-28 18:24:14,774 | INFO | total number of ended hypotheses: 184
2026-01-28 18:24:14,776 | INFO | best hypo: pour<space>la<space>famille<space>pour<space>ma<space>mère<space>et<space>que<space>j'en<space>étais<space>puni<space>par<space>ailleurs<space>quand<space>j'en<space>ai<space>pas

2026-01-28 18:24:14,778 | INFO | speech length: 12160
2026-01-28 18:24:14,820 | INFO | decoder input length: 18
2026-01-28 18:24:14,820 | INFO | max output length: 18
2026-01-28 18:24:14,820 | INFO | min output length: 1
2026-01-28 18:24:15,382 | INFO | adding <eos> in the last position in the loop
2026-01-28 18:24:15,389 | INFO | no hypothesis. Finish decoding.
2026-01-28 18:24:15,389 | INFO |  -8.05 * 0.5 =  -4.03 for decoder
2026-01-28 18:24:15,389 | INFO |  -5.81 * 0.5 =  -2.91 for ctc
2026-01-28 18:24:15,389 | INFO | total log probability: -6.93
2026-01-28 18:24:15,390 | INFO | normalized log probability: -0.35
2026-01-28 18:24:15,390 | INFO | total number of ended hypotheses: 46
2026-01-28 18:24:15,390 | INFO | best hypo: qand<space>j'en<space>ai<space>écrit

2026-01-28 18:24:15,390 | WARNING | best hypo length: 18 == max output length: 18
2026-01-28 18:24:15,390 | WARNING | decoding may be stopped by the max output length limitation, please consider to increase the maxlenratio.
2026-01-28 18:24:15,391 | INFO | speech length: 117280
2026-01-28 18:24:15,434 | INFO | decoder input length: 182
2026-01-28 18:24:15,435 | INFO | max output length: 182
2026-01-28 18:24:15,435 | INFO | min output length: 18
2026-01-28 18:24:21,348 | INFO | end detected at 123
2026-01-28 18:24:21,350 | INFO | -11.86 * 0.5 =  -5.93 for decoder
2026-01-28 18:24:21,350 | INFO |  -5.46 * 0.5 =  -2.73 for ctc
2026-01-28 18:24:21,350 | INFO | total log probability: -8.66
2026-01-28 18:24:21,350 | INFO | normalized log probability: -0.07
2026-01-28 18:24:21,350 | INFO | total number of ended hypotheses: 178
2026-01-28 18:24:21,352 | INFO | best hypo: alors<space>venons<space>à<space>la<space>main<space>oui<space>la<space>mort<space>nous<space>sommes<space>à<space>nous<space>sommes<space>assaillons<space>entre<space>les<space>deux<space>guerres<space>dans<space>les<space>années<space>vingt

2026-01-28 18:24:21,354 | INFO | speech length: 94560
2026-01-28 18:24:21,412 | INFO | decoder input length: 147
2026-01-28 18:24:21,412 | INFO | max output length: 147
2026-01-28 18:24:21,412 | INFO | min output length: 14
2026-01-28 18:24:26,272 | INFO | end detected at 108
2026-01-28 18:24:26,274 | INFO |  -8.40 * 0.5 =  -4.20 for decoder
2026-01-28 18:24:26,274 | INFO |  -1.77 * 0.5 =  -0.89 for ctc
2026-01-28 18:24:26,275 | INFO | total log probability: -5.08
2026-01-28 18:24:26,275 | INFO | normalized log probability: -0.05
2026-01-28 18:24:26,275 | INFO | total number of ended hypotheses: 208
2026-01-28 18:24:26,276 | INFO | best hypo: vous<space>avez<space>quinze<space>ans<space>et<space>demi<space>et<space>vous<space>êtes<space>sur<space>un<space>bac<space>qui<space>traverse<space>le<space>fleuve<space>c'est<space>à<space>dire<space>le<space>mécon

2026-01-28 18:24:26,278 | INFO | speech length: 42400
2026-01-28 18:24:26,326 | INFO | decoder input length: 65
2026-01-28 18:24:26,326 | INFO | max output length: 65
2026-01-28 18:24:26,327 | INFO | min output length: 6
2026-01-28 18:24:28,910 | INFO | end detected at 61
2026-01-28 18:24:28,912 | INFO |  -5.78 * 0.5 =  -2.89 for decoder
2026-01-28 18:24:28,912 | INFO |  -9.08 * 0.5 =  -4.54 for ctc
2026-01-28 18:24:28,913 | INFO | total log probability: -7.43
2026-01-28 18:24:28,913 | INFO | normalized log probability: -0.15
2026-01-28 18:24:28,913 | INFO | total number of ended hypotheses: 243
2026-01-28 18:24:28,913 | INFO | best hypo: et<space>vous<space>êtes<space>drôlement<space>attifé<space>euh<space>vous<space>voulez<space>pas

2026-01-28 18:24:28,916 | INFO | speech length: 51520
2026-01-28 18:24:28,953 | INFO | decoder input length: 80
2026-01-28 18:24:28,953 | INFO | max output length: 80
2026-01-28 18:24:28,953 | INFO | min output length: 8
2026-01-28 18:24:31,645 | INFO | end detected at 71
2026-01-28 18:24:31,647 | INFO | -15.28 * 0.5 =  -7.64 for decoder
2026-01-28 18:24:31,647 | INFO | -10.58 * 0.5 =  -5.29 for ctc
2026-01-28 18:24:31,647 | INFO | total log probability: -12.93
2026-01-28 18:24:31,647 | INFO | normalized log probability: -0.21
2026-01-28 18:24:31,647 | INFO | total number of ended hypotheses: 212
2026-01-28 18:24:31,648 | INFO | best hypo: vous<space>êtes<space>actifé<space>comment<space>agodez<space>nuit<space>j'ai<space>un<space>chapeau<space>d'hommes

2026-01-28 18:24:31,650 | INFO | speech length: 119360
2026-01-28 18:24:31,719 | INFO | decoder input length: 186
2026-01-28 18:24:31,719 | INFO | max output length: 186
2026-01-28 18:24:31,719 | INFO | min output length: 18
2026-01-28 18:24:37,479 | INFO | end detected at 116
2026-01-28 18:24:37,481 | INFO | -16.56 * 0.5 =  -8.28 for decoder
2026-01-28 18:24:37,481 | INFO | -14.27 * 0.5 =  -7.14 for ctc
2026-01-28 18:24:37,481 | INFO | total log probability: -15.41
2026-01-28 18:24:37,481 | INFO | normalized log probability: -0.14
2026-01-28 18:24:37,481 | INFO | total number of ended hypotheses: 217
2026-01-28 18:24:37,483 | INFO | best hypo: en<space>fêtre<space>oui<space>un<space>feutre<space>rose<space>couleur<space>boiderose<space>au<space>large<space>rouban<space>noir<space>mais<space>et<space>aux<space>pieds<space>j'ai<space>eu<space>ce<space>chapeau<space>là

2026-01-28 18:24:37,485 | INFO | speech length: 11360
2026-01-28 18:24:37,519 | INFO | decoder input length: 17
2026-01-28 18:24:37,520 | INFO | max output length: 17
2026-01-28 18:24:37,520 | INFO | min output length: 1
2026-01-28 18:24:38,048 | INFO | adding <eos> in the last position in the loop
2026-01-28 18:24:38,055 | INFO | no hypothesis. Finish decoding.
2026-01-28 18:24:38,055 | INFO |  -7.63 * 0.5 =  -3.81 for decoder
2026-01-28 18:24:38,055 | INFO |  -4.23 * 0.5 =  -2.12 for ctc
2026-01-28 18:24:38,055 | INFO | total log probability: -5.93
2026-01-28 18:24:38,056 | INFO | normalized log probability: -0.31
2026-01-28 18:24:38,056 | INFO | total number of ended hypotheses: 41
2026-01-28 18:24:38,056 | INFO | best hypo: vraimentje<space>l'ai<space>e

2026-01-28 18:24:38,056 | WARNING | best hypo length: 17 == max output length: 17
2026-01-28 18:24:38,056 | WARNING | decoding may be stopped by the max output length limitation, please consider to increase the maxlenratio.
2026-01-28 18:24:38,057 | INFO | speech length: 77760
2026-01-28 18:24:38,094 | INFO | decoder input length: 121
2026-01-28 18:24:38,094 | INFO | max output length: 121
2026-01-28 18:24:38,094 | INFO | min output length: 12
2026-01-28 18:24:41,005 | INFO | end detected at 66
2026-01-28 18:24:41,007 | INFO | -12.10 * 0.5 =  -6.05 for decoder
2026-01-28 18:24:41,007 | INFO | -13.35 * 0.5 =  -6.68 for ctc
2026-01-28 18:24:41,007 | INFO | total log probability: -12.72
2026-01-28 18:24:41,007 | INFO | normalized log probability: -0.22
2026-01-28 18:24:41,007 | INFO | total number of ended hypotheses: 204
2026-01-28 18:24:41,008 | INFO | best hypo: au<space>pied<space>j'ai<space>des<space>des<space>lamerors<space>des<space>sujets<space>de<space>pour<space>danser

2026-01-28 18:24:41,010 | INFO | speech length: 309120
2026-01-28 18:24:41,047 | INFO | decoder input length: 482
2026-01-28 18:24:41,047 | INFO | max output length: 482
2026-01-28 18:24:41,048 | INFO | min output length: 48
2026-01-28 18:25:12,090 | INFO | end detected at 454
2026-01-28 18:25:12,091 | INFO | -254.56 * 0.5 = -127.28 for decoder
2026-01-28 18:25:12,091 | INFO | -402.57 * 0.5 = -201.28 for ctc
2026-01-28 18:25:12,091 | INFO | total log probability: -328.56
2026-01-28 18:25:12,092 | INFO | normalized log probability: -0.74
2026-01-28 18:25:12,092 | INFO | total number of ended hypotheses: 177
2026-01-28 18:25:12,097 | INFO | best hypo: c'est<space>vraimet<space>sur<space>le<space>bac<space>évidemment<space>vous<space>devez<space>faire<space>euh<space>on<space>doit<space>vous<space>remarquer<space>aujurd'hui<space>on<space>vous<space>remarquera<space>pe<space>dans<space>c'est<space>une<space>mode<space>qui<space>se<space>porte<space>couramment<space>je<space>je<space>ne<space>fille<space>hein<space>mais<space>c'est<space>vrai<space>aujourd'hui<space>on<space>remarquera<space>pas<space>mais<space>à<space>cette<space>époque<space>on<space>vs<space>remarque<space>et<space>d'aileurs<space>quelqu'un<space>vous<space>remaré<space>qui<space>dans<space>une<space>grande<space>limousine<space>noire<space>et<space>et<space>et<space>vos<space>regards<space>se<space>croisent<space>et<space>on<space>peut<space>dire<space>est<space>ce<space>qu'on<space>peut<space>dire<space>que<space>cet<space>échange<space>de<space>regards<space>va<space>changer<space>votre

2026-01-28 18:25:12,100 | INFO | speech length: 9920
2026-01-28 18:25:12,129 | INFO | decoder input length: 15
2026-01-28 18:25:12,129 | INFO | max output length: 15
2026-01-28 18:25:12,129 | INFO | min output length: 1
2026-01-28 18:25:12,541 | INFO | end detected at 12
2026-01-28 18:25:12,543 | INFO |  -0.32 * 0.5 =  -0.16 for decoder
2026-01-28 18:25:12,543 | INFO |  -0.51 * 0.5 =  -0.26 for ctc
2026-01-28 18:25:12,543 | INFO | total log probability: -0.42
2026-01-28 18:25:12,543 | INFO | normalized log probability: -0.08
2026-01-28 18:25:12,544 | INFO | total number of ended hypotheses: 173
2026-01-28 18:25:12,544 | INFO | best hypo: oui

2026-01-28 18:25:12,545 | INFO | speech length: 94720
2026-01-28 18:25:12,581 | INFO | decoder input length: 147
2026-01-28 18:25:12,581 | INFO | max output length: 147
2026-01-28 18:25:12,581 | INFO | min output length: 14
2026-01-28 18:25:16,793 | INFO | end detected at 93
2026-01-28 18:25:16,794 | INFO | -12.85 * 0.5 =  -6.42 for decoder
2026-01-28 18:25:16,794 | INFO |  -1.28 * 0.5 =  -0.64 for ctc
2026-01-28 18:25:16,795 | INFO | total log probability: -7.07
2026-01-28 18:25:16,795 | INFO | normalized log probability: -0.08
2026-01-28 18:25:16,795 | INFO | total number of ended hypotheses: 191
2026-01-28 18:25:16,796 | INFO | best hypo: le<space>livre<space>s'applaît<space>avant<space>la<space>photographie<space>absolue<space>il<space>m'a<space>été<space>commandé<space>ce<space>livre<space>attend

2026-01-28 18:25:16,798 | INFO | speech length: 38080
2026-01-28 18:25:16,839 | INFO | decoder input length: 59
2026-01-28 18:25:16,839 | INFO | max output length: 59
2026-01-28 18:25:16,839 | INFO | min output length: 5
2026-01-28 18:25:18,880 | INFO | end detected at 57
2026-01-28 18:25:18,881 | INFO |  -4.25 * 0.5 =  -2.12 for decoder
2026-01-28 18:25:18,881 | INFO |  -2.45 * 0.5 =  -1.22 for ctc
2026-01-28 18:25:18,881 | INFO | total log probability: -3.35
2026-01-28 18:25:18,881 | INFO | normalized log probability: -0.06
2026-01-28 18:25:18,881 | INFO | total number of ended hypotheses: 169
2026-01-28 18:25:18,882 | INFO | best hypo: et<space>cette<space>photographie<space>absolue<space>n'avait<space>pas<space>été<space>prise

2026-01-28 18:25:18,884 | INFO | speech length: 13440
2026-01-28 18:25:18,924 | INFO | decoder input length: 20
2026-01-28 18:25:18,924 | INFO | max output length: 20
2026-01-28 18:25:18,924 | INFO | min output length: 2
2026-01-28 18:25:19,583 | INFO | adding <eos> in the last position in the loop
2026-01-28 18:25:19,591 | INFO | no hypothesis. Finish decoding.
2026-01-28 18:25:19,592 | INFO |  -1.36 * 0.5 =  -0.68 for decoder
2026-01-28 18:25:19,592 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 18:25:19,592 | INFO | total log probability: -0.68
2026-01-28 18:25:19,592 | INFO | normalized log probability: -0.04
2026-01-28 18:25:19,592 | INFO | total number of ended hypotheses: 111
2026-01-28 18:25:19,592 | INFO | best hypo: c'était<space>celle<space>là

2026-01-28 18:25:19,594 | INFO | speech length: 20800
2026-01-28 18:25:19,638 | INFO | decoder input length: 32
2026-01-28 18:25:19,638 | INFO | max output length: 32
2026-01-28 18:25:19,638 | INFO | min output length: 3
2026-01-28 18:25:20,605 | INFO | end detected at 28
2026-01-28 18:25:20,606 | INFO |  -1.77 * 0.5 =  -0.88 for decoder
2026-01-28 18:25:20,607 | INFO |  -0.04 * 0.5 =  -0.02 for ctc
2026-01-28 18:25:20,607 | INFO | total log probability: -0.90
2026-01-28 18:25:20,607 | INFO | normalized log probability: -0.04
2026-01-28 18:25:20,607 | INFO | total number of ended hypotheses: 170
2026-01-28 18:25:20,607 | INFO | best hypo: cet<space>instant<space>là<space>du<space>bac

2026-01-28 18:25:20,609 | INFO | speech length: 51200
2026-01-28 18:25:20,653 | INFO | decoder input length: 79
2026-01-28 18:25:20,653 | INFO | max output length: 79
2026-01-28 18:25:20,654 | INFO | min output length: 7
2026-01-28 18:25:23,876 | INFO | end detected at 74
2026-01-28 18:25:23,878 | INFO |  -6.30 * 0.5 =  -3.15 for decoder
2026-01-28 18:25:23,878 | INFO |  -2.17 * 0.5 =  -1.09 for ctc
2026-01-28 18:25:23,879 | INFO | total log probability: -4.24
2026-01-28 18:25:23,879 | INFO | normalized log probability: -0.06
2026-01-28 18:25:23,879 | INFO | total number of ended hypotheses: 183
2026-01-28 18:25:23,880 | INFO | best hypo: on<space>n'avait<space>rien<space>vu<space>qu'un<space>homme<space>une<space>auto<space>noire<space>et<space>une<space>jeune<space>fille<space>et

2026-01-28 18:25:23,883 | INFO | speech length: 24640
2026-01-28 18:25:23,931 | INFO | decoder input length: 38
2026-01-28 18:25:23,932 | INFO | max output length: 38
2026-01-28 18:25:23,932 | INFO | min output length: 3
2026-01-28 18:25:25,554 | INFO | adding <eos> in the last position in the loop
2026-01-28 18:25:25,563 | INFO | no hypothesis. Finish decoding.
2026-01-28 18:25:25,564 | INFO |  -4.90 * 0.5 =  -2.45 for decoder
2026-01-28 18:25:25,565 | INFO |  -5.00 * 0.5 =  -2.50 for ctc
2026-01-28 18:25:25,565 | INFO | total log probability: -4.95
2026-01-28 18:25:25,565 | INFO | normalized log probability: -0.14
2026-01-28 18:25:25,565 | INFO | total number of ended hypotheses: 126
2026-01-28 18:25:25,565 | INFO | best hypo: et<space>des<space>cartes<space>pour<space>un<space>digène<space>mais

2026-01-28 18:25:25,567 | INFO | speech length: 206560
2026-01-28 18:25:25,607 | INFO | decoder input length: 322
2026-01-28 18:25:25,607 | INFO | max output length: 322
2026-01-28 18:25:25,607 | INFO | min output length: 32
2026-01-28 18:25:44,387 | INFO | end detected at 272
2026-01-28 18:25:44,391 | INFO | -37.12 * 0.5 = -18.56 for decoder
2026-01-28 18:25:44,392 | INFO | -49.39 * 0.5 = -24.70 for ctc
2026-01-28 18:25:44,392 | INFO | total log probability: -43.26
2026-01-28 18:25:44,392 | INFO | normalized log probability: -0.17
2026-01-28 18:25:44,392 | INFO | total number of ended hypotheses: 241
2026-01-28 18:25:44,396 | INFO | best hypo: c'est<space>de<space>l'acte<space>tout<space>est<space>parti<space>le<space>fleuve<space>une<space>fois<space>de<space>traversé<space>qui<space>il<space>y<space>a<space>dans<space>y<space>a<space>t<space>il<space>dans<space>la<space>voiture<space>noire<space>alors<space>vous<space>nous<space>donnez<space>c'est<space>une<space>maurice<space>néon<space>bollée<space>hein<space>mais<space>je<space>n'en<space>vois<space>plus<space>de<space>ces<space>autolars<space>ah<space>oui<space>non<space>de<space>toute<space>façon<space>ils<space>ils<space>sont<space>amusés

2026-01-28 18:25:44,400 | INFO | speech length: 116001
2026-01-28 18:25:44,441 | INFO | decoder input length: 180
2026-01-28 18:25:44,442 | INFO | max output length: 180
2026-01-28 18:25:44,442 | INFO | min output length: 18
2026-01-28 18:25:52,568 | INFO | end detected at 145
2026-01-28 18:25:52,570 | INFO | -15.72 * 0.5 =  -7.86 for decoder
2026-01-28 18:25:52,570 | INFO | -22.77 * 0.5 = -11.38 for ctc
2026-01-28 18:25:52,570 | INFO | total log probability: -19.25
2026-01-28 18:25:52,570 | INFO | normalized log probability: -0.15
2026-01-28 18:25:52,570 | INFO | total number of ended hypotheses: 178
2026-01-28 18:25:52,572 | INFO | best hypo: je<space>pense<space>que<space>si<space>on<space>s'était<space>assis<space>l'un<space>devant<space>l'autre<space>on<space>aurait<space>été<space>à<space>à<space>cette<space>distance<space>là<space>avec<space>cette<space>auto<space>on<space>s'est<space>auto<space>gigantesque

2026-01-28 18:25:52,575 | INFO | speech length: 16000
2026-01-28 18:25:52,614 | INFO | decoder input length: 24
2026-01-28 18:25:52,614 | INFO | max output length: 24
2026-01-28 18:25:52,614 | INFO | min output length: 2
2026-01-28 18:25:53,502 | INFO | adding <eos> in the last position in the loop
2026-01-28 18:25:53,510 | INFO | no hypothesis. Finish decoding.
2026-01-28 18:25:53,511 | INFO |  -1.95 * 0.5 =  -0.98 for decoder
2026-01-28 18:25:53,511 | INFO |  -7.46 * 0.5 =  -3.73 for ctc
2026-01-28 18:25:53,511 | INFO | total log probability: -4.71
2026-01-28 18:25:53,511 | INFO | normalized log probability: -0.18
2026-01-28 18:25:53,511 | INFO | total number of ended hypotheses: 57
2026-01-28 18:25:53,512 | INFO | best hypo: alors<space>qui<space>vous<space>regardent

2026-01-28 18:25:53,512 | WARNING | best hypo length: 24 == max output length: 24
2026-01-28 18:25:53,512 | WARNING | decoding may be stopped by the max output length limitation, please consider to increase the maxlenratio.
2026-01-28 18:25:53,513 | INFO | speech length: 14560
2026-01-28 18:25:53,552 | INFO | decoder input length: 22
2026-01-28 18:25:53,553 | INFO | max output length: 22
2026-01-28 18:25:53,553 | INFO | min output length: 2
2026-01-28 18:25:54,371 | INFO | adding <eos> in the last position in the loop
2026-01-28 18:25:54,398 | INFO | no hypothesis. Finish decoding.
2026-01-28 18:25:54,399 | INFO |  -1.55 * 0.5 =  -0.77 for decoder
2026-01-28 18:25:54,400 | INFO |  -3.63 * 0.5 =  -1.81 for ctc
2026-01-28 18:25:54,400 | INFO | total log probability: -2.59
2026-01-28 18:25:54,400 | INFO | normalized log probability: -0.14
2026-01-28 18:25:54,400 | INFO | total number of ended hypotheses: 134
2026-01-28 18:25:54,400 | INFO | best hypo: c'est<space>un<space>chinois

2026-01-28 18:25:54,418 | INFO | speech length: 46880
2026-01-28 18:25:54,493 | INFO | decoder input length: 72
2026-01-28 18:25:54,509 | INFO | max output length: 72
2026-01-28 18:25:54,509 | INFO | min output length: 7
2026-01-28 18:25:57,333 | INFO | adding <eos> in the last position in the loop
2026-01-28 18:25:57,341 | INFO | no hypothesis. Finish decoding.
2026-01-28 18:25:57,342 | INFO | -19.97 * 0.5 =  -9.99 for decoder
2026-01-28 18:25:57,342 | INFO | -40.22 * 0.5 = -20.11 for ctc
2026-01-28 18:25:57,342 | INFO | total log probability: -30.10
2026-01-28 18:25:57,342 | INFO | normalized log probability: -0.42
2026-01-28 18:25:57,342 | INFO | total number of ended hypotheses: 88
2026-01-28 18:25:57,343 | INFO | best hypo: mais<space>vous<space>le<space>savez<space>pas<space>qui<space>dirige<space>à<space>ce<space>moment<space>si<space>je<space>sers<space>l'automobile

2026-01-28 18:25:57,344 | INFO | speech length: 11360
2026-01-28 18:25:57,380 | INFO | decoder input length: 17
2026-01-28 18:25:57,380 | INFO | max output length: 17
2026-01-28 18:25:57,380 | INFO | min output length: 1
2026-01-28 18:25:57,908 | INFO | adding <eos> in the last position in the loop
2026-01-28 18:25:57,915 | INFO | no hypothesis. Finish decoding.
2026-01-28 18:25:57,915 | INFO |  -8.01 * 0.5 =  -4.00 for decoder
2026-01-28 18:25:57,915 | INFO | -15.39 * 0.5 =  -7.70 for ctc
2026-01-28 18:25:57,915 | INFO | total log probability: -11.70
2026-01-28 18:25:57,915 | INFO | normalized log probability: -0.62
2026-01-28 18:25:57,915 | INFO | total number of ended hypotheses: 42
2026-01-28 18:25:57,915 | INFO | best hypo: et<space>puis<space>auchanfer

2026-01-28 18:25:57,915 | WARNING | best hypo length: 17 == max output length: 17
2026-01-28 18:25:57,916 | WARNING | decoding may be stopped by the max output length limitation, please consider to increase the maxlenratio.
2026-01-28 18:25:57,916 | INFO | speech length: 21120
2026-01-28 18:25:57,953 | INFO | decoder input length: 32
2026-01-28 18:25:57,953 | INFO | max output length: 32
2026-01-28 18:25:57,953 | INFO | min output length: 3
2026-01-28 18:25:58,995 | INFO | adding <eos> in the last position in the loop
2026-01-28 18:25:59,004 | INFO | no hypothesis. Finish decoding.
2026-01-28 18:25:59,005 | INFO |  -2.43 * 0.5 =  -1.22 for decoder
2026-01-28 18:25:59,005 | INFO |  -0.67 * 0.5 =  -0.33 for ctc
2026-01-28 18:25:59,005 | INFO | total log probability: -1.55
2026-01-28 18:25:59,005 | INFO | normalized log probability: -0.05
2026-01-28 18:25:59,005 | INFO | total number of ended hypotheses: 134
2026-01-28 18:25:59,006 | INFO | best hypo: il<space>a<space>douze<space>ans<space>puisque<space>vous

2026-01-28 18:25:59,007 | INFO | speech length: 31200
2026-01-28 18:25:59,043 | INFO | decoder input length: 48
2026-01-28 18:25:59,044 | INFO | max output length: 48
2026-01-28 18:25:59,044 | INFO | min output length: 4
2026-01-28 18:26:00,739 | INFO | adding <eos> in the last position in the loop
2026-01-28 18:26:00,749 | INFO | no hypothesis. Finish decoding.
2026-01-28 18:26:00,750 | INFO |  -3.69 * 0.5 =  -1.85 for decoder
2026-01-28 18:26:00,750 | INFO |  -8.23 * 0.5 =  -4.12 for ctc
2026-01-28 18:26:00,750 | INFO | total log probability: -5.96
2026-01-28 18:26:00,750 | INFO | normalized log probability: -0.15
2026-01-28 18:26:00,750 | INFO | total number of ended hypotheses: 171
2026-01-28 18:26:00,751 | INFO | best hypo: j'ai<space>quinze<space>ans<space>il<space>a<space>vingt<space>sept<space>ans<space>oui

2026-01-28 18:26:00,752 | INFO | speech length: 77920
2026-01-28 18:26:00,795 | INFO | decoder input length: 121
2026-01-28 18:26:00,795 | INFO | max output length: 121
2026-01-28 18:26:00,795 | INFO | min output length: 12
2026-01-28 18:26:04,997 | INFO | end detected at 101
2026-01-28 18:26:04,999 | INFO |  -9.96 * 0.5 =  -4.98 for decoder
2026-01-28 18:26:04,999 | INFO |  -5.45 * 0.5 =  -2.73 for ctc
2026-01-28 18:26:04,999 | INFO | total log probability: -7.70
2026-01-28 18:26:04,999 | INFO | normalized log probability: -0.08
2026-01-28 18:26:04,999 | INFO | total number of ended hypotheses: 166
2026-01-28 18:26:05,000 | INFO | best hypo: et<space>vous<space>savez<space>on<space>a<space>l'impression<space>que<space>vous<space>savez<space>qui<space>va<space>devenir<space>votre<space>amant<space>votre<space>premier<space>amant

2026-01-28 18:26:05,002 | INFO | speech length: 9280
2026-01-28 18:26:05,030 | INFO | decoder input length: 14
2026-01-28 18:26:05,030 | INFO | max output length: 14
2026-01-28 18:26:05,030 | INFO | min output length: 1
2026-01-28 18:26:05,462 | INFO | adding <eos> in the last position in the loop
2026-01-28 18:26:05,469 | INFO | no hypothesis. Finish decoding.
2026-01-28 18:26:05,469 | INFO |  -1.13 * 0.5 =  -0.56 for decoder
2026-01-28 18:26:05,469 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 18:26:05,469 | INFO | total log probability: -0.56
2026-01-28 18:26:05,469 | INFO | normalized log probability: -0.04
2026-01-28 18:26:05,470 | INFO | total number of ended hypotheses: 50
2026-01-28 18:26:05,470 | INFO | best hypo: tout<space>de<space>suite<sos/eos>

2026-01-28 18:26:05,470 | WARNING | best hypo length: 14 == max output length: 14
2026-01-28 18:26:05,470 | WARNING | decoding may be stopped by the max output length limitation, please consider to increase the maxlenratio.
2026-01-28 18:26:05,471 | INFO | speech length: 51360
2026-01-28 18:26:05,525 | INFO | decoder input length: 79
2026-01-28 18:26:05,525 | INFO | max output length: 79
2026-01-28 18:26:05,525 | INFO | min output length: 7
2026-01-28 18:26:08,270 | INFO | end detected at 66
2026-01-28 18:26:08,272 | INFO |  -6.31 * 0.5 =  -3.16 for decoder
2026-01-28 18:26:08,272 | INFO |  -3.89 * 0.5 =  -1.94 for ctc
2026-01-28 18:26:08,272 | INFO | total log probability: -5.10
2026-01-28 18:26:08,272 | INFO | normalized log probability: -0.08
2026-01-28 18:26:08,272 | INFO | total number of ended hypotheses: 202
2026-01-28 18:26:08,273 | INFO | best hypo: c'est<space>à<space>dire<space>que<space>je<space>suis<space>après<space>qu'ils<space>n'avaient<space>pas<space>de<space>plus

2026-01-28 18:26:08,275 | INFO | speech length: 11360
2026-01-28 18:26:08,344 | INFO | decoder input length: 17
2026-01-28 18:26:08,344 | INFO | max output length: 17
2026-01-28 18:26:08,344 | INFO | min output length: 1
2026-01-28 18:26:08,947 | INFO | adding <eos> in the last position in the loop
2026-01-28 18:26:08,957 | INFO | no hypothesis. Finish decoding.
2026-01-28 18:26:08,959 | INFO |  -2.95 * 0.5 =  -1.48 for decoder
2026-01-28 18:26:08,959 | INFO |  -6.40 * 0.5 =  -3.20 for ctc
2026-01-28 18:26:08,959 | INFO | total log probability: -4.67
2026-01-28 18:26:08,959 | INFO | normalized log probability: -0.39
2026-01-28 18:26:08,959 | INFO | total number of ended hypotheses: 175
2026-01-28 18:26:08,959 | INFO | best hypo: jean<space>lubak

2026-01-28 18:26:08,961 | INFO | speech length: 40000
2026-01-28 18:26:09,007 | INFO | decoder input length: 62
2026-01-28 18:26:09,007 | INFO | max output length: 62
2026-01-28 18:26:09,007 | INFO | min output length: 6
2026-01-28 18:26:11,283 | INFO | adding <eos> in the last position in the loop
2026-01-28 18:26:11,292 | INFO | no hypothesis. Finish decoding.
2026-01-28 18:26:11,293 | INFO |  -5.30 * 0.5 =  -2.65 for decoder
2026-01-28 18:26:11,293 | INFO |  -1.61 * 0.5 =  -0.81 for ctc
2026-01-28 18:26:11,293 | INFO | total log probability: -3.45
2026-01-28 18:26:11,293 | INFO | normalized log probability: -0.06
2026-01-28 18:26:11,293 | INFO | total number of ended hypotheses: 147
2026-01-28 18:26:11,294 | INFO | best hypo: mais<space>sur<space>le<space>moment<space>je<space>n'avais<space>pas<space>d'avis<space>sauf<space>que<space>j'étais

2026-01-28 18:26:11,296 | INFO | speech length: 16640
2026-01-28 18:26:11,356 | INFO | decoder input length: 25
2026-01-28 18:26:11,356 | INFO | max output length: 25
2026-01-28 18:26:11,356 | INFO | min output length: 2
2026-01-28 18:26:12,291 | INFO | adding <eos> in the last position in the loop
2026-01-28 18:26:12,300 | INFO | no hypothesis. Finish decoding.
2026-01-28 18:26:12,301 | INFO |  -3.26 * 0.5 =  -1.63 for decoder
2026-01-28 18:26:12,301 | INFO |  -7.26 * 0.5 =  -3.63 for ctc
2026-01-28 18:26:12,301 | INFO | total log probability: -5.26
2026-01-28 18:26:12,301 | INFO | normalized log probability: -0.25
2026-01-28 18:26:12,301 | INFO | total number of ended hypotheses: 145
2026-01-28 18:26:12,302 | INFO | best hypo: ah<space>c'est<space>étonné<space>par

2026-01-28 18:26:12,303 | INFO | speech length: 9760
2026-01-28 18:26:12,331 | INFO | decoder input length: 14
2026-01-28 18:26:12,331 | INFO | max output length: 14
2026-01-28 18:26:12,331 | INFO | min output length: 1
2026-01-28 18:26:12,782 | INFO | adding <eos> in the last position in the loop
2026-01-28 18:26:12,791 | INFO | no hypothesis. Finish decoding.
2026-01-28 18:26:12,793 | INFO |  -1.03 * 0.5 =  -0.51 for decoder
2026-01-28 18:26:12,793 | INFO |  -4.31 * 0.5 =  -2.15 for ctc
2026-01-28 18:26:12,793 | INFO | total log probability: -2.67
2026-01-28 18:26:12,793 | INFO | normalized log probability: -0.27
2026-01-28 18:26:12,793 | INFO | total number of ended hypotheses: 174
2026-01-28 18:26:12,793 | INFO | best hypo: tiraille

2026-01-28 18:26:12,795 | INFO | speech length: 15840
2026-01-28 18:26:12,833 | INFO | decoder input length: 24
2026-01-28 18:26:12,833 | INFO | max output length: 24
2026-01-28 18:26:12,833 | INFO | min output length: 2
2026-01-28 18:26:13,620 | INFO | adding <eos> in the last position in the loop
2026-01-28 18:26:13,630 | INFO | no hypothesis. Finish decoding.
2026-01-28 18:26:13,631 | INFO |  -1.58 * 0.5 =  -0.79 for decoder
2026-01-28 18:26:13,631 | INFO |  -2.59 * 0.5 =  -1.29 for ctc
2026-01-28 18:26:13,631 | INFO | total log probability: -2.08
2026-01-28 18:26:13,631 | INFO | normalized log probability: -0.10
2026-01-28 18:26:13,631 | INFO | total number of ended hypotheses: 170
2026-01-28 18:26:13,632 | INFO | best hypo: deux<space>milliardaires

2026-01-28 18:26:13,633 | INFO | speech length: 22560
2026-01-28 18:26:13,686 | INFO | decoder input length: 34
2026-01-28 18:26:13,686 | INFO | max output length: 34
2026-01-28 18:26:13,686 | INFO | min output length: 3
2026-01-28 18:26:14,874 | INFO | adding <eos> in the last position in the loop
2026-01-28 18:26:14,883 | INFO | no hypothesis. Finish decoding.
2026-01-28 18:26:14,884 | INFO |  -4.50 * 0.5 =  -2.25 for decoder
2026-01-28 18:26:14,884 | INFO |  -9.03 * 0.5 =  -4.51 for ctc
2026-01-28 18:26:14,885 | INFO | total log probability: -6.76
2026-01-28 18:26:14,885 | INFO | normalized log probability: -0.23
2026-01-28 18:26:14,885 | INFO | total number of ended hypotheses: 123
2026-01-28 18:26:14,885 | INFO | best hypo: c'est<space>un<space>pétolet<space>un<space>peu<space>pâté

2026-01-28 18:26:14,887 | INFO | speech length: 14240
2026-01-28 18:26:14,923 | INFO | decoder input length: 21
2026-01-28 18:26:14,923 | INFO | max output length: 21
2026-01-28 18:26:14,923 | INFO | min output length: 2
2026-01-28 18:26:15,614 | INFO | adding <eos> in the last position in the loop
2026-01-28 18:26:15,624 | INFO | no hypothesis. Finish decoding.
2026-01-28 18:26:15,626 | INFO |  -1.75 * 0.5 =  -0.88 for decoder
2026-01-28 18:26:15,626 | INFO |  -1.62 * 0.5 =  -0.81 for ctc
2026-01-28 18:26:15,626 | INFO | total log probability: -1.69
2026-01-28 18:26:15,626 | INFO | normalized log probability: -0.12
2026-01-28 18:26:15,626 | INFO | total number of ended hypotheses: 183
2026-01-28 18:26:15,627 | INFO | best hypo: et<space>y<space>attirer

2026-01-28 18:26:15,640 | INFO | Chunk: 0 | WER=12.500000 | S=4 D=3 I=0
2026-01-28 18:26:15,640 | INFO | Chunk: 1 | WER=50.000000 | S=1 D=0 I=0
2026-01-28 18:26:15,641 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:26:15,641 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:26:15,641 | INFO | Chunk: 4 | WER=33.333333 | S=0 D=0 I=2
2026-01-28 18:26:15,641 | INFO | Chunk: 5 | WER=80.000000 | S=2 D=0 I=2
2026-01-28 18:26:15,642 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:26:15,642 | INFO | Chunk: 7 | WER=50.000000 | S=2 D=0 I=1
2026-01-28 18:26:15,650 | INFO | Chunk: 8 | WER=40.000000 | S=34 D=8 I=6
2026-01-28 18:26:15,650 | INFO | Chunk: 9 | WER=14.285714 | S=2 D=0 I=1
2026-01-28 18:26:15,651 | INFO | Chunk: 10 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:26:15,651 | INFO | Chunk: 11 | WER=33.333333 | S=0 D=0 I=2
2026-01-28 18:26:15,651 | INFO | Chunk: 12 | WER=27.272727 | S=0 D=2 I=1
2026-01-28 18:26:15,651 | INFO | Chunk: 13 | WER=133.333333 | S=2 D=0 I=2
2026-01-28 18:26:15,652 | INFO | Chunk: 14 | WER=22.222222 | S=0 D=0 I=2
2026-01-28 18:26:15,652 | INFO | Chunk: 15 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:26:15,652 | INFO | Chunk: 16 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:26:15,653 | INFO | Chunk: 17 | WER=13.333333 | S=1 D=1 I=0
2026-01-28 18:26:15,653 | INFO | Chunk: 18 | WER=20.000000 | S=1 D=1 I=0
2026-01-28 18:26:15,653 | INFO | Chunk: 19 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 18:26:15,653 | INFO | Chunk: 20 | WER=75.000000 | S=2 D=1 I=0
2026-01-28 18:26:15,658 | INFO | Chunk: 21 | WER=35.106383 | S=23 D=8 I=2
2026-01-28 18:26:15,662 | INFO | Chunk: 22 | WER=41.333333 | S=17 D=8 I=6
2026-01-28 18:26:15,662 | INFO | Chunk: 23 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:26:15,662 | INFO | Chunk: 24 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:26:15,662 | INFO | Chunk: 25 | WER=100.000000 | S=4 D=0 I=0
2026-01-28 18:26:15,663 | INFO | Chunk: 26 | WER=8.333333 | S=0 D=1 I=0
2026-01-28 18:26:15,663 | INFO | Chunk: 27 | WER=27.777778 | S=2 D=1 I=2
2026-01-28 18:26:15,664 | INFO | Chunk: 28 | WER=50.000000 | S=1 D=0 I=1
2026-01-28 18:26:15,664 | INFO | Chunk: 29 | WER=26.086957 | S=5 D=1 I=0
2026-01-28 18:26:15,665 | INFO | Chunk: 30 | WER=9.090909 | S=2 D=0 I=0
2026-01-28 18:26:15,665 | INFO | Chunk: 31 | WER=30.000000 | S=2 D=1 I=0
2026-01-28 18:26:15,665 | INFO | Chunk: 32 | WER=33.333333 | S=4 D=0 I=0
2026-01-28 18:26:15,666 | INFO | Chunk: 33 | WER=34.782609 | S=5 D=2 I=1
2026-01-28 18:26:15,666 | INFO | Chunk: 34 | WER=75.000000 | S=1 D=1 I=1
2026-01-28 18:26:15,667 | INFO | Chunk: 35 | WER=46.666667 | S=4 D=3 I=0
2026-01-28 18:26:15,672 | INFO | Chunk: 36 | WER=30.526316 | S=19 D=8 I=2
2026-01-28 18:26:15,672 | INFO | Chunk: 37 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:26:15,672 | INFO | Chunk: 38 | WER=12.500000 | S=2 D=0 I=0
2026-01-28 18:26:15,673 | INFO | Chunk: 39 | WER=12.500000 | S=0 D=0 I=1
2026-01-28 18:26:15,673 | INFO | Chunk: 40 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:26:15,673 | INFO | Chunk: 41 | WER=33.333333 | S=1 D=1 I=0
2026-01-28 18:26:15,674 | INFO | Chunk: 42 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:26:15,674 | INFO | Chunk: 43 | WER=50.000000 | S=2 D=0 I=1
2026-01-28 18:26:15,676 | INFO | Chunk: 44 | WER=30.769231 | S=10 D=9 I=1
2026-01-28 18:26:15,677 | INFO | Chunk: 45 | WER=22.580645 | S=3 D=3 I=1
2026-01-28 18:26:15,677 | INFO | Chunk: 46 | WER=25.000000 | S=1 D=0 I=0
2026-01-28 18:26:15,678 | INFO | Chunk: 47 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:26:15,678 | INFO | Chunk: 48 | WER=40.000000 | S=3 D=5 I=0
2026-01-28 18:26:15,678 | INFO | Chunk: 49 | WER=50.000000 | S=1 D=1 I=0
2026-01-28 18:26:15,678 | INFO | Chunk: 50 | WER=37.500000 | S=1 D=2 I=0
2026-01-28 18:26:15,679 | INFO | Chunk: 51 | WER=11.111111 | S=0 D=0 I=1
2026-01-28 18:26:15,679 | INFO | Chunk: 52 | WER=16.666667 | S=1 D=1 I=1
2026-01-28 18:26:15,679 | INFO | Chunk: 53 | WER=66.666667 | S=0 D=0 I=2
2026-01-28 18:26:15,680 | INFO | Chunk: 54 | WER=53.333333 | S=6 D=1 I=1
2026-01-28 18:26:15,680 | INFO | Chunk: 55 | WER=100.000000 | S=2 D=1 I=0
2026-01-28 18:26:15,680 | INFO | Chunk: 56 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:26:15,681 | INFO | Chunk: 57 | WER=250.000000 | S=2 D=0 I=3
2026-01-28 18:26:15,681 | INFO | Chunk: 58 | WER=100.000000 | S=1 D=1 I=0
2026-01-28 18:26:15,681 | INFO | Chunk: 59 | WER=100.000000 | S=2 D=0 I=0
2026-01-28 18:26:15,681 | INFO | Chunk: 60 | WER=62.500000 | S=4 D=1 I=0
2026-01-28 18:26:15,681 | INFO | Chunk: 61 | WER=100.000000 | S=1 D=0 I=1
2026-01-28 18:26:16,234 | INFO | File: Rhap-D2010.wav | WER=29.812438 | S=190 D=70 I=42
2026-01-28 18:26:16,234 | INFO | ------------------------------
2026-01-28 18:26:16,235 | INFO | Conf ester Done!
2026-01-28 18:33:55,054 | INFO | Chunk: 0 | WER=28.571429 | S=10 D=5 I=1
2026-01-28 18:33:55,054 | INFO | Chunk: 1 | WER=50.000000 | S=0 D=0 I=1
2026-01-28 18:33:55,055 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:33:55,055 | INFO | Chunk: 3 | WER=10.000000 | S=1 D=0 I=0
2026-01-28 18:33:55,055 | INFO | Chunk: 4 | WER=50.000000 | S=1 D=1 I=1
2026-01-28 18:33:55,055 | INFO | Chunk: 5 | WER=40.000000 | S=1 D=1 I=0
2026-01-28 18:33:55,056 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:33:55,056 | INFO | Chunk: 7 | WER=16.666667 | S=0 D=0 I=1
2026-01-28 18:33:55,065 | INFO | Chunk: 8 | WER=38.333333 | S=17 D=29 I=0
2026-01-28 18:33:55,066 | INFO | Chunk: 9 | WER=33.333333 | S=6 D=1 I=0
2026-01-28 18:33:55,066 | INFO | Chunk: 10 | WER=12.500000 | S=0 D=0 I=1
2026-01-28 18:33:55,067 | INFO | Chunk: 11 | WER=33.333333 | S=1 D=1 I=0
2026-01-28 18:33:55,067 | INFO | Chunk: 12 | WER=54.545455 | S=2 D=3 I=1
2026-01-28 18:33:55,067 | INFO | Chunk: 13 | WER=133.333333 | S=2 D=0 I=2
2026-01-28 18:33:55,068 | INFO | Chunk: 14 | WER=22.222222 | S=1 D=1 I=0
2026-01-28 18:33:55,068 | INFO | Chunk: 15 | WER=33.333333 | S=2 D=1 I=0
2026-01-28 18:33:55,068 | INFO | Chunk: 16 | WER=25.000000 | S=0 D=1 I=0
2026-01-28 18:33:55,069 | INFO | Chunk: 17 | WER=20.000000 | S=1 D=2 I=0
2026-01-28 18:33:55,069 | INFO | Chunk: 18 | WER=30.000000 | S=0 D=2 I=1
2026-01-28 18:33:55,069 | INFO | Chunk: 19 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:33:55,069 | INFO | Chunk: 20 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:33:55,076 | INFO | Chunk: 21 | WER=23.404255 | S=13 D=9 I=0
2026-01-28 18:33:55,080 | INFO | Chunk: 22 | WER=49.333333 | S=20 D=17 I=0
2026-01-28 18:33:55,081 | INFO | Chunk: 23 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:33:55,081 | INFO | Chunk: 24 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:33:55,081 | INFO | Chunk: 25 | WER=75.000000 | S=0 D=1 I=2
2026-01-28 18:33:55,082 | INFO | Chunk: 26 | WER=33.333333 | S=3 D=1 I=0
2026-01-28 18:33:55,082 | INFO | Chunk: 27 | WER=27.777778 | S=2 D=1 I=2
2026-01-28 18:33:55,083 | INFO | Chunk: 28 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 18:33:55,083 | INFO | Chunk: 29 | WER=21.739130 | S=4 D=1 I=0
2026-01-28 18:33:55,084 | INFO | Chunk: 30 | WER=4.545455 | S=1 D=0 I=0
2026-01-28 18:33:55,085 | INFO | Chunk: 31 | WER=60.000000 | S=5 D=1 I=0
2026-01-28 18:33:55,085 | INFO | Chunk: 32 | WER=41.666667 | S=3 D=2 I=0
2026-01-28 18:33:55,086 | INFO | Chunk: 33 | WER=21.739130 | S=0 D=4 I=1
2026-01-28 18:33:55,086 | INFO | Chunk: 34 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 18:33:55,087 | INFO | Chunk: 35 | WER=46.666667 | S=5 D=2 I=0
2026-01-28 18:33:55,093 | INFO | Chunk: 36 | WER=33.684211 | S=14 D=16 I=2
2026-01-28 18:33:55,093 | INFO | Chunk: 37 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:33:55,094 | INFO | Chunk: 38 | WER=6.250000 | S=1 D=0 I=0
2026-01-28 18:33:55,094 | INFO | Chunk: 39 | WER=25.000000 | S=1 D=0 I=1
2026-01-28 18:33:55,095 | INFO | Chunk: 40 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:33:55,095 | INFO | Chunk: 41 | WER=33.333333 | S=1 D=1 I=0
2026-01-28 18:33:55,095 | INFO | Chunk: 42 | WER=31.250000 | S=5 D=0 I=0
2026-01-28 18:33:55,096 | INFO | Chunk: 43 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:33:55,099 | INFO | Chunk: 44 | WER=46.153846 | S=15 D=15 I=0
2026-01-28 18:33:55,100 | INFO | Chunk: 45 | WER=29.032258 | S=5 D=3 I=1
2026-01-28 18:33:55,100 | INFO | Chunk: 46 | WER=50.000000 | S=2 D=0 I=0
2026-01-28 18:33:55,100 | INFO | Chunk: 47 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:33:55,101 | INFO | Chunk: 48 | WER=100.000000 | S=0 D=0 I=1
2026-01-28 18:33:55,101 | INFO | Chunk: 49 | WER=35.000000 | S=3 D=4 I=0
2026-01-28 18:33:55,101 | INFO | Chunk: 50 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:33:55,102 | INFO | Chunk: 51 | WER=100.000000 | S=2 D=0 I=0
2026-01-28 18:33:55,102 | INFO | Chunk: 52 | WER=37.500000 | S=1 D=2 I=0
2026-01-28 18:33:55,102 | INFO | Chunk: 53 | WER=11.111111 | S=0 D=0 I=1
2026-01-28 18:33:55,103 | INFO | Chunk: 54 | WER=22.222222 | S=2 D=1 I=1
2026-01-28 18:33:55,103 | INFO | Chunk: 55 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 18:33:55,104 | INFO | Chunk: 56 | WER=33.333333 | S=2 D=3 I=0
2026-01-28 18:33:55,104 | INFO | Chunk: 57 | WER=100.000000 | S=3 D=0 I=0
2026-01-28 18:33:55,104 | INFO | Chunk: 58 | WER=7.142857 | S=1 D=0 I=0
2026-01-28 18:33:55,104 | INFO | Chunk: 59 | WER=250.000000 | S=2 D=0 I=3
2026-01-28 18:33:55,105 | INFO | Chunk: 60 | WER=100.000000 | S=1 D=1 I=0
2026-01-28 18:33:55,105 | INFO | Chunk: 61 | WER=100.000000 | S=2 D=0 I=0
2026-01-28 18:33:55,105 | INFO | Chunk: 62 | WER=62.500000 | S=4 D=0 I=1
2026-01-28 18:33:55,105 | INFO | Chunk: 63 | WER=100.000000 | S=1 D=0 I=1
2026-01-28 18:33:55,814 | INFO | File: Rhap-D2010.wav | WER=32.086614 | S=173 D=129 I=24
2026-01-28 18:33:55,815 | INFO | ------------------------------
2026-01-28 18:33:55,816 | INFO | hmm_tdnn Done!
2026-01-28 18:33:56,123 | INFO | ==================================Rhap-D2011.wav=========================================
2026-01-28 18:33:56,453 | INFO | Using rVAD model
2026-01-28 18:34:16,574 | INFO | Chunk: 0 | WER=28.750000 | S=5 D=2 I=16
2026-01-28 18:34:16,580 | INFO | Chunk: 1 | WER=46.666667 | S=6 D=23 I=13
2026-01-28 18:34:16,589 | INFO | Chunk: 2 | WER=43.750000 | S=5 D=21 I=16
2026-01-28 18:34:16,591 | INFO | Chunk: 3 | WER=82.926829 | S=3 D=18 I=13
2026-01-28 18:34:16,592 | INFO | Chunk: 4 | WER=103.225806 | S=24 D=2 I=6
2026-01-28 18:34:16,594 | INFO | Chunk: 5 | WER=65.000000 | S=3 D=7 I=16
2026-01-28 18:34:16,595 | INFO | Chunk: 6 | WER=200.000000 | S=5 D=0 I=5
2026-01-28 18:34:16,600 | INFO | Chunk: 7 | WER=61.111111 | S=9 D=30 I=16
2026-01-28 18:34:16,602 | INFO | Chunk: 8 | WER=94.736842 | S=16 D=2 I=0
2026-01-28 18:34:16,607 | INFO | Chunk: 9 | WER=63.768116 | S=5 D=24 I=15
2026-01-28 18:34:16,613 | INFO | Chunk: 10 | WER=46.153846 | S=1 D=16 I=13
2026-01-28 18:34:16,621 | INFO | Chunk: 11 | WER=35.106383 | S=3 D=15 I=15
2026-01-28 18:34:16,628 | INFO | Chunk: 12 | WER=61.855670 | S=7 D=37 I=16
2026-01-28 18:34:16,629 | INFO | Chunk: 13 | WER=100.000000 | S=25 D=0 I=2
2026-01-28 18:34:16,643 | INFO | Chunk: 14 | WER=56.034483 | S=9 D=36 I=20
2026-01-28 18:34:16,644 | INFO | Chunk: 15 | WER=92.592593 | S=19 D=5 I=1
2026-01-28 18:34:17,446 | INFO | File: Rhap-D2011.wav | WER=23.910841 | S=85 D=103 I=48
2026-01-28 18:34:17,447 | INFO | ------------------------------
2026-01-28 18:34:17,447 | INFO | w2vec vad chunk Done!
2026-01-28 18:34:52,100 | INFO | Chunk: 0 | WER=58.750000 | S=2 D=45 I=0
2026-01-28 18:34:52,103 | INFO | Chunk: 1 | WER=67.777778 | S=3 D=58 I=0
2026-01-28 18:34:52,106 | INFO | Chunk: 2 | WER=68.750000 | S=2 D=64 I=0
2026-01-28 18:34:52,107 | INFO | Chunk: 3 | WER=78.048780 | S=1 D=19 I=12
2026-01-28 18:34:52,108 | INFO | Chunk: 4 | WER=106.451613 | S=1 D=13 I=19
2026-01-28 18:34:52,109 | INFO | Chunk: 5 | WER=25.000000 | S=1 D=9 I=0
2026-01-28 18:34:52,109 | INFO | Chunk: 6 | WER=200.000000 | S=5 D=0 I=5
2026-01-28 18:34:52,113 | INFO | Chunk: 7 | WER=62.222222 | S=21 D=35 I=0
2026-01-28 18:34:52,113 | INFO | Chunk: 8 | WER=94.736842 | S=16 D=2 I=0
2026-01-28 18:34:52,115 | INFO | Chunk: 9 | WER=78.260870 | S=3 D=49 I=2
2026-01-28 18:34:52,116 | INFO | Chunk: 10 | WER=70.769231 | S=1 D=45 I=0
2026-01-28 18:34:52,120 | INFO | Chunk: 11 | WER=52.127660 | S=12 D=37 I=0
2026-01-28 18:34:52,121 | INFO | Chunk: 12 | WER=81.443299 | S=0 D=79 I=0
2026-01-28 18:34:52,122 | INFO | Chunk: 13 | WER=96.296296 | S=24 D=1 I=1
2026-01-28 18:34:52,126 | INFO | Chunk: 14 | WER=75.000000 | S=23 D=63 I=1
2026-01-28 18:34:52,127 | INFO | Chunk: 15 | WER=92.592593 | S=19 D=5 I=1
2026-01-28 18:34:52,402 | INFO | File: Rhap-D2011.wav | WER=62.613982 | S=91 D=505 I=22
2026-01-28 18:34:52,403 | INFO | ------------------------------
2026-01-28 18:34:52,403 | INFO | whisper med Done!
2026-01-28 18:35:39,701 | INFO | Chunk: 0 | WER=57.500000 | S=1 D=41 I=4
2026-01-28 18:35:39,704 | INFO | Chunk: 1 | WER=60.000000 | S=3 D=51 I=0
2026-01-28 18:35:39,707 | INFO | Chunk: 2 | WER=57.291667 | S=3 D=52 I=0
2026-01-28 18:35:39,708 | INFO | Chunk: 3 | WER=82.926829 | S=0 D=20 I=14
2026-01-28 18:35:39,709 | INFO | Chunk: 4 | WER=96.774194 | S=24 D=2 I=4
2026-01-28 18:35:39,710 | INFO | Chunk: 5 | WER=27.500000 | S=1 D=10 I=0
2026-01-28 18:35:39,710 | INFO | Chunk: 6 | WER=200.000000 | S=5 D=0 I=5
2026-01-28 18:35:39,714 | INFO | Chunk: 7 | WER=53.333333 | S=16 D=30 I=2
2026-01-28 18:35:39,715 | INFO | Chunk: 8 | WER=94.736842 | S=16 D=2 I=0
2026-01-28 18:35:39,716 | INFO | Chunk: 9 | WER=71.014493 | S=2 D=45 I=2
2026-01-28 18:35:39,718 | INFO | Chunk: 10 | WER=73.846154 | S=3 D=45 I=0
2026-01-28 18:35:39,721 | INFO | Chunk: 11 | WER=59.574468 | S=9 D=47 I=0
2026-01-28 18:35:39,724 | INFO | Chunk: 12 | WER=57.731959 | S=2 D=54 I=0
2026-01-28 18:35:39,724 | INFO | Chunk: 13 | WER=96.296296 | S=24 D=1 I=1
2026-01-28 18:35:39,728 | INFO | Chunk: 14 | WER=58.620690 | S=2 D=66 I=0
2026-01-28 18:35:39,729 | INFO | Chunk: 15 | WER=92.592593 | S=19 D=5 I=1
2026-01-28 18:35:40,029 | INFO | File: Rhap-D2011.wav | WER=55.420466 | S=71 D=457 I=19
2026-01-28 18:35:40,029 | INFO | ------------------------------
2026-01-28 18:35:40,029 | INFO | whisper large Done!
2026-01-28 18:35:40,204 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 18:35:40,242 | INFO | Vocabulary size: 350
2026-01-28 18:35:41,277 | INFO | Gradient checkpoint layers: []
2026-01-28 18:35:42,040 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 18:35:42,046 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 18:35:42,046 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 18:35:42,047 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 18:35:42,047 | INFO | speech length: 414560
2026-01-28 18:35:42,110 | INFO | decoder input length: 647
2026-01-28 18:35:42,110 | INFO | max output length: 647
2026-01-28 18:35:42,110 | INFO | min output length: 64
2026-01-28 18:36:08,307 | INFO | end detected at 224
2026-01-28 18:36:08,308 | INFO | -491.53 * 0.5 = -245.76 for decoder
2026-01-28 18:36:08,308 | INFO | -95.05 * 0.5 = -47.53 for ctc
2026-01-28 18:36:08,308 | INFO | total log probability: -293.29
2026-01-28 18:36:08,308 | INFO | normalized log probability: -1.34
2026-01-28 18:36:08,308 | INFO | total number of ended hypotheses: 156
2026-01-28 18:36:08,311 | INFO | best hypo: ▁il▁y▁a▁des▁produits▁qui▁vous▁changent▁tellement▁la▁vie▁qu'onsolient▁mais▁pourquoi▁on▁ne▁les▁a▁pas▁inventés▁plutôt▁et▁bien▁c'est▁le▁cas▁aujourd'hui▁de▁ce▁produit▁vraiment▁révolutionnaire▁et▁au▁début▁on▁ne▁croyez▁pas▁oi▁avec▁toute▁l'équibe▁de▁la▁boutteille▁en▁avalérie▁et▁pourtant▁aujourd'hui▁on▁y▁croit▁drre▁comme▁faite▁il▁s'agit▁tout▁simplement▁de▁ces▁cy▁une▁boule▁de▁lamage▁que▁vous▁allez▁placer▁au▁quart▁de▁votre▁linge▁sans▁rien▁d'autre▁avant▁d'émettre▁votre▁la▁machine▁à▁route

2026-01-28 18:36:08,315 | INFO | speech length: 446240
2026-01-28 18:36:08,355 | INFO | decoder input length: 696
2026-01-28 18:36:08,355 | INFO | max output length: 696
2026-01-28 18:36:08,355 | INFO | min output length: 69
2026-01-28 18:36:38,546 | INFO | end detected at 254
2026-01-28 18:36:38,547 | INFO | -557.27 * 0.5 = -278.63 for decoder
2026-01-28 18:36:38,548 | INFO | -224.88 * 0.5 = -112.44 for ctc
2026-01-28 18:36:38,548 | INFO | total log probability: -391.07
2026-01-28 18:36:38,548 | INFO | normalized log probability: -1.59
2026-01-28 18:36:38,548 | INFO | total number of ended hypotheses: 161
2026-01-28 18:36:38,551 | INFO | best hypo: ▁voilà▁ces▁manières▁incroyables▁effectivement▁vous▁oubliez▁vos▁produits▁habituels▁et▁vous▁mettez▁à▁l'intérieur▁la▁poule▁magique▁laboule▁de▁lavage▁avec▁vos▁vêtements▁et▁vous▁allez▁voir▁leurs▁réserttats▁on▁peut▁véritablement▁parles▁de▁miracles▁v▁les▁vieilleurs▁exactivements▁et▁regardez▁on▁là▁mise▁à▁l'é▁preuve▁en▁présens▁d'un▁huisiter▁pour▁vous▁prouez▁heffectivement▁à▁ce▁que▁l'on▁vous▁en▁ract▁regardez▁nous▁allons▁vous▁voir▁l'efficacité▁de▁notre▁boule▁magique▁notre▁boules▁de▁lavages▁en▁compagnie▁de▁maîtres▁de▁vingt▁dix▁huit▁siés

2026-01-28 18:36:38,554 | INFO | speech length: 474560
2026-01-28 18:36:38,597 | INFO | decoder input length: 741
2026-01-28 18:36:38,597 | INFO | max output length: 741
2026-01-28 18:36:38,598 | INFO | min output length: 74
2026-01-28 18:37:10,073 | INFO | end detected at 221
2026-01-28 18:37:10,075 | INFO | -500.39 * 0.5 = -250.19 for decoder
2026-01-28 18:37:10,076 | INFO | -170.06 * 0.5 = -85.03 for ctc
2026-01-28 18:37:10,076 | INFO | total log probability: -335.22
2026-01-28 18:37:10,076 | INFO | normalized log probability: -1.56
2026-01-28 18:37:10,076 | INFO | total number of ended hypotheses: 163
2026-01-28 18:37:10,079 | INFO | best hypo: ▁bonjours▁maître▁bonjour▁pierre▁alors▁voulez▁vous▁bien▁vérifier▁qu'il▁n'y▁a▁pas▁de▁lessive▁à▁l'intérieur▁deux▁autres▁machines▁à▁laver▁dans▁le▁tambour▁et▁également▁dans▁le▁tiroir▁alorsà▁la▁machine▁est▁vib▁parfait▁à▁le▁regarder▁on▁maître▁ici▁cette▁chemiset▁avec▁dufontain▁sur▁le▁colon▁difficileine▁à▁partir▁également▁un▁grosse▁tache▁de▁café▁constte▁des▁maîtres▁très▁sale▁très▁très▁sale▁on▁a▁ici▁un▁pantalon▁avec▁une▁grosse▁tacheage▁de▁grenadines▁le▁je▁le▁mettes▁à▁l'intérieur▁et▁pur▁à▁notre▁père▁de▁basquette▁également

2026-01-28 18:37:10,083 | INFO | speech length: 192320
2026-01-28 18:37:10,136 | INFO | decoder input length: 300
2026-01-28 18:37:10,136 | INFO | max output length: 300
2026-01-28 18:37:10,136 | INFO | min output length: 30
2026-01-28 18:37:18,105 | INFO | end detected at 106
2026-01-28 18:37:18,106 | INFO | -52.49 * 0.5 = -26.25 for decoder
2026-01-28 18:37:18,107 | INFO | -34.24 * 0.5 = -17.12 for ctc
2026-01-28 18:37:18,107 | INFO | total log probability: -43.37
2026-01-28 18:37:18,107 | INFO | normalized log probability: -0.43
2026-01-28 18:37:18,107 | INFO | total number of ended hypotheses: 168
2026-01-28 18:37:18,108 | INFO | best hypo: ▁tressaillit▁allez▁dans▁notre▁machine▁on▁va▁rajouter▁tout▁simplement▁une▁autre▁boule▁magique▁qui▁va▁remplacer▁votre▁lessive▁à▁l'intérieur▁du▁tambour▁je▁ferme▁le▁tiroir▁je▁fermement▁l'hublot▁vous▁mettez▁laissez▁les▁maîtres

2026-01-28 18:37:18,110 | INFO | speech length: 185920
2026-01-28 18:37:18,164 | INFO | decoder input length: 290
2026-01-28 18:37:18,164 | INFO | max output length: 290
2026-01-28 18:37:18,164 | INFO | min output length: 29
2026-01-28 18:37:24,367 | INFO | end detected at 83
2026-01-28 18:37:24,369 | INFO | -51.99 * 0.5 = -25.99 for decoder
2026-01-28 18:37:24,369 | INFO | -20.11 * 0.5 = -10.06 for ctc
2026-01-28 18:37:24,370 | INFO | total log probability: -36.05
2026-01-28 18:37:24,370 | INFO | normalized log probability: -0.47
2026-01-28 18:37:24,370 | INFO | total number of ended hypotheses: 176
2026-01-28 18:37:24,371 | INFO | best hypo: ▁ses▁parties▁laissées▁effectivement▁sur▁le▁hublot▁également▁sur▁le▁tiroir▁je▁mets▁en▁marche▁voilà▁et▁ses▁parties▁donc▁pour▁une▁heure▁quarante▁cinq▁de▁lavage▁et▁on▁se▁retrouve▁juste▁après

2026-01-28 18:37:24,390 | INFO | speech length: 240640
2026-01-28 18:37:24,483 | INFO | decoder input length: 375
2026-01-28 18:37:24,483 | INFO | max output length: 375
2026-01-28 18:37:24,483 | INFO | min output length: 37
2026-01-28 18:37:35,285 | INFO | end detected at 114
2026-01-28 18:37:35,287 | INFO | -213.01 * 0.5 = -106.51 for decoder
2026-01-28 18:37:35,288 | INFO | -71.53 * 0.5 = -35.76 for ctc
2026-01-28 18:37:35,288 | INFO | total log probability: -142.27
2026-01-28 18:37:35,288 | INFO | normalized log probability: -1.33
2026-01-28 18:37:35,288 | INFO | total number of ended hypotheses: 175
2026-01-28 18:37:35,290 | INFO | best hypo: ▁voilà▁le▁cycle▁de▁lavage▁déterminé▁je▁vais▁vous▁demander▁maître▁de▁briser▁le▁scellé▁sur▁notre▁hubelot▁voilà▁qui▁est▁fait▁j'ouvre▁et▁nous▁allons▁voir▁le▁résultat▁alors▁ici▁nous▁allons▁notre▁pantalons▁avec▁les▁ches▁grenadier▁voilà▁plurien

2026-01-28 18:37:35,293 | INFO | speech length: 31039
2026-01-28 18:37:35,340 | INFO | decoder input length: 47
2026-01-28 18:37:35,340 | INFO | max output length: 47
2026-01-28 18:37:35,340 | INFO | min output length: 4
2026-01-28 18:37:36,516 | INFO | end detected at 28
2026-01-28 18:37:36,517 | INFO |  -2.74 * 0.5 =  -1.37 for decoder
2026-01-28 18:37:36,517 | INFO |  -5.01 * 0.5 =  -2.51 for ctc
2026-01-28 18:37:36,517 | INFO | total log probability: -3.87
2026-01-28 18:37:36,517 | INFO | normalized log probability: -0.17
2026-01-28 18:37:36,518 | INFO | total number of ended hypotheses: 176
2026-01-28 18:37:36,518 | INFO | best hypo: ▁constatez▁maître▁comme▁moi▁je▁ne▁vois▁plus▁de▁tâches

2026-01-28 18:37:36,520 | INFO | speech length: 447200
2026-01-28 18:37:36,557 | INFO | decoder input length: 698
2026-01-28 18:37:36,557 | INFO | max output length: 698
2026-01-28 18:37:36,557 | INFO | min output length: 69
2026-01-28 18:38:03,118 | INFO | end detected at 205
2026-01-28 18:38:03,120 | INFO | -597.12 * 0.5 = -298.56 for decoder
2026-01-28 18:38:03,120 | INFO | -266.51 * 0.5 = -133.25 for ctc
2026-01-28 18:38:03,121 | INFO | total log probability: -431.81
2026-01-28 18:38:03,121 | INFO | normalized log probability: -2.19
2026-01-28 18:38:03,121 | INFO | total number of ended hypotheses: 159
2026-01-28 18:38:03,124 | INFO | best hypo: ▁c'est▁propre▁nous▁avions▁également▁ici▁notre▁chemisier▁alors▁avec▁sur▁le▁col▁dufontain▁l'ayapurien▁et▁également▁sur▁le▁côté▁plus▁de▁trace▁de▁café▁c'est▁impecccable▁et▁ensuite▁nos▁chaussurs▁qui▁sont▁ici▁où▁ahoh▁ça▁c'z▁beau'▁elles▁est▁blanches▁le▁blancs▁immatées▁comme▁les▁chaussures'▁et▁tout▁celas▁grâte▁à▁notre▁en▁bouleau▁de▁lavage▁mais'îtres▁alors▁qu'est▁ce▁qu'on▁peut▁les▁direra▁en▁concsons▁bien▁que▁sans▁savons▁rien▁qu'a▁la▁boule▁tout▁ces▁là

2026-01-28 18:38:03,127 | INFO | speech length: 97760
2026-01-28 18:38:03,178 | INFO | decoder input length: 152
2026-01-28 18:38:03,179 | INFO | max output length: 152
2026-01-28 18:38:03,179 | INFO | min output length: 15
2026-01-28 18:38:06,543 | INFO | end detected at 53
2026-01-28 18:38:06,545 | INFO |  -4.80 * 0.5 =  -2.40 for decoder
2026-01-28 18:38:06,545 | INFO |  -3.43 * 0.5 =  -1.72 for ctc
2026-01-28 18:38:06,545 | INFO | total log probability: -4.12
2026-01-28 18:38:06,545 | INFO | normalized log probability: -0.09
2026-01-28 18:38:06,545 | INFO | total number of ended hypotheses: 161
2026-01-28 18:38:06,546 | INFO | best hypo: ▁merci▁beaucoup▁maître▁donc▁la▁boule▁de▁lavage▁notre▁boule▁magique▁elle▁va▁véritablement▁remplacer▁votre▁lessive

2026-01-28 18:38:06,549 | INFO | speech length: 297600
2026-01-28 18:38:06,602 | INFO | decoder input length: 464
2026-01-28 18:38:06,602 | INFO | max output length: 464
2026-01-28 18:38:06,602 | INFO | min output length: 46
2026-01-28 18:38:21,985 | INFO | end detected at 166
2026-01-28 18:38:21,986 | INFO | -300.29 * 0.5 = -150.15 for decoder
2026-01-28 18:38:21,986 | INFO | -88.99 * 0.5 = -44.49 for ctc
2026-01-28 18:38:21,986 | INFO | total log probability: -194.64
2026-01-28 18:38:21,986 | INFO | normalized log probability: -1.22
2026-01-28 18:38:21,986 | INFO | total number of ended hypotheses: 165
2026-01-28 18:38:21,988 | INFO | best hypo: ▁alors▁comment▁ça▁marche▁eh▁bien▁les▁secrets▁il▁est▁là▁petit▁bruit▁qu'on▁entend▁que▁l'insepira▁tout▁simplement▁à▁l'intérieur▁de▁la▁boule▁magique▁il▁y▁a▁des▁billes▁en▁céramiques▁qui▁ont▁chacune▁un▁efficacité▁de▁lavages▁tout▁à▁fait▁est▁différent▁et▁là▁ce▁que▁je▁vous▁propose▁s'est▁d'écouuter▁bien▁sûrs▁le▁spécialistes▁nous▁explique▁comment▁ça▁marche▁votre▁boule▁magique

2026-01-28 18:38:21,991 | INFO | speech length: 420640
2026-01-28 18:38:22,031 | INFO | decoder input length: 656
2026-01-28 18:38:22,031 | INFO | max output length: 656
2026-01-28 18:38:22,032 | INFO | min output length: 65
2026-01-28 18:38:42,219 | INFO | end detected at 172
2026-01-28 18:38:42,220 | INFO | -342.35 * 0.5 = -171.17 for decoder
2026-01-28 18:38:42,220 | INFO | -54.39 * 0.5 = -27.20 for ctc
2026-01-28 18:38:42,220 | INFO | total log probability: -198.37
2026-01-28 18:38:42,220 | INFO | normalized log probability: -1.19
2026-01-28 18:38:42,220 | INFO | total number of ended hypotheses: 173
2026-01-28 18:38:42,222 | INFO | best hypo: ▁grâce▁au▁rayonnement▁infrarouge▁la▁boule▁magique▁accroît▁le▁mouvement▁moléculaire▁de▁l'eau▁et▁renforce▁donc▁son▁pouvoir▁nettoyant▁elle▁abaisse▁la▁tension▁superficielle▁de▁l'eau▁ce▁qui▁permet▁une▁meilleure▁pénétration▁au▁niveau▁de▁la▁fibre▁textile▁et▁plus▁en▁émettant▁les▁ons▁négatifs▁elle▁réduit▁l'hésion▁entre▁les▁fibres▁ce▁qui▁va▁faciliter▁l'élimination▁des▁impurtes

2026-01-28 18:38:42,225 | INFO | speech length: 466240
2026-01-28 18:38:42,288 | INFO | decoder input length: 728
2026-01-28 18:38:42,288 | INFO | max output length: 728
2026-01-28 18:38:42,288 | INFO | min output length: 72
2026-01-28 18:39:13,972 | INFO | end detected at 257
2026-01-28 18:39:13,974 | INFO | -564.69 * 0.5 = -282.34 for decoder
2026-01-28 18:39:13,974 | INFO | -127.23 * 0.5 = -63.62 for ctc
2026-01-28 18:39:13,974 | INFO | total log probability: -345.96
2026-01-28 18:39:13,974 | INFO | normalized log probability: -1.37
2026-01-28 18:39:13,974 | INFO | total number of ended hypotheses: 177
2026-01-28 18:39:13,977 | INFO | best hypo: ▁la▁boule▁magique▁capte▁le▁chlore▁présent▁dans▁l'eaudeville▁et▁protège▁l'élasticité▁et▁les▁couleurs▁du▁tissu▁ne▁contenant▁aucun▁émergent▁elle▁ne▁laisse▁aucun▁résidu▁de▁substance▁allergènes▁sur▁les▁vêtements▁ce▁qui▁écartes▁tout▁riste▁d'iryritation▁ou▁d'allergie▁et▁vous▁'allez▁faire▁en▁plus▁d'énortes▁économiques▁siz▁vous▁utilisez▁notre▁boule▁magique▁à▁raiseon▁d'une▁machine▁par▁joure▁pendant▁pas▁ans▁plus▁de▁mille▁eurs▁assez▁vraiment▁exceptionnels▁alors▁magaly▁est▁avec▁nous▁magie▁utilise▁depuis▁combien▁de▁temps▁notre▁boule▁magiques▁déplie▁des▁moins

2026-01-28 18:39:13,980 | INFO | speech length: 369280
2026-01-28 18:39:14,032 | INFO | decoder input length: 576
2026-01-28 18:39:14,032 | INFO | max output length: 576
2026-01-28 18:39:14,032 | INFO | min output length: 57
2026-01-28 18:39:35,611 | INFO | end detected at 205
2026-01-28 18:39:35,612 | INFO | -482.27 * 0.5 = -241.13 for decoder
2026-01-28 18:39:35,612 | INFO | -151.81 * 0.5 = -75.90 for ctc
2026-01-28 18:39:35,612 | INFO | total log probability: -317.04
2026-01-28 18:39:35,612 | INFO | normalized log probability: -1.58
2026-01-28 18:39:35,612 | INFO | total number of ended hypotheses: 132
2026-01-28 18:39:35,615 | INFO | best hypo: ▁dites▁nous▁comment▁est▁votre▁linge▁qu'est▁ce▁que▁vous▁en▁pensez▁de▁la▁boule▁magique▁c'est▁génial▁mon▁linge▁et▁superpropre▁il▁est▁d'une▁incroyable▁douceur▁le▁couleurs▁sont▁conservées▁leblin▁et▁tréblin▁c'est▁vraiment▁formidable▁et▁le▁brnigreux▁garçon▁qui▁sole▁non▁plus▁un▁tout▁à▁fait▁est▁un▁garçon▁qui▁se▁salie▁qui▁faisait▁les▁allergies▁auparavant▁qui▁n'en▁fait▁plus▁grâce▁au▁la▁boule▁magie▁et▁au▁niveau▁d'un▁dés▁odeurs▁est▁ce▁queuse▁c'est▁et▁eupicace▁également

2026-01-28 18:39:35,617 | INFO | speech length: 116000
2026-01-28 18:39:35,656 | INFO | decoder input length: 180
2026-01-28 18:39:35,657 | INFO | max output length: 180
2026-01-28 18:39:35,657 | INFO | min output length: 18
2026-01-28 18:39:39,601 | INFO | end detected at 66
2026-01-28 18:39:39,603 | INFO | -11.58 * 0.5 =  -5.79 for decoder
2026-01-28 18:39:39,603 | INFO |  -3.08 * 0.5 =  -1.54 for ctc
2026-01-28 18:39:39,603 | INFO | total log probability: -7.33
2026-01-28 18:39:39,603 | INFO | normalized log probability: -0.12
2026-01-28 18:39:39,603 | INFO | total number of ended hypotheses: 162
2026-01-28 18:39:39,604 | INFO | best hypo: ▁ah▁oui▁alors▁ça▁enlève▁bien▁les▁odeurs▁toutes▁les▁odeurs▁qui▁peut▁y▁avoir▁sur▁du▁laige▁et▁un▁plus▁ça▁apporte▁une▁odeur▁de▁fraîcheur

2026-01-28 18:39:39,606 | INFO | speech length: 462880
2026-01-28 18:39:39,649 | INFO | decoder input length: 722
2026-01-28 18:39:39,649 | INFO | max output length: 722
2026-01-28 18:39:39,649 | INFO | min output length: 72
2026-01-28 18:40:13,648 | INFO | end detected at 284
2026-01-28 18:40:13,650 | INFO | -686.26 * 0.5 = -343.13 for decoder
2026-01-28 18:40:13,650 | INFO | -249.50 * 0.5 = -124.75 for ctc
2026-01-28 18:40:13,650 | INFO | total log probability: -467.88
2026-01-28 18:40:13,650 | INFO | normalized log probability: -1.69
2026-01-28 18:40:13,650 | INFO | total number of ended hypotheses: 181
2026-01-28 18:40:13,654 | INFO | best hypo: ▁passez▁comme▁très▁agressé▁et▁silencieuse▁de▁la▁boule▁magique▁dans▁le▁tantographe▁et▁les▁tournes▁pré▁silenciels▁n'ont▁qu'un▁problème▁comme▁s'elle▁n'était▁pay▁elle▁n'abîme▁absolument▁pas▁le▁linge▁pas▁du▁tout▁voilà▁donc▁l'aboule▁magique'est▁vraiment▁pas▁l'idé▁à▁l'on▁plus▁et▁bien▁vous▁allez▁réser▁la▁planète▁avec▁la▁boule▁magique▁effectivement▁pu▁ce▁qu'in▁onser▁la▁quantité▁et▁le▁nomble▁de▁kilos▁de▁litress▁effectimant▁de▁produits▁nettoyages▁qui▁sont▁déversers▁partouts▁dans▁le▁monde▁parents▁et▁bien▁l'arts▁allez▁vraiment▁faire▁avec▁un▁jeunes▁geste▁et▁je▁suis▁sûr▁que▁vous▁allez▁à▁être▁convaincues▁par▁notre▁poule▁magique

2026-01-28 18:40:13,657 | INFO | speech length: 119200
2026-01-28 18:40:13,705 | INFO | decoder input length: 185
2026-01-28 18:40:13,705 | INFO | max output length: 185
2026-01-28 18:40:13,705 | INFO | min output length: 18
2026-01-28 18:40:17,532 | INFO | end detected at 69
2026-01-28 18:40:17,533 | INFO |  -5.91 * 0.5 =  -2.95 for decoder
2026-01-28 18:40:17,533 | INFO |  -5.90 * 0.5 =  -2.95 for ctc
2026-01-28 18:40:17,533 | INFO | total log probability: -5.90
2026-01-28 18:40:17,533 | INFO | normalized log probability: -0.09
2026-01-28 18:40:17,533 | INFO | total number of ended hypotheses: 154
2026-01-28 18:40:17,534 | INFO | best hypo: ▁tout▁comme▁magalie▁merci▁beaucoup▁d'être▁venu▁sur▁notre▁plateau▁et▁maintenant▁effectivement▁la▁boule▁magique▁vous▁n'utiliserez▁plus▁que▁cela

2026-01-28 18:40:17,546 | INFO | Chunk: 0 | WER=46.250000 | S=14 D=4 I=19
2026-01-28 18:40:17,551 | INFO | Chunk: 1 | WER=68.888889 | S=19 D=21 I=22
2026-01-28 18:40:17,557 | INFO | Chunk: 2 | WER=55.208333 | S=20 D=17 I=16
2026-01-28 18:40:17,558 | INFO | Chunk: 3 | WER=82.926829 | S=2 D=18 I=14
2026-01-28 18:40:17,559 | INFO | Chunk: 4 | WER=103.225806 | S=30 D=0 I=2
2026-01-28 18:40:17,561 | INFO | Chunk: 5 | WER=50.000000 | S=2 D=8 I=10
2026-01-28 18:40:17,561 | INFO | Chunk: 6 | WER=200.000000 | S=5 D=0 I=5
2026-01-28 18:40:17,566 | INFO | Chunk: 7 | WER=76.666667 | S=16 D=28 I=25
2026-01-28 18:40:17,566 | INFO | Chunk: 8 | WER=94.736842 | S=16 D=2 I=0
2026-01-28 18:40:17,570 | INFO | Chunk: 9 | WER=59.420290 | S=10 D=15 I=16
2026-01-28 18:40:17,573 | INFO | Chunk: 10 | WER=50.769231 | S=4 D=16 I=13
2026-01-28 18:40:17,578 | INFO | Chunk: 11 | WER=52.127660 | S=16 D=17 I=16
2026-01-28 18:40:17,583 | INFO | Chunk: 12 | WER=63.917526 | S=12 D=29 I=21
2026-01-28 18:40:17,584 | INFO | Chunk: 13 | WER=96.296296 | S=24 D=1 I=1
2026-01-28 18:40:17,592 | INFO | Chunk: 14 | WER=77.586207 | S=36 D=27 I=27
2026-01-28 18:40:17,592 | INFO | Chunk: 15 | WER=92.592593 | S=19 D=5 I=1
2026-01-28 18:40:18,132 | INFO | File: Rhap-D2011.wav | WER=36.068896 | S=210 D=73 I=73
2026-01-28 18:40:18,133 | INFO | ------------------------------
2026-01-28 18:40:18,133 | INFO | Conf cv Done!
2026-01-28 18:40:18,310 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 18:40:18,333 | INFO | Vocabulary size: 47
2026-01-28 18:40:19,471 | INFO | Gradient checkpoint layers: []
2026-01-28 18:40:20,282 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 18:40:20,288 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 18:40:20,288 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 18:40:20,289 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 18:40:20,292 | INFO | speech length: 414560
2026-01-28 18:40:20,344 | INFO | decoder input length: 647
2026-01-28 18:40:20,344 | INFO | max output length: 647
2026-01-28 18:40:20,344 | INFO | min output length: 64
2026-01-28 18:41:09,188 | INFO | end detected at 490
2026-01-28 18:41:09,190 | INFO | -304.36 * 0.5 = -152.18 for decoder
2026-01-28 18:41:09,190 | INFO | -54.36 * 0.5 = -27.18 for ctc
2026-01-28 18:41:09,190 | INFO | total log probability: -179.36
2026-01-28 18:41:09,190 | INFO | normalized log probability: -0.37
2026-01-28 18:41:09,190 | INFO | total number of ended hypotheses: 180
2026-01-28 18:41:09,196 | INFO | best hypo: il<space>y<space>a<space>des<space>produits<space>qui<space>vous<space>changent<space>tellement<space>la<space>vie<space>qu'on<space>se<space>dit<space>mais<space>pourquoi<space>on<space>les<space>a<space>pas<space>inventés<space>plus<space>tôt<space>eh<space>bien<space>c'est<space>le<space>cas<space>aujourd'hui<space>de<space>ce<space>produit<space>vraiment<space>révolutionnaire<space>alors<space>au<space>début<space>on<space>n'y<space>croyait<space>pas<space>de<space>trois<space>avec<space>toute<space>l'équipe<space>de<space>la<space>bouteille<space>volérie<space>et<space>pourtant<space>aujourd'hui<space>on<space>y<space>croit<space>dur<space>comme<space>faire<space>il<space>s'agit<space>tout<space>simplement<space>de<space>ceci<space>une<space>boule<space>de<space>lavage<space>que<space>vous<space>allez<space>placer<space>au<space>cheur<space>de<space>votre<space>linge<space>sans<space>rien<space>d'autre<space>avant<space>delle<space>mattre<space>votre<space>machine<space>e<space>route

2026-01-28 18:41:09,199 | INFO | speech length: 446240
2026-01-28 18:41:09,236 | INFO | decoder input length: 696
2026-01-28 18:41:09,236 | INFO | max output length: 696
2026-01-28 18:41:09,237 | INFO | min output length: 69
2026-01-28 18:42:03,035 | INFO | end detected at 538
2026-01-28 18:42:03,038 | INFO | -497.41 * 0.5 = -248.71 for decoder
2026-01-28 18:42:03,038 | INFO | -45.99 * 0.5 = -22.99 for ctc
2026-01-28 18:42:03,038 | INFO | total log probability: -271.70
2026-01-28 18:42:03,038 | INFO | normalized log probability: -0.52
2026-01-28 18:42:03,038 | INFO | total number of ended hypotheses: 219
2026-01-28 18:42:03,044 | INFO | best hypo: voilà<space>c'est<space>ça<space>c'est<space>il<space>est<space>incroyable<space>effectivement<space>vous<space>oubliez<space>vos<space>produits<space>habituels<space>et<space>vous<space>mettez<space>à<space>l'intérieur<space>la<space>boule<space>magique<space>la<space>boule<space>de<space>la<space>vage<space>avec<space>vos<space>vêtements<space>et<space>vous<space>allez<space>voir<space>leurs<space>résultats<space>on<space>peut<space>véritablement<space>parler<space>de<space>miracle<space>valérie<space>exactement<space>et<space>regardez<space>on<space>l'a<space>mise<space>à<space>l'épreuve<space>en<space>présence<space>d'un<space>huitie<space>pour<space>vous<space>prouver<space>effectivement<space>ce<space>que<space>l'on<space>vous<space>raconte<space>regardez<space>nous<space>allons<space>vous<space>pouver<space>l'efficacité<space>de<space>notre<space>boule<space>magique<space>e<space>t<space>notre<space>boule<space>de<space>la<space>vage<space>en<space>compagnie<space>de<space>maîtres<space>vint<space>dist<space>huit<space>ciest

2026-01-28 18:42:03,047 | INFO | speech length: 474560
2026-01-28 18:42:03,084 | INFO | decoder input length: 741
2026-01-28 18:42:03,084 | INFO | max output length: 741
2026-01-28 18:42:03,085 | INFO | min output length: 74
2026-01-28 18:42:57,529 | INFO | end detected at 520
2026-01-28 18:42:57,530 | INFO | -508.14 * 0.5 = -254.07 for decoder
2026-01-28 18:42:57,531 | INFO | -28.86 * 0.5 = -14.43 for ctc
2026-01-28 18:42:57,531 | INFO | total log probability: -268.50
2026-01-28 18:42:57,531 | INFO | normalized log probability: -0.52
2026-01-28 18:42:57,531 | INFO | total number of ended hypotheses: 168
2026-01-28 18:42:57,537 | INFO | best hypo: bonjour<space>maître<space>bonjour<space>pierre<space>alors<space>voulez<space>vous<space>bien<space>vérifier<space>qu'il<space>n'y<space>ait<space>pas<space>de<space>lissive<space>à<space>l'intérieur<space>de<space>notre<space>machine<space>à<space>laver<space>dans<space>le<space>tambour<space>également<space>dans<space>les<space>tiroirs<space>alors<space>la<space>machine<space>est<space>vide<space>parfait<space>alors<space>regardez<space>on<space>va<space>mettre<space>ici<space>cette<space>chemise<space>avec<space>du<space>fontin<space>sur<space>le<space>col<space>difficile<space>à<space>partir<space>également<space>une<space>grosse<space>stage<space>de<space>café<space>on<space>constate<space>he<space>met<space>très<space>simple<space>très<space>très<space>sale<space>on<space>a<space>ici<space>un<space>pantano<space>avec<space>une<space>grosse<space>tâge<space>de<space>gronadine<space>je<space>le<space>mets<space>également<space>à<space>l'intérieur<space>et<space>puis<space>on<space>a<space>notre<space>père<space>de<space>basket<space>également

2026-01-28 18:42:57,540 | INFO | speech length: 192320
2026-01-28 18:42:57,597 | INFO | decoder input length: 300
2026-01-28 18:42:57,597 | INFO | max output length: 300
2026-01-28 18:42:57,598 | INFO | min output length: 30
2026-01-28 18:43:15,017 | INFO | end detected at 229
2026-01-28 18:43:15,018 | INFO | -33.95 * 0.5 = -16.97 for decoder
2026-01-28 18:43:15,019 | INFO | -12.01 * 0.5 =  -6.00 for ctc
2026-01-28 18:43:15,019 | INFO | total log probability: -22.98
2026-01-28 18:43:15,019 | INFO | normalized log probability: -0.10
2026-01-28 18:43:15,019 | INFO | total number of ended hypotheses: 172
2026-01-28 18:43:15,022 | INFO | best hypo: très<space>salie<space>elle<space>est<space>dans<space>notre<space>machine<space>on<space>va<space>rajouter<space>tout<space>simplement<space>notre<space>boule<space>magique<space>qui<space>va<space>remplacer<space>votre<space>lessive<space>à<space>l'intérieur<space>du<space>tambour<space>je<space>ferme<space>le<space>tiroir<space>je<space>ferme<space>également<space>lubleau<space>vous<space>mettez<space>laissé<space>les<space>mettres

2026-01-28 18:43:15,024 | INFO | speech length: 185920
2026-01-28 18:43:15,080 | INFO | decoder input length: 290
2026-01-28 18:43:15,080 | INFO | max output length: 290
2026-01-28 18:43:15,080 | INFO | min output length: 29
2026-01-28 18:43:37,176 | INFO | end detected at 199
2026-01-28 18:43:37,179 | INFO | -18.13 * 0.5 =  -9.06 for decoder
2026-01-28 18:43:37,179 | INFO |  -9.71 * 0.5 =  -4.85 for ctc
2026-01-28 18:43:37,179 | INFO | total log probability: -13.92
2026-01-28 18:43:37,180 | INFO | normalized log probability: -0.07
2026-01-28 18:43:37,180 | INFO | total number of ended hypotheses: 193
2026-01-28 18:43:37,184 | INFO | best hypo: c'est<space>parti<space>les<space>célés<space>effectivement<space>sur<space>le<space>hublot<space>et<space>également<space>sur<space>le<space>terroir<space>je<space>mets<space>en<space>marche<space>voilà<space>et<space>c'est<space>parti<space>donc<space>pour<space>une<space>heure<space>quarante<space>cinq<space>de<space>la<space>vage<space>et<space>on<space>se<space>retrouve<space>juste<space>après

2026-01-28 18:43:37,188 | INFO | speech length: 240640
2026-01-28 18:43:37,258 | INFO | decoder input length: 375
2026-01-28 18:43:37,258 | INFO | max output length: 375
2026-01-28 18:43:37,258 | INFO | min output length: 37
2026-01-28 18:43:58,748 | INFO | end detected at 264
2026-01-28 18:43:58,750 | INFO | -36.93 * 0.5 = -18.46 for decoder
2026-01-28 18:43:58,750 | INFO | -13.57 * 0.5 =  -6.78 for ctc
2026-01-28 18:43:58,750 | INFO | total log probability: -25.25
2026-01-28 18:43:58,750 | INFO | normalized log probability: -0.10
2026-01-28 18:43:58,750 | INFO | total number of ended hypotheses: 202
2026-01-28 18:43:58,754 | INFO | best hypo: voilà<space>le<space>cycle<space>de<space>la<space>vase<space>est<space>terminé<space>je<space>vais<space>vous<space>demander<space>maître<space>de<space>briser<space>le<space>célé<space>sur<space>notre<space>hubleau<space>voilà<space>qui<space>est<space>fait<space>jouvre<space>et<space>nous<space>allons<space>voir<space>le<space>résultat<space>alors<space>ici<space>nous<space>avions<space>notre<space>pantalon<space>avec<space>des<space>tâches<space>de<space>grand<space>nazi<space>voilà<space>il<space>y<space>a<space>plus<space>rien<space>hein

2026-01-28 18:43:58,756 | INFO | speech length: 31039
2026-01-28 18:43:58,813 | INFO | decoder input length: 47
2026-01-28 18:43:58,813 | INFO | max output length: 47
2026-01-28 18:43:58,813 | INFO | min output length: 4
2026-01-28 18:44:00,465 | INFO | adding <eos> in the last position in the loop
2026-01-28 18:44:00,473 | INFO | no hypothesis. Finish decoding.
2026-01-28 18:44:00,473 | INFO | -18.15 * 0.5 =  -9.07 for decoder
2026-01-28 18:44:00,473 | INFO | -38.46 * 0.5 = -19.23 for ctc
2026-01-28 18:44:00,473 | INFO | total log probability: -28.30
2026-01-28 18:44:00,473 | INFO | normalized log probability: -0.63
2026-01-28 18:44:00,473 | INFO | total number of ended hypotheses: 52
2026-01-28 18:44:00,474 | INFO | best hypo: constaté<space>maître<space>moi<space>je<space>ne<space>vois<space>plus<space>d<space>tâche

2026-01-28 18:44:00,475 | INFO | speech length: 447200
2026-01-28 18:44:00,516 | INFO | decoder input length: 698
2026-01-28 18:44:00,516 | INFO | max output length: 698
2026-01-28 18:44:00,516 | INFO | min output length: 69
2026-01-28 18:44:45,969 | INFO | end detected at 452
2026-01-28 18:44:45,971 | INFO | -435.63 * 0.5 = -217.82 for decoder
2026-01-28 18:44:45,971 | INFO | -28.67 * 0.5 = -14.33 for ctc
2026-01-28 18:44:45,971 | INFO | total log probability: -232.15
2026-01-28 18:44:45,971 | INFO | normalized log probability: -0.52
2026-01-28 18:44:45,971 | INFO | total number of ended hypotheses: 200
2026-01-28 18:44:45,976 | INFO | best hypo: c'est<space>propre<space>nous<space>avions<space>également<space>ici<space>notre<space>chemisier<space>alors<space>avec<space>sur<space>le<space>col<space>du<space>fondetin<space>là<space>y<space>a<space>plus<space>rien<space>et<space>également<space>sur<space>le<space>côté<space>plus<space>de<space>traces<space>de<space>café<space>c'est<space>impeccable<space>ensuite<space>nos<space>chaussures<space>qui<space>sont<space>ici<space>ah<space>ça<space>c'est<space>beau<space>hein<space>elles<space>sont<space>blanches<space>blancs<space>immaculées<space>pour<space>les<space>chaussures<space>et<space>tout<space>cela<space>grâce<space>à<space>notre<space>boule<space>de<space>la<space>vachlette<space>alors<space>qu'est<space>ce<space>qu'on<space>peut<space>dire<space>en<space>conclusion<space>eh<space>bien<space>que<space>sons<space>savons<space>rien<space>qu'avec<space>la<space>boule<space>tous<space>cest<space>lache

2026-01-28 18:44:45,979 | INFO | speech length: 97760
2026-01-28 18:44:46,017 | INFO | decoder input length: 152
2026-01-28 18:44:46,017 | INFO | max output length: 152
2026-01-28 18:44:46,017 | INFO | min output length: 15
2026-01-28 18:44:51,297 | INFO | end detected at 119
2026-01-28 18:44:51,298 | INFO | -11.58 * 0.5 =  -5.79 for decoder
2026-01-28 18:44:51,298 | INFO |  -6.50 * 0.5 =  -3.25 for ctc
2026-01-28 18:44:51,299 | INFO | total log probability: -9.04
2026-01-28 18:44:51,299 | INFO | normalized log probability: -0.08
2026-01-28 18:44:51,299 | INFO | total number of ended hypotheses: 171
2026-01-28 18:44:51,300 | INFO | best hypo: merci<space>beaucoup<space>maître<space>donc<space>la<space>boule<space>de<space>la<space>vage<space>notre<space>boule<space>magique<space>elle<space>va<space>véritablement<space>remplacer<space>votre<space>lessive

2026-01-28 18:44:51,302 | INFO | speech length: 297600
2026-01-28 18:44:51,343 | INFO | decoder input length: 464
2026-01-28 18:44:51,344 | INFO | max output length: 464
2026-01-28 18:44:51,344 | INFO | min output length: 46
2026-01-28 18:45:20,597 | INFO | end detected at 386
2026-01-28 18:45:20,600 | INFO | -39.21 * 0.5 = -19.61 for decoder
2026-01-28 18:45:20,601 | INFO | -48.59 * 0.5 = -24.29 for ctc
2026-01-28 18:45:20,601 | INFO | total log probability: -43.90
2026-01-28 18:45:20,601 | INFO | normalized log probability: -0.12
2026-01-28 18:45:20,601 | INFO | total number of ended hypotheses: 211
2026-01-28 18:45:20,606 | INFO | best hypo: alors<space>comment<space>ça<space>marche<space>eh<space>bien<space>le<space>secret<space>il<space>est<space>là<space>oui<space>ça<space>fait<space>si<space>vous<space>risquez<space>on<space>entend<space>alors<space>c'est<space>ça<space>va<space>tout<space>simplement<space>à<space>l'intérieur<space>de<space>la<space>boule<space>magique<space>il<space>y<space>a<space>des<space>billes<space>en<space>cyramie<space>qui<space>ont<space>chacune<space>une<space>efficacité<space>de<space>la<space>vache<space>tout<space>à<space>fait<space>différente<space>et<space>là<space>ce<space>que<space>je<space>vous<space>propose<space>c'est<space>d'écouter<space>bien<space>sûr<space>le<space>spécialiste<space>nous<space>expliquer<space>comment<space>ça<space>marche<space>d'autre<space>boule<space>magique

2026-01-28 18:45:20,610 | INFO | speech length: 420640
2026-01-28 18:45:20,660 | INFO | decoder input length: 656
2026-01-28 18:45:20,660 | INFO | max output length: 656
2026-01-28 18:45:20,660 | INFO | min output length: 65
2026-01-28 18:46:01,783 | INFO | end detected at 380
2026-01-28 18:46:01,785 | INFO | -254.04 * 0.5 = -127.02 for decoder
2026-01-28 18:46:01,785 | INFO |  -8.41 * 0.5 =  -4.21 for ctc
2026-01-28 18:46:01,785 | INFO | total log probability: -131.22
2026-01-28 18:46:01,785 | INFO | normalized log probability: -0.35
2026-01-28 18:46:01,785 | INFO | total number of ended hypotheses: 213
2026-01-28 18:46:01,790 | INFO | best hypo: grâce<space>au<space>rayonnement<space>à<space>infrarouge<space>la<space>boule<space>magique<space>accroît<space>le<space>mouvement<space>moléculaire<space>de<space>l'eau<space>et<space>renforce<space>donc<space>son<space>pouvoir<space>nettoyant<space>elle<space>abaisse<space>la<space>tension<space>superficielle<space>de<space>l'eau<space>ce<space>qui<space>permet<space>une<space>meilleure<space>pénétration<space>au<space>niveau<space>de<space>la<space>fibre<space>textile<space>de<space>plus<space>en<space>émettant<space>des<space>ions<space>négatifs<space>elle<space>réduit<space>l'adhésion<space>entre<space>les<space>fibres<space>ce<space>qui<space>va<space>faciliter<space>l'élimination<space>des<space>impur

2026-01-28 18:46:01,793 | INFO | speech length: 466240
2026-01-28 18:46:01,830 | INFO | decoder input length: 728
2026-01-28 18:46:01,830 | INFO | max output length: 728
2026-01-28 18:46:01,830 | INFO | min output length: 72
2026-01-28 18:46:58,357 | INFO | end detected at 558
2026-01-28 18:46:58,359 | INFO | -516.17 * 0.5 = -258.08 for decoder
2026-01-28 18:46:58,359 | INFO | -35.85 * 0.5 = -17.93 for ctc
2026-01-28 18:46:58,360 | INFO | total log probability: -276.01
2026-01-28 18:46:58,360 | INFO | normalized log probability: -0.50
2026-01-28 18:46:58,360 | INFO | total number of ended hypotheses: 141
2026-01-28 18:46:58,368 | INFO | best hypo: la<space>boule<space>magique<space>capte<space>le<space>clore<space>présent<space>dans<space>notre<space>ville<space>et<space>protège<space>les<space>lasticités<space>et<space>les<space>couleurs<space>du<space>tissu<space>ne<space>contenant<space>aucun<space>étergent<space>elle<space>ne<space>laisse<space>aucun<space>résidu<space>de<space>substances<space>à<space>l'ère<space>gêne<space>sur<space>les<space>vêtements<space>ce<space>qui<space>écarte<space>tout<space>risque<space>d'héritation<space>ou<space>d'allergie<space>et<space>vous<space>allez<space>faire<space>en<space>plus<space>d'énormes<space>économies<space>car<space>si<space>vous<space>utilisez<space>notre<space>boule<space>magique<space>à<space>raison<space>d'une<space>machine<space>par<space>jour<space>et<space>pendant<space>trois<space>ans<space>plus<space>de<space>mile<space>euros<space>hein<space>c'est<space>vraiment<space>exceptionnel<space>alors<space>ma<space>galie<space>est<space>avec<space>nous<space>ma<space>galle<space>tilise<space>depuis<space>combien<space>de<space>temps<space>notre<space>boule<space>magique<space>mois

2026-01-28 18:46:58,371 | INFO | speech length: 369280
2026-01-28 18:46:58,414 | INFO | decoder input length: 576
2026-01-28 18:46:58,414 | INFO | max output length: 576
2026-01-28 18:46:58,414 | INFO | min output length: 57
2026-01-28 18:47:38,486 | INFO | end detected at 504
2026-01-28 18:47:38,488 | INFO | -322.01 * 0.5 = -161.00 for decoder
2026-01-28 18:47:38,488 | INFO | -39.77 * 0.5 = -19.89 for ctc
2026-01-28 18:47:38,488 | INFO | total log probability: -180.89
2026-01-28 18:47:38,488 | INFO | normalized log probability: -0.37
2026-01-28 18:47:38,488 | INFO | total number of ended hypotheses: 201
2026-01-28 18:47:38,494 | INFO | best hypo: alors<space>dites<space>nous<space>comment<space>est<space>votre<space>linge<space>qu'est<space>ce<space>que<space>vous<space>en<space>pensez<space>de<space>la<space>boule<space>magique<space>c'est<space>génial<space>mon<space>leige<space>est<space>super<space>propre<space>il<space>est<space>d'une<space>incroyable<space>douceur<space>les<space>couleurs<space>sont<space>conservées<space>le<space>blanc<space>est<space>très<space>blanc<space>c'est<space>vraiment<space>formidable<space>moi<space>et<space>puis<space>je<space>crois<space>vous<space>avez<space>des<space>garçons<space>qui<space>seulent<space>en<space>plus<space>hein<space>tout<space>à<space>fait<space>un<space>garçon<space>qui<space>se<space>sallie<space>et<space>qui<space>faisait<space>des<space>allergies<space>auparavin<space>et<space>qui<space>n'a<space>fait<space>plus<space>grâce<space>à<space>la<space>voule<space>magie<space>e<space>au<space>niveau<space>de<space>des<space>des<space>odeurs<space>est<space>ce<space>que<space>c'est<space>efficace<space>également

2026-01-28 18:47:38,496 | INFO | speech length: 116000
2026-01-28 18:47:38,539 | INFO | decoder input length: 180
2026-01-28 18:47:38,539 | INFO | max output length: 180
2026-01-28 18:47:38,539 | INFO | min output length: 18
2026-01-28 18:47:45,436 | INFO | end detected at 146
2026-01-28 18:47:45,438 | INFO | -12.17 * 0.5 =  -6.09 for decoder
2026-01-28 18:47:45,438 | INFO |  -2.94 * 0.5 =  -1.47 for ctc
2026-01-28 18:47:45,438 | INFO | total log probability: -7.55
2026-01-28 18:47:45,438 | INFO | normalized log probability: -0.05
2026-01-28 18:47:45,438 | INFO | total number of ended hypotheses: 201
2026-01-28 18:47:45,440 | INFO | best hypo: ah<space>oui<space>alors<space>ça<space>enlève<space>bien<space>les<space>odeurs<space>euh<space>toutes<space>les<space>odeurs<space>qu'il<space>peut<space>y<space>avoir<space>sur<space>du<space>lège<space>et<space>en<space>plus<space>ça<space>apporte<space>une<space>odeur<space>de<space>fraîcheur

2026-01-28 18:47:45,442 | INFO | speech length: 462880
2026-01-28 18:47:45,480 | INFO | decoder input length: 722
2026-01-28 18:47:45,480 | INFO | max output length: 722
2026-01-28 18:47:45,480 | INFO | min output length: 72
2026-01-28 18:48:42,386 | INFO | end detected at 612
2026-01-28 18:48:42,387 | INFO | -712.13 * 0.5 = -356.07 for decoder
2026-01-28 18:48:42,387 | INFO | -184.65 * 0.5 = -92.33 for ctc
2026-01-28 18:48:42,388 | INFO | total log probability: -448.39
2026-01-28 18:48:42,388 | INFO | normalized log probability: -0.74
2026-01-28 18:48:42,388 | INFO | total number of ended hypotheses: 156
2026-01-28 18:48:42,395 | INFO | best hypo: ça<space>c'es<space>quand<space>même<space>très<space>a<space>escalé<space>silencieuse<space>la<space>boule<space>magique<space>dans<space>le<space>tems<space>pour<space>la<space>fin<space>du<space>tour<space>et<space>si<space>la<space>sias<space>aucun<space>problème<space>comme<space>ça<space>n'y<space>était<space>pas<space>elle<space>n'abîme<space>absolument<space>pas<space>le<space>linge<space>pas<space>du<space>tout<space>voilà<space>donc<space>là<space>vous<space>le<space>magique<space>c'est<space>vraiment<space>l'idéal<space>et<space>en<space>plus<space>eh<space>bien<space>vous<space>allez<space>préserver<space>la<space>planète<space>avec<space>la<space>boule<space>magique<space>effectivement<space>puisque<space>quand<space>on<space>sait<space>la<space>quantité<space>est<space>le<space>nombre<space>de<space>kilos<space>de<space>litres<space>effactivement<space>d<space>predu<space>nettoyages<space>qui<space>sont<space>déversé<space>s<space>partout<space>heun<space>dans<space>le<space>monde<space>parean<space>et<space>bien<space>là<space>vous<space>allez<space>vraiment<space>faireun<space>jeune<space>geste<space>je<space>suis<space>sûr<space>que<space>vous<space>allez<space>être<space>convainc<space>par<space>notre<space>boule<space>magique

2026-01-28 18:48:42,398 | INFO | speech length: 119200
2026-01-28 18:48:42,454 | INFO | decoder input length: 185
2026-01-28 18:48:42,454 | INFO | max output length: 185
2026-01-28 18:48:42,454 | INFO | min output length: 18
2026-01-28 18:48:55,247 | INFO | end detected at 146
2026-01-28 18:48:55,249 | INFO | -11.71 * 0.5 =  -5.85 for decoder
2026-01-28 18:48:55,249 | INFO |  -3.18 * 0.5 =  -1.59 for ctc
2026-01-28 18:48:55,249 | INFO | total log probability: -7.45
2026-01-28 18:48:55,249 | INFO | normalized log probability: -0.05
2026-01-28 18:48:55,249 | INFO | total number of ended hypotheses: 173
2026-01-28 18:48:55,252 | INFO | best hypo: tout<space>comme<space>magali<space>merci<space>beaucoup<space>d'être<space>venu<space>sur<space>notre<space>plateau<space>et<space>maintenant<space>effectivement<space>la<space>boule<space>magique<space>vous<space>n'utiliserez<space>plus<space>que<space>cela

2026-01-28 18:48:55,277 | INFO | Chunk: 0 | WER=30.000000 | S=6 D=1 I=17
2026-01-28 18:48:55,288 | INFO | Chunk: 1 | WER=48.888889 | S=6 D=16 I=22
2026-01-28 18:48:55,300 | INFO | Chunk: 2 | WER=46.875000 | S=12 D=16 I=17
2026-01-28 18:48:55,303 | INFO | Chunk: 3 | WER=78.048780 | S=1 D=17 I=14
2026-01-28 18:48:55,306 | INFO | Chunk: 4 | WER=109.677419 | S=3 D=12 I=19
2026-01-28 18:48:55,309 | INFO | Chunk: 5 | WER=75.000000 | S=4 D=8 I=18
2026-01-28 18:48:55,310 | INFO | Chunk: 6 | WER=180.000000 | S=5 D=0 I=4
2026-01-28 18:48:55,320 | INFO | Chunk: 7 | WER=56.666667 | S=6 D=24 I=21
2026-01-28 18:48:55,321 | INFO | Chunk: 8 | WER=94.736842 | S=17 D=1 I=0
2026-01-28 18:48:55,328 | INFO | Chunk: 9 | WER=59.420290 | S=8 D=13 I=20
2026-01-28 18:48:55,335 | INFO | Chunk: 10 | WER=44.615385 | S=1 D=15 I=13
2026-01-28 18:48:55,346 | INFO | Chunk: 11 | WER=47.872340 | S=9 D=15 I=21
2026-01-28 18:48:55,359 | INFO | Chunk: 12 | WER=46.391753 | S=8 D=18 I=19
2026-01-28 18:48:55,361 | INFO | Chunk: 13 | WER=100.000000 | S=25 D=0 I=2
2026-01-28 18:48:55,378 | INFO | Chunk: 14 | WER=57.758621 | S=19 D=24 I=24
2026-01-28 18:48:55,380 | INFO | Chunk: 15 | WER=92.592593 | S=19 D=5 I=1
2026-01-28 18:48:56,570 | INFO | File: Rhap-D2011.wav | WER=24.113475 | S=125 D=33 I=80
2026-01-28 18:48:56,570 | INFO | ------------------------------
2026-01-28 18:48:56,570 | INFO | Conf ester Done!
2026-01-28 18:52:16,343 | INFO | Chunk: 0 | WER=31.250000 | S=5 D=4 I=16
2026-01-28 18:52:16,350 | INFO | Chunk: 1 | WER=44.444444 | S=3 D=19 I=18
2026-01-28 18:52:16,358 | INFO | Chunk: 2 | WER=39.583333 | S=7 D=14 I=17
2026-01-28 18:52:16,360 | INFO | Chunk: 3 | WER=85.365854 | S=4 D=17 I=14
2026-01-28 18:52:16,361 | INFO | Chunk: 4 | WER=96.774194 | S=1 D=12 I=17
2026-01-28 18:52:16,363 | INFO | Chunk: 5 | WER=67.500000 | S=2 D=8 I=17
2026-01-28 18:52:16,364 | INFO | Chunk: 6 | WER=200.000000 | S=5 D=0 I=5
2026-01-28 18:52:16,370 | INFO | Chunk: 7 | WER=57.777778 | S=8 D=23 I=21
2026-01-28 18:52:16,371 | INFO | Chunk: 8 | WER=94.736842 | S=16 D=2 I=0
2026-01-28 18:52:16,376 | INFO | Chunk: 9 | WER=59.420290 | S=7 D=18 I=16
2026-01-28 18:52:16,379 | INFO | Chunk: 10 | WER=47.692308 | S=2 D=16 I=13
2026-01-28 18:52:16,387 | INFO | Chunk: 11 | WER=44.680851 | S=7 D=14 I=21
2026-01-28 18:52:16,395 | INFO | Chunk: 12 | WER=50.515464 | S=8 D=23 I=18
2026-01-28 18:52:16,396 | INFO | Chunk: 13 | WER=96.296296 | S=24 D=1 I=1
2026-01-28 18:52:16,406 | INFO | Chunk: 14 | WER=47.413793 | S=11 D=26 I=18
2026-01-28 18:52:16,407 | INFO | Chunk: 15 | WER=92.592593 | S=19 D=6 I=0
2026-01-28 18:52:17,140 | INFO | File: Rhap-D2011.wav | WER=19.858156 | S=101 D=43 I=52
2026-01-28 18:52:17,140 | INFO | ------------------------------
2026-01-28 18:52:17,140 | INFO | hmm_tdnn Done!
2026-01-28 18:52:17,318 | INFO | ==================================Rhap-D2012.wav=========================================
2026-01-28 18:52:17,783 | INFO | Using rVAD model
2026-01-28 18:52:57,939 | INFO | Chunk: 0 | WER=79.310345 | S=13 D=6 I=27
2026-01-28 18:52:57,945 | INFO | Chunk: 1 | WER=75.675676 | S=22 D=43 I=19
2026-01-28 18:52:57,947 | INFO | Chunk: 2 | WER=108.108108 | S=33 D=0 I=7
2026-01-28 18:52:57,951 | INFO | Chunk: 3 | WER=54.545455 | S=9 D=27 I=12
2026-01-28 18:52:57,957 | INFO | Chunk: 4 | WER=58.000000 | S=7 D=28 I=23
2026-01-28 18:52:57,962 | INFO | Chunk: 5 | WER=56.043956 | S=6 D=23 I=22
2026-01-28 18:52:57,967 | INFO | Chunk: 6 | WER=71.910112 | S=7 D=24 I=33
2026-01-28 18:52:57,972 | INFO | Chunk: 7 | WER=63.809524 | S=13 D=40 I=14
2026-01-28 18:52:57,977 | INFO | Chunk: 8 | WER=57.731959 | S=8 D=33 I=15
2026-01-28 18:52:57,982 | INFO | Chunk: 9 | WER=56.179775 | S=7 D=19 I=24
2026-01-28 18:52:57,986 | INFO | Chunk: 10 | WER=83.783784 | S=12 D=27 I=23
2026-01-28 18:52:58,443 | INFO | File: Rhap-D2012.wav | WER=26.198083 | S=127 D=85 I=34
2026-01-28 18:52:58,443 | INFO | ------------------------------
2026-01-28 18:52:58,443 | INFO | w2vec vad chunk Done!
2026-01-28 18:53:25,271 | INFO | Chunk: 0 | WER=67.241379 | S=10 D=29 I=0
2026-01-28 18:53:25,273 | INFO | Chunk: 1 | WER=84.684685 | S=7 D=86 I=1
2026-01-28 18:53:25,275 | INFO | Chunk: 2 | WER=100.000000 | S=33 D=2 I=2
2026-01-28 18:53:25,277 | INFO | Chunk: 3 | WER=56.818182 | S=1 D=49 I=0
2026-01-28 18:53:25,281 | INFO | Chunk: 4 | WER=63.000000 | S=23 D=40 I=0
2026-01-28 18:53:25,284 | INFO | Chunk: 5 | WER=63.736264 | S=2 D=56 I=0
2026-01-28 18:53:25,287 | INFO | Chunk: 6 | WER=47.191011 | S=1 D=40 I=1
2026-01-28 18:53:25,289 | INFO | Chunk: 7 | WER=78.095238 | S=1 D=81 I=0
2026-01-28 18:53:25,292 | INFO | Chunk: 8 | WER=58.762887 | S=4 D=51 I=2
2026-01-28 18:53:25,293 | INFO | Chunk: 9 | WER=92.134831 | S=2 D=80 I=0
2026-01-28 18:53:25,296 | INFO | Chunk: 10 | WER=66.216216 | S=19 D=30 I=0
2026-01-28 18:53:25,502 | INFO | File: Rhap-D2012.wav | WER=65.388711 | S=64 D=544 I=6
2026-01-28 18:53:25,502 | INFO | ------------------------------
2026-01-28 18:53:25,503 | INFO | whisper med Done!
2026-01-28 18:54:02,919 | INFO | Chunk: 0 | WER=41.379310 | S=19 D=5 I=0
2026-01-28 18:54:02,922 | INFO | Chunk: 1 | WER=81.981982 | S=5 D=86 I=0
2026-01-28 18:54:02,923 | INFO | Chunk: 2 | WER=110.810811 | S=33 D=0 I=8
2026-01-28 18:54:02,926 | INFO | Chunk: 3 | WER=48.863636 | S=2 D=38 I=3
2026-01-28 18:54:02,930 | INFO | Chunk: 4 | WER=56.000000 | S=13 D=43 I=0
2026-01-28 18:54:02,932 | INFO | Chunk: 5 | WER=64.835165 | S=3 D=56 I=0
2026-01-28 18:54:02,936 | INFO | Chunk: 6 | WER=49.438202 | S=7 D=34 I=3
2026-01-28 18:54:02,938 | INFO | Chunk: 7 | WER=77.142857 | S=0 D=81 I=0
2026-01-28 18:54:02,941 | INFO | Chunk: 8 | WER=55.670103 | S=8 D=46 I=0
2026-01-28 18:54:02,943 | INFO | Chunk: 9 | WER=91.011236 | S=1 D=80 I=0
2026-01-28 18:54:02,944 | INFO | Chunk: 10 | WER=72.972973 | S=5 D=49 I=0
2026-01-28 18:54:03,168 | INFO | File: Rhap-D2012.wav | WER=62.939297 | S=63 D=516 I=12
2026-01-28 18:54:03,169 | INFO | ------------------------------
2026-01-28 18:54:03,169 | INFO | whisper large Done!
2026-01-28 18:54:03,346 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 18:54:03,384 | INFO | Vocabulary size: 350
2026-01-28 18:54:05,002 | INFO | Gradient checkpoint layers: []
2026-01-28 18:54:05,834 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 18:54:05,838 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 18:54:05,838 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 18:54:05,839 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 18:54:05,839 | INFO | speech length: 405280
2026-01-28 18:54:05,888 | INFO | decoder input length: 632
2026-01-28 18:54:05,889 | INFO | max output length: 632
2026-01-28 18:54:05,889 | INFO | min output length: 63
2026-01-28 18:54:29,325 | INFO | end detected at 213
2026-01-28 18:54:29,328 | INFO | -512.97 * 0.5 = -256.48 for decoder
2026-01-28 18:54:29,328 | INFO | -129.45 * 0.5 = -64.73 for ctc
2026-01-28 18:54:29,328 | INFO | total log probability: -321.21
2026-01-28 18:54:29,328 | INFO | normalized log probability: -1.54
2026-01-28 18:54:29,328 | INFO | total number of ended hypotheses: 192
2026-01-28 18:54:29,331 | INFO | best hypo: ▁deux▁préludes▁de▁karol▁schimanovski▁extrait▁de▁son▁opus▁un▁deux▁mille▁neuf▁cent▁sixième▁et▁le▁septième▁interprété▁par▁maria▁yudina▁a▁enregistré▁en▁mille▁neuf▁cent▁cinquante▁si▁on▁n'avait▁pas▁choisi▁udina▁par▁haza▁si▁dievano▁non▁je▁l'a▁le▁choisi▁parce▁que▁c'était▁une▁élève▁de▁bomenfeld▁felis▁bron▁menfeld▁l'oncle▁de▁cheminocing▁tempspparaître▁tout▁ailleurs▁don▁'on▁peut▁penser▁que▁c'est▁une▁tradition▁ici▁qu'il▁est▁repsentait▁l'on▁peut▁penser▁que▁'

2026-01-28 18:54:29,335 | INFO | speech length: 442080
2026-01-28 18:54:29,379 | INFO | decoder input length: 690
2026-01-28 18:54:29,379 | INFO | max output length: 690
2026-01-28 18:54:29,379 | INFO | min output length: 69
2026-01-28 18:55:00,604 | INFO | end detected at 231
2026-01-28 18:55:00,608 | INFO | -554.54 * 0.5 = -277.27 for decoder
2026-01-28 18:55:00,608 | INFO | -150.65 * 0.5 = -75.32 for ctc
2026-01-28 18:55:00,608 | INFO | total log probability: -352.59
2026-01-28 18:55:00,608 | INFO | normalized log probability: -1.57
2026-01-28 18:55:00,608 | INFO | total number of ended hypotheses: 189
2026-01-28 18:55:00,612 | INFO | best hypo: ▁à▁l'époque▁de▁schumanowski▁ont▁joué▁se▁prélude▁comme▁ça▁que▁par▁exemple▁neuhaus▁le▁cousin▁schumanovski▁le▁jouait▁de▁cette▁façon▁aussi▁lancé▁à▁la▁fois▁comme▁un▁diresque▁prélude▁s'a▁fait▁passer▁à▁'fredachappa▁et▁matchu▁face▁à▁chaupina▁dutchu▁s'ap▁plutôt▁dans▁une▁mouvance▁valnérienne▁le▁premier▁qu'on▁a'ait▁imentendu▁et▁très▁tristanien▁de▁leurs▁à▁un▁moment▁le▁motif▁avec▁les▁monté▁chromatique▁du▁d'houser▁du▁désir▁de▁tristan▁qui▁apparaît▁et▁c'est▁l'ocasion▁de▁dire▁qu'

2026-01-28 18:55:00,615 | INFO | speech length: 234240
2026-01-28 18:55:00,658 | INFO | decoder input length: 365
2026-01-28 18:55:00,658 | INFO | max output length: 365
2026-01-28 18:55:00,659 | INFO | min output length: 36
2026-01-28 18:55:10,188 | INFO | end detected at 103
2026-01-28 18:55:10,190 | INFO | -90.41 * 0.5 = -45.20 for decoder
2026-01-28 18:55:10,190 | INFO | -17.07 * 0.5 =  -8.54 for ctc
2026-01-28 18:55:10,190 | INFO | total log probability: -53.74
2026-01-28 18:55:10,190 | INFO | normalized log probability: -0.55
2026-01-28 18:55:10,190 | INFO | total number of ended hypotheses: 159
2026-01-28 18:55:10,192 | INFO | best hypo: ▁la▁découverte▁de▁wagner▁pour▁schumanowski▁a▁peut▁être▁déterminé▁sa▁vocation▁musicale▁il▁a▁treize▁ans▁il▁est▁à▁l'opéra▁de▁vienne▁et▁il▁entend▁loengrin▁et▁il▁a▁dit▁ensuite▁à▁plusieurs▁reprises▁a▁été▁vraiment▁pour▁lui▁une▁révélation

2026-01-28 18:55:10,195 | INFO | speech length: 460320
2026-01-28 18:55:10,245 | INFO | decoder input length: 718
2026-01-28 18:55:10,245 | INFO | max output length: 718
2026-01-28 18:55:10,245 | INFO | min output length: 71
2026-01-28 18:55:37,952 | INFO | end detected at 196
2026-01-28 18:55:37,954 | INFO | -451.80 * 0.5 = -225.90 for decoder
2026-01-28 18:55:37,954 | INFO | -96.23 * 0.5 = -48.11 for ctc
2026-01-28 18:55:37,954 | INFO | total log probability: -274.01
2026-01-28 18:55:37,955 | INFO | normalized log probability: -1.46
2026-01-28 18:55:37,955 | INFO | total number of ended hypotheses: 186
2026-01-28 18:55:37,957 | INFO | best hypo: ▁je▁me▁demande▁d'ailleurs▁si▁l'arrivée▁du▁berger▁dans▁le▁roi▁roger▁ce▁berger▁qui▁amène▁d'autre▁d'autres▁valeurs▁si▁vous▁voulez▁dans▁un▁monde▁qui▁figé▁ne▁rappellent▁pas▁un▁peu▁l'arrivée▁de▁louenvrd▁mais▁je▁ne▁voudrais▁pas▁faire▁de▁la▁psychalyse▁à▁trois▁sous▁c'est▁à▁la▁fois▁ce▁moment▁là▁wingin▁et▁pas▁siphal▁impson▁ça▁rait▁que▁le▁mangerine▁est▁partifal▁effectivement▁la▁pass▁dous▁de▁votre▁ouvrage▁d'

2026-01-28 18:55:37,960 | INFO | speech length: 442720
2026-01-28 18:55:38,014 | INFO | decoder input length: 691
2026-01-28 18:55:38,014 | INFO | max output length: 691
2026-01-28 18:55:38,014 | INFO | min output length: 69
2026-01-28 18:56:06,441 | INFO | end detected at 243
2026-01-28 18:56:06,442 | INFO | -541.58 * 0.5 = -270.79 for decoder
2026-01-28 18:56:06,442 | INFO | -148.35 * 0.5 = -74.18 for ctc
2026-01-28 18:56:06,442 | INFO | total log probability: -344.96
2026-01-28 18:56:06,443 | INFO | normalized log probability: -1.45
2026-01-28 18:56:06,443 | INFO | total number of ended hypotheses: 136
2026-01-28 18:56:06,446 | INFO | best hypo: ▁monumentale▁biographie▁de▁schimanovski▁jolissant▁le▁portrait▁de▁notre▁de▁votre▁héros▁vers▁vingt▁six▁ans▁à▁peu▁près▁par▁un▁ami▁qui▁l'attend▁à▁la▁gare▁il▁était▁d'une▁bonne▁taille▁très▁élégamment▁vêtue▁boîtée▁légèrement▁de▁la▁jambe▁gauche▁qui▁sanstre▁plus▁courte▁que▁la▁droite▁trahissait▁discrètement▁à▁la▁chaque▁pas▁un▁raidississement▁de▁l'articulation▁la▁lavalière▁et▁soie▁noire▁de▁la▁la▁photographie▁de▁berlin▁deux▁mille▁neuf▁cent▁six▁avaient▁fait▁place▁à▁une▁cravate▁parfaitement▁nouée▁de▁la▁couleur▁idéalement▁messortie▁de▁avec▁le▁on▁de▁sa▁pèlerine

2026-01-28 18:56:06,448 | INFO | speech length: 445600
2026-01-28 18:56:06,495 | INFO | decoder input length: 695
2026-01-28 18:56:06,495 | INFO | max output length: 695
2026-01-28 18:56:06,496 | INFO | min output length: 69
2026-01-28 18:56:33,497 | INFO | end detected at 232
2026-01-28 18:56:33,498 | INFO | -509.40 * 0.5 = -254.70 for decoder
2026-01-28 18:56:33,498 | INFO | -121.28 * 0.5 = -60.64 for ctc
2026-01-28 18:56:33,498 | INFO | total log probability: -315.34
2026-01-28 18:56:33,498 | INFO | normalized log probability: -1.40
2026-01-28 18:56:33,498 | INFO | total number of ended hypotheses: 165
2026-01-28 18:56:33,501 | INFO | best hypo: ▁toutes▁les▁négoces▁de▁giménesquettes▁et▁du▁meilleur▁genre▁et▁tout▁cela▁n'était▁qu'une▁sorte▁d'accompagnement▁pour▁le▁charme▁qui▁émanait▁de▁la▁personne▁de▁l'artiste▁par▁lequel▁il▁attirait▁aussitôt▁étaient▁visibles▁dans▁la▁dourceur▁du▁regard▁de▁ses▁grands▁yeux▁vert▁foncésait▁alors▁ectivement▁schimmanovski▁était▁une▁homme▁de▁très▁élégant▁et▁il▁avait▁un▁côté▁aristocrate▁un▁côté▁d'handy▁a▁côté▁et▁esthète▁qu'il▁gardea▁jusqu'à▁la▁fin▁de▁sa▁vie▁et▁il▁était▁très▁attaché▁à▁la▁beauté▁des▁choses▁ce▁qui

2026-01-28 18:56:33,504 | INFO | speech length: 471680
2026-01-28 18:56:33,576 | INFO | decoder input length: 736
2026-01-28 18:56:33,576 | INFO | max output length: 736
2026-01-28 18:56:33,576 | INFO | min output length: 73
2026-01-28 18:57:08,254 | INFO | end detected at 230
2026-01-28 18:57:08,256 | INFO | -715.64 * 0.5 = -357.82 for decoder
2026-01-28 18:57:08,256 | INFO | -352.69 * 0.5 = -176.35 for ctc
2026-01-28 18:57:08,256 | INFO | total log probability: -534.16
2026-01-28 18:57:08,256 | INFO | normalized log probability: -2.38
2026-01-28 18:57:08,256 | INFO | total number of ended hypotheses: 173
2026-01-28 18:57:08,259 | INFO | best hypo: ▁on▁explique▁que▁mais▁il▁aimait▁bien▁descendre▁dans▁les▁rivaux▁hôtels▁dans▁les▁grands▁hôtels▁et▁à▁la▁fin▁de▁sa▁vie▁quand▁il▁est▁en▁france▁d'ailleurs▁essayer▁de▁soigner▁mais▁en▁vain▁sa▁tuberculose▁et▁durant▁s'il▁s'il▁jouil▁aller▁dans▁un▁bruit▁buit▁de▁caricature▁d'ant▁'enfin▁et▁s'il▁jouits▁descendre▁dans▁un▁'hôtels▁de▁la▁cette▁catéegorie▁'anspope▁à▁la▁peraine▁et▁il▁fette▁dire▁de▁la▁guerre▁de▁va▁être▁sa▁famie▁avec▁un▁grand▁changement▁de▁fortte▁de▁là▁ne▁somme▁encore▁devant▁guerre▁somme▁dans▁cette

2026-01-28 18:57:08,262 | INFO | speech length: 461120
2026-01-28 18:57:08,315 | INFO | decoder input length: 720
2026-01-28 18:57:08,316 | INFO | max output length: 720
2026-01-28 18:57:08,316 | INFO | min output length: 72
2026-01-28 18:57:37,803 | INFO | end detected at 221
2026-01-28 18:57:37,805 | INFO | -717.08 * 0.5 = -358.54 for decoder
2026-01-28 18:57:37,805 | INFO | -182.32 * 0.5 = -91.16 for ctc
2026-01-28 18:57:37,805 | INFO | total log probability: -449.70
2026-01-28 18:57:37,805 | INFO | normalized log probability: -2.09
2026-01-28 18:57:37,805 | INFO | total number of ended hypotheses: 173
2026-01-28 18:57:37,808 | INFO | best hypo: ▁pologne▁encore▁occupé▁mais▁où▁la▁vie▁artistique▁est▁très▁importante▁et▁où▁se▁coalise▁en▁quelque▁sorte▁le▁mouvement▁de▁la▁jeune▁pologne▁la▁jeune▁pologne▁c'ait▁d'amour▁un▁mouvement▁artistique▁est▁ensuite▁'est▁devenus▁du▁côté▁de▁la▁musique▁à▁la▁jeune▁pologne▁en▁musique▁avec▁quatre▁ositeurs▁tous▁de▁ses▁élèves▁d'aurs▁de▁nos▁skowskis▁et▁qui▁se▁retrouve▁'ilia▁skowski▁ryjitzki▁la▁fit▁telberg▁qui▁ensuite▁va▁abeandonnery▁la▁composition▁pour▁la▁dire▁dire▁'orchestre▁et▁puis

2026-01-28 18:57:37,811 | INFO | speech length: 455040
2026-01-28 18:57:37,855 | INFO | decoder input length: 710
2026-01-28 18:57:37,855 | INFO | max output length: 710
2026-01-28 18:57:37,855 | INFO | min output length: 71
2026-01-28 18:58:02,316 | INFO | end detected at 197
2026-01-28 18:58:02,318 | INFO | -450.57 * 0.5 = -225.29 for decoder
2026-01-28 18:58:02,318 | INFO | -145.65 * 0.5 = -72.82 for ctc
2026-01-28 18:58:02,318 | INFO | total log probability: -298.11
2026-01-28 18:58:02,318 | INFO | normalized log probability: -1.56
2026-01-28 18:58:02,318 | INFO | total number of ended hypotheses: 165
2026-01-28 18:58:02,321 | INFO | best hypo: ▁il▁y▁a▁eu▁chezluto▁lors▁suite▁un▁groupe▁qui▁va▁très▁vite▁se▁désunir▁parce▁qu'il▁y▁a▁des▁jalousies▁à▁des▁inimitiés▁les▁uns▁ont▁plus▁de▁succès▁que▁les▁autres▁l'ont▁de▁ce▁n'est▁pas▁fait▁pour▁cimenter▁ce▁groupe▁mais▁qui▁enfin▁et▁c'▁est▁intéresstant▁que▁parce▁qu'on▁allasse▁voulait▁quatre▁mousquetaire▁jay▁la▁jeune▁musique▁polonaise▁qui▁veulait▁en▁découdre▁avec▁le▁cons▁avatisme▁de▁varsovie▁et▁qui▁fondre▁société▁d'édition▁dont▁le▁siège▁est▁à▁berlin

2026-01-28 18:58:02,323 | INFO | speech length: 442720
2026-01-28 18:58:02,376 | INFO | decoder input length: 691
2026-01-28 18:58:02,376 | INFO | max output length: 691
2026-01-28 18:58:02,377 | INFO | min output length: 69
2026-01-28 18:58:27,972 | INFO | end detected at 214
2026-01-28 18:58:27,974 | INFO | -528.01 * 0.5 = -264.00 for decoder
2026-01-28 18:58:27,975 | INFO | -143.35 * 0.5 = -71.67 for ctc
2026-01-28 18:58:27,975 | INFO | total log probability: -335.68
2026-01-28 18:58:27,975 | INFO | normalized log probability: -1.61
2026-01-28 18:58:27,975 | INFO | total number of ended hypotheses: 182
2026-01-28 18:58:27,977 | INFO | best hypo: ▁il▁a▁surévidemment▁un▁plus▁grand▁rayonnement▁que▁varsovie▁et▁c'est▁dans▁cet▁esprit▁qu'il▁est▁proposé▁par▁chin▁manuscrit▁l'ouverture▁du▁concert▁de▁la▁puce▁douze▁alors▁l'ouverture▁du▁concert▁qui▁date▁de▁mille▁neuf▁cent▁cinq▁neuf▁cent▁six▁date▁de▁justement▁de▁ces▁années▁là▁dix▁années▁de▁la▁jeune▁pologne▁que▁on▁voit▁les▁marque▁'on▁val▁'entendre▁et▁la▁marque▁de▁straus▁à▁cette▁très▁très▁évidente▁de▁cette▁partis▁et▁elle▁recommunique▁tous▁dans▁ces▁culte▁de▁straus▁et▁un▁moment▁ojitzki▁raconte▁dans▁ses▁souvenirs

2026-01-28 18:58:27,980 | INFO | speech length: 393920
2026-01-28 18:58:28,023 | INFO | decoder input length: 615
2026-01-28 18:58:28,023 | INFO | max output length: 615
2026-01-28 18:58:28,023 | INFO | min output length: 61
2026-01-28 18:58:50,238 | INFO | end detected at 207
2026-01-28 18:58:50,239 | INFO | -492.52 * 0.5 = -246.26 for decoder
2026-01-28 18:58:50,240 | INFO | -177.53 * 0.5 = -88.77 for ctc
2026-01-28 18:58:50,240 | INFO | total log probability: -335.02
2026-01-28 18:58:50,240 | INFO | normalized log probability: -1.67
2026-01-28 18:58:50,240 | INFO | total number of ended hypotheses: 172
2026-01-28 18:58:50,242 | INFO | best hypo: ▁peu▁il▁jouait▁à▁quatre▁mains▁à▁touche▁trans▁schtrousske▁szymanowski▁jouait▁aussi▁à▁quatre▁mains▁avec▁le▁cousin▁de▁lahauski▁pianiste▁bien▁connu▁et▁rugizsky▁die▁ne▁pouvait▁pas▁s'▁empêchers▁à▁la▁fin▁de▁mauvais▁transefiguration▁de▁faire▁raisonnér▁et▁raisonnér▁et▁raisonnr▁et▁encore▁ses▁accords▁qui▁nous▁échantaient▁pas▁facés▁à▁extrer▁de▁cette▁ouverture▁des▁connerts▁inter'prétée▁par▁l'orches▁national▁de▁la▁radio▁polonaise▁directée▁de▁kzimir▁kor

2026-01-28 18:58:50,254 | INFO | Chunk: 0 | WER=81.034483 | S=16 D=3 I=28
2026-01-28 18:58:50,260 | INFO | Chunk: 1 | WER=85.585586 | S=33 D=42 I=20
2026-01-28 18:58:50,261 | INFO | Chunk: 2 | WER=105.405405 | S=33 D=0 I=6
2026-01-28 18:58:50,266 | INFO | Chunk: 3 | WER=59.090909 | S=4 D=28 I=20
2026-01-28 18:58:50,271 | INFO | Chunk: 4 | WER=66.000000 | S=9 D=30 I=27
2026-01-28 18:58:50,277 | INFO | Chunk: 5 | WER=72.527473 | S=16 D=24 I=26
2026-01-28 18:58:50,282 | INFO | Chunk: 6 | WER=92.134831 | S=22 D=25 I=35
2026-01-28 18:58:50,287 | INFO | Chunk: 7 | WER=69.523810 | S=14 D=40 I=19
2026-01-28 18:58:50,293 | INFO | Chunk: 8 | WER=62.886598 | S=12 D=29 I=20
2026-01-28 18:58:50,298 | INFO | Chunk: 9 | WER=65.168539 | S=13 D=19 I=26
2026-01-28 18:58:50,301 | INFO | Chunk: 10 | WER=97.297297 | S=19 D=26 I=27
2026-01-28 18:58:50,785 | INFO | File: Rhap-D2012.wav | WER=38.977636 | S=208 D=85 I=73
2026-01-28 18:58:50,785 | INFO | ------------------------------
2026-01-28 18:58:50,785 | INFO | Conf cv Done!
2026-01-28 18:58:50,966 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 18:58:50,989 | INFO | Vocabulary size: 47
2026-01-28 18:58:52,147 | INFO | Gradient checkpoint layers: []
2026-01-28 18:58:52,883 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 18:58:52,888 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 18:58:52,889 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 18:58:52,889 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 18:58:52,892 | INFO | speech length: 405280
2026-01-28 18:58:52,949 | INFO | decoder input length: 632
2026-01-28 18:58:52,949 | INFO | max output length: 632
2026-01-28 18:58:52,949 | INFO | min output length: 63
2026-01-28 18:59:39,136 | INFO | end detected at 486
2026-01-28 18:59:39,138 | INFO | -418.42 * 0.5 = -209.21 for decoder
2026-01-28 18:59:39,138 | INFO | -83.76 * 0.5 = -41.88 for ctc
2026-01-28 18:59:39,138 | INFO | total log probability: -251.09
2026-01-28 18:59:39,138 | INFO | normalized log probability: -0.52
2026-01-28 18:59:39,138 | INFO | total number of ended hypotheses: 169
2026-01-28 18:59:39,144 | INFO | best hypo: deux<space>préludes<space>de<space>karol<space>schimanowski<space>extrait<space>de<space>son<space>opus<space>un<space>de<space>mille<space>neuf<space>cent<space>le<space>sixième<space>et<space>le<space>septième<space>interprété<space>par<space>euh<space>maria<space>youdina<space>enregistré<space>mille<space>neuf<space>cent<space>cinquante<space>six<space>vous<space>n'avez<space>pas<space>choisi<space>youdina<space>par<space>hasard<space>il<space>dit<space>vrai<space>nous<space>non<space>je<space>l'ai<space>choisi<space>parce<space>que<space>c'était<space>une<space>une<space>élève<space>de<space>boumendefeld<space>de<space>filiks<space>pour<space>menfeld<space>donc<space>les<space>chimats<space>d'aussi<space>dont<space>on<space>en<space>parlait<space>tout<space>à<space>l'heure<space>donc<space>on<space>peut<space>penser<space>que<space>c'est<space>ue<space>tradition<space>euh<space>ici<space>quiest<space>représentaie<space>t<space>on<space>peut<space>penser<space>que

2026-01-28 18:59:39,147 | INFO | speech length: 442080
2026-01-28 18:59:39,184 | INFO | decoder input length: 690
2026-01-28 18:59:39,184 | INFO | max output length: 690
2026-01-28 18:59:39,184 | INFO | min output length: 69
2026-01-28 19:00:31,320 | INFO | end detected at 551
2026-01-28 19:00:31,321 | INFO | -476.82 * 0.5 = -238.41 for decoder
2026-01-28 19:00:31,322 | INFO | -70.39 * 0.5 = -35.20 for ctc
2026-01-28 19:00:31,322 | INFO | total log probability: -273.61
2026-01-28 19:00:31,322 | INFO | normalized log probability: -0.50
2026-01-28 19:00:31,322 | INFO | total number of ended hypotheses: 185
2026-01-28 19:00:31,328 | INFO | best hypo: euh<space>à<space>l'époque<space>de<space>schimanowski<space>ont<space>joué<space>c'est<space>prébute<space>comme<space>ça<space>que<space>par<space>exemple<space>dans<space>jaros<space>le<space>kusan<space>chimanowski<space>est<space>joué<space>euh<space>de<space>cette<space>façon<space>aussi<space>alors<space>c'est<space>à<space>la<space>fois<space>euh<space>comment<space>dire<space>est<space>ce<space>que<space>prélude<space>ça<space>fait<space>passer<space>évidemment<space>frédéric<space>chapin<space>mais<space>moi<space>je<space>trouve<space>pas<space>ça<space>shopinien<space>du<space>tout<space>je<space>trouve<space>ça<space>plutôt<space>dans<space>une<space>mouvance<space>alnérienne<space>bah<space>le<space>premier<space>qu'on<space>a<space>entendu<space>est<space>très<space>tristanlien<space>il<space>y<space>a<space>d'a<space>leure<space>à<space>un<space>moment<space>le<space>le<space>motif<space>euh<space>motieut<space>monté<space>craumatique<space>du<space>dil<space>dat<space>motif<space>du<space>désir<space>de<space>tristan<space>qui<space>apparaît<space>et<space>c'est<space>l'occasion<space>de<space>dire<space>que

2026-01-28 19:00:31,331 | INFO | speech length: 234240
2026-01-28 19:00:31,391 | INFO | decoder input length: 365
2026-01-28 19:00:31,392 | INFO | max output length: 365
2026-01-28 19:00:31,392 | INFO | min output length: 36
2026-01-28 19:00:47,931 | INFO | end detected at 249
2026-01-28 19:00:47,933 | INFO | -33.94 * 0.5 = -16.97 for decoder
2026-01-28 19:00:47,934 | INFO | -24.67 * 0.5 = -12.33 for ctc
2026-01-28 19:00:47,934 | INFO | total log probability: -29.30
2026-01-28 19:00:47,934 | INFO | normalized log probability: -0.12
2026-01-28 19:00:47,934 | INFO | total number of ended hypotheses: 228
2026-01-28 19:00:47,937 | INFO | best hypo: la<space>découverte<space>de<space>wagner<space>pu<space>schimanowski<space>a<space>peut<space>être<space>déterminé<space>sa<space>vocation<space>musicale<space>il<space>y<space>a<space>treize<space>ans<space>il<space>est<space>à<space>l'opéra<space>de<space>vienne<space>il<space>entend<space>l'ohengrine<space>et<space>il<space>l'a<space>dit<space>ensuite<space>à<space>plusieurs<space>reprises<space>ça<space>a<space>été<space>vraiment<space>pour<space>lui<space>une<space>une<space>révélation

2026-01-28 19:00:47,940 | INFO | speech length: 460320
2026-01-28 19:00:47,976 | INFO | decoder input length: 718
2026-01-28 19:00:47,976 | INFO | max output length: 718
2026-01-28 19:00:47,976 | INFO | min output length: 71
2026-01-28 19:01:32,924 | INFO | end detected at 428
2026-01-28 19:01:32,926 | INFO | -329.90 * 0.5 = -164.95 for decoder
2026-01-28 19:01:32,927 | INFO | -88.44 * 0.5 = -44.22 for ctc
2026-01-28 19:01:32,927 | INFO | total log probability: -209.17
2026-01-28 19:01:32,927 | INFO | normalized log probability: -0.50
2026-01-28 19:01:32,927 | INFO | total number of ended hypotheses: 197
2026-01-28 19:01:32,932 | INFO | best hypo: je<space>ne<space>me<space>demande<space>d'ailleurs<space>si<space>euh<space>l'arrivée<space>du<space>berger<space>dans<space>le<space>droit<space>roger<space>ce<space>berger<space>qui<space>euh<space>amène<space>euh<space>d'autres<space>d'autres<space>valeurs<space>si<space>vous<space>voulez<space>dans<space>un<space>monde<space>ce<space>qui<space>est<space>figé<space>euh<space>ne<space>rappelle<space>pas<space>un<space>peu<space>l'arrivée<space>de<space>l'oinerie<space>mais<space>je<space>ne<space>voudrais<space>pas<space>faire<space>de<space>la<space>psychanalyse<space>à<space>trois<space>sous<space>c'est<space>à<space>la<space>foisà<space>ce<space>moment<space>là<space>ça<space>pourait<space>être<space>le<space>ceune<space>si<space>n'est<space>pasiphales<space>effectivement<space>als<space>pas<space>sans<space>doune<space>de<space>notre<space>ouvrage<space>don<space>cet

2026-01-28 19:01:32,935 | INFO | speech length: 442720
2026-01-28 19:01:32,983 | INFO | decoder input length: 691
2026-01-28 19:01:32,983 | INFO | max output length: 691
2026-01-28 19:01:32,983 | INFO | min output length: 69
2026-01-28 19:02:26,324 | INFO | end detected at 553
2026-01-28 19:02:26,325 | INFO | -596.16 * 0.5 = -298.08 for decoder
2026-01-28 19:02:26,326 | INFO | -54.00 * 0.5 = -27.00 for ctc
2026-01-28 19:02:26,326 | INFO | total log probability: -325.08
2026-01-28 19:02:26,326 | INFO | normalized log probability: -0.59
2026-01-28 19:02:26,326 | INFO | total number of ended hypotheses: 164
2026-01-28 19:02:26,334 | INFO | best hypo: monumentale<space>biographie<space>de<space>schimanowsky<space>je<space>lis<space>son<space>le<space>portrait<space>de<space>notre<space>de<space>votre<space>héros<space>vers<space>vingt<space>six<space>ans<space>à<space>peu<space>près<space>euh<space>fait<space>par<space>un<space>ami<space>qui<space>l'attend<space>à<space>la<space>garde<space>il<space>était<space>d'une<space>belle<space>taille<space>très<space>élégamment<space>vêtu<space>boîter<space>légèrement<space>de<space>l'agenbe<space>gauche<space>qui<space>sans<space>être<space>plus<space>courte<space>que<space>la<space>droite<space>trahissaie<space>discrètement<space>à<space>chaque<space>pas<space>à<space>unraidissement<space>de<space>l'articulation<space>la<space>lavallière<space>de<space>soie<space>noire<space>de<space>la<space>photographie<space>de<space>berline<space>de<space>mille<space>neuf<space>cent<space>six<space>avait<space>fait<space>plac<space>à<space>une<space>cravate<space>parfaitement<space>nouée<space>de<space>couleurs<space>idéalement<space>sortie<space>avec<space>le<space>tout<space>de<space>sa<space>pélerine

2026-01-28 19:02:26,337 | INFO | speech length: 445600
2026-01-28 19:02:26,381 | INFO | decoder input length: 695
2026-01-28 19:02:26,381 | INFO | max output length: 695
2026-01-28 19:02:26,381 | INFO | min output length: 69
2026-01-28 19:03:17,503 | INFO | end detected at 495
2026-01-28 19:03:17,504 | INFO | -552.04 * 0.5 = -276.02 for decoder
2026-01-28 19:03:17,504 | INFO | -17.15 * 0.5 =  -8.58 for ctc
2026-01-28 19:03:17,504 | INFO | total log probability: -284.60
2026-01-28 19:03:17,504 | INFO | normalized log probability: -0.58
2026-01-28 19:03:17,504 | INFO | total number of ended hypotheses: 173
2026-01-28 19:03:17,511 | INFO | best hypo: toute<space>l'élégance<space>de<space>schimanos<space>qui<space>était<space>du<space>meilleur<space>genre<space>et<space>tout<space>cela<space>n'était<space>qu'une<space>sorte<space>d'accompagnement<space>pour<space>le<space>charme<space>qui<space>est<space>manée<space>de<space>la<space>personne<space>de<space>l'artiste<space>par<space>lequel<space>il<space>a<space>tiré<space>aussitôt<space>il<space>était<space>visible<space>dans<space>la<space>douceur<space>du<space>regard<space>de<space>ses<space>grands<space>yeux<space>vers<space>foncés<space>alors<space>effectivement<space>euh<space>schimanowvski<space>était<space>un<space>homme<space>très<space>élégant<space>il<space>avait<space>un<space>côté<space>aristocrate<space>un<space>côté<space>dhandi<space>un<space>côté<space>estêt<space>quil<space>gardera<space>jusqu'à<space>la<space>fin<space>de<space>sa<space>vie<space>et<space>il<space>était<space>très<space>attaché<space>à<space>la<space>beauté<space>des<space>choses<space>ce<space>qui

2026-01-28 19:03:17,513 | INFO | speech length: 471680
2026-01-28 19:03:17,558 | INFO | decoder input length: 736
2026-01-28 19:03:17,559 | INFO | max output length: 736
2026-01-28 19:03:17,559 | INFO | min output length: 73
2026-01-28 19:04:16,213 | INFO | end detected at 558
2026-01-28 19:04:16,216 | INFO | -582.45 * 0.5 = -291.22 for decoder
2026-01-28 19:04:16,217 | INFO | -93.60 * 0.5 = -46.80 for ctc
2026-01-28 19:04:16,217 | INFO | total log probability: -338.02
2026-01-28 19:04:16,217 | INFO | normalized log probability: -0.62
2026-01-28 19:04:16,217 | INFO | total number of ended hypotheses: 223
2026-01-28 19:04:16,228 | INFO | best hypo: explique<space>que<space>euh<space>ben<space>il<space>aimait<space>bien<space>descendre<space>dans<space>les<space>pays<space>vous<space>êtes<space>dans<space>les<space>grands<space>hôtels<space>et<space>à<space>la<space>fin<space>de<space>sa<space>vie<space>euh<space>quand<space>il<space>est<space>euh<space>en<space>france<space>d'ailleurs<space>pour<space>essayer<space>de<space>soigner<space>en<space>vain<space>euh<space>sa<space>tuberculose<space>euh<space>durant<space>euh<space>si<space>si<space>je<space>vois<space>aller<space>dans<space>un<space>bouigoui<space>euh<space>je<space>caricature<space>un<space>petit<space>peu<space>mais<space>enfin<space>euh<space>si<space>je<space>vois<space>descendre<space>euh<space>dans<space>un<space>hôtel<space>le<space>troisième<space>catégorie<space>alors<space>ça<space>ce<space>beau<space>plus<space>la<space>peine<space>ti<space>faut<space>dire<space>que<space>la<space>guerre<space>va<space>être<space>pour<space>sa<space>famille<space>a<space>a<space>un<space>grand<space>changement<space>de<space>fortune<space>mais<space>là<space>nous<space>sommes<space>encore<space>ant<space>guere<space>cest<space>dire<space>nousomme<space>danst

2026-01-28 19:04:16,233 | INFO | speech length: 461120
2026-01-28 19:04:16,290 | INFO | decoder input length: 720
2026-01-28 19:04:16,290 | INFO | max output length: 720
2026-01-28 19:04:16,290 | INFO | min output length: 72
2026-01-28 19:05:14,387 | INFO | end detected at 528
2026-01-28 19:05:14,388 | INFO | -517.99 * 0.5 = -259.00 for decoder
2026-01-28 19:05:14,388 | INFO | -43.72 * 0.5 = -21.86 for ctc
2026-01-28 19:05:14,388 | INFO | total log probability: -280.85
2026-01-28 19:05:14,388 | INFO | normalized log probability: -0.54
2026-01-28 19:05:14,388 | INFO | total number of ended hypotheses: 159
2026-01-28 19:05:14,395 | INFO | best hypo: pologne<space>euh<space>encore<space>occupée<space>mais<space>où<space>euh<space>la<space>la<space>vie<space>artistique<space>est<space>est<space>très<space>importante<space>et<space>où<space>se<space>coalise<space>en<space>quelque<space>sorte<space>le<space>le<space>mouvement<space>de<space>la<space>jeune<space>pologne<space>alors<space>la<space>jeune<space>pologne<space>c'est<space>l'amour<space>à<space>un<space>mouvement<space>artistique<space>et<space>ensuite<space>c'est<space>devenu<space>du<space>côté<space>de<space>la<space>musique<space>la<space>jeune<space>pologne<space>en<space>musique<space>avec<space>quatre<space>compositeurs<space>euh<space>tous<space>ces<space>élèves<space>d'ailleurs<space>de<space>loskovsky<space>qui<space>se<space>retrouvent<space>il<space>y<space>a<space>schimanowski<space>l<space>y<space>a<space>rugitsky<space>y<space>a<space>fitelberg<space>qui<space>qui<space>ensuite<space>euh<space>va<space>abandonner<space>la<space>composition<space>pour<space>la<space>dircon<space>e<space>dir<space>e<space>con<space>orchestre<space>et<space>puis

2026-01-28 19:05:14,397 | INFO | speech length: 455040
2026-01-28 19:05:14,436 | INFO | decoder input length: 710
2026-01-28 19:05:14,436 | INFO | max output length: 710
2026-01-28 19:05:14,436 | INFO | min output length: 71
2026-01-28 19:06:04,794 | INFO | end detected at 470
2026-01-28 19:06:04,795 | INFO | -222.02 * 0.5 = -111.01 for decoder
2026-01-28 19:06:04,796 | INFO | -24.17 * 0.5 = -12.08 for ctc
2026-01-28 19:06:04,796 | INFO | total log probability: -123.09
2026-01-28 19:06:04,796 | INFO | normalized log probability: -0.27
2026-01-28 19:06:04,796 | INFO | total number of ended hypotheses: 161
2026-01-28 19:06:04,802 | INFO | best hypo: il<space>y<space>a<space>euh<space>chez<space>l'auto<space>alors<space>c'est<space>un<space>groupe<space>qui<space>va<space>très<space>vite<space>se<space>désunir<space>parce<space>qu'il<space>y<space>a<space>des<space>des<space>jalousies<space>y<space>a<space>des<space>inimitiés<space>euh<space>les<space>uns<space>ont<space>plus<space>de<space>succès<space>que<space>les<space>autres<space>donc<space>ce<space>n'est<space>pas<space>fait<space>pour<space>simenter<space>ce<space>groupe<space>mais<space>enfin<space>euh<space>c'est<space>intéressant<space>parce<space>qu'on<space>a<space>là<space>si<space>vous<space>voulez<space>quatre<space>mousculteurs<space>de<space>euh<space>la<space>jeune<space>musique<space>polonaise<space>qui<space>veulent<space>en<space>découdre<space>avec<space>le<space>conservatisme<space>de<space>varsovie<space>et<space>qui<space>euh<space>h<space>fonde<space>une<space>société<space>d'idition<space>dont<space>le<space>siège<space>est<space>à<space>berlin

2026-01-28 19:06:04,804 | INFO | speech length: 442720
2026-01-28 19:06:04,867 | INFO | decoder input length: 691
2026-01-28 19:06:04,867 | INFO | max output length: 691
2026-01-28 19:06:04,867 | INFO | min output length: 69
2026-01-28 19:06:54,880 | INFO | end detected at 520
2026-01-28 19:06:54,882 | INFO | -415.06 * 0.5 = -207.53 for decoder
2026-01-28 19:06:54,882 | INFO | -62.11 * 0.5 = -31.06 for ctc
2026-01-28 19:06:54,882 | INFO | total log probability: -238.59
2026-01-28 19:06:54,882 | INFO | normalized log probability: -0.47
2026-01-28 19:06:54,882 | INFO | total number of ended hypotheses: 172
2026-01-28 19:06:54,889 | INFO | best hypo: qui<space>est<space>sûr<space>éidemment<space>un<space>plus<space>grand<space>rayonnement<space>que<space>varsovie<space>et<space>c'est<space>dans<space>cet<space>esprit<space>qui<space>est<space>proposé<space>par<space>shilmanesky<space>l'ouverture<space>de<space>concert<space>sur<space>la<space>plus<space>douze<space>alors<space>l'ouverture<space>de<space>consoeurs<space>qui<space>date<space>de<space>de<space>mille<space>neuf<space>cent<space>cinq<space>neuf<space>cent<space>six<space>date<space>de<space>justement<space>de<space>ces<space>années<space>là<space>des<space>années<space>de<space>la<space>jeune<space>pologne<space>on<space>voit<space>la<space>marque<space>on<space>va<space>l'entendre<space>la<space>marque<space>de<space>strangue<space>ça<space>c'est<space>très<space>très<space>évident<space>dans<space>cette<space>particie<space>e<space>alons<space>i<space>communie<space>t<space>tous<space>dans<space>ce<space>culte<space>de<space>strance<space>et<space>à<space>un<space>moment<space>où<space>gisky<space>raconte<space>dans<space>ses<space>souvenirs

2026-01-28 19:06:54,892 | INFO | speech length: 393920
2026-01-28 19:06:54,936 | INFO | decoder input length: 615
2026-01-28 19:06:54,936 | INFO | max output length: 615
2026-01-28 19:06:54,936 | INFO | min output length: 61
2026-01-28 19:07:35,151 | INFO | end detected at 458
2026-01-28 19:07:35,152 | INFO | -205.81 * 0.5 = -102.90 for decoder
2026-01-28 19:07:35,152 | INFO | -59.97 * 0.5 = -29.98 for ctc
2026-01-28 19:07:35,152 | INFO | total log probability: -132.89
2026-01-28 19:07:35,153 | INFO | normalized log probability: -0.29
2026-01-28 19:07:35,153 | INFO | total number of ended hypotheses: 171
2026-01-28 19:07:35,159 | INFO | best hypo: que<space>il<space>jouait<space>à<space>quatre<space>vingts<space>à<space>touche<space>france<space>je<space>pense<space>que<space>schimanowski<space>jouait<space>aussi<space>à<space>quatre<space>vingts<space>avec<space>le<space>cousin<space>de<space>raos<space>qui<space>est<space>pianiste<space>euh<space>bien<space>connu<space>et<space>rugiski<space>dit<space>on<space>ne<space>pouvait<space>pas<space>s'empêcher<space>à<space>la<space>fin<space>du<space>mot<space>rétransfiguration<space>de<space>faire<space>raisonner<space>raisonner<space>et<space>raisonner<space>et<space>encore<space>ces<space>accords<space>qui<space>nous<space>ont<space>chanté<space>là<space>c'est<space>un<space>extrait<space>de<space>cette<space>ouverture<space>de<space>concert<space>interprété<space>par<space>l'orchestre<space>national<space>de<space>la<space>radio<space>polonaise<space>direction<space>qusi<space>mirecore

2026-01-28 19:07:35,170 | INFO | Chunk: 0 | WER=81.034483 | S=12 D=1 I=34
2026-01-28 19:07:35,177 | INFO | Chunk: 1 | WER=71.171171 | S=20 D=31 I=28
2026-01-28 19:07:35,179 | INFO | Chunk: 2 | WER=116.216216 | S=31 D=1 I=11
2026-01-28 19:07:35,183 | INFO | Chunk: 3 | WER=62.500000 | S=4 D=26 I=25
2026-01-28 19:07:35,189 | INFO | Chunk: 4 | WER=59.000000 | S=9 D=26 I=24
2026-01-28 19:07:35,195 | INFO | Chunk: 5 | WER=61.538462 | S=8 D=23 I=25
2026-01-28 19:07:35,200 | INFO | Chunk: 6 | WER=84.269663 | S=15 D=20 I=40
2026-01-28 19:07:35,207 | INFO | Chunk: 7 | WER=64.761905 | S=6 D=34 I=28
2026-01-28 19:07:35,212 | INFO | Chunk: 8 | WER=47.422680 | S=5 D=21 I=20
2026-01-28 19:07:35,217 | INFO | Chunk: 9 | WER=60.674157 | S=11 D=16 I=27
2026-01-28 19:07:35,221 | INFO | Chunk: 10 | WER=93.243243 | S=19 D=22 I=28
2026-01-28 19:07:35,745 | INFO | File: Rhap-D2012.wav | WER=28.966986 | S=157 D=23 I=92
2026-01-28 19:07:35,745 | INFO | ------------------------------
2026-01-28 19:07:35,745 | INFO | Conf ester Done!
2026-01-28 19:11:12,520 | INFO | Chunk: 0 | WER=74.137931 | S=13 D=1 I=29
2026-01-28 19:11:12,530 | INFO | Chunk: 1 | WER=70.270270 | S=16 D=36 I=26
2026-01-28 19:11:12,532 | INFO | Chunk: 2 | WER=113.513514 | S=33 D=0 I=9
2026-01-28 19:11:12,538 | INFO | Chunk: 3 | WER=52.272727 | S=5 D=25 I=16
2026-01-28 19:11:12,546 | INFO | Chunk: 4 | WER=58.000000 | S=5 D=27 I=26
2026-01-28 19:11:12,553 | INFO | Chunk: 5 | WER=65.934066 | S=9 D=23 I=28
2026-01-28 19:11:12,561 | INFO | Chunk: 6 | WER=76.404494 | S=6 D=25 I=37
2026-01-28 19:11:12,569 | INFO | Chunk: 7 | WER=65.714286 | S=6 D=36 I=27
2026-01-28 19:11:12,577 | INFO | Chunk: 8 | WER=47.422680 | S=4 D=22 I=20
2026-01-28 19:11:12,584 | INFO | Chunk: 9 | WER=53.932584 | S=9 D=15 I=24
2026-01-28 19:11:12,589 | INFO | Chunk: 10 | WER=82.432432 | S=20 D=18 I=23
2026-01-28 19:11:13,272 | INFO | File: Rhap-D2012.wav | WER=23.855165 | S=121 D=33 I=70
2026-01-28 19:11:13,272 | INFO | ------------------------------
2026-01-28 19:11:13,273 | INFO | hmm_tdnn Done!
2026-01-28 19:11:13,452 | INFO | ==================================Rhap-D2013.wav=========================================
2026-01-28 19:11:13,664 | INFO | Using rVAD model
2026-01-28 19:11:31,053 | INFO | Chunk: 0 | WER=8.411215 | S=6 D=2 I=1
2026-01-28 19:11:31,060 | INFO | Chunk: 1 | WER=4.597701 | S=3 D=1 I=0
2026-01-28 19:11:31,068 | INFO | Chunk: 2 | WER=7.920792 | S=7 D=1 I=0
2026-01-28 19:11:31,073 | INFO | Chunk: 3 | WER=14.705882 | S=8 D=1 I=1
2026-01-28 19:11:31,081 | INFO | Chunk: 4 | WER=3.846154 | S=3 D=0 I=1
2026-01-28 19:11:31,082 | INFO | Chunk: 5 | WER=16.666667 | S=2 D=0 I=0
2026-01-28 19:11:31,090 | INFO | Chunk: 6 | WER=9.183673 | S=7 D=2 I=0
2026-01-28 19:11:31,093 | INFO | Chunk: 7 | WER=10.000000 | S=4 D=2 I=0
2026-01-28 19:11:31,389 | INFO | File: Rhap-D2013.wav | WER=8.163265 | S=40 D=9 I=3
2026-01-28 19:11:31,390 | INFO | ------------------------------
2026-01-28 19:11:31,390 | INFO | w2vec vad chunk Done!
2026-01-28 19:11:53,596 | INFO | Chunk: 0 | WER=64.485981 | S=4 D=64 I=1
2026-01-28 19:11:53,601 | INFO | Chunk: 1 | WER=55.172414 | S=7 D=38 I=3
2026-01-28 19:11:53,606 | INFO | Chunk: 2 | WER=56.435644 | S=6 D=51 I=0
2026-01-28 19:11:53,609 | INFO | Chunk: 3 | WER=54.411765 | S=4 D=32 I=1
2026-01-28 19:11:53,614 | INFO | Chunk: 4 | WER=64.423077 | S=11 D=53 I=3
2026-01-28 19:11:53,614 | INFO | Chunk: 5 | WER=8.333333 | S=1 D=0 I=0
2026-01-28 19:11:53,617 | INFO | Chunk: 6 | WER=88.775510 | S=3 D=84 I=0
2026-01-28 19:11:53,619 | INFO | Chunk: 7 | WER=31.666667 | S=4 D=15 I=0
2026-01-28 19:11:53,762 | INFO | File: Rhap-D2013.wav | WER=60.282575 | S=41 D=336 I=7
2026-01-28 19:11:53,762 | INFO | ------------------------------
2026-01-28 19:11:53,762 | INFO | whisper med Done!
2026-01-28 19:12:25,876 | INFO | Chunk: 0 | WER=52.336449 | S=12 D=43 I=1
2026-01-28 19:12:25,879 | INFO | Chunk: 1 | WER=59.770115 | S=4 D=47 I=1
2026-01-28 19:12:25,882 | INFO | Chunk: 2 | WER=59.405941 | S=7 D=53 I=0
2026-01-28 19:12:25,885 | INFO | Chunk: 3 | WER=38.235294 | S=13 D=11 I=2
2026-01-28 19:12:25,888 | INFO | Chunk: 4 | WER=64.423077 | S=2 D=65 I=0
2026-01-28 19:12:25,889 | INFO | Chunk: 5 | WER=25.000000 | S=2 D=1 I=0
2026-01-28 19:12:25,892 | INFO | Chunk: 6 | WER=60.204082 | S=2 D=57 I=0
2026-01-28 19:12:25,894 | INFO | Chunk: 7 | WER=28.333333 | S=5 D=12 I=0
2026-01-28 19:12:26,015 | INFO | File: Rhap-D2013.wav | WER=53.218210 | S=48 D=288 I=3
2026-01-28 19:12:26,015 | INFO | ------------------------------
2026-01-28 19:12:26,015 | INFO | whisper large Done!
2026-01-28 19:12:26,190 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 19:12:26,229 | INFO | Vocabulary size: 350
2026-01-28 19:12:27,842 | INFO | Gradient checkpoint layers: []
2026-01-28 19:12:28,876 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:12:28,884 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:12:28,885 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:12:28,886 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 19:12:28,886 | INFO | speech length: 439360
2026-01-28 19:12:28,959 | INFO | decoder input length: 686
2026-01-28 19:12:28,959 | INFO | max output length: 686
2026-01-28 19:12:28,959 | INFO | min output length: 68
2026-01-28 19:13:07,691 | INFO | end detected at 265
2026-01-28 19:13:07,693 | INFO | -636.00 * 0.5 = -318.00 for decoder
2026-01-28 19:13:07,693 | INFO | -139.38 * 0.5 = -69.69 for ctc
2026-01-28 19:13:07,693 | INFO | total log probability: -387.69
2026-01-28 19:13:07,693 | INFO | normalized log probability: -1.49
2026-01-28 19:13:07,693 | INFO | total number of ended hypotheses: 156
2026-01-28 19:13:07,697 | INFO | best hypo: ▁et▁louis▁dominique▁si▁le▁premier▁tour▁a▁été▁un▁avertissement▁pour▁le▁gouvernement▁le▁second▁ressemble▁fort▁à▁une▁déroute▁et▁rien▁n'arrête▁la▁vaguerose▁écrivent▁les▁dernières▁nouvelles▁d'alsace▁pour▁l'indépendant▁du▁midices▁et▁la▁gifles▁et▁est▁c'est▁l'aoujouteue▁de'▁nicholas▁sarkoszy▁qui▁doit▁être▁brûlante▁que▁la▁france▁et▁rose▁constate▁la▁voie▁du▁nord▁pour▁l'union▁à▁reimse▁pas▁de▁rhétorique▁de▁plateau▁télévisés▁qui▁tiennent▁ses▁bel▁et▁bien▁la▁béréesina▁pour▁la▁droite▁alors▁vote▁sanction▁ou▁pas▁oui▁c'étter▁bien▁un▁vote▁sanction▁pour▁jeancristophe▁gisbert▁dans▁la▁dépêche▁du▁midi

2026-01-28 19:13:07,702 | INFO | speech length: 400640
2026-01-28 19:13:07,777 | INFO | decoder input length: 625
2026-01-28 19:13:07,777 | INFO | max output length: 625
2026-01-28 19:13:07,777 | INFO | min output length: 62
2026-01-28 19:13:46,798 | INFO | end detected at 233
2026-01-28 19:13:46,799 | INFO | -509.49 * 0.5 = -254.74 for decoder
2026-01-28 19:13:46,799 | INFO | -102.03 * 0.5 = -51.01 for ctc
2026-01-28 19:13:46,799 | INFO | total log probability: -305.76
2026-01-28 19:13:46,799 | INFO | normalized log probability: -1.34
2026-01-28 19:13:46,799 | INFO | total number of ended hypotheses: 144
2026-01-28 19:13:46,802 | INFO | best hypo: ▁avec▁un▁cinglant▁désaveux▁pour▁nicolas▁sarkozy▁et▁le▁gouvernement▁de▁françois▁fillon▁il▁faut▁s'appeler▁rachida▁d'hatti▁écrit▁aussi▁libération▁champagne▁pour▁oser▁affirmeer▁qu'il▁ne▁s'agit▁pas▁d'un▁vote▁sanctions▁pour▁le▁parisien▁et▁aujourd'hui▁en▁france▁à▁la▁une▁maison'est▁plus▁d'une▁vagues▁c'est▁une▁défer▁lente▁pour▁la▁gauche▁à▁la▁une▁de▁libérationon▁qui▁ne▁boute▁pas▁son▁plaisir▁une▁caricature▁de▁niclas▁sarkozy▁pour▁à▁tâtiner▁sous▁cette▁exclamation▁et▁bling▁dans▁son▁éditoriement▁dans▁libération

2026-01-28 19:13:46,805 | INFO | speech length: 465440
2026-01-28 19:13:46,844 | INFO | decoder input length: 726
2026-01-28 19:13:46,844 | INFO | max output length: 726
2026-01-28 19:13:46,844 | INFO | min output length: 72
2026-01-28 19:14:19,073 | INFO | end detected at 259
2026-01-28 19:14:19,074 | INFO | -618.96 * 0.5 = -309.48 for decoder
2026-01-28 19:14:19,074 | INFO | -199.97 * 0.5 = -99.99 for ctc
2026-01-28 19:14:19,074 | INFO | total log probability: -409.47
2026-01-28 19:14:19,074 | INFO | normalized log probability: -1.61
2026-01-28 19:14:19,074 | INFO | total number of ended hypotheses: 149
2026-01-28 19:14:19,078 | INFO | best hypo: ▁laurent▁geoffrin▁parle▁lui▁aussi▁de▁désaveux▁cinglant▁même▁si▁dit▁il▁les▁responsables▁de▁l'ump▁s'obstine▁à▁nier▁l'évidence▁dans▁un▁accès▁de▁mauvaise▁foi▁s▁soviétique▁et▁dans▁un▁festival▁de▁la▁langue▁de▁bois▁l'ump▁voulait▁croire▁à▁la▁bourrasque▁c'est▁une▁tornade▁p▁pour▁le▁figaroeux▁aussi▁la▁gauche▁a▁transteformer▁l'esscai▁du▁premier▁tour▁le▁figaro▁qui▁reconnait▁l'ecrasante▁victoire▁de▁la▁gauche▁pour▁le▁parti▁sotialiste▁et▁chrétienne▁mousotte▁désormais▁au▁pouvoir▁dans▁la▁majoruité▁des▁grandes▁et▁villes▁et▁des▁ville▁moyennes▁de▁conseiless▁sénéraux▁et▁dans▁vingt▁régions▁sur▁vingt▁deux

2026-01-28 19:14:19,082 | INFO | speech length: 329120
2026-01-28 19:14:19,134 | INFO | decoder input length: 513
2026-01-28 19:14:19,135 | INFO | max output length: 513
2026-01-28 19:14:19,135 | INFO | min output length: 51
2026-01-28 19:14:36,837 | INFO | end detected at 176
2026-01-28 19:14:36,839 | INFO | -295.65 * 0.5 = -147.82 for decoder
2026-01-28 19:14:36,839 | INFO | -90.77 * 0.5 = -45.38 for ctc
2026-01-28 19:14:36,839 | INFO | total log probability: -193.21
2026-01-28 19:14:36,839 | INFO | normalized log probability: -1.14
2026-01-28 19:14:36,839 | INFO | total number of ended hypotheses: 158
2026-01-28 19:14:36,842 | INFO | best hypo: ▁dispose▁d'une▁force▁de▁frappe▁territoriale▁considérable▁alors▁qu'elles▁le▁sont▁tirées▁de▁la▁wagrose▁faut▁il▁que▁le▁gouvernement▁pour▁tenir▁compte▁du▁vote▁des▁français▁change▁sa▁politique▁quel▁naïf▁auraient▁pu▁le▁penser▁et▁bien▁non▁répontaitienne▁moujotte▁dans▁le▁figaro▁au▁contraire▁il▁faut▁qu'élérer▁le▁rythme▁du▁changement▁réformes▁plus▁vite▁et▁plus▁fortte▁comme▁en▁écho▁de▁libération▁laurent▁joffrin

2026-01-28 19:14:36,845 | INFO | speech length: 464800
2026-01-28 19:14:36,954 | INFO | decoder input length: 725
2026-01-28 19:14:36,955 | INFO | max output length: 725
2026-01-28 19:14:36,955 | INFO | min output length: 72
2026-01-28 19:15:13,943 | INFO | end detected at 290
2026-01-28 19:15:13,945 | INFO | -695.18 * 0.5 = -347.59 for decoder
2026-01-28 19:15:13,945 | INFO | -411.94 * 0.5 = -205.97 for ctc
2026-01-28 19:15:13,945 | INFO | total log probability: -553.56
2026-01-28 19:15:13,945 | INFO | normalized log probability: -1.96
2026-01-28 19:15:13,945 | INFO | total number of ended hypotheses: 183
2026-01-28 19:15:13,948 | INFO | best hypo: ▁qui▁n'est▁évidemment▁pas▁convaincu▁balaye▁d'un▁ricanement▁la▁pensée▁détiennent▁mouchote▁les▁français▁disconvains▁dans▁la▁mauvaise▁direction▁avec▁une▁seule▁réponse▁du▁côté▁ump▁il▁f▁faut▁y▁aller▁plus▁vite▁voyant▁la▁maintenant▁l'autre▁enjeu▁largement▁évoqué▁avant▁ligne▁municipal▁ou▁scrutin▁local▁ou▁national▁et▁bien▁une▁réponset▁dans▁le▁journal▁dans▁de▁haute▁les▁marne▁'am▁moin▁de▁conscidérer▁que▁tous▁les▁maires▁que▁droite▁et▁qui▁ont▁et▁batte▁étaient▁fonscièrement▁mauvaise▁mais▁il▁faut▁avoir▁dans▁le▁réuseultat▁des▁municipales▁et▁bien▁cantanonales▁à▁un▁mes▁mesage▁forts▁envoyé▁à▁l'anxcutif▁mesage▁national▁donc▁mais▁quels▁message

2026-01-28 19:15:13,951 | INFO | speech length: 68800
2026-01-28 19:15:13,999 | INFO | decoder input length: 107
2026-01-28 19:15:13,999 | INFO | max output length: 107
2026-01-28 19:15:13,999 | INFO | min output length: 10
2026-01-28 19:15:15,787 | INFO | end detected at 38
2026-01-28 19:15:15,788 | INFO |  -5.72 * 0.5 =  -2.86 for decoder
2026-01-28 19:15:15,788 | INFO |  -9.41 * 0.5 =  -4.71 for ctc
2026-01-28 19:15:15,789 | INFO | total log probability: -7.56
2026-01-28 19:15:15,789 | INFO | normalized log probability: -0.23
2026-01-28 19:15:15,789 | INFO | total number of ended hypotheses: 165
2026-01-28 19:15:15,789 | INFO | best hypo: ▁pour▁jules▁clobert▁dans▁nora▁claire▁le▁test▁politique▁reste▁à▁déchiffrer

2026-01-28 19:15:15,791 | INFO | speech length: 440000
2026-01-28 19:15:15,828 | INFO | decoder input length: 687
2026-01-28 19:15:15,828 | INFO | max output length: 687
2026-01-28 19:15:15,829 | INFO | min output length: 68
2026-01-28 19:15:46,842 | INFO | end detected at 253
2026-01-28 19:15:46,843 | INFO | -446.99 * 0.5 = -223.49 for decoder
2026-01-28 19:15:46,843 | INFO | -160.19 * 0.5 = -80.10 for ctc
2026-01-28 19:15:46,843 | INFO | total log probability: -303.59
2026-01-28 19:15:46,843 | INFO | normalized log probability: -1.23
2026-01-28 19:15:46,843 | INFO | total number of ended hypotheses: 162
2026-01-28 19:15:46,846 | INFO | best hypo: ▁pour▁journaux▁jean▁christophe▁qui▁souligne▁également▁la▁faiblesse▁de▁la▁mobilisation▁des▁électeurs▁hier▁c'est▁même▁le▁triomphe▁d'un▁revenant▁écrit▁l'alsace▁on▁l'avait▁un▁peu▁oublié▁après▁les▁records▁de▁la▁présidentielle▁mais▁l'abstentionnisme▁est▁de▁retour▁et▁avec▁lui▁le▁fausché▁très▁larage▁entre▁le▁pouvoir▁et▁les▁citoyen▁enfinste▁le▁cas▁de▁marseille▁marseille▁fut▁pour▁lump▁pour▁toute▁de▁la▁majorité▁présaidentielle▁'une▁boué▁dans▁le▁naufrage▁d'un▁souard▁municipe▁écrit▁réger▁thèche▁dans▁midit▁libre▁un▁sauvetage▁miraculeux▁à▁bien▁des▁écarts▁écrits▁la▁prov

2026-01-28 19:15:46,849 | INFO | speech length: 274720
2026-01-28 19:15:46,902 | INFO | decoder input length: 428
2026-01-28 19:15:46,902 | INFO | max output length: 428
2026-01-28 19:15:46,902 | INFO | min output length: 42
2026-01-28 19:16:00,030 | INFO | end detected at 152
2026-01-28 19:16:00,032 | INFO | -162.25 * 0.5 = -81.12 for decoder
2026-01-28 19:16:00,032 | INFO | -43.73 * 0.5 = -21.87 for ctc
2026-01-28 19:16:00,032 | INFO | total log probability: -102.99
2026-01-28 19:16:00,032 | INFO | normalized log probability: -0.70
2026-01-28 19:16:00,032 | INFO | total number of ended hypotheses: 188
2026-01-28 19:16:00,034 | INFO | best hypo: ▁marseille▁occupe▁une▁place▁à▁part▁et▁ce▁matin▁le▁président▁et▁le▁gouvernement▁peuvent▁dire▁mercy▁marseille▁alors▁que▁va▁faire▁maintenant▁le▁président▁se▁demande▁le▁progrès▁à▁lyon▁elle▁va▁sans▁doute▁faire▁la▁même▁chose▁qu'avant▁pronostic▁francis▁brochet▁mais▁autrement▁le▁même▁fonds▁sans▁ré▁forme▁autrement▁vit▁les▁réformes▁sans▁l'arollexe

2026-01-28 19:16:00,048 | INFO | Chunk: 0 | WER=24.299065 | S=19 D=4 I=3
2026-01-28 19:16:00,053 | INFO | Chunk: 1 | WER=21.839080 | S=13 D=1 I=5
2026-01-28 19:16:00,059 | INFO | Chunk: 2 | WER=20.792079 | S=16 D=0 I=5
2026-01-28 19:16:00,063 | INFO | Chunk: 3 | WER=26.470588 | S=11 D=3 I=4
2026-01-28 19:16:00,069 | INFO | Chunk: 4 | WER=41.346154 | S=25 D=7 I=11
2026-01-28 19:16:00,070 | INFO | Chunk: 5 | WER=25.000000 | S=3 D=0 I=0
2026-01-28 19:16:00,075 | INFO | Chunk: 6 | WER=22.448980 | S=17 D=3 I=2
2026-01-28 19:16:00,078 | INFO | Chunk: 7 | WER=16.666667 | S=9 D=1 I=0
2026-01-28 19:16:00,303 | INFO | File: Rhap-D2013.wav | WER=25.431711 | S=113 D=19 I=30
2026-01-28 19:16:00,303 | INFO | ------------------------------
2026-01-28 19:16:00,303 | INFO | Conf cv Done!
2026-01-28 19:16:00,481 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 19:16:00,504 | INFO | Vocabulary size: 47
2026-01-28 19:16:01,645 | INFO | Gradient checkpoint layers: []
2026-01-28 19:16:02,350 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:16:02,358 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:16:02,364 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:16:02,365 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 19:16:02,368 | INFO | speech length: 439360
2026-01-28 19:16:02,417 | INFO | decoder input length: 686
2026-01-28 19:16:02,417 | INFO | max output length: 686
2026-01-28 19:16:02,418 | INFO | min output length: 68
2026-01-28 19:16:54,642 | INFO | end detected at 572
2026-01-28 19:16:54,644 | INFO | -537.19 * 0.5 = -268.59 for decoder
2026-01-28 19:16:54,644 | INFO | -37.27 * 0.5 = -18.64 for ctc
2026-01-28 19:16:54,644 | INFO | total log probability: -287.23
2026-01-28 19:16:54,644 | INFO | normalized log probability: -0.51
2026-01-28 19:16:54,644 | INFO | total number of ended hypotheses: 171
2026-01-28 19:16:54,651 | INFO | best hypo: eh<space>oui<space>dominique<space>si<space>le<space>premier<space>tour<space>a<space>été<space>un<space>avertissement<space>pour<space>le<space>gouvernement<space>le<space>second<space>ressemble<space>fort<space>à<space>une<space>déroute<space>et<space>rien<space>n'arrête<space>la<space>vague<space>rose<space>écrivent<space>les<space>dernières<space>nouvelles<space>d'alsace<space>pour<space>l'indépendant<space>du<space>midi<space>c'est<space>la<space>giffle<space>et<space>c'est<space>l'ajout<space>de<space>nicolas<space>sarkozy<space>qui<space>doit<space>être<space>brûlante<space>la<space>france<space>et<space>rose<space>constate<space>la<space>voix<space>du<space>nord<space>pour<space>l'union<space>hareins<space>pas<space>de<space>rhétorique<space>de<space>plateau<space>télévisé<space>qui<space>tienne<space>c'est<space>bet<space>et<space>bien<space>la<space>bersina<space>pour<space>la<space>droite<space>alors<space>vote<space>sanction<space>ou<space>pas<space>oui<space>c'ét<space>it<space>bien<space>un<space>vote<space>sanction<space>pour<space>jean<space>christophe<space>gisbert<space>dans<space>la<space>dépêche<space>du<space>midi

2026-01-28 19:16:54,653 | INFO | speech length: 400640
2026-01-28 19:16:54,697 | INFO | decoder input length: 625
2026-01-28 19:16:54,697 | INFO | max output length: 625
2026-01-28 19:16:54,697 | INFO | min output length: 62
2026-01-28 19:17:38,259 | INFO | end detected at 487
2026-01-28 19:17:38,260 | INFO | -417.92 * 0.5 = -208.96 for decoder
2026-01-28 19:17:38,261 | INFO | -26.02 * 0.5 = -13.01 for ctc
2026-01-28 19:17:38,261 | INFO | total log probability: -221.97
2026-01-28 19:17:38,261 | INFO | normalized log probability: -0.46
2026-01-28 19:17:38,261 | INFO | total number of ended hypotheses: 148
2026-01-28 19:17:38,268 | INFO | best hypo: avec<space>un<space>cinglant<space>désaveu<space>pour<space>nicolas<space>sarkozy<space>et<space>le<space>gouvernement<space>de<space>françois<space>fillon<space>il<space>faut<space>s'appeler<space>rachida<space>dati<space>écrit<space>aussi<space>libération<space>champagne<space>pour<space>oser<space>affirmer<space>qu'il<space>ne<space>s'agit<space>pas<space>d'un<space>vote<space>sanction<space>pour<space>le<space>parisien<space>aujourd'hui<space>en<space>france<space>à<space>la<space>une<space>ce<space>n'est<space>plus<space>une<space>vague<space>c'est<space>une<space>déferlante<space>pour<space>la<space>gauche<space>à<space>la<space>une<space>de<space>libération<space>qui<space>ne<space>boute<space>pas<space>son<space>plaisir<space>une<space>caricature<space>de<space>nicolas<space>darkozy<space>tour<space>ratatiné<space>sous<space>cette<space>exclamation<space>et<space>bleng<space>dans<space>son<space>éditorial<space>dans<space>libération

2026-01-28 19:17:38,271 | INFO | speech length: 465440
2026-01-28 19:17:38,313 | INFO | decoder input length: 726
2026-01-28 19:17:38,313 | INFO | max output length: 726
2026-01-28 19:17:38,314 | INFO | min output length: 72
2026-01-28 19:18:40,736 | INFO | end detected at 571
2026-01-28 19:18:40,737 | INFO | -631.05 * 0.5 = -315.53 for decoder
2026-01-28 19:18:40,737 | INFO | -25.02 * 0.5 = -12.51 for ctc
2026-01-28 19:18:40,738 | INFO | total log probability: -328.04
2026-01-28 19:18:40,738 | INFO | normalized log probability: -0.58
2026-01-28 19:18:40,738 | INFO | total number of ended hypotheses: 161
2026-01-28 19:18:40,745 | INFO | best hypo: laurent<space>joffra<space>parle<space>lui<space>aussi<space>de<space>désaveu<space>cinglant<space>même<space>si<space>dit<space>il<space>les<space>responsables<space>de<space>l'ump<space>s'obstinent<space>à<space>nier<space>l'évidence<space>dans<space>un<space>accès<space>de<space>mauvaise<space>foi<space>soviétique<space>et<space>dans<space>un<space>festival<space>de<space>langue<space>de<space>bois<space>l'ump<space>voulait<space>croire<space>à<space>la<space>bourrasque<space>c'est<space>une<space>tournade<space>pour<space>le<space>figaro<space>aussi<space>la<space>gauche<space>a<space>transformé<space>l'essai<space>du<space>premier<space>tour<space>le<space>figaro<space>qui<space>reconnaît<space>l'écrasante<space>victoire<space>de<space>la<space>gauche<space>le<space>parti<space>socialiste<space>é<space>crit<space>étienne<space>mougeotte<space>désormais<space>au<space>pouvoir<space>dans<space>la<space>majorté<space>des<space>grandes<space>villes<space>et<space>des<space>villes<space>moyenne<space>des<space>conseils<space>généraux<space>et<space>dans<space>vingt<space>régions<space>sur<space>vingt<space>deux

2026-01-28 19:18:40,747 | INFO | speech length: 329120
2026-01-28 19:18:40,791 | INFO | decoder input length: 513
2026-01-28 19:18:40,791 | INFO | max output length: 513
2026-01-28 19:18:40,791 | INFO | min output length: 51
2026-01-28 19:19:16,007 | INFO | end detected at 422
2026-01-28 19:19:16,009 | INFO | -40.23 * 0.5 = -20.11 for decoder
2026-01-28 19:19:16,010 | INFO | -10.96 * 0.5 =  -5.48 for ctc
2026-01-28 19:19:16,010 | INFO | total log probability: -25.59
2026-01-28 19:19:16,010 | INFO | normalized log probability: -0.06
2026-01-28 19:19:16,010 | INFO | total number of ended hypotheses: 199
2026-01-28 19:19:16,015 | INFO | best hypo: dispose<space>d'une<space>force<space>de<space>frappe<space>territoriale<space>considérable<space>alors<space>quelle<space>leçon<space>tirer<space>de<space>la<space>vague<space>rosse<space>faut<space>il<space>que<space>le<space>gouvernement<space>pour<space>tenir<space>compte<space>du<space>vote<space>des<space>français<space>change<space>sa<space>politique<space>quelques<space>naïfs<space>auraient<space>pu<space>le<space>penser<space>eh<space>bien<space>non<space>répond<space>étienne<space>mougeotte<space>dans<space>le<space>figaro<space>au<space>contraire<space>il<space>faut<space>accélérer<space>le<space>rythme<space>du<space>changement<space>les<space>réformes<space>plus<space>vite<space>et<space>plus<space>fort<space>comme<space>en<space>écho<space>dans<space>libération<space>laurent<space>joffrin

2026-01-28 19:19:16,017 | INFO | speech length: 464800
2026-01-28 19:19:16,058 | INFO | decoder input length: 725
2026-01-28 19:19:16,058 | INFO | max output length: 725
2026-01-28 19:19:16,058 | INFO | min output length: 72
2026-01-28 19:20:16,536 | INFO | end detected at 599
2026-01-28 19:20:16,537 | INFO | -731.75 * 0.5 = -365.88 for decoder
2026-01-28 19:20:16,537 | INFO | -29.76 * 0.5 = -14.88 for ctc
2026-01-28 19:20:16,537 | INFO | total log probability: -380.76
2026-01-28 19:20:16,537 | INFO | normalized log probability: -0.64
2026-01-28 19:20:16,537 | INFO | total number of ended hypotheses: 164
2026-01-28 19:20:16,544 | INFO | best hypo: qui<space>n'est<space>évidemment<space>pas<space>convaincu<space>balaïda<space>ricanement<space>la<space>pensée<space>d'étienne<space>mougeotte<space>les<space>français<space>disent<space>qu'on<space>va<space>dans<space>la<space>mauvaise<space>direction<space>une<space>seule<space>réponse<space>du<space>côté<space>de<space>l'ump<space>il<space>faut<space>y<space>aller<space>plus<space>vite<space>voyons<space>maintenant<space>l'autre<space>enjeu<space>largement<space>évoqué<space>avant<space>legne<space>municipale<space>scrutin<space>local<space>ou<space>national<space>eh<space>bien<space>une<space>réponse<space>dans<space>le<space>journal<space>de<space>la<space>haute<space>marne<space>à<space>moins<space>de<space>considérer<space>que<space>tous<space>les<space>maires<space>de<space>droite<space>qui<space>ont<space>été<space>batous<space>étaient<space>foncièrement<space>mauvais<space>il<space>faut<space>voir<space>dans<space>le<space>résultat<space>des<space>municipales<space>et<space>des<space>cantonales<space>un<space>message<space>fort<space>envoyé<space>à<space>lexécutif<space>message<space>national<space>donc<space>mais<space>quel<space>message

2026-01-28 19:20:16,547 | INFO | speech length: 68800
2026-01-28 19:20:16,584 | INFO | decoder input length: 107
2026-01-28 19:20:16,585 | INFO | max output length: 107
2026-01-28 19:20:16,585 | INFO | min output length: 10
2026-01-28 19:20:20,525 | INFO | end detected at 84
2026-01-28 19:20:20,526 | INFO | -11.83 * 0.5 =  -5.92 for decoder
2026-01-28 19:20:20,526 | INFO |  -5.26 * 0.5 =  -2.63 for ctc
2026-01-28 19:20:20,526 | INFO | total log probability: -8.55
2026-01-28 19:20:20,527 | INFO | normalized log probability: -0.11
2026-01-28 19:20:20,527 | INFO | total number of ended hypotheses: 206
2026-01-28 19:20:20,528 | INFO | best hypo: pour<space>jucques<space>laubert<space>dans<space>nord<space>et<space>clair<space>le<space>test<space>politique<space>reste<space>à<space>déchiffre

2026-01-28 19:20:20,530 | INFO | speech length: 440000
2026-01-28 19:20:20,572 | INFO | decoder input length: 687
2026-01-28 19:20:20,572 | INFO | max output length: 687
2026-01-28 19:20:20,572 | INFO | min output length: 68
2026-01-28 19:21:12,131 | INFO | end detected at 564
2026-01-28 19:21:12,132 | INFO | -590.30 * 0.5 = -295.15 for decoder
2026-01-28 19:21:12,132 | INFO | -35.05 * 0.5 = -17.53 for ctc
2026-01-28 19:21:12,132 | INFO | total log probability: -312.68
2026-01-28 19:21:12,132 | INFO | normalized log probability: -0.56
2026-01-28 19:21:12,132 | INFO | total number of ended hypotheses: 160
2026-01-28 19:21:12,139 | INFO | best hypo: vos<space>journaux<space>jean<space>christophe<space>qui<space>souligne<space>également<space>la<space>faiblesse<space>de<space>la<space>mobilisation<space>des<space>électeurs<space>hier<space>c'st<space>même<space>le<space>triomphe<space>d'un<space>revenant<space>écrit<space>l'alsace<space>on<space>l'avait<space>un<space>peu<space>oublié<space>après<space>les<space>records<space>de<space>la<space>présidentielle<space>mais<space>l'abstentionnisme<space>est<space>de<space>retour<space>et<space>avec<space>lui<space>le<space>fossé<space>très<space>large<space>entre<space>le<space>pouvoir<space>et<space>les<space>citoyens<space>enfin<space>reste<space>le<space>cas<space>marseille<space>marseille<space>fut<space>pour<space>l'ump<space>pour<space>toute<space>la<space>majorité<space>présidentielle<space>une<space>bouée<space>d<space>ns<space>le<space>naufrage<space>d'un<space>soir<space>de<space>municipale<space>écrit<space>roger<space>ence<space>dons<space>midi<space>libre<space>un<space>sauvetage<space>miraculeux<space>à<space>bien<space>des<space>écarts<space>écrit<space>la<space>promence

2026-01-28 19:21:12,141 | INFO | speech length: 274720
2026-01-28 19:21:12,199 | INFO | decoder input length: 428
2026-01-28 19:21:12,199 | INFO | max output length: 428
2026-01-28 19:21:12,199 | INFO | min output length: 42
2026-01-28 19:21:45,467 | INFO | end detected at 346
2026-01-28 19:21:45,469 | INFO | -43.00 * 0.5 = -21.50 for decoder
2026-01-28 19:21:45,469 | INFO |  -4.71 * 0.5 =  -2.36 for ctc
2026-01-28 19:21:45,469 | INFO | total log probability: -23.86
2026-01-28 19:21:45,469 | INFO | normalized log probability: -0.07
2026-01-28 19:21:45,469 | INFO | total number of ended hypotheses: 193
2026-01-28 19:21:45,474 | INFO | best hypo: marseille<space>occupe<space>une<space>place<space>à<space>part<space>et<space>ce<space>matin<space>le<space>président<space>et<space>le<space>gouvernement<space>peuvent<space>dire<space>merci<space>marseille<space>alors<space>que<space>va<space>faire<space>maintenant<space>le<space>président<space>se<space>demande<space>le<space>progrès<space>à<space>lyon<space>il<space>va<space>sans<space>doute<space>faire<space>la<space>même<space>chose<space>qu'avant<space>pronostic<space>francis<space>brochet<space>mais<space>autrement<space>le<space>même<space>fond<space>sans<space>la<space>forme<space>autrement<space>dit<space>les<space>réformes<space>sans<space>la<space>relaique

2026-01-28 19:21:45,487 | INFO | Chunk: 0 | WER=11.214953 | S=9 D=1 I=2
2026-01-28 19:21:45,491 | INFO | Chunk: 1 | WER=4.597701 | S=4 D=0 I=0
2026-01-28 19:21:45,498 | INFO | Chunk: 2 | WER=6.930693 | S=6 D=0 I=1
2026-01-28 19:21:45,501 | INFO | Chunk: 3 | WER=2.941176 | S=1 D=0 I=1
2026-01-28 19:21:45,507 | INFO | Chunk: 4 | WER=8.653846 | S=5 D=3 I=1
2026-01-28 19:21:45,508 | INFO | Chunk: 5 | WER=41.666667 | S=4 D=0 I=1
2026-01-28 19:21:45,513 | INFO | Chunk: 6 | WER=9.183673 | S=8 D=0 I=1
2026-01-28 19:21:45,516 | INFO | Chunk: 7 | WER=6.666667 | S=3 D=1 I=0
2026-01-28 19:21:45,734 | INFO | File: Rhap-D2013.wav | WER=8.163265 | S=40 D=5 I=7
2026-01-28 19:21:45,734 | INFO | ------------------------------
2026-01-28 19:21:45,735 | INFO | Conf ester Done!
2026-01-28 19:23:15,192 | INFO | Chunk: 0 | WER=3.738318 | S=3 D=0 I=1
2026-01-28 19:23:15,199 | INFO | Chunk: 1 | WER=6.896552 | S=5 D=0 I=1
2026-01-28 19:23:15,207 | INFO | Chunk: 2 | WER=4.950495 | S=4 D=0 I=1
2026-01-28 19:23:15,212 | INFO | Chunk: 3 | WER=10.294118 | S=5 D=0 I=2
2026-01-28 19:23:15,221 | INFO | Chunk: 4 | WER=5.769231 | S=4 D=1 I=1
2026-01-28 19:23:15,221 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 19:23:15,229 | INFO | Chunk: 6 | WER=5.102041 | S=5 D=0 I=0
2026-01-28 19:23:15,233 | INFO | Chunk: 7 | WER=11.666667 | S=3 D=2 I=2
2026-01-28 19:23:15,532 | INFO | File: Rhap-D2013.wav | WER=6.279435 | S=29 D=3 I=8
2026-01-28 19:23:15,532 | INFO | ------------------------------
2026-01-28 19:23:15,532 | INFO | hmm_tdnn Done!
2026-01-28 19:23:15,709 | INFO | ==================================Rhap-M0001.wav=========================================
2026-01-28 19:23:15,876 | INFO | Using rVAD model
2026-01-28 19:23:20,473 | INFO | Chunk: 0 | WER=40.000000 | S=1 D=0 I=1
2026-01-28 19:23:20,474 | INFO | Chunk: 1 | WER=28.000000 | S=4 D=3 I=0
2026-01-28 19:23:20,475 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 19:23:20,475 | INFO | Chunk: 3 | WER=50.000000 | S=2 D=2 I=0
2026-01-28 19:23:20,476 | INFO | Chunk: 4 | WER=50.000000 | S=3 D=3 I=0
2026-01-28 19:23:20,476 | INFO | Chunk: 5 | WER=18.181818 | S=1 D=1 I=0
2026-01-28 19:23:20,477 | INFO | Chunk: 6 | WER=75.000000 | S=2 D=0 I=1
2026-01-28 19:23:20,479 | INFO | Chunk: 7 | WER=9.756098 | S=2 D=2 I=0
2026-01-28 19:23:20,480 | INFO | Chunk: 8 | WER=25.000000 | S=2 D=1 I=0
2026-01-28 19:23:20,480 | INFO | Chunk: 9 | WER=50.000000 | S=1 D=0 I=0
2026-01-28 19:23:20,516 | INFO | File: Rhap-M0001.wav | WER=25.196850 | S=18 D=12 I=2
2026-01-28 19:23:20,516 | INFO | ------------------------------
2026-01-28 19:23:20,516 | INFO | w2vec vad chunk Done!
2026-01-28 19:23:28,504 | INFO | Chunk: 0 | WER=40.000000 | S=1 D=0 I=1
2026-01-28 19:23:28,505 | INFO | Chunk: 1 | WER=36.000000 | S=6 D=2 I=1
2026-01-28 19:23:28,505 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 19:23:28,505 | INFO | Chunk: 3 | WER=37.500000 | S=2 D=1 I=0
2026-01-28 19:23:28,506 | INFO | Chunk: 4 | WER=41.666667 | S=4 D=1 I=0
2026-01-28 19:23:28,506 | INFO | Chunk: 5 | WER=9.090909 | S=0 D=1 I=0
2026-01-28 19:23:28,506 | INFO | Chunk: 6 | WER=25.000000 | S=0 D=1 I=0
2026-01-28 19:23:28,507 | INFO | Chunk: 7 | WER=19.512195 | S=5 D=3 I=0
2026-01-28 19:23:28,508 | INFO | Chunk: 8 | WER=25.000000 | S=2 D=1 I=0
2026-01-28 19:23:28,508 | INFO | Chunk: 9 | WER=100.000000 | S=1 D=1 I=0
2026-01-28 19:23:28,518 | INFO | File: Rhap-M0001.wav | WER=26.771654 | S=21 D=11 I=2
2026-01-28 19:23:28,518 | INFO | ------------------------------
2026-01-28 19:23:28,519 | INFO | whisper med Done!
2026-01-28 19:23:39,102 | INFO | Chunk: 0 | WER=60.000000 | S=1 D=0 I=2
2026-01-28 19:23:39,103 | INFO | Chunk: 1 | WER=36.000000 | S=7 D=2 I=0
2026-01-28 19:23:39,103 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 19:23:39,103 | INFO | Chunk: 3 | WER=50.000000 | S=3 D=1 I=0
2026-01-28 19:23:39,104 | INFO | Chunk: 4 | WER=25.000000 | S=2 D=1 I=0
2026-01-28 19:23:39,104 | INFO | Chunk: 5 | WER=9.090909 | S=1 D=0 I=0
2026-01-28 19:23:39,104 | INFO | Chunk: 6 | WER=25.000000 | S=0 D=1 I=0
2026-01-28 19:23:39,105 | INFO | Chunk: 7 | WER=19.512195 | S=4 D=4 I=0
2026-01-28 19:23:39,106 | INFO | Chunk: 8 | WER=25.000000 | S=2 D=1 I=0
2026-01-28 19:23:39,106 | INFO | Chunk: 9 | WER=100.000000 | S=1 D=1 I=0
2026-01-28 19:23:39,116 | INFO | File: Rhap-M0001.wav | WER=26.771654 | S=21 D=11 I=2
2026-01-28 19:23:39,116 | INFO | ------------------------------
2026-01-28 19:23:39,116 | INFO | whisper large Done!
2026-01-28 19:23:39,278 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 19:23:39,317 | INFO | Vocabulary size: 350
2026-01-28 19:23:40,295 | INFO | Gradient checkpoint layers: []
2026-01-28 19:23:41,052 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:23:41,056 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:23:41,057 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:23:41,057 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 19:23:41,057 | INFO | speech length: 55040
2026-01-28 19:23:41,102 | INFO | decoder input length: 85
2026-01-28 19:23:41,102 | INFO | max output length: 85
2026-01-28 19:23:41,102 | INFO | min output length: 8
2026-01-28 19:23:42,101 | INFO | end detected at 22
2026-01-28 19:23:42,102 | INFO |  -1.79 * 0.5 =  -0.90 for decoder
2026-01-28 19:23:42,102 | INFO |  -0.48 * 0.5 =  -0.24 for ctc
2026-01-28 19:23:42,102 | INFO | total log probability: -1.13
2026-01-28 19:23:42,102 | INFO | normalized log probability: -0.06
2026-01-28 19:23:42,102 | INFO | total number of ended hypotheses: 142
2026-01-28 19:23:42,102 | INFO | best hypo: ▁eh▁bien▁tu▁prends▁le▁boulevard

2026-01-28 19:23:42,105 | INFO | speech length: 125280
2026-01-28 19:23:42,154 | INFO | decoder input length: 195
2026-01-28 19:23:42,154 | INFO | max output length: 195
2026-01-28 19:23:42,154 | INFO | min output length: 19
2026-01-28 19:23:46,032 | INFO | end detected at 68
2026-01-28 19:23:46,033 | INFO | -11.72 * 0.5 =  -5.86 for decoder
2026-01-28 19:23:46,033 | INFO | -21.24 * 0.5 = -10.62 for ctc
2026-01-28 19:23:46,033 | INFO | total log probability: -16.48
2026-01-28 19:23:46,033 | INFO | normalized log probability: -0.27
2026-01-28 19:23:46,033 | INFO | total number of ended hypotheses: 165
2026-01-28 19:23:46,034 | INFO | best hypo: ▁la▁qui▁part▁de▁verchavand▁date▁bouvard▁qui▁passe▁à▁côté▁d'habitat▁je▁continue▁tu▁vas▁arriver▁sur▁la▁place▁victor▁hugo

2026-01-28 19:23:46,036 | INFO | speech length: 36960
2026-01-28 19:23:46,076 | INFO | decoder input length: 57
2026-01-28 19:23:46,076 | INFO | max output length: 57
2026-01-28 19:23:46,076 | INFO | min output length: 5
2026-01-28 19:23:47,128 | INFO | end detected at 25
2026-01-28 19:23:47,129 | INFO |  -2.23 * 0.5 =  -1.12 for decoder
2026-01-28 19:23:47,130 | INFO |  -0.23 * 0.5 =  -0.12 for ctc
2026-01-28 19:23:47,130 | INFO | total log probability: -1.23
2026-01-28 19:23:47,130 | INFO | normalized log probability: -0.06
2026-01-28 19:23:47,130 | INFO | total number of ended hypotheses: 139
2026-01-28 19:23:47,130 | INFO | best hypo: ▁la▁place▁victor▁hugo▁à▁la▁banque

2026-01-28 19:23:47,132 | INFO | speech length: 27200
2026-01-28 19:23:47,183 | INFO | decoder input length: 42
2026-01-28 19:23:47,183 | INFO | max output length: 42
2026-01-28 19:23:47,184 | INFO | min output length: 4
2026-01-28 19:23:48,634 | INFO | end detected at 21
2026-01-28 19:23:48,636 | INFO |  -2.81 * 0.5 =  -1.41 for decoder
2026-01-28 19:23:48,637 | INFO |  -6.26 * 0.5 =  -3.13 for ctc
2026-01-28 19:23:48,637 | INFO | total log probability: -4.54
2026-01-28 19:23:48,637 | INFO | normalized log probability: -0.28
2026-01-28 19:23:48,637 | INFO | total number of ended hypotheses: 155
2026-01-28 19:23:48,637 | INFO | best hypo: ▁c'est▁l'angle▁du▁prend▁droite

2026-01-28 19:23:48,640 | INFO | speech length: 51040
2026-01-28 19:23:48,701 | INFO | decoder input length: 79
2026-01-28 19:23:48,701 | INFO | max output length: 79
2026-01-28 19:23:48,701 | INFO | min output length: 7
2026-01-28 19:23:51,424 | INFO | end detected at 35
2026-01-28 19:23:51,427 | INFO |  -3.35 * 0.5 =  -1.68 for decoder
2026-01-28 19:23:51,427 | INFO |  -8.01 * 0.5 =  -4.01 for ctc
2026-01-28 19:23:51,427 | INFO | total log probability: -5.68
2026-01-28 19:23:51,427 | INFO | normalized log probability: -0.20
2026-01-28 19:23:51,427 | INFO | total number of ended hypotheses: 179
2026-01-28 19:23:51,428 | INFO | best hypo: ▁tu▁longeais▁les▁rails▁du▁crâne▁jusqu'à▁la▁pergonnette

2026-01-28 19:23:51,432 | INFO | speech length: 38560
2026-01-28 19:23:51,491 | INFO | decoder input length: 59
2026-01-28 19:23:51,491 | INFO | max output length: 59
2026-01-28 19:23:51,491 | INFO | min output length: 5
2026-01-28 19:23:53,545 | INFO | end detected at 28
2026-01-28 19:23:53,547 | INFO |  -2.08 * 0.5 =  -1.04 for decoder
2026-01-28 19:23:53,547 | INFO |  -0.98 * 0.5 =  -0.49 for ctc
2026-01-28 19:23:53,547 | INFO | total log probability: -1.53
2026-01-28 19:23:53,548 | INFO | normalized log probability: -0.06
2026-01-28 19:23:53,548 | INFO | total number of ended hypotheses: 155
2026-01-28 19:23:53,548 | INFO | best hypo: ▁tu▁continues▁dans▁la▁vie▁ville▁tu▁prends▁la▁grande▁rue

2026-01-28 19:23:53,551 | INFO | speech length: 48160
2026-01-28 19:23:53,610 | INFO | decoder input length: 74
2026-01-28 19:23:53,610 | INFO | max output length: 74
2026-01-28 19:23:53,610 | INFO | min output length: 7
2026-01-28 19:23:54,462 | INFO | end detected at 16
2026-01-28 19:23:54,464 | INFO |  -0.87 * 0.5 =  -0.43 for decoder
2026-01-28 19:23:54,464 | INFO |  -6.99 * 0.5 =  -3.50 for ctc
2026-01-28 19:23:54,464 | INFO | total log probability: -3.93
2026-01-28 19:23:54,464 | INFO | normalized log probability: -0.36
2026-01-28 19:23:54,464 | INFO | total number of ended hypotheses: 170
2026-01-28 19:23:54,465 | INFO | best hypo: ▁après▁tu▁bifurques

2026-01-28 19:23:54,467 | INFO | speech length: 221120
2026-01-28 19:23:54,511 | INFO | decoder input length: 345
2026-01-28 19:23:54,512 | INFO | max output length: 345
2026-01-28 19:23:54,512 | INFO | min output length: 34
2026-01-28 19:24:03,036 | INFO | end detected at 100
2026-01-28 19:24:03,038 | INFO | -61.48 * 0.5 = -30.74 for decoder
2026-01-28 19:24:03,038 | INFO | -31.60 * 0.5 = -15.80 for ctc
2026-01-28 19:24:03,038 | INFO | total log probability: -46.54
2026-01-28 19:24:03,038 | INFO | normalized log probability: -0.50
2026-01-28 19:24:03,038 | INFO | total number of ended hypotheses: 189
2026-01-28 19:24:03,039 | INFO | best hypo: ▁il▁y▁a▁une▁type▁bifurcation▁juste▁avant▁la▁place▁du▁tribunal▁tu▁passes▁à▁côté▁d'une▁petite▁fontaine▁t'arrive▁place▁aux▁herbes▁avec▁une▁sorte▁halle▁quoi▁de▁de▁de▁structure▁métallique▁tutinues▁la▁rue

2026-01-28 19:24:03,042 | INFO | speech length: 52160
2026-01-28 19:24:03,079 | INFO | decoder input length: 81
2026-01-28 19:24:03,079 | INFO | max output length: 81
2026-01-28 19:24:03,079 | INFO | min output length: 8
2026-01-28 19:24:04,313 | INFO | end detected at 28
2026-01-28 19:24:04,314 | INFO |  -2.11 * 0.5 =  -1.06 for decoder
2026-01-28 19:24:04,314 | INFO |  -0.81 * 0.5 =  -0.40 for ctc
2026-01-28 19:24:04,314 | INFO | total log probability: -1.46
2026-01-28 19:24:04,314 | INFO | normalized log probability: -0.06
2026-01-28 19:24:04,314 | INFO | total number of ended hypotheses: 146
2026-01-28 19:24:04,315 | INFO | best hypo: ▁la▁petite▁rue▁est▁arrive▁à▁la▁fontaine▁place▁notre▁dame

2026-01-28 19:24:04,319 | INFO | Chunk: 0 | WER=40.000000 | S=1 D=0 I=1
2026-01-28 19:24:04,320 | INFO | Chunk: 1 | WER=28.000000 | S=5 D=2 I=0
2026-01-28 19:24:04,320 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 19:24:04,321 | INFO | Chunk: 3 | WER=62.500000 | S=4 D=1 I=0
2026-01-28 19:24:04,321 | INFO | Chunk: 4 | WER=41.666667 | S=3 D=2 I=0
2026-01-28 19:24:04,321 | INFO | Chunk: 5 | WER=9.090909 | S=1 D=0 I=0
2026-01-28 19:24:04,321 | INFO | Chunk: 6 | WER=25.000000 | S=0 D=1 I=0
2026-01-28 19:24:04,323 | INFO | Chunk: 7 | WER=17.073171 | S=4 D=3 I=0
2026-01-28 19:24:04,323 | INFO | Chunk: 8 | WER=25.000000 | S=2 D=1 I=0
2026-01-28 19:24:04,333 | INFO | File: Rhap-M0001.wav | WER=24.800000 | S=20 D=10 I=1
2026-01-28 19:24:04,333 | INFO | ------------------------------
2026-01-28 19:24:04,333 | INFO | Conf cv Done!
2026-01-28 19:24:04,482 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 19:24:04,504 | INFO | Vocabulary size: 47
2026-01-28 19:24:05,453 | INFO | Gradient checkpoint layers: []
2026-01-28 19:24:06,071 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:24:06,075 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:24:06,075 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:24:06,076 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 19:24:06,078 | INFO | speech length: 55040
2026-01-28 19:24:06,117 | INFO | decoder input length: 85
2026-01-28 19:24:06,117 | INFO | max output length: 85
2026-01-28 19:24:06,117 | INFO | min output length: 8
2026-01-28 19:24:07,918 | INFO | end detected at 44
2026-01-28 19:24:07,920 | INFO |  -4.55 * 0.5 =  -2.27 for decoder
2026-01-28 19:24:07,920 | INFO |  -0.89 * 0.5 =  -0.44 for ctc
2026-01-28 19:24:07,920 | INFO | total log probability: -2.72
2026-01-28 19:24:07,920 | INFO | normalized log probability: -0.07
2026-01-28 19:24:07,920 | INFO | total number of ended hypotheses: 180
2026-01-28 19:24:07,921 | INFO | best hypo: eh<space>ben<space>euh<space>tu<space>prends<space>le<space>boulevard<space>euh

2026-01-28 19:24:07,923 | INFO | speech length: 125280
2026-01-28 19:24:07,959 | INFO | decoder input length: 195
2026-01-28 19:24:07,959 | INFO | max output length: 195
2026-01-28 19:24:07,959 | INFO | min output length: 19
2026-01-28 19:24:15,049 | INFO | end detected at 137
2026-01-28 19:24:15,050 | INFO | -19.75 * 0.5 =  -9.87 for decoder
2026-01-28 19:24:15,050 | INFO | -13.42 * 0.5 =  -6.71 for ctc
2026-01-28 19:24:15,050 | INFO | total log probability: -16.58
2026-01-28 19:24:15,050 | INFO | normalized log probability: -0.13
2026-01-28 19:24:15,050 | INFO | total number of ended hypotheses: 163
2026-01-28 19:24:15,052 | INFO | best hypo: là<space>qui<space>part<space>euh<space>de<space>vêtre<space>chavande<space>être<space>boulevard<space>qui<space>passe<space>à<space>côté<space>d'halita<space>je<space>continue<space>tu<space>vas<space>arriver<space>sur<space>la<space>place<space>euh<space>victor<space>hugo

2026-01-28 19:24:15,055 | INFO | speech length: 36960
2026-01-28 19:24:15,090 | INFO | decoder input length: 57
2026-01-28 19:24:15,090 | INFO | max output length: 57
2026-01-28 19:24:15,090 | INFO | min output length: 5
2026-01-28 19:24:16,648 | INFO | end detected at 42
2026-01-28 19:24:16,651 | INFO |  -2.98 * 0.5 =  -1.49 for decoder
2026-01-28 19:24:16,651 | INFO |  -1.16 * 0.5 =  -0.58 for ctc
2026-01-28 19:24:16,651 | INFO | total log probability: -2.07
2026-01-28 19:24:16,651 | INFO | normalized log probability: -0.06
2026-01-28 19:24:16,651 | INFO | total number of ended hypotheses: 184
2026-01-28 19:24:16,652 | INFO | best hypo: la<space>place<space>victor<space>hugo<space>à<space>la<space>banque

2026-01-28 19:24:16,654 | INFO | speech length: 27200
2026-01-28 19:24:16,693 | INFO | decoder input length: 42
2026-01-28 19:24:16,693 | INFO | max output length: 42
2026-01-28 19:24:16,694 | INFO | min output length: 4
2026-01-28 19:24:18,047 | INFO | end detected at 38
2026-01-28 19:24:18,049 | INFO |  -6.37 * 0.5 =  -3.18 for decoder
2026-01-28 19:24:18,049 | INFO |  -8.49 * 0.5 =  -4.24 for ctc
2026-01-28 19:24:18,049 | INFO | total log probability: -7.43
2026-01-28 19:24:18,050 | INFO | normalized log probability: -0.25
2026-01-28 19:24:18,050 | INFO | total number of ended hypotheses: 224
2026-01-28 19:24:18,050 | INFO | best hypo: est<space>l'angle<space>qui<space>prend<space>droite

2026-01-28 19:24:18,052 | INFO | speech length: 51040
2026-01-28 19:24:18,094 | INFO | decoder input length: 79
2026-01-28 19:24:18,094 | INFO | max output length: 79
2026-01-28 19:24:18,094 | INFO | min output length: 7
2026-01-28 19:24:20,609 | INFO | end detected at 66
2026-01-28 19:24:20,611 | INFO |  -8.15 * 0.5 =  -4.07 for decoder
2026-01-28 19:24:20,611 | INFO | -17.47 * 0.5 =  -8.74 for ctc
2026-01-28 19:24:20,611 | INFO | total log probability: -12.81
2026-01-28 19:24:20,611 | INFO | normalized log probability: -0.22
2026-01-28 19:24:20,611 | INFO | total number of ended hypotheses: 196
2026-01-28 19:24:20,612 | INFO | best hypo: tu<space>longes<space>les<space>les<space>rails<space>du<space>drame<space>jusqu'à<space>la<space>place<space>gonette

2026-01-28 19:24:20,614 | INFO | speech length: 38560
2026-01-28 19:24:20,651 | INFO | decoder input length: 59
2026-01-28 19:24:20,651 | INFO | max output length: 59
2026-01-28 19:24:20,651 | INFO | min output length: 5
2026-01-28 19:24:22,847 | INFO | adding <eos> in the last position in the loop
2026-01-28 19:24:22,855 | INFO | no hypothesis. Finish decoding.
2026-01-28 19:24:22,856 | INFO |  -5.90 * 0.5 =  -2.95 for decoder
2026-01-28 19:24:22,857 | INFO |  -5.15 * 0.5 =  -2.58 for ctc
2026-01-28 19:24:22,857 | INFO | total log probability: -5.53
2026-01-28 19:24:22,857 | INFO | normalized log probability: -0.10
2026-01-28 19:24:22,857 | INFO | total number of ended hypotheses: 112
2026-01-28 19:24:22,858 | INFO | best hypo: tu<space>continues<space>dans<space>la<space>vie<space>ville<space>tu<space>prends<space>la<space>grande<space>rue

2026-01-28 19:24:22,859 | INFO | speech length: 48160
2026-01-28 19:24:22,895 | INFO | decoder input length: 74
2026-01-28 19:24:22,895 | INFO | max output length: 74
2026-01-28 19:24:22,895 | INFO | min output length: 7
2026-01-28 19:24:24,380 | INFO | end detected at 37
2026-01-28 19:24:24,383 | INFO |  -3.26 * 0.5 =  -1.63 for decoder
2026-01-28 19:24:24,383 | INFO |  -5.20 * 0.5 =  -2.60 for ctc
2026-01-28 19:24:24,383 | INFO | total log probability: -4.23
2026-01-28 19:24:24,383 | INFO | normalized log probability: -0.14
2026-01-28 19:24:24,383 | INFO | total number of ended hypotheses: 218
2026-01-28 19:24:24,384 | INFO | best hypo: et<space>euh<space>après<space>tu<space>bissures<space>que

2026-01-28 19:24:24,386 | INFO | speech length: 221120
2026-01-28 19:24:24,429 | INFO | decoder input length: 345
2026-01-28 19:24:24,429 | INFO | max output length: 345
2026-01-28 19:24:24,429 | INFO | min output length: 34
2026-01-28 19:24:39,110 | INFO | end detected at 222
2026-01-28 19:24:39,112 | INFO | -30.19 * 0.5 = -15.10 for decoder
2026-01-28 19:24:39,112 | INFO |  -9.67 * 0.5 =  -4.83 for ctc
2026-01-28 19:24:39,112 | INFO | total log probability: -19.93
2026-01-28 19:24:39,112 | INFO | normalized log probability: -0.09
2026-01-28 19:24:39,112 | INFO | total number of ended hypotheses: 198
2026-01-28 19:24:39,115 | INFO | best hypo: il<space>y<space>a<space>une<space>petite<space>bifurcation<space>juste<space>avant<space>la<space>place<space>du<space>tribunal<space>qui<space>passe<space>à<space>côté<space>d'une<space>petite<space>fontaine<space>t'arrive<space>place<space>aux<space>herbes<space>avec<space>une<space>une<space>sorte<space>de<space>hale<space>quoi<space>de<space>de<space>de<space>structures<space>métalliques<space>tu<space>continues<space>la<space>rue

2026-01-28 19:24:39,117 | INFO | speech length: 52160
2026-01-28 19:24:39,159 | INFO | decoder input length: 81
2026-01-28 19:24:39,159 | INFO | max output length: 81
2026-01-28 19:24:39,159 | INFO | min output length: 8
2026-01-28 19:24:41,691 | INFO | end detected at 66
2026-01-28 19:24:41,692 | INFO |  -5.04 * 0.5 =  -2.52 for decoder
2026-01-28 19:24:41,692 | INFO |  -0.89 * 0.5 =  -0.45 for ctc
2026-01-28 19:24:41,693 | INFO | total log probability: -2.97
2026-01-28 19:24:41,693 | INFO | normalized log probability: -0.05
2026-01-28 19:24:41,693 | INFO | total number of ended hypotheses: 167
2026-01-28 19:24:41,694 | INFO | best hypo: la<space>petite<space>rue<space>est<space>arrive<space>à<space>la<space>fontaine<space>euh<space>place<space>notre<space>dame

2026-01-28 19:24:41,699 | INFO | Chunk: 0 | WER=80.000000 | S=1 D=0 I=3
2026-01-28 19:24:41,700 | INFO | Chunk: 1 | WER=36.000000 | S=6 D=1 I=2
2026-01-28 19:24:41,700 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 19:24:41,701 | INFO | Chunk: 3 | WER=62.500000 | S=3 D=2 I=0
2026-01-28 19:24:41,701 | INFO | Chunk: 4 | WER=16.666667 | S=2 D=0 I=0
2026-01-28 19:24:41,701 | INFO | Chunk: 5 | WER=9.090909 | S=1 D=0 I=0
2026-01-28 19:24:41,702 | INFO | Chunk: 6 | WER=75.000000 | S=1 D=0 I=2
2026-01-28 19:24:41,703 | INFO | Chunk: 7 | WER=17.073171 | S=7 D=0 I=0
2026-01-28 19:24:41,703 | INFO | Chunk: 8 | WER=33.333333 | S=2 D=1 I=1
2026-01-28 19:24:41,714 | INFO | File: Rhap-M0001.wav | WER=28.000000 | S=23 D=4 I=8
2026-01-28 19:24:41,714 | INFO | ------------------------------
2026-01-28 19:24:41,714 | INFO | Conf ester Done!
2026-01-28 19:25:53,265 | INFO | Chunk: 0 | WER=100.000000 | S=3 D=0 I=2
2026-01-28 19:25:53,266 | INFO | Chunk: 1 | WER=44.000000 | S=9 D=2 I=0
2026-01-28 19:25:53,267 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 19:25:53,267 | INFO | Chunk: 3 | WER=50.000000 | S=3 D=1 I=0
2026-01-28 19:25:53,267 | INFO | Chunk: 4 | WER=33.333333 | S=4 D=0 I=0
2026-01-28 19:25:53,268 | INFO | Chunk: 5 | WER=45.454545 | S=4 D=1 I=0
2026-01-28 19:25:53,268 | INFO | Chunk: 6 | WER=50.000000 | S=1 D=1 I=0
2026-01-28 19:25:53,270 | INFO | Chunk: 7 | WER=24.390244 | S=7 D=2 I=1
2026-01-28 19:25:53,270 | INFO | Chunk: 8 | WER=41.666667 | S=3 D=1 I=1
2026-01-28 19:25:53,271 | INFO | Chunk: 9 | WER=150.000000 | S=2 D=0 I=1
2026-01-28 19:25:53,285 | INFO | File: Rhap-M0001.wav | WER=38.582677 | S=36 D=8 I=5
2026-01-28 19:25:53,285 | INFO | ------------------------------
2026-01-28 19:25:53,285 | INFO | hmm_tdnn Done!
2026-01-28 19:25:53,467 | INFO | ==================================Rhap-M0002.wav=========================================
2026-01-28 19:25:53,630 | INFO | Using rVAD model
2026-01-28 19:25:59,202 | INFO | Chunk: 0 | WER=24.489796 | S=5 D=7 I=0
2026-01-28 19:25:59,209 | INFO | Chunk: 1 | WER=16.470588 | S=9 D=4 I=1
2026-01-28 19:25:59,209 | INFO | Chunk: 2 | WER=44.444444 | S=5 D=3 I=0
2026-01-28 19:25:59,211 | INFO | Chunk: 3 | WER=9.677419 | S=3 D=0 I=0
2026-01-28 19:25:59,235 | INFO | File: Rhap-M0002.wav | WER=20.218579 | S=22 D=14 I=1
2026-01-28 19:25:59,235 | INFO | ------------------------------
2026-01-28 19:25:59,235 | INFO | w2vec vad chunk Done!
2026-01-28 19:26:07,880 | INFO | Chunk: 0 | WER=32.653061 | S=3 D=13 I=0
2026-01-28 19:26:07,884 | INFO | Chunk: 1 | WER=51.764706 | S=0 D=43 I=1
2026-01-28 19:26:07,884 | INFO | Chunk: 2 | WER=22.222222 | S=2 D=2 I=0
2026-01-28 19:26:07,886 | INFO | Chunk: 3 | WER=6.451613 | S=2 D=0 I=0
2026-01-28 19:26:07,905 | INFO | File: Rhap-M0002.wav | WER=36.065574 | S=7 D=58 I=1
2026-01-28 19:26:07,905 | INFO | ------------------------------
2026-01-28 19:26:07,905 | INFO | whisper med Done!
2026-01-28 19:26:19,182 | INFO | Chunk: 0 | WER=57.142857 | S=3 D=25 I=0
2026-01-28 19:26:19,186 | INFO | Chunk: 1 | WER=42.352941 | S=5 D=29 I=2
2026-01-28 19:26:19,187 | INFO | Chunk: 2 | WER=22.222222 | S=3 D=1 I=0
2026-01-28 19:26:19,188 | INFO | Chunk: 3 | WER=3.225806 | S=1 D=0 I=0
2026-01-28 19:26:19,208 | INFO | File: Rhap-M0002.wav | WER=37.704918 | S=12 D=55 I=2
2026-01-28 19:26:19,208 | INFO | ------------------------------
2026-01-28 19:26:19,208 | INFO | whisper large Done!
2026-01-28 19:26:19,365 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 19:26:19,414 | INFO | Vocabulary size: 350
2026-01-28 19:26:20,413 | INFO | Gradient checkpoint layers: []
2026-01-28 19:26:21,064 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:26:21,068 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:26:21,068 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:26:21,069 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 19:26:21,069 | INFO | speech length: 329760
2026-01-28 19:26:21,118 | INFO | decoder input length: 514
2026-01-28 19:26:21,119 | INFO | max output length: 514
2026-01-28 19:26:21,119 | INFO | min output length: 51
2026-01-28 19:26:31,645 | INFO | end detected at 104
2026-01-28 19:26:31,646 | INFO | -196.31 * 0.5 = -98.16 for decoder
2026-01-28 19:26:31,646 | INFO | -61.27 * 0.5 = -30.64 for ctc
2026-01-28 19:26:31,646 | INFO | total log probability: -128.79
2026-01-28 19:26:31,646 | INFO | normalized log probability: -1.29
2026-01-28 19:26:31,646 | INFO | total number of ended hypotheses: 154
2026-01-28 19:26:31,648 | INFO | best hypo: ▁non▁queueyez▁une▁jeune▁fille▁tout▁en▁noir▁qui▁en▁se▁baladant▁dans▁la▁rue▁a▁vu▁une▁vitrine▁de▁pâtissier▁boulonger▁et▁puis▁pendant▁qu'elle▁regardait▁cette▁vitrine▁ou▁nulle▁il▁le▁est▁quelqu'un▁peu▁arrivait▁pour▁livrer▁du▁pain▁et▁des▁gâteaux

2026-01-28 19:26:31,651 | INFO | speech length: 403840
2026-01-28 19:26:31,699 | INFO | decoder input length: 630
2026-01-28 19:26:31,699 | INFO | max output length: 630
2026-01-28 19:26:31,699 | INFO | min output length: 63
2026-01-28 19:26:51,270 | INFO | end detected at 176
2026-01-28 19:26:51,271 | INFO | -414.04 * 0.5 = -207.02 for decoder
2026-01-28 19:26:51,272 | INFO | -140.69 * 0.5 = -70.34 for ctc
2026-01-28 19:26:51,272 | INFO | total log probability: -277.36
2026-01-28 19:26:51,272 | INFO | normalized log probability: -1.62
2026-01-28 19:26:51,272 | INFO | total number of ended hypotheses: 165
2026-01-28 19:26:51,274 | INFO | best hypo: ▁et▁pendant▁qu'il▁était▁à▁l'intérieur▁de▁la▁boutique▁qu'il▁avait▁laissé▁son▁camion▁ouvert▁elle▁en▁a▁profité▁pour▁voler▁une▁baguette▁de▁pins▁et▁s'enfuir▁mais▁à▁ce▁moment▁là▁il▁a▁une▁dame▁peu▁plus▁âgée▁avec▁un▁chapeau▁qu'il▁avait▁vue▁elle▁s'est▁mis▁d▁courir▁et▁allait▁et▁rentrer▁d▁charvilie▁chappline▁elle▁y▁sont▁tombés▁de▁tous▁deux▁la▁d'âme▁plus▁égé▁que▁le▁chapeau▁la▁dénonçait▁d'abord▁au▁boulanger

2026-01-28 19:26:51,276 | INFO | speech length: 84000
2026-01-28 19:26:51,329 | INFO | decoder input length: 130
2026-01-28 19:26:51,329 | INFO | max output length: 130
2026-01-28 19:26:51,329 | INFO | min output length: 13
2026-01-28 19:26:53,392 | INFO | end detected at 41
2026-01-28 19:26:53,395 | INFO |  -9.52 * 0.5 =  -4.76 for decoder
2026-01-28 19:26:53,395 | INFO | -13.26 * 0.5 =  -6.63 for ctc
2026-01-28 19:26:53,395 | INFO | total log probability: -11.39
2026-01-28 19:26:53,395 | INFO | normalized log probability: -0.33
2026-01-28 19:26:53,395 | INFO | total number of ended hypotheses: 178
2026-01-28 19:26:53,396 | INFO | best hypo: ▁puis▁comme▁il▁est▁tous▁les▁deux▁par▁terre▁le▁banger▁est▁arrivé▁avec▁la▁poupée▁sison

2026-01-28 19:26:53,398 | INFO | speech length: 176160
2026-01-28 19:26:53,441 | INFO | decoder input length: 274
2026-01-28 19:26:53,441 | INFO | max output length: 274
2026-01-28 19:26:53,441 | INFO | min output length: 27
2026-01-28 19:26:58,862 | INFO | end detected at 78
2026-01-28 19:26:58,864 | INFO |  -8.71 * 0.5 =  -4.36 for decoder
2026-01-28 19:26:58,864 | INFO |  -1.86 * 0.5 =  -0.93 for ctc
2026-01-28 19:26:58,864 | INFO | total log probability: -5.28
2026-01-28 19:26:58,864 | INFO | normalized log probability: -0.07
2026-01-28 19:26:58,864 | INFO | total number of ended hypotheses: 164
2026-01-28 19:26:58,865 | INFO | best hypo: ▁discutée▁et▁visiblement▁même▁s'est▁un▁film▁muet▁on▁comprend▁que▁charlie▁chaplin▁s'accuse▁à▁la▁place▁de▁la▁jeune▁fille▁qui▁donc▁reste▁toute▁seule▁fort▁étonnée

2026-01-28 19:26:58,872 | INFO | Chunk: 0 | WER=24.489796 | S=7 D=4 I=1
2026-01-28 19:26:58,877 | INFO | Chunk: 1 | WER=31.764706 | S=20 D=4 I=3
2026-01-28 19:26:58,877 | INFO | Chunk: 2 | WER=33.333333 | S=5 D=1 I=0
2026-01-28 19:26:58,878 | INFO | Chunk: 3 | WER=9.677419 | S=2 D=1 I=0
2026-01-28 19:26:58,897 | INFO | File: Rhap-M0002.wav | WER=26.229508 | S=34 D=10 I=4
2026-01-28 19:26:58,897 | INFO | ------------------------------
2026-01-28 19:26:58,897 | INFO | Conf cv Done!
2026-01-28 19:26:59,082 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 19:26:59,105 | INFO | Vocabulary size: 47
2026-01-28 19:27:00,245 | INFO | Gradient checkpoint layers: []
2026-01-28 19:27:00,867 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:27:00,871 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:27:00,871 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:27:00,872 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 19:27:00,875 | INFO | speech length: 329760
2026-01-28 19:27:00,916 | INFO | decoder input length: 514
2026-01-28 19:27:00,916 | INFO | max output length: 514
2026-01-28 19:27:00,916 | INFO | min output length: 51
2026-01-28 19:27:34,254 | INFO | end detected at 362
2026-01-28 19:27:34,257 | INFO | -66.25 * 0.5 = -33.12 for decoder
2026-01-28 19:27:34,257 | INFO | -66.16 * 0.5 = -33.08 for ctc
2026-01-28 19:27:34,257 | INFO | total log probability: -66.20
2026-01-28 19:27:34,257 | INFO | normalized log probability: -0.19
2026-01-28 19:27:34,257 | INFO | total number of ended hypotheses: 213
2026-01-28 19:27:34,263 | INFO | best hypo: donc<space>euh<space>il<space>y<space>a<space>une<space>jeune<space>fille<space>pour<space>cent<space>hesitation<space>tout<space>en<space>noir<space>qui<space>pour<space>cent<space>hesitation<space>on<space>se<space>baladant<space>dans<space>la<space>rue<space>a<space>vu<space>une<space>vitrine<space>pour<space>cent<space>hesitation<space>de<space>pâtissiers<space>bongers<space>et<space>puis<space>pendant<space>qu'elle<space>pour<space>cent<space>hesitation<space>regardez<space>cette<space>vitrine<space>pour<space>cent<space>hesitation<space>y<space>est<space>quelqu'un<space>qui<space>arrivait<space>pour<space>livrer<space>pour<space>cent<space>hesitation<space>du<space>pain<space>et<space>des<space>gâteaux

2026-01-28 19:27:34,266 | INFO | speech length: 403840
2026-01-28 19:27:34,304 | INFO | decoder input length: 630
2026-01-28 19:27:34,304 | INFO | max output length: 630
2026-01-28 19:27:34,304 | INFO | min output length: 63
2026-01-28 19:28:14,817 | INFO | end detected at 417
2026-01-28 19:28:14,819 | INFO | -136.16 * 0.5 = -68.08 for decoder
2026-01-28 19:28:14,819 | INFO | -37.10 * 0.5 = -18.55 for ctc
2026-01-28 19:28:14,819 | INFO | total log probability: -86.63
2026-01-28 19:28:14,819 | INFO | normalized log probability: -0.21
2026-01-28 19:28:14,819 | INFO | total number of ended hypotheses: 221
2026-01-28 19:28:14,825 | INFO | best hypo: et<space>pendant<space>qu'il<space>était<space>à<space>l'intérieur<space>de<space>la<space>boutique<space>il<space>avait<space>laissé<space>son<space>camion<space>ouvert<space>euh<space>elle<space>en<space>a<space>profité<space>pour<space>voler<space>une<space>baguette<space>de<space>pain<space>elle<space>s'en<space>fire<space>mais<space>à<space>ce<space>moment<space>là<space>il<space>y<space>a<space>une<space>dame<space>un<space>peu<space>plus<space>âgée<space>avec<space>un<space>chapeau<space>qu'il<space>a<space>vue<space>elle<space>s'est<space>mise<space>à<space>courir<space>elle<space>est<space>rentrée<space>euh<space>dans<space>char<space>discipline<space>elle<space>ils<space>sont<space>tombés<space>tous<space>les<space>deux<space>la<space>dame<space>euh<space>faisait<space>avec<space>le<space>chapeau<space>l'a<space>dénoncée<space>d'abord<space>ou<space>boulanger

2026-01-28 19:28:14,828 | INFO | speech length: 84000
2026-01-28 19:28:14,875 | INFO | decoder input length: 130
2026-01-28 19:28:14,875 | INFO | max output length: 130
2026-01-28 19:28:14,875 | INFO | min output length: 13
2026-01-28 19:28:19,255 | INFO | end detected at 97
2026-01-28 19:28:19,258 | INFO | -16.35 * 0.5 =  -8.17 for decoder
2026-01-28 19:28:19,258 | INFO | -18.17 * 0.5 =  -9.08 for ctc
2026-01-28 19:28:19,258 | INFO | total log probability: -17.26
2026-01-28 19:28:19,258 | INFO | normalized log probability: -0.21
2026-01-28 19:28:19,258 | INFO | total number of ended hypotheses: 223
2026-01-28 19:28:19,260 | INFO | best hypo: puis<space>euh<space>comme<space>ils<space>tous<space>les<space>deux<space>par<space>terre<space>bon<space>j'est<space>arrivé<space>avec<space>la<space>police<space>ils<space>ont

2026-01-28 19:28:19,262 | INFO | speech length: 176160
2026-01-28 19:28:19,305 | INFO | decoder input length: 274
2026-01-28 19:28:19,305 | INFO | max output length: 274
2026-01-28 19:28:19,305 | INFO | min output length: 27
2026-01-28 19:28:29,929 | INFO | end detected at 181
2026-01-28 19:28:29,931 | INFO | -17.76 * 0.5 =  -8.88 for decoder
2026-01-28 19:28:29,931 | INFO |  -9.07 * 0.5 =  -4.53 for ctc
2026-01-28 19:28:29,931 | INFO | total log probability: -13.41
2026-01-28 19:28:29,931 | INFO | normalized log probability: -0.08
2026-01-28 19:28:29,931 | INFO | total number of ended hypotheses: 191
2026-01-28 19:28:29,933 | INFO | best hypo: discuter<space>et<space>visiblement<space>même<space>si<space>c'est<space>un<space>film<space>mué<space>euh<space>on<space>comprend<space>que<space>euh<space>chaque<space>chapeline<space>s'accuse<space>à<space>la<space>place<space>de<space>la<space>jeune<space>fille<space>qui<space>donc<space>euh<space>reste<space>toute<space>seule<space>sans<space>rétonner

2026-01-28 19:28:29,942 | INFO | Chunk: 0 | WER=51.020408 | S=10 D=0 I=15
2026-01-28 19:28:29,947 | INFO | Chunk: 1 | WER=16.470588 | S=10 D=1 I=3
2026-01-28 19:28:29,947 | INFO | Chunk: 2 | WER=22.222222 | S=2 D=1 I=1
2026-01-28 19:28:29,948 | INFO | Chunk: 3 | WER=29.032258 | S=6 D=0 I=3
2026-01-28 19:28:29,969 | INFO | File: Rhap-M0002.wav | WER=28.415301 | S=28 D=2 I=22
2026-01-28 19:28:29,969 | INFO | ------------------------------
2026-01-28 19:28:29,969 | INFO | Conf ester Done!
2026-01-28 19:29:26,066 | INFO | Chunk: 0 | WER=51.020408 | S=11 D=11 I=3
2026-01-28 19:29:26,072 | INFO | Chunk: 1 | WER=23.529412 | S=11 D=5 I=4
2026-01-28 19:29:26,073 | INFO | Chunk: 2 | WER=55.555556 | S=7 D=3 I=0
2026-01-28 19:29:26,074 | INFO | Chunk: 3 | WER=29.032258 | S=8 D=0 I=1
2026-01-28 19:29:26,099 | INFO | File: Rhap-M0002.wav | WER=34.972678 | S=37 D=19 I=8
2026-01-28 19:29:26,099 | INFO | ------------------------------
2026-01-28 19:29:26,099 | INFO | hmm_tdnn Done!
2026-01-28 19:29:26,276 | INFO | ==================================Rhap-M0003.wav=========================================
2026-01-28 19:29:26,450 | INFO | Using rVAD model
2026-01-28 19:29:35,046 | INFO | Chunk: 0 | WER=100.000000 | S=1 D=0 I=2
2026-01-28 19:29:35,048 | INFO | Chunk: 1 | WER=27.777778 | S=1 D=6 I=3
2026-01-28 19:29:35,048 | INFO | Chunk: 2 | WER=23.076923 | S=3 D=1 I=2
2026-01-28 19:29:35,054 | INFO | Chunk: 3 | WER=24.742268 | S=9 D=11 I=4
2026-01-28 19:29:35,058 | INFO | Chunk: 4 | WER=29.411765 | S=9 D=10 I=6
2026-01-28 19:29:35,058 | INFO | Chunk: 5 | WER=114.285714 | S=0 D=3 I=5
2026-01-28 19:29:35,059 | INFO | Chunk: 6 | WER=59.375000 | S=6 D=7 I=6
2026-01-28 19:29:35,059 | INFO | Chunk: 7 | WER=120.000000 | S=5 D=0 I=1
2026-01-28 19:29:35,060 | INFO | Chunk: 8 | WER=100.000000 | S=3 D=1 I=0
2026-01-28 19:29:35,104 | INFO | File: Rhap-M0003.wav | WER=26.101695 | S=37 D=25 I=15
2026-01-28 19:29:35,104 | INFO | ------------------------------
2026-01-28 19:29:35,104 | INFO | w2vec vad chunk Done!
2026-01-28 19:29:45,004 | INFO | Chunk: 0 | WER=100.000000 | S=0 D=1 I=2
2026-01-28 19:29:45,005 | INFO | Chunk: 1 | WER=27.777778 | S=0 D=6 I=4
2026-01-28 19:29:45,006 | INFO | Chunk: 2 | WER=15.384615 | S=2 D=0 I=2
2026-01-28 19:29:45,008 | INFO | Chunk: 3 | WER=69.072165 | S=1 D=66 I=0
2026-01-28 19:29:45,010 | INFO | Chunk: 4 | WER=83.529412 | S=4 D=66 I=1
2026-01-28 19:29:45,010 | INFO | Chunk: 5 | WER=114.285714 | S=0 D=3 I=5
2026-01-28 19:29:45,011 | INFO | Chunk: 6 | WER=59.375000 | S=7 D=9 I=3
2026-01-28 19:29:45,011 | INFO | Chunk: 7 | WER=120.000000 | S=5 D=0 I=1
2026-01-28 19:29:45,011 | INFO | Chunk: 8 | WER=100.000000 | S=4 D=0 I=0
2026-01-28 19:29:45,038 | INFO | File: Rhap-M0003.wav | WER=60.338983 | S=23 D=144 I=11
2026-01-28 19:29:45,038 | INFO | ------------------------------
2026-01-28 19:29:45,038 | INFO | whisper med Done!
2026-01-28 19:29:59,331 | INFO | Chunk: 0 | WER=166.666667 | S=1 D=0 I=4
2026-01-28 19:29:59,333 | INFO | Chunk: 1 | WER=30.555556 | S=1 D=7 I=3
2026-01-28 19:29:59,334 | INFO | Chunk: 2 | WER=15.384615 | S=2 D=0 I=2
2026-01-28 19:29:59,337 | INFO | Chunk: 3 | WER=67.010309 | S=0 D=65 I=0
2026-01-28 19:29:59,340 | INFO | Chunk: 4 | WER=74.117647 | S=2 D=60 I=1
2026-01-28 19:29:59,340 | INFO | Chunk: 5 | WER=114.285714 | S=0 D=3 I=5
2026-01-28 19:29:59,341 | INFO | Chunk: 6 | WER=53.125000 | S=2 D=9 I=6
2026-01-28 19:29:59,342 | INFO | Chunk: 7 | WER=180.000000 | S=5 D=0 I=4
2026-01-28 19:29:59,342 | INFO | Chunk: 8 | WER=125.000000 | S=4 D=0 I=1
2026-01-28 19:29:59,381 | INFO | File: Rhap-M0003.wav | WER=56.610169 | S=15 D=135 I=17
2026-01-28 19:29:59,381 | INFO | ------------------------------
2026-01-28 19:29:59,381 | INFO | whisper large Done!
2026-01-28 19:29:59,538 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 19:29:59,587 | INFO | Vocabulary size: 350
2026-01-28 19:30:00,514 | INFO | Gradient checkpoint layers: []
2026-01-28 19:30:01,171 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:30:01,176 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:30:01,176 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:30:01,176 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 19:30:01,177 | INFO | speech length: 30400
2026-01-28 19:30:01,216 | INFO | decoder input length: 47
2026-01-28 19:30:01,216 | INFO | max output length: 47
2026-01-28 19:30:01,216 | INFO | min output length: 4
2026-01-28 19:30:02,065 | INFO | end detected at 19
2026-01-28 19:30:02,066 | INFO |  -1.28 * 0.5 =  -0.64 for decoder
2026-01-28 19:30:02,066 | INFO |  -0.99 * 0.5 =  -0.49 for ctc
2026-01-28 19:30:02,066 | INFO | total log probability: -1.14
2026-01-28 19:30:02,066 | INFO | normalized log probability: -0.08
2026-01-28 19:30:02,066 | INFO | total number of ended hypotheses: 145
2026-01-28 19:30:02,067 | INFO | best hypo: ▁la▁cathédrale▁notre▁dame

2026-01-28 19:30:02,069 | INFO | speech length: 188160
2026-01-28 19:30:02,115 | INFO | decoder input length: 293
2026-01-28 19:30:02,115 | INFO | max output length: 293
2026-01-28 19:30:02,115 | INFO | min output length: 29
2026-01-28 19:30:07,965 | INFO | end detected at 82
2026-01-28 19:30:07,966 | INFO | -48.25 * 0.5 = -24.12 for decoder
2026-01-28 19:30:07,966 | INFO | -17.40 * 0.5 =  -8.70 for ctc
2026-01-28 19:30:07,966 | INFO | total log probability: -32.82
2026-01-28 19:30:07,966 | INFO | normalized log probability: -0.44
2026-01-28 19:30:07,966 | INFO | total number of ended hypotheses: 171
2026-01-28 19:30:07,968 | INFO | best hypo: ▁en▁fait▁donc▁là▁vous▁partez▁vous▁remontez▁le▁grand▁boulevard▁là▁donc▁vous▁allez▁jusqu'au▁bout▁jusqu'à▁un▁grand▁carrefour▁là▁où▁vous▁allez▁voir▁le▁cinéma▁la▁neuf▁chavin

2026-01-28 19:30:07,970 | INFO | speech length: 145280
2026-01-28 19:30:08,019 | INFO | decoder input length: 226
2026-01-28 19:30:08,019 | INFO | max output length: 226
2026-01-28 19:30:08,019 | INFO | min output length: 22
2026-01-28 19:30:11,612 | INFO | end detected at 57
2026-01-28 19:30:11,613 | INFO |  -8.45 * 0.5 =  -4.23 for decoder
2026-01-28 19:30:11,613 | INFO | -12.60 * 0.5 =  -6.30 for ctc
2026-01-28 19:30:11,614 | INFO | total log probability: -10.53
2026-01-28 19:30:11,614 | INFO | normalized log probability: -0.21
2026-01-28 19:30:11,614 | INFO | total number of ended hypotheses: 179
2026-01-28 19:30:11,614 | INFO | best hypo: ▁et▁quand▁vous▁serez▁donc▁à▁la▁nef▁chavant▁alors▁la▁et▁vous▁vous▁passer▁à▁droite▁du▁cinéma▁une▁petite▁rue▁qui▁part▁à▁droite▁du▁cinéma

2026-01-28 19:30:11,617 | INFO | speech length: 472800
2026-01-28 19:30:11,660 | INFO | decoder input length: 738
2026-01-28 19:30:11,660 | INFO | max output length: 738
2026-01-28 19:30:11,660 | INFO | min output length: 73
2026-01-28 19:30:36,104 | INFO | end detected at 195
2026-01-28 19:30:36,106 | INFO | -527.18 * 0.5 = -263.59 for decoder
2026-01-28 19:30:36,106 | INFO | -127.09 * 0.5 = -63.54 for ctc
2026-01-28 19:30:36,106 | INFO | total log probability: -327.13
2026-01-28 19:30:36,106 | INFO | normalized log probability: -1.74
2026-01-28 19:30:36,107 | INFO | total number of ended hypotheses: 177
2026-01-28 19:30:36,109 | INFO | best hypo: ▁ce▁qui▁en▁fait▁qui▁c'est▁sur▁la▁ligne▁du▁tram▁là▁vous▁suivez▁la▁ligne▁du▁tram▁et▁vous▁remontez▁jusqu'à▁là▁une▁grande▁place▁à▁la▁place▁de▁verdun▁là▁est▁la▁place▁de▁verdun▁à▁la▁préfecture▁à▁la▁côte▁repère▁et▁à▁la▁préfecture▁dontin▁vous▁suivez▁sur▁la▁place▁de▁verdun▁la▁préfecture▁est▁sur▁vos▁droite▁vous▁vous▁continuez▁tout▁droit▁et▁faite▁balain▁vous▁passez▁en▁fait▁c'est▁la▁ligne▁du▁tramoujours▁la▁voûte▁vous▁suivez▁la▁ligne▁du▁tram▁qui▁passe▁vers

2026-01-28 19:30:36,112 | INFO | speech length: 450400
2026-01-28 19:30:36,172 | INFO | decoder input length: 703
2026-01-28 19:30:36,172 | INFO | max output length: 703
2026-01-28 19:30:36,173 | INFO | min output length: 70
2026-01-28 19:31:08,720 | INFO | end detected at 183
2026-01-28 19:31:08,721 | INFO | -342.61 * 0.5 = -171.31 for decoder
2026-01-28 19:31:08,721 | INFO | -112.30 * 0.5 = -56.15 for ctc
2026-01-28 19:31:08,721 | INFO | total log probability: -227.45
2026-01-28 19:31:08,721 | INFO | normalized log probability: -1.29
2026-01-28 19:31:08,722 | INFO | total number of ended hypotheses: 183
2026-01-28 19:31:08,724 | INFO | best hypo: ▁schrocks▁est▁une▁ancienne▁caserne▁pour▁là▁et▁vous▁suivez▁toujours▁la▁ligne▁du▁tram▁et▁là▁vous▁tombez▁donc▁à▁la▁maison▁du▁tourisme▁dans▁la▁maison▁du▁tourisme▁à▁la▁maison▁du▁tourisme▁vous▁allez▁continuez▁à▁droite▁de▁la▁maison▁du▁tourisme▁et▁passez▁et▁là▁vous▁passez▁devant▁les▁halles▁sainte▁clarc▁et▁on▁est▁un▁marchéz▁couvert▁et▁là▁et▁là▁d'aque▁donc▁que▁vous▁continuez▁toujoursoujours▁vous▁suivez▁la▁ligne▁du▁tram▁enfer

2026-01-28 19:31:08,727 | INFO | speech length: 46080
2026-01-28 19:31:08,787 | INFO | decoder input length: 71
2026-01-28 19:31:08,788 | INFO | max output length: 71
2026-01-28 19:31:08,788 | INFO | min output length: 7
2026-01-28 19:31:10,108 | INFO | end detected at 30
2026-01-28 19:31:10,109 | INFO |  -4.28 * 0.5 =  -2.14 for decoder
2026-01-28 19:31:10,110 | INFO | -11.60 * 0.5 =  -5.80 for ctc
2026-01-28 19:31:10,110 | INFO | total log probability: -7.94
2026-01-28 19:31:10,110 | INFO | normalized log probability: -0.36
2026-01-28 19:31:10,110 | INFO | total number of ended hypotheses: 195
2026-01-28 19:31:10,110 | INFO | best hypo: ▁et▁puis▁bas▁vous▁tomber▁place▁bas▁plasmo

2026-01-28 19:31:10,112 | INFO | speech length: 115840
2026-01-28 19:31:10,166 | INFO | decoder input length: 180
2026-01-28 19:31:10,166 | INFO | max output length: 180
2026-01-28 19:31:10,166 | INFO | min output length: 18
2026-01-28 19:31:14,352 | INFO | end detected at 74
2026-01-28 19:31:14,354 | INFO | -20.85 * 0.5 = -10.42 for decoder
2026-01-28 19:31:14,354 | INFO | -15.86 * 0.5 =  -7.93 for ctc
2026-01-28 19:31:14,354 | INFO | total log probability: -18.35
2026-01-28 19:31:14,354 | INFO | normalized log probability: -0.28
2026-01-28 19:31:14,354 | INFO | total number of ended hypotheses: 188
2026-01-28 19:31:14,355 | INFO | best hypo: ▁eh▁belle▁laquelle▁la▁cathédrale▁est▁la▁sur▁la▁place▁notre▁dame▁vous▁vous▁êtes▁obligé▁de▁passer▁de▁vin▁en▁fait▁un▁fait▁toulon▁vous▁suivez▁la▁ligne▁du▁tout

2026-01-28 19:31:14,357 | INFO | speech length: 20160
2026-01-28 19:31:14,398 | INFO | decoder input length: 31
2026-01-28 19:31:14,398 | INFO | max output length: 31
2026-01-28 19:31:14,398 | INFO | min output length: 3
2026-01-28 19:31:15,185 | INFO | end detected at 20
2026-01-28 19:31:15,187 | INFO |  -5.16 * 0.5 =  -2.58 for decoder
2026-01-28 19:31:15,187 | INFO | -12.56 * 0.5 =  -6.28 for ctc
2026-01-28 19:31:15,187 | INFO | total log probability: -8.86
2026-01-28 19:31:15,187 | INFO | normalized log probability: -0.74
2026-01-28 19:31:15,187 | INFO | total number of ended hypotheses: 185
2026-01-28 19:31:15,187 | INFO | best hypo: ▁il▁n'est▁pas▁bien▁compliqué

2026-01-28 19:31:15,189 | INFO | speech length: 8960
2026-01-28 19:31:15,221 | INFO | decoder input length: 13
2026-01-28 19:31:15,221 | INFO | max output length: 13
2026-01-28 19:31:15,221 | INFO | min output length: 1
2026-01-28 19:31:15,664 | INFO | adding <eos> in the last position in the loop
2026-01-28 19:31:15,673 | INFO | no hypothesis. Finish decoding.
2026-01-28 19:31:15,674 | INFO |  -5.56 * 0.5 =  -2.78 for decoder
2026-01-28 19:31:15,674 | INFO | -10.89 * 0.5 =  -5.45 for ctc
2026-01-28 19:31:15,674 | INFO | total log probability: -8.23
2026-01-28 19:31:15,674 | INFO | normalized log probability: -0.82
2026-01-28 19:31:15,675 | INFO | total number of ended hypotheses: 133
2026-01-28 19:31:15,675 | INFO | best hypo: ▁vous▁n'est▁pas▁vôtre

2026-01-28 19:31:15,680 | INFO | Chunk: 0 | WER=100.000000 | S=0 D=1 I=2
2026-01-28 19:31:15,682 | INFO | Chunk: 1 | WER=25.000000 | S=0 D=6 I=3
2026-01-28 19:31:15,682 | INFO | Chunk: 2 | WER=15.384615 | S=2 D=0 I=2
2026-01-28 19:31:15,688 | INFO | Chunk: 3 | WER=26.804124 | S=14 D=8 I=4
2026-01-28 19:31:15,692 | INFO | Chunk: 4 | WER=40.000000 | S=15 D=12 I=7
2026-01-28 19:31:15,692 | INFO | Chunk: 5 | WER=114.285714 | S=1 D=3 I=4
2026-01-28 19:31:15,693 | INFO | Chunk: 6 | WER=65.625000 | S=8 D=7 I=6
2026-01-28 19:31:15,693 | INFO | Chunk: 7 | WER=120.000000 | S=5 D=0 I=1
2026-01-28 19:31:15,694 | INFO | Chunk: 8 | WER=125.000000 | S=4 D=0 I=1
2026-01-28 19:31:15,738 | INFO | File: Rhap-M0003.wav | WER=32.881356 | S=46 D=29 I=22
2026-01-28 19:31:15,738 | INFO | ------------------------------
2026-01-28 19:31:15,738 | INFO | Conf cv Done!
2026-01-28 19:31:15,919 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 19:31:15,941 | INFO | Vocabulary size: 47
2026-01-28 19:31:17,089 | INFO | Gradient checkpoint layers: []
2026-01-28 19:31:17,833 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:31:17,839 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:31:17,840 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:31:17,840 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 19:31:17,843 | INFO | speech length: 30400
2026-01-28 19:31:17,888 | INFO | decoder input length: 47
2026-01-28 19:31:17,888 | INFO | max output length: 47
2026-01-28 19:31:17,888 | INFO | min output length: 4
2026-01-28 19:31:19,658 | INFO | end detected at 40
2026-01-28 19:31:19,660 | INFO |  -2.91 * 0.5 =  -1.46 for decoder
2026-01-28 19:31:19,660 | INFO |  -0.80 * 0.5 =  -0.40 for ctc
2026-01-28 19:31:19,660 | INFO | total log probability: -1.86
2026-01-28 19:31:19,660 | INFO | normalized log probability: -0.06
2026-01-28 19:31:19,660 | INFO | total number of ended hypotheses: 189
2026-01-28 19:31:19,661 | INFO | best hypo: alors<space>la<space>cathédrale<space>notre<space>dame

2026-01-28 19:31:19,664 | INFO | speech length: 188160
2026-01-28 19:31:19,705 | INFO | decoder input length: 293
2026-01-28 19:31:19,705 | INFO | max output length: 293
2026-01-28 19:31:19,705 | INFO | min output length: 29
2026-01-28 19:31:31,594 | INFO | end detected at 192
2026-01-28 19:31:31,596 | INFO | -17.59 * 0.5 =  -8.80 for decoder
2026-01-28 19:31:31,596 | INFO |  -6.30 * 0.5 =  -3.15 for ctc
2026-01-28 19:31:31,596 | INFO | total log probability: -11.95
2026-01-28 19:31:31,596 | INFO | normalized log probability: -0.06
2026-01-28 19:31:31,596 | INFO | total number of ended hypotheses: 214
2026-01-28 19:31:31,599 | INFO | best hypo: en<space>fait<space>donc<space>là<space>bah<space>vous<space>partez<space>là<space>vous<space>remontez<space>le<space>grand<space>boulevard<space>là<space>euh<space>donc<space>vous<space>allez<space>jusqu'au<space>bout<space>jusqu'à<space>un<space>grand<space>carrefour<space>là<space>où<space>vous<space>allez<space>voir<space>le<space>le<space>cinéma<space>la<space>nève<space>chavant

2026-01-28 19:31:31,602 | INFO | speech length: 145280
2026-01-28 19:31:31,639 | INFO | decoder input length: 226
2026-01-28 19:31:31,639 | INFO | max output length: 226
2026-01-28 19:31:31,639 | INFO | min output length: 22
2026-01-28 19:31:39,306 | INFO | end detected at 144
2026-01-28 19:31:39,308 | INFO | -20.17 * 0.5 = -10.09 for decoder
2026-01-28 19:31:39,308 | INFO |  -7.53 * 0.5 =  -3.77 for ctc
2026-01-28 19:31:39,308 | INFO | total log probability: -13.85
2026-01-28 19:31:39,308 | INFO | normalized log probability: -0.10
2026-01-28 19:31:39,308 | INFO | total number of ended hypotheses: 198
2026-01-28 19:31:39,310 | INFO | best hypo: et<space>quand<space>vous<space>serez<space>donc<space>à<space>un<space>nef<space>chavant<space>alors<space>là<space>euh<space>i<space>vous<space>vous<space>passez<space>un<space>droit<space>du<space>cinéma<space>une<space>petite<space>rue<space>qui<space>part<space>à<space>droite<space>du<space>cinéma

2026-01-28 19:31:39,313 | INFO | speech length: 472800
2026-01-28 19:31:39,354 | INFO | decoder input length: 738
2026-01-28 19:31:39,355 | INFO | max output length: 738
2026-01-28 19:31:39,355 | INFO | min output length: 73
2026-01-28 19:32:30,555 | INFO | end detected at 503
2026-01-28 19:32:30,556 | INFO | -440.98 * 0.5 = -220.49 for decoder
2026-01-28 19:32:30,556 | INFO | -33.11 * 0.5 = -16.56 for ctc
2026-01-28 19:32:30,556 | INFO | total log probability: -237.05
2026-01-28 19:32:30,556 | INFO | normalized log probability: -0.48
2026-01-28 19:32:30,556 | INFO | total number of ended hypotheses: 151
2026-01-28 19:32:30,562 | INFO | best hypo: euh<space>qui<space>en<space>fait<space>qui<space>c'est<space>sur<space>la<space>ligne<space>du<space>train<space>hein<space>vous<space>suivez<space>la<space>ligne<space>du<space>trame<space>et<space>vous<space>remontez<space>euh<space>jusqu'à<space>la<space>une<space>grande<space>place<space>et<space>la<space>place<space>de<space>verdin<space>là<space>sur<space>la<space>place<space>de<space>verdailles<space>à<space>hum<space>la<space>préfecture<space>voilà<space>comme<space>repère<space>et<space>à<space>la<space>préfecture<space>donc<space>quand<space>vous<space>arrivez<space>sur<space>la<space>place<space>de<space>verda<space>la<space>préfecture<space>est<space>sur<space>votre<space>droite<space>pour<space>vous<space>continuez<space>tout<space>droit<space>en<space>fait<space>euh<space>bah<space>là<space>euh<space>vous<space>passez<space>euh<space>ven<space>en<space>fait<space>c'est<space>la<space>ligne<space>du<space>trame<space>toujours<space>là<space>vous<space>t<space>vous<space>suivez<space>la<space>ligne<space>du<space>trame<space>qui<space>passe<space>vers<space>la

2026-01-28 19:32:30,565 | INFO | speech length: 450400
2026-01-28 19:32:30,625 | INFO | decoder input length: 703
2026-01-28 19:32:30,626 | INFO | max output length: 703
2026-01-28 19:32:30,626 | INFO | min output length: 70
2026-01-28 19:33:17,232 | INFO | end detected at 462
2026-01-28 19:33:17,233 | INFO | -298.47 * 0.5 = -149.23 for decoder
2026-01-28 19:33:17,233 | INFO | -45.50 * 0.5 = -22.75 for ctc
2026-01-28 19:33:17,233 | INFO | total log probability: -171.99
2026-01-28 19:33:17,233 | INFO | normalized log probability: -0.38
2026-01-28 19:33:17,233 | INFO | total number of ended hypotheses: 159
2026-01-28 19:33:17,239 | INFO | best hypo: je<space>crois<space>que<space>c'est<space>une<space>ancienne<space>caserne<space>voilà<space>et<space>vous<space>suivez<space>toujours<space>la<space>ligne<space>du<space>drame<space>et<space>là<space>vous<space>tombez<space>donc<space>euh<space>à<space>la<space>maison<space>du<space>tourisme<space>donc<space>la<space>maison<space>du<space>tourisme<space>à<space>la<space>maison<space>du<space>tourisme<space>donc<space>vous<space>allez<space>continuer<space>s<space>à<space>droite<space>de<space>la<space>maison<space>du<space>tourisme<space>passez<space>de<space>euh<space>et<space>là<space>vous<space>passez<space>devant<space>euh<space>les<space>als<space>sainte<space>clerc<space>c'est<space>en<space>fait<space>un<space>marché<space>couvert<space>voilà<space>et<space>là<space>donc<space>donc<space>euh<space>vous<space>continuez<space>vous<space>un<space>ban<space>toujour<space>toujours<space>vous<space>suivez<space>la<space>ligne<space>du<space>drame<space>en<space>fait

2026-01-28 19:33:17,241 | INFO | speech length: 46080
2026-01-28 19:33:17,279 | INFO | decoder input length: 71
2026-01-28 19:33:17,279 | INFO | max output length: 71
2026-01-28 19:33:17,279 | INFO | min output length: 7
2026-01-28 19:33:19,322 | INFO | end detected at 55
2026-01-28 19:33:19,324 | INFO |  -7.34 * 0.5 =  -3.67 for decoder
2026-01-28 19:33:19,324 | INFO |  -5.48 * 0.5 =  -2.74 for ctc
2026-01-28 19:33:19,324 | INFO | total log probability: -6.41
2026-01-28 19:33:19,324 | INFO | normalized log probability: -0.13
2026-01-28 19:33:19,324 | INFO | total number of ended hypotheses: 194
2026-01-28 19:33:19,325 | INFO | best hypo: euh<space>et<space>puis<space>ben<space>vous<space>tombez<space>place<space>ma<space>place<space>nous

2026-01-28 19:33:19,327 | INFO | speech length: 115840
2026-01-28 19:33:19,363 | INFO | decoder input length: 180
2026-01-28 19:33:19,363 | INFO | max output length: 180
2026-01-28 19:33:19,363 | INFO | min output length: 18
2026-01-28 19:33:27,333 | INFO | end detected at 166
2026-01-28 19:33:27,335 | INFO | -15.67 * 0.5 =  -7.83 for decoder
2026-01-28 19:33:27,335 | INFO | -19.64 * 0.5 =  -9.82 for ctc
2026-01-28 19:33:27,335 | INFO | total log probability: -17.65
2026-01-28 19:33:27,335 | INFO | normalized log probability: -0.11
2026-01-28 19:33:27,335 | INFO | total number of ended hypotheses: 178
2026-01-28 19:33:27,337 | INFO | best hypo: eh<space>bien<space>la<space>k<space>la<space>la<space>cathédrale<space>elle<space>est<space>sur<space>la<space>place<space>notre<space>dame<space>vous<space>vous<space>êtes<space>obligée<space>de<space>passer<space>devant<space>en<space>fait<space>en<space>en<space>fait<space>tout<space>le<space>long<space>vous<space>suivez<space>la<space>ligne<space>du

2026-01-28 19:33:27,340 | INFO | speech length: 20160
2026-01-28 19:33:27,380 | INFO | decoder input length: 31
2026-01-28 19:33:27,380 | INFO | max output length: 31
2026-01-28 19:33:27,380 | INFO | min output length: 3
2026-01-28 19:33:28,393 | INFO | adding <eos> in the last position in the loop
2026-01-28 19:33:28,401 | INFO | no hypothesis. Finish decoding.
2026-01-28 19:33:28,402 | INFO |  -2.70 * 0.5 =  -1.35 for decoder
2026-01-28 19:33:28,402 | INFO |  -0.68 * 0.5 =  -0.34 for ctc
2026-01-28 19:33:28,402 | INFO | total log probability: -1.69
2026-01-28 19:33:28,402 | INFO | normalized log probability: -0.06
2026-01-28 19:33:28,402 | INFO | total number of ended hypotheses: 94
2026-01-28 19:33:28,403 | INFO | best hypo: c'est<space>pas<space>bien<space>compliqué<space>y<space>a

2026-01-28 19:33:28,404 | INFO | speech length: 8960
2026-01-28 19:33:28,432 | INFO | decoder input length: 13
2026-01-28 19:33:28,432 | INFO | max output length: 13
2026-01-28 19:33:28,432 | INFO | min output length: 1
2026-01-28 19:33:28,832 | INFO | adding <eos> in the last position in the loop
2026-01-28 19:33:28,839 | INFO | no hypothesis. Finish decoding.
2026-01-28 19:33:28,839 | INFO |  -5.93 * 0.5 =  -2.96 for decoder
2026-01-28 19:33:28,839 | INFO | -21.18 * 0.5 = -10.59 for ctc
2026-01-28 19:33:28,839 | INFO | total log probability: -13.55
2026-01-28 19:33:28,839 | INFO | normalized log probability: -0.90
2026-01-28 19:33:28,839 | INFO | total number of ended hypotheses: 40
2026-01-28 19:33:28,840 | INFO | best hypo: oui<space>pas<space>votre

2026-01-28 19:33:28,840 | WARNING | best hypo length: 13 == max output length: 13
2026-01-28 19:33:28,840 | WARNING | decoding may be stopped by the max output length limitation, please consider to increase the maxlenratio.
2026-01-28 19:33:28,845 | INFO | Chunk: 0 | WER=66.666667 | S=0 D=0 I=2
2026-01-28 19:33:28,846 | INFO | Chunk: 1 | WER=25.000000 | S=0 D=4 I=5
2026-01-28 19:33:28,847 | INFO | Chunk: 2 | WER=26.923077 | S=4 D=0 I=3
2026-01-28 19:33:28,853 | INFO | Chunk: 3 | WER=27.835052 | S=11 D=5 I=11
2026-01-28 19:33:28,858 | INFO | Chunk: 4 | WER=25.882353 | S=6 D=5 I=11
2026-01-28 19:33:28,858 | INFO | Chunk: 5 | WER=114.285714 | S=1 D=2 I=5
2026-01-28 19:33:28,859 | INFO | Chunk: 6 | WER=37.500000 | S=4 D=3 I=5
2026-01-28 19:33:28,859 | INFO | Chunk: 7 | WER=140.000000 | S=5 D=0 I=2
2026-01-28 19:33:28,859 | INFO | Chunk: 8 | WER=100.000000 | S=3 D=1 I=0
2026-01-28 19:33:28,909 | INFO | File: Rhap-M0003.wav | WER=25.423729 | S=27 D=12 I=36
2026-01-28 19:33:28,909 | INFO | ------------------------------
2026-01-28 19:33:28,909 | INFO | Conf ester Done!
2026-01-28 19:34:53,821 | INFO | Chunk: 0 | WER=66.666667 | S=0 D=0 I=2
2026-01-28 19:34:53,823 | INFO | Chunk: 1 | WER=30.555556 | S=2 D=5 I=4
2026-01-28 19:34:53,824 | INFO | Chunk: 2 | WER=11.538462 | S=1 D=0 I=2
2026-01-28 19:34:53,832 | INFO | Chunk: 3 | WER=23.711340 | S=12 D=5 I=6
2026-01-28 19:34:53,838 | INFO | Chunk: 4 | WER=24.705882 | S=8 D=5 I=8
2026-01-28 19:34:53,839 | INFO | Chunk: 5 | WER=128.571429 | S=7 D=0 I=2
2026-01-28 19:34:53,840 | INFO | Chunk: 6 | WER=59.375000 | S=13 D=3 I=3
2026-01-28 19:34:53,840 | INFO | Chunk: 7 | WER=100.000000 | S=5 D=0 I=0
2026-01-28 19:34:53,840 | INFO | Chunk: 8 | WER=100.000000 | S=2 D=2 I=0
2026-01-28 19:34:53,905 | INFO | File: Rhap-M0003.wav | WER=25.423729 | S=46 D=11 I=18
2026-01-28 19:34:53,905 | INFO | ------------------------------
2026-01-28 19:34:53,905 | INFO | hmm_tdnn Done!
2026-01-28 19:34:54,081 | INFO | ==================================Rhap-M0004.wav=========================================
2026-01-28 19:34:54,253 | INFO | Using rVAD model
2026-01-28 19:34:55,645 | INFO | Chunk: 0 | WER=40.000000 | S=3 D=1 I=0
2026-01-28 19:34:55,646 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 19:34:55,647 | INFO | Chunk: 2 | WER=50.000000 | S=2 D=2 I=0
2026-01-28 19:34:55,647 | INFO | Chunk: 3 | WER=25.000000 | S=1 D=1 I=0
2026-01-28 19:34:55,652 | INFO | File: Rhap-M0004.wav | WER=22.222222 | S=6 D=4 I=0
2026-01-28 19:34:55,652 | INFO | ------------------------------
2026-01-28 19:34:55,652 | INFO | w2vec vad chunk Done!
2026-01-28 19:34:58,709 | INFO | Chunk: 0 | WER=60.000000 | S=4 D=2 I=0
2026-01-28 19:34:58,710 | INFO | Chunk: 1 | WER=10.526316 | S=1 D=1 I=0
2026-01-28 19:34:58,710 | INFO | Chunk: 2 | WER=12.500000 | S=1 D=0 I=0
2026-01-28 19:34:58,711 | INFO | Chunk: 3 | WER=25.000000 | S=2 D=0 I=0
2026-01-28 19:34:58,714 | INFO | File: Rhap-M0004.wav | WER=24.444444 | S=8 D=3 I=0
2026-01-28 19:34:58,714 | INFO | ------------------------------
2026-01-28 19:34:58,714 | INFO | whisper med Done!
2026-01-28 19:35:03,030 | INFO | Chunk: 0 | WER=40.000000 | S=2 D=2 I=0
2026-01-28 19:35:03,031 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 19:35:03,031 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 19:35:03,032 | INFO | Chunk: 3 | WER=50.000000 | S=4 D=0 I=0
2026-01-28 19:35:03,036 | INFO | File: Rhap-M0004.wav | WER=17.777778 | S=6 D=2 I=0
2026-01-28 19:35:03,036 | INFO | ------------------------------
2026-01-28 19:35:03,036 | INFO | whisper large Done!
2026-01-28 19:35:03,194 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 19:35:03,244 | INFO | Vocabulary size: 350
2026-01-28 19:35:04,859 | INFO | Gradient checkpoint layers: []
2026-01-28 19:35:05,624 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:35:05,628 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:35:05,629 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:35:05,629 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 19:35:05,630 | INFO | speech length: 43200
2026-01-28 19:35:05,669 | INFO | decoder input length: 67
2026-01-28 19:35:05,669 | INFO | max output length: 67
2026-01-28 19:35:05,669 | INFO | min output length: 6
2026-01-28 19:35:07,118 | INFO | end detected at 34
2026-01-28 19:35:07,119 | INFO | -11.05 * 0.5 =  -5.53 for decoder
2026-01-28 19:35:07,119 | INFO |  -6.53 * 0.5 =  -3.26 for ctc
2026-01-28 19:35:07,119 | INFO | total log probability: -8.79
2026-01-28 19:35:07,119 | INFO | normalized log probability: -0.30
2026-01-28 19:35:07,119 | INFO | total number of ended hypotheses: 175
2026-01-28 19:35:07,120 | INFO | best hypo: ▁une▁montée▁d'escaliers▁fut▁descente▁jusqu'au▁cinémaster

2026-01-28 19:35:07,123 | INFO | speech length: 73440
2026-01-28 19:35:07,167 | INFO | decoder input length: 114
2026-01-28 19:35:07,167 | INFO | max output length: 114
2026-01-28 19:35:07,167 | INFO | min output length: 11
2026-01-28 19:35:09,998 | INFO | end detected at 55
2026-01-28 19:35:10,000 | INFO |  -7.26 * 0.5 =  -3.63 for decoder
2026-01-28 19:35:10,000 | INFO |  -6.63 * 0.5 =  -3.32 for ctc
2026-01-28 19:35:10,000 | INFO | total log probability: -6.95
2026-01-28 19:35:10,000 | INFO | normalized log probability: -0.14
2026-01-28 19:35:10,000 | INFO | total number of ended hypotheses: 186
2026-01-28 19:35:10,001 | INFO | best hypo: ▁arrivés▁au▁cinémastar▁ils▁font▁continuer▁tout▁droit▁jusqu'en▁bas▁de▁la▁rue▁jusqu'au▁rond▁point

2026-01-28 19:35:10,003 | INFO | speech length: 26080
2026-01-28 19:35:10,057 | INFO | decoder input length: 40
2026-01-28 19:35:10,057 | INFO | max output length: 40
2026-01-28 19:35:10,057 | INFO | min output length: 4
2026-01-28 19:35:11,527 | INFO | end detected at 21
2026-01-28 19:35:11,530 | INFO |  -3.54 * 0.5 =  -1.77 for decoder
2026-01-28 19:35:11,530 | INFO |  -7.66 * 0.5 =  -3.83 for ctc
2026-01-28 19:35:11,530 | INFO | total log probability: -5.60
2026-01-28 19:35:11,530 | INFO | normalized log probability: -0.35
2026-01-28 19:35:11,530 | INFO | total number of ended hypotheses: 161
2026-01-28 19:35:11,531 | INFO | best hypo: ▁pouvons▁point▁être▁tourné▁à▁gauche

2026-01-28 19:35:11,534 | INFO | speech length: 26880
2026-01-28 19:35:11,584 | INFO | decoder input length: 41
2026-01-28 19:35:11,584 | INFO | max output length: 41
2026-01-28 19:35:11,584 | INFO | min output length: 4
2026-01-28 19:35:12,605 | INFO | end detected at 22
2026-01-28 19:35:12,608 | INFO |  -1.55 * 0.5 =  -0.78 for decoder
2026-01-28 19:35:12,608 | INFO |  -1.79 * 0.5 =  -0.90 for ctc
2026-01-28 19:35:12,608 | INFO | total log probability: -1.67
2026-01-28 19:35:12,608 | INFO | normalized log probability: -0.10
2026-01-28 19:35:12,608 | INFO | total number of ended hypotheses: 164
2026-01-28 19:35:12,609 | INFO | best hypo: ▁et▁tout▁droit▁la▁guerre▁est▁en▁face

2026-01-28 19:35:12,615 | INFO | Chunk: 0 | WER=70.000000 | S=6 D=1 I=0
2026-01-28 19:35:12,616 | INFO | Chunk: 1 | WER=26.315789 | S=4 D=1 I=0
2026-01-28 19:35:12,616 | INFO | Chunk: 2 | WER=62.500000 | S=3 D=2 I=0
2026-01-28 19:35:12,617 | INFO | Chunk: 3 | WER=12.500000 | S=1 D=0 I=0
2026-01-28 19:35:12,621 | INFO | File: Rhap-M0004.wav | WER=40.000000 | S=14 D=4 I=0
2026-01-28 19:35:12,621 | INFO | ------------------------------
2026-01-28 19:35:12,621 | INFO | Conf cv Done!
2026-01-28 19:35:12,829 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 19:35:12,858 | INFO | Vocabulary size: 47
2026-01-28 19:35:14,124 | INFO | Gradient checkpoint layers: []
2026-01-28 19:35:14,787 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:35:14,792 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:35:14,792 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:35:14,792 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 19:35:14,795 | INFO | speech length: 43200
2026-01-28 19:35:14,834 | INFO | decoder input length: 67
2026-01-28 19:35:14,834 | INFO | max output length: 67
2026-01-28 19:35:14,834 | INFO | min output length: 6
2026-01-28 19:35:17,208 | INFO | end detected at 64
2026-01-28 19:35:17,210 | INFO | -10.59 * 0.5 =  -5.30 for decoder
2026-01-28 19:35:17,210 | INFO | -13.22 * 0.5 =  -6.61 for ctc
2026-01-28 19:35:17,211 | INFO | total log probability: -11.91
2026-01-28 19:35:17,211 | INFO | normalized log probability: -0.21
2026-01-28 19:35:17,211 | INFO | total number of ended hypotheses: 229
2026-01-28 19:35:17,211 | INFO | best hypo: monter<space>les<space>escaliers<space>qui<space>descende<space>jusqu'au<space>cinéma<space>star

2026-01-28 19:35:17,214 | INFO | speech length: 73440
2026-01-28 19:35:17,250 | INFO | decoder input length: 114
2026-01-28 19:35:17,251 | INFO | max output length: 114
2026-01-28 19:35:17,251 | INFO | min output length: 11
2026-01-28 19:35:21,315 | INFO | end detected at 99
2026-01-28 19:35:21,317 | INFO | -14.58 * 0.5 =  -7.29 for decoder
2026-01-28 19:35:21,317 | INFO | -15.22 * 0.5 =  -7.61 for ctc
2026-01-28 19:35:21,317 | INFO | total log probability: -14.90
2026-01-28 19:35:21,317 | INFO | normalized log probability: -0.16
2026-01-28 19:35:21,317 | INFO | total number of ended hypotheses: 184
2026-01-28 19:35:21,318 | INFO | best hypo: arrivé<space>au<space>cinéma<space>star<space>et<space>pour<space>continuer<space>tout<space>droit<space>jusqu'au<space>bat<space>de<space>la<space>rue<space>jusqu'au<space>roncoin

2026-01-28 19:35:21,321 | INFO | speech length: 26080
2026-01-28 19:35:21,358 | INFO | decoder input length: 40
2026-01-28 19:35:21,359 | INFO | max output length: 40
2026-01-28 19:35:21,359 | INFO | min output length: 4
2026-01-28 19:35:22,705 | INFO | adding <eos> in the last position in the loop
2026-01-28 19:35:22,714 | INFO | no hypothesis. Finish decoding.
2026-01-28 19:35:22,715 | INFO |  -8.73 * 0.5 =  -4.36 for decoder
2026-01-28 19:35:22,715 | INFO | -12.16 * 0.5 =  -6.08 for ctc
2026-01-28 19:35:22,715 | INFO | total log probability: -10.45
2026-01-28 19:35:22,715 | INFO | normalized log probability: -0.27
2026-01-28 19:35:22,716 | INFO | total number of ended hypotheses: 130
2026-01-28 19:35:22,716 | INFO | best hypo: pour<space>requoi<space>ils<space>sont<space>tournés<space>à<space>gauche

2026-01-28 19:35:22,718 | INFO | speech length: 26880
2026-01-28 19:35:22,759 | INFO | decoder input length: 41
2026-01-28 19:35:22,759 | INFO | max output length: 41
2026-01-28 19:35:22,759 | INFO | min output length: 4
2026-01-28 19:35:24,145 | INFO | adding <eos> in the last position in the loop
2026-01-28 19:35:24,155 | INFO | no hypothesis. Finish decoding.
2026-01-28 19:35:24,156 | INFO |  -5.74 * 0.5 =  -2.87 for decoder
2026-01-28 19:35:24,156 | INFO |  -4.15 * 0.5 =  -2.07 for ctc
2026-01-28 19:35:24,156 | INFO | total log probability: -4.94
2026-01-28 19:35:24,156 | INFO | normalized log probability: -0.14
2026-01-28 19:35:24,156 | INFO | total number of ended hypotheses: 163
2026-01-28 19:35:24,156 | INFO | best hypo: et<space>pour<space>quoi<space>la<space>guerre<space>est<space>en<space>face

2026-01-28 19:35:24,162 | INFO | Chunk: 0 | WER=40.000000 | S=3 D=1 I=0
2026-01-28 19:35:24,163 | INFO | Chunk: 1 | WER=31.578947 | S=5 D=1 I=0
2026-01-28 19:35:24,163 | INFO | Chunk: 2 | WER=75.000000 | S=5 D=1 I=0
2026-01-28 19:35:24,163 | INFO | Chunk: 3 | WER=37.500000 | S=3 D=0 I=0
2026-01-28 19:35:24,167 | INFO | File: Rhap-M0004.wav | WER=42.222222 | S=16 D=3 I=0
2026-01-28 19:35:24,167 | INFO | ------------------------------
2026-01-28 19:35:24,167 | INFO | Conf ester Done!
2026-01-28 19:35:50,892 | INFO | Chunk: 0 | WER=40.000000 | S=3 D=1 I=0
2026-01-28 19:35:50,894 | INFO | Chunk: 1 | WER=26.315789 | S=3 D=2 I=0
2026-01-28 19:35:50,894 | INFO | Chunk: 2 | WER=62.500000 | S=2 D=3 I=0
2026-01-28 19:35:50,895 | INFO | Chunk: 3 | WER=50.000000 | S=2 D=2 I=0
2026-01-28 19:35:50,901 | INFO | File: Rhap-M0004.wav | WER=40.000000 | S=10 D=8 I=0
2026-01-28 19:35:50,901 | INFO | ------------------------------
2026-01-28 19:35:50,902 | INFO | hmm_tdnn Done!
2026-01-28 19:35:51,114 | INFO | ==================================Rhap-M0005.wav=========================================
2026-01-28 19:35:51,309 | INFO | Using rVAD model
2026-01-28 19:35:54,888 | INFO | Chunk: 0 | WER=73.684211 | S=2 D=2 I=10
2026-01-28 19:35:54,889 | INFO | Chunk: 1 | WER=94.444444 | S=14 D=2 I=1
2026-01-28 19:35:54,889 | INFO | Chunk: 2 | WER=100.000000 | S=7 D=3 I=0
2026-01-28 19:35:54,890 | INFO | Chunk: 3 | WER=107.142857 | S=14 D=0 I=1
2026-01-28 19:35:54,892 | INFO | Chunk: 4 | WER=50.000000 | S=2 D=16 I=12
2026-01-28 19:35:54,903 | INFO | File: Rhap-M0005.wav | WER=34.710744 | S=17 D=12 I=13
2026-01-28 19:35:54,903 | INFO | ------------------------------
2026-01-28 19:35:54,903 | INFO | w2vec vad chunk Done!
2026-01-28 19:36:01,260 | INFO | Chunk: 0 | WER=21.052632 | S=1 D=0 I=3
2026-01-28 19:36:01,262 | INFO | Chunk: 1 | WER=94.444444 | S=14 D=2 I=1
2026-01-28 19:36:01,263 | INFO | Chunk: 2 | WER=80.000000 | S=2 D=5 I=1
2026-01-28 19:36:01,264 | INFO | Chunk: 3 | WER=107.142857 | S=14 D=0 I=1
2026-01-28 19:36:01,269 | INFO | Chunk: 4 | WER=48.333333 | S=3 D=26 I=0
2026-01-28 19:36:01,303 | INFO | File: Rhap-M0005.wav | WER=32.231405 | S=10 D=28 I=1
2026-01-28 19:36:01,303 | INFO | ------------------------------
2026-01-28 19:36:01,303 | INFO | whisper med Done!
2026-01-28 19:36:10,554 | INFO | Chunk: 0 | WER=57.894737 | S=1 D=0 I=10
2026-01-28 19:36:10,555 | INFO | Chunk: 1 | WER=94.444444 | S=16 D=0 I=1
2026-01-28 19:36:10,555 | INFO | Chunk: 2 | WER=100.000000 | S=9 D=1 I=0
2026-01-28 19:36:10,556 | INFO | Chunk: 3 | WER=107.142857 | S=14 D=0 I=1
2026-01-28 19:36:10,558 | INFO | Chunk: 4 | WER=53.333333 | S=2 D=30 I=0
2026-01-28 19:36:10,570 | INFO | File: Rhap-M0005.wav | WER=28.099174 | S=13 D=20 I=1
2026-01-28 19:36:10,570 | INFO | ------------------------------
2026-01-28 19:36:10,570 | INFO | whisper large Done!
2026-01-28 19:36:10,727 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 19:36:10,777 | INFO | Vocabulary size: 350
2026-01-28 19:36:11,850 | INFO | Gradient checkpoint layers: []
2026-01-28 19:36:12,582 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:36:12,587 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:36:12,587 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:36:12,587 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 19:36:12,588 | INFO | speech length: 156480
2026-01-28 19:36:12,628 | INFO | decoder input length: 244
2026-01-28 19:36:12,628 | INFO | max output length: 244
2026-01-28 19:36:12,628 | INFO | min output length: 24
2026-01-28 19:36:16,659 | INFO | end detected at 62
2026-01-28 19:36:16,661 | INFO | -15.88 * 0.5 =  -7.94 for decoder
2026-01-28 19:36:16,661 | INFO | -14.18 * 0.5 =  -7.09 for ctc
2026-01-28 19:36:16,661 | INFO | total log probability: -15.03
2026-01-28 19:36:16,661 | INFO | normalized log probability: -0.27
2026-01-28 19:36:16,661 | INFO | total number of ended hypotheses: 165
2026-01-28 19:36:16,662 | INFO | best hypo: ▁donc▁alors▁pour▁aller▁à▁la▁nef▁chavon▁donc▁il▁faut▁prendre▁l'avenueller▁tout▁au▁bout▁de▁l'avenue▁ensuite▁au▁feu▁vous▁tournez▁à▁droite

2026-01-28 19:36:16,665 | INFO | speech length: 114880
2026-01-28 19:36:16,708 | INFO | decoder input length: 179
2026-01-28 19:36:16,708 | INFO | max output length: 179
2026-01-28 19:36:16,708 | INFO | min output length: 17
2026-01-28 19:36:19,805 | INFO | end detected at 51
2026-01-28 19:36:19,807 | INFO |  -3.88 * 0.5 =  -1.94 for decoder
2026-01-28 19:36:19,807 | INFO |  -3.05 * 0.5 =  -1.53 for ctc
2026-01-28 19:36:19,807 | INFO | total log probability: -3.47
2026-01-28 19:36:19,807 | INFO | normalized log probability: -0.08
2026-01-28 19:36:19,807 | INFO | total number of ended hypotheses: 168
2026-01-28 19:36:19,808 | INFO | best hypo: ▁il▁semble▁que▁vous▁continuez▁à▁longer▁l'avenue▁jusqu'à▁l'arrêt▁albert▁premier▁de▁belgique

2026-01-28 19:36:19,810 | INFO | speech length: 45280
2026-01-28 19:36:19,850 | INFO | decoder input length: 70
2026-01-28 19:36:19,850 | INFO | max output length: 70
2026-01-28 19:36:19,850 | INFO | min output length: 7
2026-01-28 19:36:20,952 | INFO | end detected at 25
2026-01-28 19:36:20,954 | INFO | -10.22 * 0.5 =  -5.11 for decoder
2026-01-28 19:36:20,954 | INFO |  -6.50 * 0.5 =  -3.25 for ctc
2026-01-28 19:36:20,954 | INFO | total log probability: -8.36
2026-01-28 19:36:20,954 | INFO | normalized log probability: -0.46
2026-01-28 19:36:20,954 | INFO | total number of ended hypotheses: 175
2026-01-28 19:36:20,955 | INFO | best hypo: ▁voilà▁donc▁déclaré▁tra▁madère▁premier▁aber▁premier

2026-01-28 19:36:20,956 | INFO | speech length: 47200
2026-01-28 19:36:20,991 | INFO | decoder input length: 73
2026-01-28 19:36:20,991 | INFO | max output length: 73
2026-01-28 19:36:20,991 | INFO | min output length: 7
2026-01-28 19:36:22,885 | INFO | end detected at 38
2026-01-28 19:36:22,886 | INFO |  -3.31 * 0.5 =  -1.66 for decoder
2026-01-28 19:36:22,886 | INFO |  -0.50 * 0.5 =  -0.25 for ctc
2026-01-28 19:36:22,886 | INFO | total log probability: -1.91
2026-01-28 19:36:22,886 | INFO | normalized log probability: -0.06
2026-01-28 19:36:22,886 | INFO | total number of ended hypotheses: 143
2026-01-28 19:36:22,887 | INFO | best hypo: ▁et▁une▁fois▁que▁vous▁êtes▁arrivé▁à▁l'arrêt▁vous▁allez▁sur▁votre▁gauche

2026-01-28 19:36:22,889 | INFO | speech length: 254240
2026-01-28 19:36:22,933 | INFO | decoder input length: 396
2026-01-28 19:36:22,933 | INFO | max output length: 396
2026-01-28 19:36:22,933 | INFO | min output length: 39
2026-01-28 19:36:34,488 | INFO | end detected at 141
2026-01-28 19:36:34,489 | INFO | -157.81 * 0.5 = -78.91 for decoder
2026-01-28 19:36:34,490 | INFO | -40.71 * 0.5 = -20.35 for ctc
2026-01-28 19:36:34,490 | INFO | total log probability: -99.26
2026-01-28 19:36:34,490 | INFO | normalized log probability: -0.72
2026-01-28 19:36:34,490 | INFO | total number of ended hypotheses: 162
2026-01-28 19:36:34,492 | INFO | best hypo: ▁vous▁face▁à▁l'arrêt▁et▁vous▁allez▁toujours▁tout▁droit▁et▁normalement▁vous▁continuez▁toujours▁toujours▁tout▁droit▁et▁vous▁allez▁tomber▁sur▁la▁neige▁avance▁à▁un▁grand▁complexe▁ou▁vous▁pouvez▁pas▁louper▁voilà▁et▁en▁fait▁fautitoujoursivre▁la▁ligne▁de▁tram▁comme▁ça▁vous▁pouvez▁pas▁vous▁perdre▁doncque▁voilà

2026-01-28 19:36:34,499 | INFO | Chunk: 0 | WER=73.684211 | S=2 D=2 I=10
2026-01-28 19:36:34,499 | INFO | Chunk: 1 | WER=94.444444 | S=14 D=2 I=1
2026-01-28 19:36:34,499 | INFO | Chunk: 2 | WER=100.000000 | S=8 D=2 I=0
2026-01-28 19:36:34,500 | INFO | Chunk: 3 | WER=107.142857 | S=14 D=0 I=1
2026-01-28 19:36:34,502 | INFO | Chunk: 4 | WER=58.333333 | S=7 D=17 I=11
2026-01-28 19:36:34,512 | INFO | File: Rhap-M0005.wav | WER=33.884298 | S=17 D=12 I=12
2026-01-28 19:36:34,512 | INFO | ------------------------------
2026-01-28 19:36:34,513 | INFO | Conf cv Done!
2026-01-28 19:36:34,696 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 19:36:34,719 | INFO | Vocabulary size: 47
2026-01-28 19:36:35,649 | INFO | Gradient checkpoint layers: []
2026-01-28 19:36:36,335 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:36:36,339 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:36:36,340 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:36:36,340 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 19:36:36,343 | INFO | speech length: 156480
2026-01-28 19:36:36,403 | INFO | decoder input length: 244
2026-01-28 19:36:36,403 | INFO | max output length: 244
2026-01-28 19:36:36,403 | INFO | min output length: 24
2026-01-28 19:36:45,810 | INFO | end detected at 150
2026-01-28 19:36:45,814 | INFO | -12.38 * 0.5 =  -6.19 for decoder
2026-01-28 19:36:45,814 | INFO |  -6.09 * 0.5 =  -3.04 for ctc
2026-01-28 19:36:45,814 | INFO | total log probability: -9.23
2026-01-28 19:36:45,815 | INFO | normalized log probability: -0.07
2026-01-28 19:36:45,815 | INFO | total number of ended hypotheses: 229
2026-01-28 19:36:45,817 | INFO | best hypo: donc<space>alors<space>pour<space>aller<space>à<space>la<space>nef<space>chavant<space>donc<space>il<space>faut<space>prendre<space>l'avenue<space>là<space>aller<space>tout<space>au<space>bout<space>de<space>l'avenue<space>ensuite<space>au<space>feu<space>vous<space>tournez<space>à<space>droite

2026-01-28 19:36:45,821 | INFO | speech length: 114880
2026-01-28 19:36:45,862 | INFO | decoder input length: 179
2026-01-28 19:36:45,862 | INFO | max output length: 179
2026-01-28 19:36:45,862 | INFO | min output length: 17
2026-01-28 19:36:52,129 | INFO | end detected at 108
2026-01-28 19:36:52,131 | INFO | -18.97 * 0.5 =  -9.48 for decoder
2026-01-28 19:36:52,131 | INFO | -10.86 * 0.5 =  -5.43 for ctc
2026-01-28 19:36:52,131 | INFO | total log probability: -14.92
2026-01-28 19:36:52,131 | INFO | normalized log probability: -0.15
2026-01-28 19:36:52,131 | INFO | total number of ended hypotheses: 199
2026-01-28 19:36:52,133 | INFO | best hypo: et<space>il<space>me<space>semble<space>que<space>vous<space>continuez<space>à<space>longer<space>euh<space>l'avenue<space>jusqu'à<space>l'arrêt<space>euh<space>albert<space>premier<space>belgi

2026-01-28 19:36:52,136 | INFO | speech length: 45280
2026-01-28 19:36:52,181 | INFO | decoder input length: 70
2026-01-28 19:36:52,181 | INFO | max output length: 70
2026-01-28 19:36:52,181 | INFO | min output length: 7
2026-01-28 19:36:55,023 | INFO | end detected at 66
2026-01-28 19:36:55,025 | INFO | -14.73 * 0.5 =  -7.36 for decoder
2026-01-28 19:36:55,025 | INFO | -18.66 * 0.5 =  -9.33 for ctc
2026-01-28 19:36:55,025 | INFO | total log probability: -16.69
2026-01-28 19:36:55,025 | INFO | normalized log probability: -0.32
2026-01-28 19:36:55,025 | INFO | total number of ended hypotheses: 199
2026-01-28 19:36:55,026 | INFO | best hypo: voilà<space>donc<space>j'arrêtera<space>ma<space>der<space>première<space>d<space>un<space>premier

2026-01-28 19:36:55,029 | INFO | speech length: 47200
2026-01-28 19:36:55,084 | INFO | decoder input length: 73
2026-01-28 19:36:55,084 | INFO | max output length: 73
2026-01-28 19:36:55,084 | INFO | min output length: 7
2026-01-28 19:36:58,190 | INFO | adding <eos> in the last position in the loop
2026-01-28 19:36:58,200 | INFO | no hypothesis. Finish decoding.
2026-01-28 19:36:58,201 | INFO | -13.40 * 0.5 =  -6.70 for decoder
2026-01-28 19:36:58,201 | INFO |  -9.34 * 0.5 =  -4.67 for ctc
2026-01-28 19:36:58,201 | INFO | total log probability: -11.37
2026-01-28 19:36:58,201 | INFO | normalized log probability: -0.16
2026-01-28 19:36:58,201 | INFO | total number of ended hypotheses: 123
2026-01-28 19:36:58,202 | INFO | best hypo: et<space>une<space>fois<space>que<space>vous<space>êtes<space>arrivé<space>à<space>la<space>rêve<space>vous<space>alez<space>sur<space>votre<space>gauche

2026-01-28 19:36:58,203 | INFO | speech length: 254240
2026-01-28 19:36:58,258 | INFO | decoder input length: 396
2026-01-28 19:36:58,258 | INFO | max output length: 396
2026-01-28 19:36:58,258 | INFO | min output length: 39
2026-01-28 19:37:25,976 | INFO | end detected at 321
2026-01-28 19:37:25,979 | INFO | -26.12 * 0.5 = -13.06 for decoder
2026-01-28 19:37:25,979 | INFO |  -5.23 * 0.5 =  -2.62 for ctc
2026-01-28 19:37:25,979 | INFO | total log probability: -15.68
2026-01-28 19:37:25,979 | INFO | normalized log probability: -0.05
2026-01-28 19:37:25,979 | INFO | total number of ended hypotheses: 215
2026-01-28 19:37:25,983 | INFO | best hypo: donc<space>face<space>à<space>l'arrêt<space>et<space>vous<space>allez<space>toujours<space>tout<space>droit<space>et<space>normalement<space>vous<space>continuez<space>toujours<space>toujours<space>tout<space>droit<space>et<space>vous<space>allez<space>tomber<space>sur<space>la<space>neige<space>chavance<space>un<space>grand<space>complexe<space>vous<space>pouvez<space>pas<space>louper<space>voilà<space>et<space>en<space>fait<space>faut<space>toujours<space>suivre<space>euh<space>la<space>la<space>ligne<space>de<space>drame<space>comme<space>ça<space>vous<space>pouvez<space>pas<space>vous<space>perdre<space>donc<space>euh<space>voilà

2026-01-28 19:37:25,990 | INFO | Chunk: 0 | WER=52.631579 | S=0 D=0 I=10
2026-01-28 19:37:25,991 | INFO | Chunk: 1 | WER=100.000000 | S=16 D=0 I=2
2026-01-28 19:37:25,991 | INFO | Chunk: 2 | WER=100.000000 | S=10 D=0 I=0
2026-01-28 19:37:25,991 | INFO | Chunk: 3 | WER=107.142857 | S=14 D=0 I=1
2026-01-28 19:37:25,994 | INFO | Chunk: 4 | WER=58.333333 | S=2 D=18 I=15
2026-01-28 19:37:26,005 | INFO | File: Rhap-M0005.wav | WER=34.710744 | S=16 D=8 I=18
2026-01-28 19:37:26,005 | INFO | ------------------------------
2026-01-28 19:37:26,005 | INFO | Conf ester Done!
2026-01-28 19:38:07,706 | INFO | Chunk: 0 | WER=89.473684 | S=4 D=2 I=11
2026-01-28 19:38:07,707 | INFO | Chunk: 1 | WER=88.888889 | S=16 D=0 I=0
2026-01-28 19:38:07,707 | INFO | Chunk: 2 | WER=100.000000 | S=6 D=2 I=2
2026-01-28 19:38:07,708 | INFO | Chunk: 3 | WER=107.142857 | S=14 D=0 I=1
2026-01-28 19:38:07,711 | INFO | Chunk: 4 | WER=55.000000 | S=3 D=18 I=12
2026-01-28 19:38:07,726 | INFO | File: Rhap-M0005.wav | WER=31.404959 | S=14 D=10 I=14
2026-01-28 19:38:07,726 | INFO | ------------------------------
2026-01-28 19:38:07,726 | INFO | hmm_tdnn Done!
2026-01-28 19:38:07,995 | INFO | ==================================Rhap-M0006.wav=========================================
2026-01-28 19:38:08,235 | INFO | Using rVAD model
2026-01-28 19:38:10,611 | INFO | Chunk: 0 | WER=20.454545 | S=2 D=4 I=3
2026-01-28 19:38:10,613 | INFO | Chunk: 1 | WER=55.319149 | S=5 D=13 I=8
2026-01-28 19:38:10,622 | INFO | File: Rhap-M0006.wav | WER=35.164835 | S=10 D=14 I=8
2026-01-28 19:38:10,622 | INFO | ------------------------------
2026-01-28 19:38:10,622 | INFO | w2vec vad chunk Done!
2026-01-28 19:38:15,800 | INFO | Chunk: 0 | WER=34.090909 | S=5 D=6 I=4
2026-01-28 19:38:15,802 | INFO | Chunk: 1 | WER=53.191489 | S=12 D=11 I=2
2026-01-28 19:38:15,809 | INFO | File: Rhap-M0006.wav | WER=39.560440 | S=21 D=13 I=2
2026-01-28 19:38:15,809 | INFO | ------------------------------
2026-01-28 19:38:15,809 | INFO | whisper med Done!
2026-01-28 19:38:22,621 | INFO | Chunk: 0 | WER=29.545455 | S=5 D=3 I=5
2026-01-28 19:38:22,622 | INFO | Chunk: 1 | WER=38.297872 | S=3 D=15 I=0
2026-01-28 19:38:22,629 | INFO | File: Rhap-M0006.wav | WER=28.571429 | S=11 D=14 I=1
2026-01-28 19:38:22,629 | INFO | ------------------------------
2026-01-28 19:38:22,629 | INFO | whisper large Done!
2026-01-28 19:38:22,859 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 19:38:22,897 | INFO | Vocabulary size: 350
2026-01-28 19:38:23,890 | INFO | Gradient checkpoint layers: []
2026-01-28 19:38:24,663 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:38:24,669 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:38:24,669 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:38:24,670 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 19:38:24,670 | INFO | speech length: 212960
2026-01-28 19:38:24,726 | INFO | decoder input length: 332
2026-01-28 19:38:24,726 | INFO | max output length: 332
2026-01-28 19:38:24,726 | INFO | min output length: 33
2026-01-28 19:38:34,596 | INFO | end detected at 113
2026-01-28 19:38:34,599 | INFO | -103.99 * 0.5 = -52.00 for decoder
2026-01-28 19:38:34,599 | INFO | -66.27 * 0.5 = -33.13 for ctc
2026-01-28 19:38:34,599 | INFO | total log probability: -85.13
2026-01-28 19:38:34,599 | INFO | normalized log probability: -0.80
2026-01-28 19:38:34,599 | INFO | total number of ended hypotheses: 169
2026-01-28 19:38:34,601 | INFO | best hypo: ▁le▁web▁a▁pour▁aller▁à▁la▁neufch▁avance▁très▁simple▁tu▁prends▁tout▁droit▁là▁tu▁vois▁et▁quand▁arrives▁au▁bout▁de▁la▁ligne▁droite▁à▁l'intersection▁carrefour▁tut▁tournas▁droite▁et▁il▁continue▁tout▁droit▁jusqu'à▁vert▁de▁belge

2026-01-28 19:38:34,607 | INFO | speech length: 164160
2026-01-28 19:38:34,664 | INFO | decoder input length: 256
2026-01-28 19:38:34,665 | INFO | max output length: 256
2026-01-28 19:38:34,665 | INFO | min output length: 25
2026-01-28 19:38:42,782 | INFO | end detected at 105
2026-01-28 19:38:42,785 | INFO | -42.27 * 0.5 = -21.13 for decoder
2026-01-28 19:38:42,785 | INFO | -18.34 * 0.5 =  -9.17 for ctc
2026-01-28 19:38:42,785 | INFO | total log probability: -30.30
2026-01-28 19:38:42,785 | INFO | normalized log probability: -0.31
2026-01-28 19:38:42,785 | INFO | total number of ended hypotheses: 188
2026-01-28 19:38:42,787 | INFO | best hypo: ▁vous▁droit▁tu▁verras▁à▁un▁arrêt▁de▁tram▁quand▁arrive▁à▁l'arrêt▁de▁tram▁là▁tu▁tournes▁à▁gauche▁tu▁suis▁la▁ligne▁du▁tram▁jusqu'à▁jusqu'à▁basqu'à▁la▁nef▁avant▁et▁tu▁verras▁ça▁un▁arrêt▁de▁train

2026-01-28 19:38:42,798 | INFO | Chunk: 0 | WER=45.454545 | S=8 D=6 I=6
2026-01-28 19:38:42,800 | INFO | Chunk: 1 | WER=44.680851 | S=4 D=10 I=7
2026-01-28 19:38:42,809 | INFO | File: Rhap-M0006.wav | WER=39.560440 | S=15 D=12 I=9
2026-01-28 19:38:42,809 | INFO | ------------------------------
2026-01-28 19:38:42,809 | INFO | Conf cv Done!
2026-01-28 19:38:43,069 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 19:38:43,098 | INFO | Vocabulary size: 47
2026-01-28 19:38:44,020 | INFO | Gradient checkpoint layers: []
2026-01-28 19:38:44,838 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:38:44,842 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:38:44,842 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:38:44,843 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 19:38:44,846 | INFO | speech length: 212960
2026-01-28 19:38:44,898 | INFO | decoder input length: 332
2026-01-28 19:38:44,898 | INFO | max output length: 332
2026-01-28 19:38:44,898 | INFO | min output length: 33
2026-01-28 19:39:00,862 | INFO | end detected at 263
2026-01-28 19:39:00,864 | INFO | -33.78 * 0.5 = -16.89 for decoder
2026-01-28 19:39:00,864 | INFO | -24.49 * 0.5 = -12.24 for ctc
2026-01-28 19:39:00,864 | INFO | total log probability: -29.14
2026-01-28 19:39:00,864 | INFO | normalized log probability: -0.11
2026-01-28 19:39:00,864 | INFO | total number of ended hypotheses: 205
2026-01-28 19:39:00,868 | INFO | best hypo: ouais<space>ben<space>pour<space>aller<space>à<space>la<space>neuf<space>chavants<space>c'est<space>très<space>simple<space>tu<space>prends<space>tout<space>droit<space>là<space>tu<space>vois<space>et<space>euh<space>quand<space>arrive<space>au<space>bout<space>de<space>la<space>la<space>ligne<space>droite<space>à<space>l'interception<space>au<space>carrefour<space>tu<space>tournes<space>à<space>droite<space>ensuite<space>tu<space>continues<space>tout<space>droit<space>jusqu'à<space>albert<space>prenez<space>le<space>belge

2026-01-28 19:39:00,871 | INFO | speech length: 164160
2026-01-28 19:39:00,908 | INFO | decoder input length: 256
2026-01-28 19:39:00,908 | INFO | max output length: 256
2026-01-28 19:39:00,908 | INFO | min output length: 25
2026-01-28 19:39:12,140 | INFO | end detected at 200
2026-01-28 19:39:12,142 | INFO | -24.23 * 0.5 = -12.11 for decoder
2026-01-28 19:39:12,142 | INFO | -11.70 * 0.5 =  -5.85 for ctc
2026-01-28 19:39:12,142 | INFO | total log probability: -17.96
2026-01-28 19:39:12,142 | INFO | normalized log probability: -0.09
2026-01-28 19:39:12,142 | INFO | total number of ended hypotheses: 201
2026-01-28 19:39:12,145 | INFO | best hypo: droite<space>tu<space>verras<space>y<space>a<space>un<space>arrêt<space>de<space>trame<space>quand<space>arrive<space>à<space>l'arrêt<space>de<space>trame<space>là<space>tu<space>tournes<space>à<space>gauche<space>tu<space>suis<space>la<space>ligne<space>du<space>trame<space>jusqu'à<space>jusqu'à<space>ben<space>jusqu'à<space>la<space>neuf<space>chavant<space>et<space>tu<space>verras<space>ça<space>un<space>arrêtera

2026-01-28 19:39:12,153 | INFO | Chunk: 0 | WER=27.272727 | S=4 D=1 I=7
2026-01-28 19:39:12,154 | INFO | Chunk: 1 | WER=46.808511 | S=6 D=10 I=6
2026-01-28 19:39:12,161 | INFO | File: Rhap-M0006.wav | WER=31.868132 | S=13 D=7 I=9
2026-01-28 19:39:12,161 | INFO | ------------------------------
2026-01-28 19:39:12,162 | INFO | Conf ester Done!
2026-01-28 19:39:37,546 | INFO | Chunk: 0 | WER=40.909091 | S=7 D=3 I=8
2026-01-28 19:39:37,548 | INFO | Chunk: 1 | WER=63.829787 | S=8 D=17 I=5
2026-01-28 19:39:37,557 | INFO | File: Rhap-M0006.wav | WER=46.153846 | S=19 D=15 I=8
2026-01-28 19:39:37,557 | INFO | ------------------------------
2026-01-28 19:39:37,557 | INFO | hmm_tdnn Done!
2026-01-28 19:39:37,763 | INFO | ==================================Rhap-M0007.wav=========================================
2026-01-28 19:39:37,944 | INFO | Using rVAD model
2026-01-28 19:39:42,630 | INFO | Chunk: 0 | WER=19.565217 | S=5 D=4 I=0
2026-01-28 19:39:42,631 | INFO | Chunk: 1 | WER=50.000000 | S=2 D=2 I=2
2026-01-28 19:39:42,632 | INFO | Chunk: 2 | WER=33.333333 | S=3 D=3 I=0
2026-01-28 19:39:42,632 | INFO | Chunk: 3 | WER=26.666667 | S=2 D=1 I=1
2026-01-28 19:39:42,633 | INFO | Chunk: 4 | WER=20.512821 | S=4 D=4 I=0
2026-01-28 19:39:42,634 | INFO | Chunk: 5 | WER=19.354839 | S=4 D=0 I=2
2026-01-28 19:39:42,649 | INFO | File: Rhap-M0007.wav | WER=24.223602 | S=20 D=14 I=5
2026-01-28 19:39:42,649 | INFO | ------------------------------
2026-01-28 19:39:42,649 | INFO | w2vec vad chunk Done!
2026-01-28 19:39:50,806 | INFO | Chunk: 0 | WER=26.086957 | S=3 D=9 I=0
2026-01-28 19:39:50,807 | INFO | Chunk: 1 | WER=8.333333 | S=0 D=1 I=0
2026-01-28 19:39:50,807 | INFO | Chunk: 2 | WER=22.222222 | S=1 D=3 I=0
2026-01-28 19:39:50,808 | INFO | Chunk: 3 | WER=26.666667 | S=3 D=0 I=1
2026-01-28 19:39:50,808 | INFO | Chunk: 4 | WER=69.230769 | S=2 D=25 I=0
2026-01-28 19:39:50,809 | INFO | Chunk: 5 | WER=6.451613 | S=1 D=0 I=1
2026-01-28 19:39:50,823 | INFO | File: Rhap-M0007.wav | WER=31.055901 | S=10 D=38 I=2
2026-01-28 19:39:50,823 | INFO | ------------------------------
2026-01-28 19:39:50,823 | INFO | whisper med Done!
2026-01-28 19:40:02,201 | INFO | Chunk: 0 | WER=19.565217 | S=2 D=7 I=0
2026-01-28 19:40:02,201 | INFO | Chunk: 1 | WER=8.333333 | S=0 D=1 I=0
2026-01-28 19:40:02,202 | INFO | Chunk: 2 | WER=22.222222 | S=2 D=2 I=0
2026-01-28 19:40:02,202 | INFO | Chunk: 3 | WER=26.666667 | S=3 D=0 I=1
2026-01-28 19:40:02,203 | INFO | Chunk: 4 | WER=71.794872 | S=2 D=26 I=0
2026-01-28 19:40:02,204 | INFO | Chunk: 5 | WER=6.451613 | S=1 D=0 I=1
2026-01-28 19:40:02,217 | INFO | File: Rhap-M0007.wav | WER=29.813665 | S=10 D=36 I=2
2026-01-28 19:40:02,217 | INFO | ------------------------------
2026-01-28 19:40:02,217 | INFO | whisper large Done!
2026-01-28 19:40:02,349 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 19:40:02,387 | INFO | Vocabulary size: 350
2026-01-28 19:40:03,412 | INFO | Gradient checkpoint layers: []
2026-01-28 19:40:04,255 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:40:04,261 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:40:04,261 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:40:04,262 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 19:40:04,262 | INFO | speech length: 217280
2026-01-28 19:40:04,321 | INFO | decoder input length: 339
2026-01-28 19:40:04,321 | INFO | max output length: 339
2026-01-28 19:40:04,322 | INFO | min output length: 33
2026-01-28 19:40:13,554 | INFO | end detected at 104
2026-01-28 19:40:13,556 | INFO | -57.86 * 0.5 = -28.93 for decoder
2026-01-28 19:40:13,556 | INFO | -25.08 * 0.5 = -12.54 for ctc
2026-01-28 19:40:13,556 | INFO | total log probability: -41.47
2026-01-28 19:40:13,556 | INFO | normalized log probability: -0.42
2026-01-28 19:40:13,556 | INFO | total number of ended hypotheses: 181
2026-01-28 19:40:13,558 | INFO | best hypo: ▁ben▁prenez▁la▁route▁là▁vous▁sortez▁la▁commune▁de▁pont▁à▁ma▁frais▁vous▁allez▁rêver▁à▁un▁moment▁par▁l'échangeur▁de▁l'autoroute▁vous▁continuez▁à▁vous▁prenez▁le▁rond▁point▁tout▁droit▁vous▁passez▁le▁pont▁au▁dessus▁de▁la▁voie▁fer

2026-01-28 19:40:13,562 | INFO | speech length: 65440
2026-01-28 19:40:13,610 | INFO | decoder input length: 101
2026-01-28 19:40:13,610 | INFO | max output length: 101
2026-01-28 19:40:13,610 | INFO | min output length: 10
2026-01-28 19:40:15,281 | INFO | end detected at 35
2026-01-28 19:40:15,283 | INFO |  -3.23 * 0.5 =  -1.62 for decoder
2026-01-28 19:40:15,284 | INFO |  -3.26 * 0.5 =  -1.63 for ctc
2026-01-28 19:40:15,284 | INFO | total log probability: -3.25
2026-01-28 19:40:15,284 | INFO | normalized log probability: -0.11
2026-01-28 19:40:15,284 | INFO | total number of ended hypotheses: 158
2026-01-28 19:40:15,284 | INFO | best hypo: ▁après▁vous▁continuez▁la▁direction▁saint▁jean▁c'est▁direction▁la▁piscine

2026-01-28 19:40:15,286 | INFO | speech length: 64800
2026-01-28 19:40:15,322 | INFO | decoder input length: 100
2026-01-28 19:40:15,322 | INFO | max output length: 100
2026-01-28 19:40:15,322 | INFO | min output length: 10
2026-01-28 19:40:17,505 | INFO | end detected at 39
2026-01-28 19:40:17,507 | INFO |  -3.44 * 0.5 =  -1.72 for decoder
2026-01-28 19:40:17,507 | INFO |  -4.88 * 0.5 =  -2.44 for ctc
2026-01-28 19:40:17,507 | INFO | total log probability: -4.16
2026-01-28 19:40:17,507 | INFO | normalized log probability: -0.13
2026-01-28 19:40:17,507 | INFO | total number of ended hypotheses: 166
2026-01-28 19:40:17,508 | INFO | best hypo: ▁après▁vous▁continuez▁à▁vous▁suivre▁la▁route▁et▁en▁grand▁tournant▁une▁grande▁ligne▁droite

2026-01-28 19:40:17,510 | INFO | speech length: 62240
2026-01-28 19:40:17,550 | INFO | decoder input length: 96
2026-01-28 19:40:17,550 | INFO | max output length: 96
2026-01-28 19:40:17,550 | INFO | min output length: 9
2026-01-28 19:40:19,318 | INFO | end detected at 38
2026-01-28 19:40:19,319 | INFO |  -6.41 * 0.5 =  -3.20 for decoder
2026-01-28 19:40:19,319 | INFO | -12.43 * 0.5 =  -6.22 for ctc
2026-01-28 19:40:19,319 | INFO | total log probability: -9.42
2026-01-28 19:40:19,319 | INFO | normalized log probability: -0.29
2026-01-28 19:40:19,319 | INFO | total number of ended hypotheses: 175
2026-01-28 19:40:19,320 | INFO | best hypo: ▁il▁avait▁à▁l'entrée▁de▁saint▁jean▁de▁marianne▁où▁situé▁un▁garage▁fiat

2026-01-28 19:40:19,322 | INFO | speech length: 195680
2026-01-28 19:40:19,374 | INFO | decoder input length: 305
2026-01-28 19:40:19,374 | INFO | max output length: 305
2026-01-28 19:40:19,374 | INFO | min output length: 30
2026-01-28 19:40:26,192 | INFO | end detected at 96
2026-01-28 19:40:26,194 | INFO | -72.27 * 0.5 = -36.14 for decoder
2026-01-28 19:40:26,194 | INFO | -12.33 * 0.5 =  -6.16 for ctc
2026-01-28 19:40:26,194 | INFO | total log probability: -42.30
2026-01-28 19:40:26,194 | INFO | normalized log probability: -0.48
2026-01-28 19:40:26,194 | INFO | total number of ended hypotheses: 178
2026-01-28 19:40:26,195 | INFO | best hypo: ▁vous▁prenaisserons▁point▁à▁gauche▁dura▁géant▁ajusta▁sur▁la▁droite▁et▁vous▁continuez▁au▁prochain▁rond▁point▁prenez▁à▁gauche▁vous▁descendez▁à▁la▁route▁vers▁les▁sapeurs▁pompiers▁en▁bas▁de▁la▁route

2026-01-28 19:40:26,198 | INFO | speech length: 130880
2026-01-28 19:40:26,243 | INFO | decoder input length: 204
2026-01-28 19:40:26,243 | INFO | max output length: 204
2026-01-28 19:40:26,243 | INFO | min output length: 20
2026-01-28 19:40:30,659 | INFO | end detected at 74
2026-01-28 19:40:30,661 | INFO | -22.82 * 0.5 = -11.41 for decoder
2026-01-28 19:40:30,661 | INFO | -10.05 * 0.5 =  -5.02 for ctc
2026-01-28 19:40:30,661 | INFO | total log probability: -16.44
2026-01-28 19:40:30,661 | INFO | normalized log probability: -0.24
2026-01-28 19:40:30,662 | INFO | total number of ended hypotheses: 157
2026-01-28 19:40:30,663 | INFO | best hypo: ▁vous▁avez▁après▁au▁fond▁de▁la▁route▁prenez▁à▁gauche▁devant▁doretel▁vous▁continuez▁tout▁droit▁et▁vous▁arrivez▁en▁face▁de▁la▁gare▁au▁bout▁de▁la▁ligne▁droite

2026-01-28 19:40:30,675 | INFO | Chunk: 0 | WER=28.260870 | S=5 D=4 I=4
2026-01-28 19:40:30,676 | INFO | Chunk: 1 | WER=16.666667 | S=0 D=1 I=1
2026-01-28 19:40:30,678 | INFO | Chunk: 2 | WER=38.888889 | S=3 D=3 I=1
2026-01-28 19:40:30,678 | INFO | Chunk: 3 | WER=26.666667 | S=4 D=0 I=0
2026-01-28 19:40:30,681 | INFO | Chunk: 4 | WER=28.205128 | S=5 D=5 I=1
2026-01-28 19:40:30,683 | INFO | Chunk: 5 | WER=12.903226 | S=2 D=1 I=1
2026-01-28 19:40:30,713 | INFO | File: Rhap-M0007.wav | WER=25.465839 | S=19 D=14 I=8
2026-01-28 19:40:30,713 | INFO | ------------------------------
2026-01-28 19:40:30,714 | INFO | Conf cv Done!
2026-01-28 19:40:30,927 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 19:40:30,950 | INFO | Vocabulary size: 47
2026-01-28 19:40:31,850 | INFO | Gradient checkpoint layers: []
2026-01-28 19:40:32,565 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:40:32,569 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:40:32,569 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:40:32,570 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 19:40:32,573 | INFO | speech length: 217280
2026-01-28 19:40:32,619 | INFO | decoder input length: 339
2026-01-28 19:40:32,619 | INFO | max output length: 339
2026-01-28 19:40:32,619 | INFO | min output length: 33
2026-01-28 19:40:51,211 | INFO | end detected at 255
2026-01-28 19:40:51,213 | INFO | -34.07 * 0.5 = -17.04 for decoder
2026-01-28 19:40:51,214 | INFO | -25.96 * 0.5 = -12.98 for ctc
2026-01-28 19:40:51,214 | INFO | total log probability: -30.02
2026-01-28 19:40:51,214 | INFO | normalized log probability: -0.12
2026-01-28 19:40:51,214 | INFO | total number of ended hypotheses: 174
2026-01-28 19:40:51,217 | INFO | best hypo: ben<space>vous<space>prenez<space>la<space>route<space>là<space>vous<space>sortez<space>euh<space>de<space>la<space>commune<space>de<space>ponta<space>à<space>ma<space>freud<space>vous<space>allez<space>arriver<space>à<space>un<space>moment<space>sur<space>les<space>changeurs<space>de<space>l'autoroute<space>vous<space>continuez<space>vous<space>prenez<space>le<space>rond<space>pointe<space>coup<space>droit<space>vous<space>passez<space>le<space>pont<space>euh<space>au<space>dessus<space>de<space>la<space>voie<space>ferrée

2026-01-28 19:40:51,220 | INFO | speech length: 65440
2026-01-28 19:40:51,271 | INFO | decoder input length: 101
2026-01-28 19:40:51,271 | INFO | max output length: 101
2026-01-28 19:40:51,272 | INFO | min output length: 10
2026-01-28 19:40:54,786 | INFO | end detected at 87
2026-01-28 19:40:54,788 | INFO | -13.19 * 0.5 =  -6.59 for decoder
2026-01-28 19:40:54,788 | INFO |  -8.46 * 0.5 =  -4.23 for ctc
2026-01-28 19:40:54,788 | INFO | total log probability: -10.83
2026-01-28 19:40:54,788 | INFO | normalized log probability: -0.14
2026-01-28 19:40:54,788 | INFO | total number of ended hypotheses: 179
2026-01-28 19:40:54,789 | INFO | best hypo: après<space>vous<space>continuez<space>à<space>direction<space>saint<space>gens<space>euh<space>c'est<space>dire<space>que<space>sont<space>la<space>piscine

2026-01-28 19:40:54,791 | INFO | speech length: 64800
2026-01-28 19:40:54,829 | INFO | decoder input length: 100
2026-01-28 19:40:54,829 | INFO | max output length: 100
2026-01-28 19:40:54,829 | INFO | min output length: 10
2026-01-28 19:40:58,616 | INFO | end detected at 96
2026-01-28 19:40:58,618 | INFO |  -9.05 * 0.5 =  -4.52 for decoder
2026-01-28 19:40:58,618 | INFO |  -4.17 * 0.5 =  -2.09 for ctc
2026-01-28 19:40:58,618 | INFO | total log probability: -6.61
2026-01-28 19:40:58,618 | INFO | normalized log probability: -0.07
2026-01-28 19:40:58,618 | INFO | total number of ended hypotheses: 196
2026-01-28 19:40:58,619 | INFO | best hypo: après<space>vous<space>continuez<space>vous<space>suivez<space>la<space>route<space>mais<space>un<space>grand<space>tournant<space>une<space>grande<space>ligne<space>droite

2026-01-28 19:40:58,621 | INFO | speech length: 62240
2026-01-28 19:40:58,676 | INFO | decoder input length: 96
2026-01-28 19:40:58,676 | INFO | max output length: 96
2026-01-28 19:40:58,677 | INFO | min output length: 9
2026-01-28 19:41:02,033 | INFO | end detected at 83
2026-01-28 19:41:02,035 | INFO | -18.06 * 0.5 =  -9.03 for decoder
2026-01-28 19:41:02,035 | INFO | -11.95 * 0.5 =  -5.98 for ctc
2026-01-28 19:41:02,035 | INFO | total log probability: -15.01
2026-01-28 19:41:02,035 | INFO | normalized log probability: -0.20
2026-01-28 19:41:02,035 | INFO | total number of ended hypotheses: 218
2026-01-28 19:41:02,036 | INFO | best hypo: gravais<space>à<space>l'entrée<space>de<space>saint<space>jende<space>marienne<space>euh<space>aussi<space>tu<space>es<space>un<space>garage<space>fiat

2026-01-28 19:41:02,039 | INFO | speech length: 195680
2026-01-28 19:41:02,076 | INFO | decoder input length: 305
2026-01-28 19:41:02,076 | INFO | max output length: 305
2026-01-28 19:41:02,076 | INFO | min output length: 30
2026-01-28 19:41:16,216 | INFO | end detected at 225
2026-01-28 19:41:16,219 | INFO | -26.81 * 0.5 = -13.40 for decoder
2026-01-28 19:41:16,219 | INFO | -17.80 * 0.5 =  -8.90 for ctc
2026-01-28 19:41:16,219 | INFO | total log probability: -22.31
2026-01-28 19:41:16,219 | INFO | normalized log probability: -0.11
2026-01-28 19:41:16,219 | INFO | total number of ended hypotheses: 239
2026-01-28 19:41:16,222 | INFO | best hypo: vous<space>prenez<space>seron<space>point<space>à<space>gauche<space>il<space>y<space>aura<space>géant<space>euh<space>juste<space>sur<space>votre<space>droite<space>après<space>vous<space>continuez<space>à<space>au<space>prochain<space>ron<space>point<space>vous<space>prenez<space>à<space>gauche<space>vous<space>descendez<space>la<space>route<space>vers<space>les<space>sapeurs<space>pompiers<space>en<space>bas<space>de<space>la<space>route

2026-01-28 19:41:16,225 | INFO | speech length: 130880
2026-01-28 19:41:16,268 | INFO | decoder input length: 204
2026-01-28 19:41:16,268 | INFO | max output length: 204
2026-01-28 19:41:16,268 | INFO | min output length: 20
2026-01-28 19:41:25,283 | INFO | end detected at 174
2026-01-28 19:41:25,285 | INFO | -21.41 * 0.5 = -10.70 for decoder
2026-01-28 19:41:25,285 | INFO | -12.29 * 0.5 =  -6.14 for ctc
2026-01-28 19:41:25,285 | INFO | total log probability: -16.85
2026-01-28 19:41:25,285 | INFO | normalized log probability: -0.10
2026-01-28 19:41:25,285 | INFO | total number of ended hypotheses: 185
2026-01-28 19:41:25,288 | INFO | best hypo: vous<space>avez<space>appris<space>au<space>fond<space>de<space>la<space>route<space>vous<space>prenez<space>à<space>gauche<space>devant<space>deux<space>rhôtels<space>vous<space>continuez<space>tout<space>droit<space>et<space>vous<space>arrivez<space>en<space>face<space>de<space>la<space>garre<space>au<space>bout<space>de<space>la<space>ligne<space>droite

2026-01-28 19:41:25,295 | INFO | Chunk: 0 | WER=30.434783 | S=7 D=2 I=5
2026-01-28 19:41:25,295 | INFO | Chunk: 1 | WER=58.333333 | S=2 D=1 I=4
2026-01-28 19:41:25,296 | INFO | Chunk: 2 | WER=22.222222 | S=1 D=3 I=0
2026-01-28 19:41:25,296 | INFO | Chunk: 3 | WER=53.333333 | S=6 D=1 I=1
2026-01-28 19:41:25,297 | INFO | Chunk: 4 | WER=12.820513 | S=2 D=1 I=2
2026-01-28 19:41:25,298 | INFO | Chunk: 5 | WER=19.354839 | S=4 D=0 I=2
2026-01-28 19:41:25,315 | INFO | File: Rhap-M0007.wav | WER=27.329193 | S=22 D=8 I=14
2026-01-28 19:41:25,315 | INFO | ------------------------------
2026-01-28 19:41:25,315 | INFO | Conf ester Done!
2026-01-28 19:42:22,472 | INFO | Chunk: 0 | WER=34.782609 | S=10 D=4 I=2
2026-01-28 19:42:22,473 | INFO | Chunk: 1 | WER=41.666667 | S=4 D=1 I=0
2026-01-28 19:42:22,473 | INFO | Chunk: 2 | WER=27.777778 | S=2 D=2 I=1
2026-01-28 19:42:22,474 | INFO | Chunk: 3 | WER=33.333333 | S=2 D=1 I=2
2026-01-28 19:42:22,476 | INFO | Chunk: 4 | WER=17.948718 | S=4 D=1 I=2
2026-01-28 19:42:22,477 | INFO | Chunk: 5 | WER=32.258065 | S=6 D=0 I=4
2026-01-28 19:42:22,499 | INFO | File: Rhap-M0007.wav | WER=29.813665 | S=28 D=9 I=11
2026-01-28 19:42:22,499 | INFO | ------------------------------
2026-01-28 19:42:22,499 | INFO | hmm_tdnn Done!
2026-01-28 19:42:22,678 | INFO | ==================================Rhap-M0008.wav=========================================
2026-01-28 19:42:22,837 | INFO | Using rVAD model
2026-01-28 19:42:24,366 | INFO | Chunk: 0 | WER=26.086957 | S=2 D=3 I=1
2026-01-28 19:42:24,367 | INFO | Chunk: 1 | WER=26.315789 | S=1 D=4 I=0
2026-01-28 19:42:24,368 | INFO | Chunk: 2 | WER=62.500000 | S=0 D=2 I=3
2026-01-28 19:42:24,372 | INFO | File: Rhap-M0008.wav | WER=32.000000 | S=3 D=9 I=4
2026-01-28 19:42:24,372 | INFO | ------------------------------
2026-01-28 19:42:24,373 | INFO | w2vec vad chunk Done!
2026-01-28 19:42:28,051 | INFO | Chunk: 0 | WER=21.739130 | S=1 D=2 I=2
2026-01-28 19:42:28,051 | INFO | Chunk: 1 | WER=5.263158 | S=0 D=1 I=0
2026-01-28 19:42:28,054 | INFO | Chunk: 2 | WER=62.500000 | S=0 D=2 I=3
2026-01-28 19:42:28,058 | INFO | File: Rhap-M0008.wav | WER=22.000000 | S=1 D=5 I=5
2026-01-28 19:42:28,058 | INFO | ------------------------------
2026-01-28 19:42:28,059 | INFO | whisper med Done!
2026-01-28 19:42:32,275 | INFO | Chunk: 0 | WER=21.739130 | S=1 D=2 I=2
2026-01-28 19:42:32,276 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 19:42:32,276 | INFO | Chunk: 2 | WER=62.500000 | S=0 D=2 I=3
2026-01-28 19:42:32,280 | INFO | File: Rhap-M0008.wav | WER=20.000000 | S=1 D=4 I=5
2026-01-28 19:42:32,280 | INFO | ------------------------------
2026-01-28 19:42:32,280 | INFO | whisper large Done!
2026-01-28 19:42:32,414 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 19:42:32,451 | INFO | Vocabulary size: 350
2026-01-28 19:42:34,077 | INFO | Gradient checkpoint layers: []
2026-01-28 19:42:35,118 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:42:35,126 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:42:35,127 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:42:35,128 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 19:42:35,128 | INFO | speech length: 122720
2026-01-28 19:42:35,187 | INFO | decoder input length: 191
2026-01-28 19:42:35,187 | INFO | max output length: 191
2026-01-28 19:42:35,187 | INFO | min output length: 19
2026-01-28 19:42:39,099 | INFO | end detected at 51
2026-01-28 19:42:39,100 | INFO |  -7.27 * 0.5 =  -3.63 for decoder
2026-01-28 19:42:39,100 | INFO | -10.18 * 0.5 =  -5.09 for ctc
2026-01-28 19:42:39,100 | INFO | total log probability: -8.72
2026-01-28 19:42:39,100 | INFO | normalized log probability: -0.19
2026-01-28 19:42:39,100 | INFO | total number of ended hypotheses: 159
2026-01-28 19:42:39,101 | INFO | best hypo: ▁vous▁allez▁à▁remillo▁et▁ensuite▁vous▁prenez▁la▁direction▁de▁saint▁jean▁vous▁prenez▁la▁grande▁ligne▁droite▁et▁vous▁tournez

2026-01-28 19:42:39,105 | INFO | speech length: 81280
2026-01-28 19:42:39,166 | INFO | decoder input length: 126
2026-01-28 19:42:39,166 | INFO | max output length: 126
2026-01-28 19:42:39,166 | INFO | min output length: 12
2026-01-28 19:42:43,156 | INFO | end detected at 42
2026-01-28 19:42:43,159 | INFO |  -5.91 * 0.5 =  -2.95 for decoder
2026-01-28 19:42:43,159 | INFO |  -5.50 * 0.5 =  -2.75 for ctc
2026-01-28 19:42:43,159 | INFO | total log probability: -5.70
2026-01-28 19:42:43,159 | INFO | normalized log probability: -0.16
2026-01-28 19:42:43,159 | INFO | total number of ended hypotheses: 167
2026-01-28 19:42:43,160 | INFO | best hypo: ▁a▁gauche▁vous▁passez▁devant▁les▁pompiers▁ensuite▁et▁ligne▁droite▁vous▁allez▁à▁droite

2026-01-28 19:42:43,164 | INFO | speech length: 38560
2026-01-28 19:42:43,217 | INFO | decoder input length: 59
2026-01-28 19:42:43,217 | INFO | max output length: 59
2026-01-28 19:42:43,217 | INFO | min output length: 5
2026-01-28 19:42:44,806 | INFO | end detected at 24
2026-01-28 19:42:44,807 | INFO |  -1.47 * 0.5 =  -0.74 for decoder
2026-01-28 19:42:44,808 | INFO |  -0.75 * 0.5 =  -0.38 for ctc
2026-01-28 19:42:44,808 | INFO | total log probability: -1.11
2026-01-28 19:42:44,808 | INFO | normalized log probability: -0.06
2026-01-28 19:42:44,808 | INFO | total number of ended hypotheses: 165
2026-01-28 19:42:44,808 | INFO | best hypo: ▁et▁vous▁continuez▁vous▁tomber▁sur▁la▁gare

2026-01-28 19:42:44,814 | INFO | Chunk: 0 | WER=17.391304 | S=1 D=2 I=1
2026-01-28 19:42:44,815 | INFO | Chunk: 1 | WER=26.315789 | S=1 D=4 I=0
2026-01-28 19:42:44,815 | INFO | Chunk: 2 | WER=87.500000 | S=1 D=3 I=3
2026-01-28 19:42:44,819 | INFO | File: Rhap-M0008.wav | WER=32.000000 | S=3 D=9 I=4
2026-01-28 19:42:44,819 | INFO | ------------------------------
2026-01-28 19:42:44,819 | INFO | Conf cv Done!
2026-01-28 19:42:45,012 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 19:42:45,083 | INFO | Vocabulary size: 47
2026-01-28 19:42:46,299 | INFO | Gradient checkpoint layers: []
2026-01-28 19:42:47,019 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:42:47,024 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:42:47,024 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:42:47,024 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 19:42:47,027 | INFO | speech length: 122720
2026-01-28 19:42:47,068 | INFO | decoder input length: 191
2026-01-28 19:42:47,068 | INFO | max output length: 191
2026-01-28 19:42:47,068 | INFO | min output length: 19
2026-01-28 19:42:54,171 | INFO | end detected at 147
2026-01-28 19:42:54,173 | INFO | -22.68 * 0.5 = -11.34 for decoder
2026-01-28 19:42:54,173 | INFO |  -7.42 * 0.5 =  -3.71 for ctc
2026-01-28 19:42:54,173 | INFO | total log probability: -15.05
2026-01-28 19:42:54,173 | INFO | normalized log probability: -0.11
2026-01-28 19:42:54,173 | INFO | total number of ended hypotheses: 169
2026-01-28 19:42:54,175 | INFO | best hypo: vous<space>allez<space>un<space>rumillot<space>et<space>euh<space>ensuite<space>vous<space>prenez<space>la<space>direction<space>de<space>saint<space>jean<space>vous<space>p<space>vous<space>prenez<space>la<space>grande<space>ligne<space>droite<space>et<space>vous<space>tournez<space>un

2026-01-28 19:42:54,178 | INFO | speech length: 81280
2026-01-28 19:42:54,221 | INFO | decoder input length: 126
2026-01-28 19:42:54,221 | INFO | max output length: 126
2026-01-28 19:42:54,221 | INFO | min output length: 12
2026-01-28 19:42:58,511 | INFO | end detected at 101
2026-01-28 19:42:58,513 | INFO |  -9.43 * 0.5 =  -4.72 for decoder
2026-01-28 19:42:58,513 | INFO |  -2.00 * 0.5 =  -1.00 for ctc
2026-01-28 19:42:58,513 | INFO | total log probability: -5.72
2026-01-28 19:42:58,513 | INFO | normalized log probability: -0.06
2026-01-28 19:42:58,513 | INFO | total number of ended hypotheses: 175
2026-01-28 19:42:58,514 | INFO | best hypo: à<space>gauche<space>vous<space>passez<space>devant<space>les<space>pompiers<space>ensuite<space>il<space>y<space>a<space>une<space>ligne<space>droite<space>vous<space>allez<space>à<space>droite

2026-01-28 19:42:58,516 | INFO | speech length: 38560
2026-01-28 19:42:58,552 | INFO | decoder input length: 59
2026-01-28 19:42:58,552 | INFO | max output length: 59
2026-01-28 19:42:58,552 | INFO | min output length: 5
2026-01-28 19:43:00,846 | INFO | end detected at 55
2026-01-28 19:43:00,848 | INFO |  -9.63 * 0.5 =  -4.82 for decoder
2026-01-28 19:43:00,848 | INFO |  -3.87 * 0.5 =  -1.93 for ctc
2026-01-28 19:43:00,848 | INFO | total log probability: -6.75
2026-01-28 19:43:00,848 | INFO | normalized log probability: -0.13
2026-01-28 19:43:00,848 | INFO | total number of ended hypotheses: 170
2026-01-28 19:43:00,849 | INFO | best hypo: et<space>euh<space>vous<space>continuez<space>et<space>vous<space>tombez<space>sur<space>la<space>garre

2026-01-28 19:43:00,855 | INFO | Chunk: 0 | WER=26.086957 | S=3 D=0 I=3
2026-01-28 19:43:00,856 | INFO | Chunk: 1 | WER=5.263158 | S=0 D=1 I=0
2026-01-28 19:43:00,856 | INFO | Chunk: 2 | WER=75.000000 | S=0 D=2 I=4
2026-01-28 19:43:00,860 | INFO | File: Rhap-M0008.wav | WER=26.000000 | S=3 D=3 I=7
2026-01-28 19:43:00,860 | INFO | ------------------------------
2026-01-28 19:43:00,860 | INFO | Conf ester Done!
2026-01-28 19:43:24,243 | INFO | Chunk: 0 | WER=39.130435 | S=4 D=2 I=3
2026-01-28 19:43:24,244 | INFO | Chunk: 1 | WER=5.263158 | S=1 D=0 I=0
2026-01-28 19:43:24,244 | INFO | Chunk: 2 | WER=75.000000 | S=0 D=3 I=3
2026-01-28 19:43:24,249 | INFO | File: Rhap-M0008.wav | WER=32.000000 | S=5 D=5 I=6
2026-01-28 19:43:24,250 | INFO | ------------------------------
2026-01-28 19:43:24,250 | INFO | hmm_tdnn Done!
2026-01-28 19:43:24,503 | INFO | ==================================Rhap-M0009.wav=========================================
2026-01-28 19:43:24,704 | INFO | Using rVAD model
2026-01-28 19:43:31,623 | INFO | Chunk: 0 | WER=26.041667 | S=11 D=13 I=1
2026-01-28 19:43:31,623 | INFO | Chunk: 1 | WER=38.461538 | S=1 D=1 I=3
2026-01-28 19:43:31,627 | INFO | Chunk: 2 | WER=32.786885 | S=10 D=10 I=0
2026-01-28 19:43:31,627 | INFO | Chunk: 3 | WER=63.636364 | S=10 D=2 I=2
2026-01-28 19:43:31,654 | INFO | File: Rhap-M0009.wav | WER=30.729167 | S=31 D=24 I=4
2026-01-28 19:43:31,654 | INFO | ------------------------------
2026-01-28 19:43:31,654 | INFO | w2vec vad chunk Done!
2026-01-28 19:43:37,878 | INFO | Chunk: 0 | WER=71.875000 | S=4 D=65 I=0
2026-01-28 19:43:37,879 | INFO | Chunk: 1 | WER=7.692308 | S=0 D=1 I=0
2026-01-28 19:43:37,880 | INFO | Chunk: 2 | WER=62.295082 | S=2 D=36 I=0
2026-01-28 19:43:37,880 | INFO | Chunk: 3 | WER=54.545455 | S=8 D=4 I=0
2026-01-28 19:43:37,892 | INFO | File: Rhap-M0009.wav | WER=62.500000 | S=14 D=106 I=0
2026-01-28 19:43:37,892 | INFO | ------------------------------
2026-01-28 19:43:37,892 | INFO | whisper med Done!
2026-01-28 19:43:48,092 | INFO | Chunk: 0 | WER=66.666667 | S=5 D=59 I=0
2026-01-28 19:43:48,092 | INFO | Chunk: 1 | WER=30.769231 | S=0 D=1 I=3
2026-01-28 19:43:48,094 | INFO | Chunk: 2 | WER=44.262295 | S=5 D=19 I=3
2026-01-28 19:43:48,095 | INFO | Chunk: 3 | WER=63.636364 | S=11 D=2 I=1
2026-01-28 19:43:48,109 | INFO | File: Rhap-M0009.wav | WER=53.645833 | S=21 D=78 I=4
2026-01-28 19:43:48,109 | INFO | ------------------------------
2026-01-28 19:43:48,109 | INFO | whisper large Done!
2026-01-28 19:43:48,244 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 19:43:48,310 | INFO | Vocabulary size: 350
2026-01-28 19:43:49,753 | INFO | Gradient checkpoint layers: []
2026-01-28 19:43:50,477 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:43:50,481 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:43:50,482 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:43:50,482 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 19:43:50,483 | INFO | speech length: 427680
2026-01-28 19:43:50,538 | INFO | decoder input length: 667
2026-01-28 19:43:50,538 | INFO | max output length: 667
2026-01-28 19:43:50,538 | INFO | min output length: 66
2026-01-28 19:44:14,094 | INFO | end detected at 197
2026-01-28 19:44:14,095 | INFO | -436.76 * 0.5 = -218.38 for decoder
2026-01-28 19:44:14,095 | INFO | -142.52 * 0.5 = -71.26 for ctc
2026-01-28 19:44:14,095 | INFO | total log probability: -289.64
2026-01-28 19:44:14,095 | INFO | normalized log probability: -1.51
2026-01-28 19:44:14,095 | INFO | total number of ended hypotheses: 155
2026-01-28 19:44:14,097 | INFO | best hypo: ▁alors▁là▁tu▁vois▁donc▁tu▁continues▁tout▁droit▁ou▁droite▁tu▁traverses▁toute▁arrivant▁un▁rond▁point▁au▁rond▁point▁ces▁tout▁droits▁directions▁saint▁jean▁de▁mauriennes▁dans▁toute▁toujours▁st▁jean▁de▁maurienne▁tups▁au▁rond▁point▁donc▁toujours▁tout▁droit▁avec▁tu▁monte▁une▁grande▁grande▁ligne▁droite▁tu▁et▁passe▁devant▁la▁pisincine▁et▁un▁stades▁côté▁et▁arrives▁à▁un▁rond▁point▁platurant▁dans▁le▁centre▁t'arrivant▁un▁rond▁point▁si▁donc▁à▁gauche

2026-01-28 19:44:14,101 | INFO | speech length: 104960
2026-01-28 19:44:14,142 | INFO | decoder input length: 163
2026-01-28 19:44:14,142 | INFO | max output length: 163
2026-01-28 19:44:14,142 | INFO | min output length: 16
2026-01-28 19:44:16,338 | INFO | end detected at 37
2026-01-28 19:44:16,340 | INFO | -14.66 * 0.5 =  -7.33 for decoder
2026-01-28 19:44:16,340 | INFO | -19.95 * 0.5 =  -9.97 for ctc
2026-01-28 19:44:16,340 | INFO | total log probability: -17.30
2026-01-28 19:44:16,340 | INFO | normalized log probability: -0.56
2026-01-28 19:44:16,340 | INFO | total number of ended hypotheses: 175
2026-01-28 19:44:16,341 | INFO | best hypo: ▁ensuite▁donc▁go▁schlash▁rox▁se▁sera▁indiqué▁déjà▁gare▁mais▁bon▁tu

2026-01-28 19:44:16,343 | INFO | speech length: 334080
2026-01-28 19:44:16,389 | INFO | decoder input length: 521
2026-01-28 19:44:16,389 | INFO | max output length: 521
2026-01-28 19:44:16,389 | INFO | min output length: 52
2026-01-28 19:44:29,500 | INFO | end detected at 119
2026-01-28 19:44:29,501 | INFO | -219.36 * 0.5 = -109.68 for decoder
2026-01-28 19:44:29,502 | INFO | -103.21 * 0.5 = -51.60 for ctc
2026-01-28 19:44:29,502 | INFO | total log probability: -161.28
2026-01-28 19:44:29,502 | INFO | normalized log probability: -1.41
2026-01-28 19:44:29,502 | INFO | total number of ended hypotheses: 155
2026-01-28 19:44:29,503 | INFO | best hypo: ▁il▁traverse▁le▁champ▁de▁foie▁t'arrive▁un▁autre▁rond▁point▁placé▁encore▁à▁gauche▁et▁tu▁descends▁avec▁une▁route▁qui▁descends▁tu▁passe▁devant▁les▁pompiers▁et▁ensuite▁'▁tubavant▁la▁bastille▁scie'▁et▁ensuite▁c'est▁la▁première▁'▁deuxième▁ou▁troisième▁ou▁troisième▁lignembre▁à▁droite

2026-01-28 19:44:29,505 | INFO | speech length: 89600
2026-01-28 19:44:29,554 | INFO | decoder input length: 139
2026-01-28 19:44:29,554 | INFO | max output length: 139
2026-01-28 19:44:29,554 | INFO | min output length: 13
2026-01-28 19:44:32,422 | INFO | end detected at 53
2026-01-28 19:44:32,424 | INFO | -12.25 * 0.5 =  -6.13 for decoder
2026-01-28 19:44:32,424 | INFO | -13.68 * 0.5 =  -6.84 for ctc
2026-01-28 19:44:32,424 | INFO | total log probability: -12.96
2026-01-28 19:44:32,424 | INFO | normalized log probability: -0.29
2026-01-28 19:44:32,424 | INFO | total number of ended hypotheses: 201
2026-01-28 19:44:32,425 | INFO | best hypo: ▁et▁tu▁montes▁et▁voilà▁t▁arrive▁devant▁la▁gare▁après▁la▁sin▁indiqué▁cabinet▁téléphone▁et▁voilà

2026-01-28 19:44:32,439 | INFO | Chunk: 0 | WER=43.750000 | S=23 D=17 I=2
2026-01-28 19:44:32,440 | INFO | Chunk: 1 | WER=76.923077 | S=4 D=3 I=3
2026-01-28 19:44:32,443 | INFO | Chunk: 2 | WER=45.901639 | S=14 D=13 I=1
2026-01-28 19:44:32,444 | INFO | Chunk: 3 | WER=50.000000 | S=7 D=4 I=0
2026-01-28 19:44:32,468 | INFO | File: Rhap-M0009.wav | WER=44.270833 | S=46 D=35 I=4
2026-01-28 19:44:32,468 | INFO | ------------------------------
2026-01-28 19:44:32,469 | INFO | Conf cv Done!
2026-01-28 19:44:32,678 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 19:44:32,707 | INFO | Vocabulary size: 47
2026-01-28 19:44:34,443 | INFO | Gradient checkpoint layers: []
2026-01-28 19:44:35,095 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:44:35,100 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:44:35,100 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:44:35,100 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 19:44:35,103 | INFO | speech length: 427680
2026-01-28 19:44:35,145 | INFO | decoder input length: 667
2026-01-28 19:44:35,146 | INFO | max output length: 667
2026-01-28 19:44:35,146 | INFO | min output length: 66
2026-01-28 19:45:25,876 | INFO | end detected at 502
2026-01-28 19:45:25,878 | INFO | -452.39 * 0.5 = -226.19 for decoder
2026-01-28 19:45:25,878 | INFO | -132.70 * 0.5 = -66.35 for ctc
2026-01-28 19:45:25,878 | INFO | total log probability: -292.55
2026-01-28 19:45:25,878 | INFO | normalized log probability: -0.59
2026-01-28 19:45:25,878 | INFO | total number of ended hypotheses: 130
2026-01-28 19:45:25,884 | INFO | best hypo: alors<space>là<space>tu<space>vois<space>donc<space>tu<space>continues<space>te<space>fous<space>droite<space>ou<space>droite<space>tu<space>traverses<space>tout<space>arrive<space>à<space>un<space>gros<space>point<space>oh<space>on<space>prend<space>c'est<space>tout<space>droit<space>direction<space>saint<space>jean<space>de<space>marienne<space>pour<space>cent<space>hesitation<space>donc<space>tout<space>toujours<space>son<space>genre<space>de<space>marienne<space>tu<space>passes<space>un<space>autre<space>on<space>point<space>donc<space>toujours<space>tout<space>droit<space>après<space>tje<space>monte<space>une<space>grande<space>grande<space>ligne<space>droite<space>tu<space>passes<space>devant<space>la<space>piscine<space>il<space>y<space>a<space>un<space>stade<space>aussi<space>à<space>côté<space>est<space>arrive<space>à<space>un<space>rone<space>poine<space>done<space>gaus<space>te<space>rontre<space>dans<space>le<space>centre<space>ville<space>tarrive<space>à<space>ue<space>ron<space>poine<space>c'est<space>donc<space>e<space>gauche

2026-01-28 19:45:25,886 | INFO | speech length: 104960
2026-01-28 19:45:25,923 | INFO | decoder input length: 163
2026-01-28 19:45:25,923 | INFO | max output length: 163
2026-01-28 19:45:25,923 | INFO | min output length: 16
2026-01-28 19:45:30,953 | INFO | end detected at 108
2026-01-28 19:45:30,955 | INFO |  -8.71 * 0.5 =  -4.36 for decoder
2026-01-28 19:45:30,956 | INFO | -22.98 * 0.5 = -11.49 for ctc
2026-01-28 19:45:30,956 | INFO | total log probability: -15.85
2026-01-28 19:45:30,956 | INFO | normalized log probability: -0.19
2026-01-28 19:45:30,956 | INFO | total number of ended hypotheses: 218
2026-01-28 19:45:30,957 | INFO | best hypo: euh<space>ensuite<space>donc<space>gauche<space>là<space>je<space>crois<space>que<space>ça<space>sera<space>indiqué<space>déjà<space>gare<space>euh<space>mais<space>bon<space>tu

2026-01-28 19:45:30,959 | INFO | speech length: 334080
2026-01-28 19:45:31,000 | INFO | decoder input length: 521
2026-01-28 19:45:31,000 | INFO | max output length: 521
2026-01-28 19:45:31,000 | INFO | min output length: 52
2026-01-28 19:45:59,312 | INFO | end detected at 333
2026-01-28 19:45:59,314 | INFO | -109.09 * 0.5 = -54.55 for decoder
2026-01-28 19:45:59,314 | INFO | -40.55 * 0.5 = -20.28 for ctc
2026-01-28 19:45:59,314 | INFO | total log probability: -74.82
2026-01-28 19:45:59,314 | INFO | normalized log probability: -0.23
2026-01-28 19:45:59,314 | INFO | total number of ended hypotheses: 176
2026-01-28 19:45:59,319 | INFO | best hypo: je<space>traverse<space>le<space>champ<space>de<space>foire<space>ça<space>arrive<space>un<space>autre<space>point<space>là<space>c'est<space>encore<space>à<space>gauche<space>pour<space>tu<space>descend<space>en<space>fait<space>c'est<space>une<space>route<space>qui<space>descend<space>je<space>passe<space>devant<space>les<space>pompiers<space>et<space>pour<space>cent<space>hesitation<space>ensuite<space>premier<space>tu<space>passes<space>dans<space>la<space>bastille<space>aussi<space>et<space>ensuite<space>c'est<space>la<space>première<space>ou<space>fin<space>euh<space>deuxième<space>ou<space>troisième<space>troisième<space>il<space>temps<space>à<space>droite

2026-01-28 19:45:59,321 | INFO | speech length: 89600
2026-01-28 19:45:59,362 | INFO | decoder input length: 139
2026-01-28 19:45:59,362 | INFO | max output length: 139
2026-01-28 19:45:59,362 | INFO | min output length: 13
2026-01-28 19:46:04,313 | INFO | end detected at 115
2026-01-28 19:46:04,315 | INFO | -28.53 * 0.5 = -14.27 for decoder
2026-01-28 19:46:04,315 | INFO | -16.09 * 0.5 =  -8.05 for ctc
2026-01-28 19:46:04,315 | INFO | total log probability: -22.31
2026-01-28 19:46:04,315 | INFO | normalized log probability: -0.21
2026-01-28 19:46:04,315 | INFO | total number of ended hypotheses: 210
2026-01-28 19:46:04,317 | INFO | best hypo: et<space>tu<space>montes<space>et<space>voilà<space>ça<space>arrive<space>devant<space>là<space>y<space>a<space>connaissant<space>diké<space>il<space>y<space>a<space>a<space>une<space>cabine<space>de<space>téléphone<space>et<space>voilà

2026-01-28 19:46:04,328 | INFO | Chunk: 0 | WER=40.625000 | S=32 D=3 I=4
2026-01-28 19:46:04,328 | INFO | Chunk: 1 | WER=38.461538 | S=1 D=0 I=4
2026-01-28 19:46:04,331 | INFO | Chunk: 2 | WER=32.786885 | S=13 D=3 I=4
2026-01-28 19:46:04,332 | INFO | Chunk: 3 | WER=72.727273 | S=7 D=4 I=5
2026-01-28 19:46:04,353 | INFO | File: Rhap-M0009.wav | WER=38.541667 | S=51 D=8 I=15
2026-01-28 19:46:04,353 | INFO | ------------------------------
2026-01-28 19:46:04,354 | INFO | Conf ester Done!
2026-01-28 19:47:11,323 | INFO | Chunk: 0 | WER=42.708333 | S=22 D=16 I=3
2026-01-28 19:47:11,323 | INFO | Chunk: 1 | WER=23.076923 | S=1 D=1 I=1
2026-01-28 19:47:11,327 | INFO | Chunk: 2 | WER=45.901639 | S=18 D=8 I=2
2026-01-28 19:47:11,328 | INFO | Chunk: 3 | WER=86.363636 | S=15 D=3 I=1
2026-01-28 19:47:11,354 | INFO | File: Rhap-M0009.wav | WER=44.270833 | S=56 D=25 I=4
2026-01-28 19:47:11,354 | INFO | ------------------------------
2026-01-28 19:47:11,354 | INFO | hmm_tdnn Done!
2026-01-28 19:47:11,532 | INFO | ==================================Rhap-M0010.wav=========================================
2026-01-28 19:47:11,692 | INFO | Using rVAD model
2026-01-28 19:47:13,156 | INFO | Chunk: 0 | WER=27.586207 | S=4 D=4 I=0
2026-01-28 19:47:13,158 | INFO | Chunk: 1 | WER=24.242424 | S=4 D=3 I=1
2026-01-28 19:47:13,163 | INFO | File: Rhap-M0010.wav | WER=25.806452 | S=8 D=7 I=1
2026-01-28 19:47:13,164 | INFO | ------------------------------
2026-01-28 19:47:13,164 | INFO | w2vec vad chunk Done!
2026-01-28 19:47:16,845 | INFO | Chunk: 0 | WER=24.137931 | S=3 D=4 I=0
2026-01-28 19:47:16,847 | INFO | Chunk: 1 | WER=21.212121 | S=3 D=3 I=1
2026-01-28 19:47:16,851 | INFO | File: Rhap-M0010.wav | WER=22.580645 | S=6 D=7 I=1
2026-01-28 19:47:16,851 | INFO | ------------------------------
2026-01-28 19:47:16,851 | INFO | whisper med Done!
2026-01-28 19:47:21,609 | INFO | Chunk: 0 | WER=13.793103 | S=1 D=3 I=0
2026-01-28 19:47:21,610 | INFO | Chunk: 1 | WER=30.303030 | S=4 D=5 I=1
2026-01-28 19:47:21,614 | INFO | File: Rhap-M0010.wav | WER=22.580645 | S=5 D=8 I=1
2026-01-28 19:47:21,614 | INFO | ------------------------------
2026-01-28 19:47:21,615 | INFO | whisper large Done!
2026-01-28 19:47:21,783 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 19:47:21,821 | INFO | Vocabulary size: 350
2026-01-28 19:47:23,023 | INFO | Gradient checkpoint layers: []
2026-01-28 19:47:23,900 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:47:23,917 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:47:23,919 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:47:23,920 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 19:47:23,920 | INFO | speech length: 108320
2026-01-28 19:47:24,020 | INFO | decoder input length: 168
2026-01-28 19:47:24,020 | INFO | max output length: 168
2026-01-28 19:47:24,020 | INFO | min output length: 16
2026-01-28 19:47:27,673 | INFO | end detected at 66
2026-01-28 19:47:27,675 | INFO | -10.96 * 0.5 =  -5.48 for decoder
2026-01-28 19:47:27,675 | INFO | -11.52 * 0.5 =  -5.76 for ctc
2026-01-28 19:47:27,675 | INFO | total log probability: -11.24
2026-01-28 19:47:27,675 | INFO | normalized log probability: -0.19
2026-01-28 19:47:27,675 | INFO | total number of ended hypotheses: 186
2026-01-28 19:47:27,676 | INFO | best hypo: ▁alors▁tu▁vas▁sur▁forum▁là▁ensuite▁des▁escaliers▁et▁descends▁les▁escaliers▁tu▁vas▁tout▁droit▁tu▁vas▁passer▁dans▁un▁magasin▁de▁pantalon

2026-01-28 19:47:27,680 | INFO | speech length: 144160
2026-01-28 19:47:27,726 | INFO | decoder input length: 224
2026-01-28 19:47:27,726 | INFO | max output length: 224
2026-01-28 19:47:27,726 | INFO | min output length: 22
2026-01-28 19:47:32,916 | INFO | end detected at 83
2026-01-28 19:47:32,919 | INFO | -11.50 * 0.5 =  -5.75 for decoder
2026-01-28 19:47:32,920 | INFO | -19.80 * 0.5 =  -9.90 for ctc
2026-01-28 19:47:32,920 | INFO | total log probability: -15.65
2026-01-28 19:47:32,920 | INFO | normalized log probability: -0.21
2026-01-28 19:47:32,920 | INFO | total number of ended hypotheses: 213
2026-01-28 19:47:32,921 | INFO | best hypo: ▁ensuite▁tu▁vas▁te▁retrouver▁vers▁le▁cinéma▁star▁puis▁descends▁toute▁la▁pente▁ensuite▁vers▁un▁rond▁point▁puis▁tourne▁à▁gauche▁et▁en▁face▁néanmoins▁de▁ravoir▁la▁guerre

2026-01-28 19:47:32,930 | INFO | Chunk: 0 | WER=24.137931 | S=3 D=4 I=0
2026-01-28 19:47:32,931 | INFO | Chunk: 1 | WER=33.333333 | S=7 D=3 I=1
2026-01-28 19:47:32,938 | INFO | File: Rhap-M0010.wav | WER=29.032258 | S=10 D=7 I=1
2026-01-28 19:47:32,938 | INFO | ------------------------------
2026-01-28 19:47:32,938 | INFO | Conf cv Done!
2026-01-28 19:47:33,148 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 19:47:33,177 | INFO | Vocabulary size: 47
2026-01-28 19:47:34,063 | INFO | Gradient checkpoint layers: []
2026-01-28 19:47:34,721 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:47:34,725 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:47:34,725 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:47:34,726 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 19:47:34,729 | INFO | speech length: 108320
2026-01-28 19:47:34,770 | INFO | decoder input length: 168
2026-01-28 19:47:34,771 | INFO | max output length: 168
2026-01-28 19:47:34,771 | INFO | min output length: 16
2026-01-28 19:47:41,568 | INFO | end detected at 149
2026-01-28 19:47:41,570 | INFO | -26.53 * 0.5 = -13.27 for decoder
2026-01-28 19:47:41,570 | INFO | -36.43 * 0.5 = -18.22 for ctc
2026-01-28 19:47:41,570 | INFO | total log probability: -31.48
2026-01-28 19:47:41,570 | INFO | normalized log probability: -0.23
2026-01-28 19:47:41,570 | INFO | total number of ended hypotheses: 223
2026-01-28 19:47:41,572 | INFO | best hypo: alors<space>tu<space>vas<space>sur<space>le<space>forum<space>là<space>ensuite<space>c'est<space>des<space>descaliers<space>il<space>descend<space>les<space>escaliers<space>tu<space>bats<space>tou<space>droit<space>tu<space>vas<space>passer<space>mon<space>magasin<space>de<space>patamon

2026-01-28 19:47:41,575 | INFO | speech length: 144160
2026-01-28 19:47:41,611 | INFO | decoder input length: 224
2026-01-28 19:47:41,611 | INFO | max output length: 224
2026-01-28 19:47:41,612 | INFO | min output length: 22
2026-01-28 19:47:50,026 | INFO | end detected at 160
2026-01-28 19:47:50,029 | INFO | -33.75 * 0.5 = -16.88 for decoder
2026-01-28 19:47:50,029 | INFO | -32.82 * 0.5 = -16.41 for ctc
2026-01-28 19:47:50,029 | INFO | total log probability: -33.28
2026-01-28 19:47:50,029 | INFO | normalized log probability: -0.22
2026-01-28 19:47:50,029 | INFO | total number of ended hypotheses: 185
2026-01-28 19:47:50,031 | INFO | best hypo: ensuite<space>tu<space>bas<space>tu<space>retrouves<space>vers<space>le<space>cinéma<space>star<space>qui<space>descend<space>toute<space>la<space>pente<space>ensuite<space>tu<space>berras<space>hon<space>point<space>son<space>la<space>gauche<space>et<space>en<space>face<space>normal<space>devra<space>voilà<space>y<space>a

2026-01-28 19:47:50,038 | INFO | Chunk: 0 | WER=37.931034 | S=9 D=2 I=0
2026-01-28 19:47:50,039 | INFO | Chunk: 1 | WER=48.484848 | S=13 D=3 I=0
2026-01-28 19:47:50,044 | INFO | File: Rhap-M0010.wav | WER=43.548387 | S=22 D=5 I=0
2026-01-28 19:47:50,044 | INFO | ------------------------------
2026-01-28 19:47:50,044 | INFO | Conf ester Done!
2026-01-28 19:48:15,607 | INFO | Chunk: 0 | WER=65.517241 | S=8 D=11 I=0
2026-01-28 19:48:15,608 | INFO | Chunk: 1 | WER=54.545455 | S=11 D=7 I=0
2026-01-28 19:48:15,614 | INFO | File: Rhap-M0010.wav | WER=59.677419 | S=19 D=18 I=0
2026-01-28 19:48:15,614 | INFO | ------------------------------
2026-01-28 19:48:15,614 | INFO | hmm_tdnn Done!
2026-01-28 19:48:15,875 | INFO | ==================================Rhap-M0011.wav=========================================
2026-01-28 19:48:16,116 | INFO | Using rVAD model
2026-01-28 19:48:21,908 | INFO | Chunk: 0 | WER=30.769231 | S=3 D=1 I=0
2026-01-28 19:48:21,911 | INFO | Chunk: 1 | WER=19.444444 | S=4 D=2 I=1
2026-01-28 19:48:21,912 | INFO | Chunk: 2 | WER=11.111111 | S=2 D=0 I=0
2026-01-28 19:48:21,915 | INFO | Chunk: 3 | WER=21.875000 | S=3 D=3 I=1
2026-01-28 19:48:21,916 | INFO | Chunk: 4 | WER=76.470588 | S=10 D=3 I=0
2026-01-28 19:48:21,916 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 19:48:21,919 | INFO | Chunk: 6 | WER=13.888889 | S=3 D=2 I=0
2026-01-28 19:48:21,951 | INFO | File: Rhap-M0011.wav | WER=24.203822 | S=25 D=11 I=2
2026-01-28 19:48:21,951 | INFO | ------------------------------
2026-01-28 19:48:21,952 | INFO | w2vec vad chunk Done!
2026-01-28 19:48:28,938 | INFO | Chunk: 0 | WER=23.076923 | S=1 D=2 I=0
2026-01-28 19:48:28,939 | INFO | Chunk: 1 | WER=44.444444 | S=0 D=16 I=0
2026-01-28 19:48:28,939 | INFO | Chunk: 2 | WER=5.555556 | S=1 D=0 I=0
2026-01-28 19:48:28,940 | INFO | Chunk: 3 | WER=65.625000 | S=4 D=17 I=0
2026-01-28 19:48:28,940 | INFO | Chunk: 4 | WER=52.941176 | S=4 D=5 I=0
2026-01-28 19:48:28,940 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 19:48:28,941 | INFO | Chunk: 6 | WER=22.222222 | S=2 D=5 I=1
2026-01-28 19:48:28,953 | INFO | File: Rhap-M0011.wav | WER=36.942675 | S=12 D=45 I=1
2026-01-28 19:48:28,953 | INFO | ------------------------------
2026-01-28 19:48:28,953 | INFO | whisper med Done!
2026-01-28 19:48:38,505 | INFO | Chunk: 0 | WER=7.692308 | S=1 D=0 I=0
2026-01-28 19:48:38,506 | INFO | Chunk: 1 | WER=50.000000 | S=2 D=16 I=0
2026-01-28 19:48:38,506 | INFO | Chunk: 2 | WER=11.111111 | S=2 D=0 I=0
2026-01-28 19:48:38,507 | INFO | Chunk: 3 | WER=28.125000 | S=5 D=4 I=0
2026-01-28 19:48:38,508 | INFO | Chunk: 4 | WER=70.588235 | S=5 D=7 I=0
2026-01-28 19:48:38,508 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 19:48:38,509 | INFO | Chunk: 6 | WER=41.666667 | S=7 D=8 I=0
2026-01-28 19:48:38,521 | INFO | File: Rhap-M0011.wav | WER=36.305732 | S=22 D=35 I=0
2026-01-28 19:48:38,521 | INFO | ------------------------------
2026-01-28 19:48:38,521 | INFO | whisper large Done!
2026-01-28 19:48:38,696 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 19:48:38,735 | INFO | Vocabulary size: 350
2026-01-28 19:48:39,670 | INFO | Gradient checkpoint layers: []
2026-01-28 19:48:40,352 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:48:40,356 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:48:40,357 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:48:40,357 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 19:48:40,357 | INFO | speech length: 66560
2026-01-28 19:48:40,404 | INFO | decoder input length: 103
2026-01-28 19:48:40,404 | INFO | max output length: 103
2026-01-28 19:48:40,404 | INFO | min output length: 10
2026-01-28 19:48:41,914 | INFO | end detected at 31
2026-01-28 19:48:41,916 | INFO |  -7.63 * 0.5 =  -3.82 for decoder
2026-01-28 19:48:41,916 | INFO |  -6.00 * 0.5 =  -3.00 for ctc
2026-01-28 19:48:41,916 | INFO | total log probability: -6.82
2026-01-28 19:48:41,916 | INFO | normalized log probability: -0.27
2026-01-28 19:48:41,916 | INFO | total number of ended hypotheses: 187
2026-01-28 19:48:41,917 | INFO | best hypo: ▁alors▁ton▁pois▁les▁deux▁la▁place▁notre▁dame▁à▁la▁nef▁chavant

2026-01-28 19:48:41,920 | INFO | speech length: 152640
2026-01-28 19:48:41,956 | INFO | decoder input length: 238
2026-01-28 19:48:41,956 | INFO | max output length: 238
2026-01-28 19:48:41,956 | INFO | min output length: 23
2026-01-28 19:48:47,048 | INFO | end detected at 81
2026-01-28 19:48:47,049 | INFO | -18.28 * 0.5 =  -9.14 for decoder
2026-01-28 19:48:47,049 | INFO | -12.44 * 0.5 =  -6.22 for ctc
2026-01-28 19:48:47,050 | INFO | total log probability: -15.36
2026-01-28 19:48:47,050 | INFO | normalized log probability: -0.21
2026-01-28 19:48:47,050 | INFO | total number of ended hypotheses: 183
2026-01-28 19:48:47,051 | INFO | best hypo: ▁je▁prolonge▁la▁place▁notre▁dame▁par▁la▁place▁sainte▁claire▁je▁continue▁tout▁droit▁à▁évoluer▁stendhal▁et▁je▁tourne▁dans▁la▁rue▁du▁nom▁d'un▁géographe▁je▁me▁souviens▁plus▁lequel

2026-01-28 19:48:47,053 | INFO | speech length: 79840
2026-01-28 19:48:47,107 | INFO | decoder input length: 124
2026-01-28 19:48:47,107 | INFO | max output length: 124
2026-01-28 19:48:47,107 | INFO | min output length: 12
2026-01-28 19:48:49,395 | INFO | end detected at 46
2026-01-28 19:48:49,396 | INFO |  -6.72 * 0.5 =  -3.36 for decoder
2026-01-28 19:48:49,397 | INFO |  -2.76 * 0.5 =  -1.38 for ctc
2026-01-28 19:48:49,397 | INFO | total log probability: -4.74
2026-01-28 19:48:49,397 | INFO | normalized log probability: -0.12
2026-01-28 19:48:49,397 | INFO | total number of ended hypotheses: 169
2026-01-28 19:48:49,397 | INFO | best hypo: ▁et▁je▁passe▁sous▁le▁lycée▁stendhal▁un▁petit▁passage▁qui▁me▁mène▁vers▁le▁square▁des▁pots

2026-01-28 19:48:49,399 | INFO | speech length: 165440
2026-01-28 19:48:49,471 | INFO | decoder input length: 258
2026-01-28 19:48:49,471 | INFO | max output length: 258
2026-01-28 19:48:49,471 | INFO | min output length: 25
2026-01-28 19:48:55,311 | INFO | end detected at 86
2026-01-28 19:48:55,313 | INFO | -29.68 * 0.5 = -14.84 for decoder
2026-01-28 19:48:55,313 | INFO |  -9.21 * 0.5 =  -4.61 for ctc
2026-01-28 19:48:55,313 | INFO | total log probability: -19.44
2026-01-28 19:48:55,314 | INFO | normalized log probability: -0.25
2026-01-28 19:48:55,314 | INFO | total number of ended hypotheses: 208
2026-01-28 19:48:55,315 | INFO | best hypo: ▁le▁square▁des▁postes▁tu▁peux▁rejoindre▁la▁place▁de▁l'étoile▁et▁là▁i▁deux▁itinéraires▁je▁crois▁ou▁bien▁je▁prends▁la▁rue▁lesdiguières▁et▁le▁boulevard▁edouard▁re

2026-01-28 19:48:55,318 | INFO | speech length: 72960
2026-01-28 19:48:55,367 | INFO | decoder input length: 113
2026-01-28 19:48:55,367 | INFO | max output length: 113
2026-01-28 19:48:55,367 | INFO | min output length: 11
2026-01-28 19:48:57,353 | INFO | end detected at 35
2026-01-28 19:48:57,354 | INFO | -12.17 * 0.5 =  -6.08 for decoder
2026-01-28 19:48:57,355 | INFO | -12.41 * 0.5 =  -6.21 for ctc
2026-01-28 19:48:57,355 | INFO | total log probability: -12.29
2026-01-28 19:48:57,355 | INFO | normalized log probability: -0.42
2026-01-28 19:48:57,355 | INFO | total number of ended hypotheses: 178
2026-01-28 19:48:57,355 | INFO | best hypo: ▁et▁non▁sait▁plus▁elle▁doit▁resser▁et▁mene▁c'est▁figer▁la▁charge

2026-01-28 19:48:57,358 | INFO | speech length: 10400
2026-01-28 19:48:57,403 | INFO | decoder input length: 15
2026-01-28 19:48:57,403 | INFO | max output length: 15
2026-01-28 19:48:57,404 | INFO | min output length: 1
2026-01-28 19:48:57,977 | INFO | end detected at 13
2026-01-28 19:48:57,978 | INFO |  -2.23 * 0.5 =  -1.11 for decoder
2026-01-28 19:48:57,978 | INFO |  -0.68 * 0.5 =  -0.34 for ctc
2026-01-28 19:48:57,978 | INFO | total log probability: -1.45
2026-01-28 19:48:57,978 | INFO | normalized log probability: -0.18
2026-01-28 19:48:57,978 | INFO | total number of ended hypotheses: 154
2026-01-28 19:48:57,979 | INFO | best hypo: ▁non▁c'est▁même

2026-01-28 19:48:57,981 | INFO | speech length: 142720
2026-01-28 19:48:58,031 | INFO | decoder input length: 222
2026-01-28 19:48:58,031 | INFO | max output length: 222
2026-01-28 19:48:58,032 | INFO | min output length: 22
2026-01-28 19:49:02,811 | INFO | end detected at 66
2026-01-28 19:49:02,813 | INFO | -22.00 * 0.5 = -11.00 for decoder
2026-01-28 19:49:02,813 | INFO | -27.70 * 0.5 = -13.85 for ctc
2026-01-28 19:49:02,813 | INFO | total log probability: -24.85
2026-01-28 19:49:02,813 | INFO | normalized log probability: -0.42
2026-01-28 19:49:02,813 | INFO | total number of ended hypotheses: 188
2026-01-28 19:49:02,814 | INFO | best hypo: ▁je▁m'en▁bouge▁ou▁à▁l'arrière▁petite▁rue▁mais▁dans▁je▁ne▁sais▁pas▁le▁non▁une▁petite▁rue▁en▁qui▁tourne▁un▁peu▁et▁je▁j'arrive▁je▁tombe▁sur▁la▁nef▁chavan

2026-01-28 19:49:02,822 | INFO | Chunk: 0 | WER=30.769231 | S=4 D=0 I=0
2026-01-28 19:49:02,824 | INFO | Chunk: 1 | WER=13.888889 | S=3 D=2 I=0
2026-01-28 19:49:02,825 | INFO | Chunk: 2 | WER=16.666667 | S=3 D=0 I=0
2026-01-28 19:49:02,826 | INFO | Chunk: 3 | WER=18.750000 | S=5 D=1 I=0
2026-01-28 19:49:02,826 | INFO | Chunk: 4 | WER=64.705882 | S=8 D=3 I=0
2026-01-28 19:49:02,827 | INFO | Chunk: 5 | WER=20.000000 | S=0 D=1 I=0
2026-01-28 19:49:02,828 | INFO | Chunk: 6 | WER=30.555556 | S=5 D=3 I=3
2026-01-28 19:49:02,848 | INFO | File: Rhap-M0011.wav | WER=26.114650 | S=28 D=10 I=3
2026-01-28 19:49:02,848 | INFO | ------------------------------
2026-01-28 19:49:02,848 | INFO | Conf cv Done!
2026-01-28 19:49:03,056 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 19:49:03,085 | INFO | Vocabulary size: 47
2026-01-28 19:49:04,038 | INFO | Gradient checkpoint layers: []
2026-01-28 19:49:04,687 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:49:04,691 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:49:04,692 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:49:04,692 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 19:49:04,695 | INFO | speech length: 66560
2026-01-28 19:49:04,734 | INFO | decoder input length: 103
2026-01-28 19:49:04,734 | INFO | max output length: 103
2026-01-28 19:49:04,734 | INFO | min output length: 10
2026-01-28 19:49:07,739 | INFO | end detected at 73
2026-01-28 19:49:07,741 | INFO |  -6.76 * 0.5 =  -3.38 for decoder
2026-01-28 19:49:07,741 | INFO |  -5.17 * 0.5 =  -2.58 for ctc
2026-01-28 19:49:07,741 | INFO | total log probability: -5.96
2026-01-28 19:49:07,741 | INFO | normalized log probability: -0.09
2026-01-28 19:49:07,741 | INFO | total number of ended hypotheses: 200
2026-01-28 19:49:07,742 | INFO | best hypo: alors<space>quand<space>pour<space>aller<space>deux<space>la<space>place<space>notre<space>dame<space>à<space>la<space>nef<space>chavant

2026-01-28 19:49:07,745 | INFO | speech length: 152640
2026-01-28 19:49:07,782 | INFO | decoder input length: 238
2026-01-28 19:49:07,783 | INFO | max output length: 238
2026-01-28 19:49:07,783 | INFO | min output length: 23
2026-01-28 19:49:18,808 | INFO | end detected at 193
2026-01-28 19:49:18,810 | INFO | -25.03 * 0.5 = -12.51 for decoder
2026-01-28 19:49:18,810 | INFO | -12.51 * 0.5 =  -6.25 for ctc
2026-01-28 19:49:18,810 | INFO | total log probability: -18.77
2026-01-28 19:49:18,810 | INFO | normalized log probability: -0.10
2026-01-28 19:49:18,810 | INFO | total number of ended hypotheses: 194
2026-01-28 19:49:18,813 | INFO | best hypo: je<space>prolonge<space>la<space>place<space>notre<space>dame<space>par<space>la<space>place<space>sainte<space>claire<space>je<space>continue<space>le<space>droit<space>j'arrivolise<space>standale<space>mais<space>je<space>tourne<space>dans<space>la<space>rue<space>du<space>nom<space>d'un<space>géographe<space>je<space>me<space>souviens<space>plus<space>lequel

2026-01-28 19:49:18,815 | INFO | speech length: 79840
2026-01-28 19:49:18,851 | INFO | decoder input length: 124
2026-01-28 19:49:18,851 | INFO | max output length: 124
2026-01-28 19:49:18,851 | INFO | min output length: 12
2026-01-28 19:49:22,894 | INFO | end detected at 95
2026-01-28 19:49:22,895 | INFO | -11.43 * 0.5 =  -5.71 for decoder
2026-01-28 19:49:22,895 | INFO |  -5.24 * 0.5 =  -2.62 for ctc
2026-01-28 19:49:22,895 | INFO | total log probability: -8.33
2026-01-28 19:49:22,895 | INFO | normalized log probability: -0.09
2026-01-28 19:49:22,895 | INFO | total number of ended hypotheses: 162
2026-01-28 19:49:22,897 | INFO | best hypo: et<space>je<space>passe<space>sous<space>l'élycée<space>standald<space>un<space>petit<space>passage<space>qui<space>me<space>mène<space>vers<space>le<space>square<space>des<space>postes

2026-01-28 19:49:22,898 | INFO | speech length: 165440
2026-01-28 19:49:22,940 | INFO | decoder input length: 258
2026-01-28 19:49:22,940 | INFO | max output length: 258
2026-01-28 19:49:22,940 | INFO | min output length: 25
2026-01-28 19:49:34,165 | INFO | end detected at 169
2026-01-28 19:49:34,167 | INFO | -17.91 * 0.5 =  -8.96 for decoder
2026-01-28 19:49:34,167 | INFO | -12.37 * 0.5 =  -6.18 for ctc
2026-01-28 19:49:34,167 | INFO | total log probability: -15.14
2026-01-28 19:49:34,167 | INFO | normalized log probability: -0.09
2026-01-28 19:49:34,168 | INFO | total number of ended hypotheses: 212
2026-01-28 19:49:34,170 | INFO | best hypo: je<space>cors<space>des<space>postes<space>je<space>peux<space>rejoindre<space>la<space>place<space>de<space>l'étoile<space>et<space>là<space>j'ai<space>deux<space>itinéraires<space>je<space>crois<space>ou<space>bien<space>je<space>prends<space>la<space>rue<space>le<space>diguière<space>et<space>le<space>boulevard<space>edouard<space>khey

2026-01-28 19:49:34,174 | INFO | speech length: 72960
2026-01-28 19:49:34,217 | INFO | decoder input length: 113
2026-01-28 19:49:34,217 | INFO | max output length: 113
2026-01-28 19:49:34,217 | INFO | min output length: 11
2026-01-28 19:49:37,266 | INFO | end detected at 71
2026-01-28 19:49:37,267 | INFO | -15.39 * 0.5 =  -7.69 for decoder
2026-01-28 19:49:37,267 | INFO | -11.43 * 0.5 =  -5.71 for ctc
2026-01-28 19:49:37,267 | INFO | total log probability: -13.41
2026-01-28 19:49:37,268 | INFO | normalized log probability: -0.22
2026-01-28 19:49:37,268 | INFO | total number of ended hypotheses: 183
2026-01-28 19:49:37,269 | INFO | best hypo: et<space>non<space>c'est<space>plus<space>doire<space>c'est<space>et<space>me<space>c'est<space>filiser<space>la<space>charge

2026-01-28 19:49:37,271 | INFO | speech length: 10400
2026-01-28 19:49:37,314 | INFO | decoder input length: 15
2026-01-28 19:49:37,314 | INFO | max output length: 15
2026-01-28 19:49:37,314 | INFO | min output length: 1
2026-01-28 19:49:37,784 | INFO | adding <eos> in the last position in the loop
2026-01-28 19:49:37,791 | INFO | no hypothesis. Finish decoding.
2026-01-28 19:49:37,792 | INFO |  -1.26 * 0.5 =  -0.63 for decoder
2026-01-28 19:49:37,792 | INFO |  -0.34 * 0.5 =  -0.17 for ctc
2026-01-28 19:49:37,792 | INFO | total log probability: -0.80
2026-01-28 19:49:37,792 | INFO | normalized log probability: -0.05
2026-01-28 19:49:37,792 | INFO | total number of ended hypotheses: 55
2026-01-28 19:49:37,792 | INFO | best hypo: non<space>c'est<space>même<sos/eos>

2026-01-28 19:49:37,792 | WARNING | best hypo length: 15 == max output length: 15
2026-01-28 19:49:37,792 | WARNING | decoding may be stopped by the max output length limitation, please consider to increase the maxlenratio.
2026-01-28 19:49:37,793 | INFO | speech length: 142720
2026-01-28 19:49:37,841 | INFO | decoder input length: 222
2026-01-28 19:49:37,842 | INFO | max output length: 222
2026-01-28 19:49:37,842 | INFO | min output length: 22
2026-01-28 19:49:52,692 | INFO | end detected at 163
2026-01-28 19:49:52,695 | INFO | -18.82 * 0.5 =  -9.41 for decoder
2026-01-28 19:49:52,696 | INFO | -10.35 * 0.5 =  -5.18 for ctc
2026-01-28 19:49:52,696 | INFO | total log probability: -14.59
2026-01-28 19:49:52,696 | INFO | normalized log probability: -0.09
2026-01-28 19:49:52,696 | INFO | total number of ended hypotheses: 205
2026-01-28 19:49:52,698 | INFO | best hypo: je<space>m'en<space>bouge<space>ou<space>alors<space>y<space>a<space>une<space>petite<space>rue<space>mais<space>dont<space>je<space>ne<space>sais<space>pas<space>le<space>nom<space>une<space>petite<space>rue<space>en<space>qui<space>tourne<space>un<space>peu<space>et<space>je<space>j'arrive<space>je<space>tombe<space>sur<space>la<space>neuf<space>chavant

2026-01-28 19:49:52,706 | INFO | Chunk: 0 | WER=15.384615 | S=2 D=0 I=0
2026-01-28 19:49:52,707 | INFO | Chunk: 1 | WER=16.666667 | S=4 D=2 I=0
2026-01-28 19:49:52,708 | INFO | Chunk: 2 | WER=27.777778 | S=5 D=0 I=0
2026-01-28 19:49:52,709 | INFO | Chunk: 3 | WER=15.625000 | S=4 D=0 I=1
2026-01-28 19:49:52,710 | INFO | Chunk: 4 | WER=47.058824 | S=4 D=3 I=1
2026-01-28 19:49:52,710 | INFO | Chunk: 5 | WER=40.000000 | S=1 D=0 I=1
2026-01-28 19:49:52,712 | INFO | Chunk: 6 | WER=13.888889 | S=2 D=1 I=2
2026-01-28 19:49:52,732 | INFO | File: Rhap-M0011.wav | WER=21.019108 | S=22 D=6 I=5
2026-01-28 19:49:52,732 | INFO | ------------------------------
2026-01-28 19:49:52,732 | INFO | Conf ester Done!
2026-01-28 19:50:48,694 | INFO | Chunk: 0 | WER=23.076923 | S=3 D=0 I=0
2026-01-28 19:50:48,696 | INFO | Chunk: 1 | WER=13.888889 | S=4 D=0 I=1
2026-01-28 19:50:48,697 | INFO | Chunk: 2 | WER=27.777778 | S=4 D=1 I=0
2026-01-28 19:50:48,698 | INFO | Chunk: 3 | WER=31.250000 | S=6 D=2 I=2
2026-01-28 19:50:48,699 | INFO | Chunk: 4 | WER=52.941176 | S=7 D=2 I=0
2026-01-28 19:50:48,699 | INFO | Chunk: 5 | WER=20.000000 | S=1 D=0 I=0
2026-01-28 19:50:48,701 | INFO | Chunk: 6 | WER=13.888889 | S=2 D=1 I=2
2026-01-28 19:50:48,721 | INFO | File: Rhap-M0011.wav | WER=24.203822 | S=27 D=6 I=5
2026-01-28 19:50:48,722 | INFO | ------------------------------
2026-01-28 19:50:48,722 | INFO | hmm_tdnn Done!
2026-01-28 19:50:48,950 | INFO | ==================================Rhap-M0012.wav=========================================
2026-01-28 19:50:49,149 | INFO | Using rVAD model
2026-01-28 19:50:50,249 | INFO | Chunk: 0 | WER=22.222222 | S=7 D=1 I=2
2026-01-28 19:50:50,256 | INFO | File: Rhap-M0012.wav | WER=22.222222 | S=7 D=1 I=2
2026-01-28 19:50:50,256 | INFO | ------------------------------
2026-01-28 19:50:50,257 | INFO | w2vec vad chunk Done!
2026-01-28 19:50:54,357 | INFO | Chunk: 0 | WER=48.888889 | S=7 D=13 I=2
2026-01-28 19:50:54,364 | INFO | File: Rhap-M0012.wav | WER=48.888889 | S=7 D=13 I=2
2026-01-28 19:50:54,364 | INFO | ------------------------------
2026-01-28 19:50:54,364 | INFO | whisper med Done!
2026-01-28 19:50:59,944 | INFO | Chunk: 0 | WER=44.444444 | S=13 D=2 I=5
2026-01-28 19:50:59,948 | INFO | File: Rhap-M0012.wav | WER=44.444444 | S=13 D=2 I=5
2026-01-28 19:50:59,948 | INFO | ------------------------------
2026-01-28 19:50:59,948 | INFO | whisper large Done!
2026-01-28 19:51:00,124 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 19:51:00,162 | INFO | Vocabulary size: 350
2026-01-28 19:51:01,118 | INFO | Gradient checkpoint layers: []
2026-01-28 19:51:02,028 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:51:02,033 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:51:02,034 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:51:02,034 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 19:51:02,035 | INFO | speech length: 271840
2026-01-28 19:51:02,084 | INFO | decoder input length: 424
2026-01-28 19:51:02,084 | INFO | max output length: 424
2026-01-28 19:51:02,085 | INFO | min output length: 42
2026-01-28 19:51:13,206 | INFO | end detected at 129
2026-01-28 19:51:13,208 | INFO | -158.25 * 0.5 = -79.12 for decoder
2026-01-28 19:51:13,208 | INFO | -34.93 * 0.5 = -17.47 for ctc
2026-01-28 19:51:13,208 | INFO | total log probability: -96.59
2026-01-28 19:51:13,208 | INFO | normalized log probability: -0.79
2026-01-28 19:51:13,208 | INFO | total number of ended hypotheses: 158
2026-01-28 19:51:13,210 | INFO | best hypo: ▁non▁que▁j'irai▁à▁pied▁en▁longeant▁là▁donc▁jusqu'au▁cinéma▁neufchavan▁ensuite▁je▁continuerai▁jusqu'à▁la▁préfecture▁je▁couperais▁par▁la▁place▁grenette▁donc▁après▁je▁longerai▁le▁bouvard▁gambettache▁traverse▁donc▁le▁cours▁jean▁jaurès▁et▁j'arrive▁alse▁lorraine

2026-01-28 19:51:13,224 | INFO | Chunk: 0 | WER=26.666667 | S=8 D=2 I=2
2026-01-28 19:51:13,231 | INFO | File: Rhap-M0012.wav | WER=26.666667 | S=8 D=2 I=2
2026-01-28 19:51:13,231 | INFO | ------------------------------
2026-01-28 19:51:13,231 | INFO | Conf cv Done!
2026-01-28 19:51:13,603 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 19:51:13,633 | INFO | Vocabulary size: 47
2026-01-28 19:51:15,462 | INFO | Gradient checkpoint layers: []
2026-01-28 19:51:16,516 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:51:16,522 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:51:16,522 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:51:16,523 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 19:51:16,527 | INFO | speech length: 271840
2026-01-28 19:51:16,579 | INFO | decoder input length: 424
2026-01-28 19:51:16,579 | INFO | max output length: 424
2026-01-28 19:51:16,579 | INFO | min output length: 42
2026-01-28 19:51:41,565 | INFO | end detected at 294
2026-01-28 19:51:41,568 | INFO | -36.26 * 0.5 = -18.13 for decoder
2026-01-28 19:51:41,568 | INFO | -19.21 * 0.5 =  -9.61 for ctc
2026-01-28 19:51:41,568 | INFO | total log probability: -27.74
2026-01-28 19:51:41,568 | INFO | normalized log probability: -0.10
2026-01-28 19:51:41,568 | INFO | total number of ended hypotheses: 204
2026-01-28 19:51:41,573 | INFO | best hypo: donc<space>euh<space>j'irais<space>à<space>pied<space>euh<space>en<space>longeant<space>là<space>donc<space>euh<space>jusqu'au<space>cinéma<space>neuf<space>chavant<space>ensuite<space>euh<space>je<space>continuerai<space>jusqu'à<space>la<space>préfecture<space>je<space>couperais<space>par<space>la<space>place<space>grenette<space>donc<space>après<space>je<space>longerais<space>le<space>boulevard<space>euh<space>gambéta<space>je<space>traverse<space>donc<space>le<space>cours<space>jean<space>jaurès<space>et<space>j'arrive<space>euh<space>à<space>alsace<space>lorenne

2026-01-28 19:51:41,583 | INFO | Chunk: 0 | WER=24.444444 | S=3 D=0 I=8
2026-01-28 19:51:41,588 | INFO | File: Rhap-M0012.wav | WER=24.444444 | S=3 D=0 I=8
2026-01-28 19:51:41,588 | INFO | ------------------------------
2026-01-28 19:51:41,588 | INFO | Conf ester Done!
2026-01-28 19:51:54,712 | INFO | Chunk: 0 | WER=28.888889 | S=7 D=3 I=3
2026-01-28 19:51:54,717 | INFO | File: Rhap-M0012.wav | WER=28.888889 | S=7 D=3 I=3
2026-01-28 19:51:54,737 | INFO | ------------------------------
2026-01-28 19:51:54,737 | INFO | hmm_tdnn Done!
2026-01-28 19:51:55,050 | INFO | ==================================Rhap-M0013.wav=========================================
2026-01-28 19:51:55,230 | INFO | Using rVAD model
2026-01-28 19:51:58,311 | INFO | Chunk: 0 | WER=15.294118 | S=5 D=7 I=1
2026-01-28 19:51:58,314 | INFO | Chunk: 1 | WER=13.333333 | S=4 D=6 I=0
2026-01-28 19:51:58,329 | INFO | File: Rhap-M0013.wav | WER=14.375000 | S=9 D=13 I=1
2026-01-28 19:51:58,329 | INFO | ------------------------------
2026-01-28 19:51:58,329 | INFO | w2vec vad chunk Done!
2026-01-28 19:52:02,908 | INFO | Chunk: 0 | WER=72.941176 | S=2 D=60 I=0
2026-01-28 19:52:02,911 | INFO | Chunk: 1 | WER=45.333333 | S=2 D=31 I=1
2026-01-28 19:52:02,920 | INFO | File: Rhap-M0013.wav | WER=60.000000 | S=4 D=91 I=1
2026-01-28 19:52:02,920 | INFO | ------------------------------
2026-01-28 19:52:02,920 | INFO | whisper med Done!
2026-01-28 19:52:10,531 | INFO | Chunk: 0 | WER=49.411765 | S=5 D=37 I=0
2026-01-28 19:52:10,534 | INFO | Chunk: 1 | WER=41.333333 | S=3 D=27 I=1
2026-01-28 19:52:10,545 | INFO | File: Rhap-M0013.wav | WER=45.625000 | S=8 D=64 I=1
2026-01-28 19:52:10,545 | INFO | ------------------------------
2026-01-28 19:52:10,545 | INFO | whisper large Done!
2026-01-28 19:52:10,711 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 19:52:10,750 | INFO | Vocabulary size: 350
2026-01-28 19:52:11,779 | INFO | Gradient checkpoint layers: []
2026-01-28 19:52:12,500 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:52:12,504 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:52:12,505 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:52:12,505 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 19:52:12,506 | INFO | speech length: 441600
2026-01-28 19:52:12,555 | INFO | decoder input length: 689
2026-01-28 19:52:12,555 | INFO | max output length: 689
2026-01-28 19:52:12,555 | INFO | min output length: 68
2026-01-28 19:52:33,196 | INFO | end detected at 169
2026-01-28 19:52:33,197 | INFO | -418.92 * 0.5 = -209.46 for decoder
2026-01-28 19:52:33,197 | INFO | -88.67 * 0.5 = -44.34 for ctc
2026-01-28 19:52:33,197 | INFO | total log probability: -253.80
2026-01-28 19:52:33,197 | INFO | normalized log probability: -1.56
2026-01-28 19:52:33,197 | INFO | total number of ended hypotheses: 134
2026-01-28 19:52:33,200 | INFO | best hypo: ▁me▁leure▁à▁pied▁même▁pour▁aller▁à▁la▁gare▁vous▁allez▁vous▁éloigner▁du▁tram▁en▁longeant▁le▁cinéma▁et▁donc▁vous▁suivez▁cette▁avenue▁tout▁le▁long▁elle▁fait▁un▁petit▁coup▁d'un▁moment▁verrez▁et▁donc▁que▁vous▁passe▁à▁côté▁de▁la▁poste▁etce▁et▁au▁bout▁d'un▁emple▁mais▁elle▁est▁arrivé▁sur▁une▁grande▁place▁avec▁une▁fontame▁aux▁milieu▁c'est▁la▁place▁victor▁hgo▁etcque▁vous▁rencontrez▁à▁son▁moment▁le▁trame

2026-01-28 19:52:33,203 | INFO | speech length: 381280
2026-01-28 19:52:33,254 | INFO | decoder input length: 595
2026-01-28 19:52:33,254 | INFO | max output length: 595
2026-01-28 19:52:33,255 | INFO | min output length: 59
2026-01-28 19:52:51,765 | INFO | end detected at 159
2026-01-28 19:52:51,766 | INFO | -443.24 * 0.5 = -221.62 for decoder
2026-01-28 19:52:51,766 | INFO | -117.81 * 0.5 = -58.91 for ctc
2026-01-28 19:52:51,766 | INFO | total log probability: -280.53
2026-01-28 19:52:51,766 | INFO | normalized log probability: -1.81
2026-01-28 19:52:51,766 | INFO | total number of ended hypotheses: 137
2026-01-28 19:52:51,769 | INFO | best hypo: ▁que▁vous▁pouvez▁longer▁que▁vous▁pouvez▁suivre▁en▁prenant▁à▁gauche▁et▁à▁ce▁moment▁là▁si▁vous▁si▁vous▁suivez▁les▁rails▁vous▁allez▁arriver▁à▁peu▁près▁je▁ne▁saisz▁pas▁combien▁de▁temps▁peu▁près▁d'êt▁être▁cinq▁cents▁être▁moi▁à▁petit▁kilomètres▁plus▁loin▁vous▁allez▁arriver▁à▁la▁gare▁c'esti▁un▁peu▁bâtiment▁vous▁verrez▁ce▁sans▁po▁problème▁vous▁renaissez▁déjà▁un▁peu

2026-01-28 19:52:51,782 | INFO | Chunk: 0 | WER=31.764706 | S=16 D=7 I=4
2026-01-28 19:52:51,787 | INFO | Chunk: 1 | WER=22.666667 | S=9 D=5 I=3
2026-01-28 19:52:51,808 | INFO | File: Rhap-M0013.wav | WER=27.500000 | S=25 D=12 I=7
2026-01-28 19:52:51,808 | INFO | ------------------------------
2026-01-28 19:52:51,808 | INFO | Conf cv Done!
2026-01-28 19:52:52,014 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 19:52:52,043 | INFO | Vocabulary size: 47
2026-01-28 19:52:53,088 | INFO | Gradient checkpoint layers: []
2026-01-28 19:52:53,848 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:52:53,853 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:52:53,853 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:52:53,853 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 19:52:53,856 | INFO | speech length: 441600
2026-01-28 19:52:53,898 | INFO | decoder input length: 689
2026-01-28 19:52:53,898 | INFO | max output length: 689
2026-01-28 19:52:53,898 | INFO | min output length: 68
2026-01-28 19:53:43,851 | INFO | end detected at 490
2026-01-28 19:53:43,852 | INFO | -427.90 * 0.5 = -213.95 for decoder
2026-01-28 19:53:43,853 | INFO | -76.80 * 0.5 = -38.40 for ctc
2026-01-28 19:53:43,853 | INFO | total log probability: -252.35
2026-01-28 19:53:43,853 | INFO | normalized log probability: -0.52
2026-01-28 19:53:43,853 | INFO | total number of ended hypotheses: 182
2026-01-28 19:53:43,859 | INFO | best hypo: alors<space>à<space>pied<space>ben<space>pour<space>euh<space>aller<space>à<space>la<space>gare<space>vous<space>allez<space>euh<space>vous<space>éloigner<space>du<space>drame<space>en<space>en<space>logeant<space>euh<space>la<space>le<space>cinéma<space>et<space>pour<space>cent<space>hesitation<space>donc<space>vous<space>suivez<space>cet<space>avenue<space>pour<space>cent<space>hesitation<space>tout<space>long<space>elle<space>fait<space>un<space>petit<space>coup<space>d'un<space>moment<space>vous<space>verrez<space>et<space>pour<space>cent<space>hesitation<space>donc<space>vous<space>passez<space>à<space>côté<space>de<space>la<space>poste<space>etc<space>et<space>au<space>bout<space>d'attendre<space>vous<space>allez<space>arriver<space>sur<space>une<space>grande<space>place<space>avec<space>une<space>fontaine<space>au<space>milieu<space>c'est<space>la<space>place<space>victor<space>heugo<space>et<space>euh<space>donc<space>eun<space>vous<space>roncontrez<space>à<space>ce<space>ment<space>là<space>le<space>le<space>drame

2026-01-28 19:53:43,861 | INFO | speech length: 381280
2026-01-28 19:53:43,901 | INFO | decoder input length: 595
2026-01-28 19:53:43,901 | INFO | max output length: 595
2026-01-28 19:53:43,901 | INFO | min output length: 59
2026-01-28 19:54:23,070 | INFO | end detected at 441
2026-01-28 19:54:23,071 | INFO | -185.05 * 0.5 = -92.52 for decoder
2026-01-28 19:54:23,071 | INFO | -33.30 * 0.5 = -16.65 for ctc
2026-01-28 19:54:23,071 | INFO | total log probability: -109.17
2026-01-28 19:54:23,071 | INFO | normalized log probability: -0.25
2026-01-28 19:54:23,071 | INFO | total number of ended hypotheses: 169
2026-01-28 19:54:23,077 | INFO | best hypo: que<space>vous<space>pouvez<space>longer<space>que<space>vous<space>pouvez<space>suivre<space>euh<space>en<space>prenant<space>à<space>gauche<space>et<space>à<space>ce<space>moment<space>là<space>si<space>vous<space>le<space>si<space>vous<space>suivez<space>les<space>rails<space>vous<space>allez<space>arriver<space>pour<space>cent<space>hesitation<space>à<space>peu<space>près<space>pour<space>cent<space>hesitation<space>je<space>sais<space>pas<space>combien<space>de<space>temps<space>à<space>peu<space>près<space>pour<space>ent<space>hesitation<space>d'être<space>cinq<space>cent<space>peut<space>être<space>moins<space>un<space>petit<space>kilomètre<space>plus<space>loin<space>vous<space>allez<space>arriver<space>à<space>la<space>gare<space>c'est<space>un<space>grand<space>bâtiment<space>vous<space>le<space>verrez<space>sans<space>problème<space>vous<space>connaissez<space>déjà<space>un<space>peu<space>ou

2026-01-28 19:54:23,087 | INFO | Chunk: 0 | WER=36.470588 | S=15 D=2 I=14
2026-01-28 19:54:23,091 | INFO | Chunk: 1 | WER=17.333333 | S=4 D=0 I=9
2026-01-28 19:54:23,108 | INFO | File: Rhap-M0013.wav | WER=27.500000 | S=19 D=2 I=23
2026-01-28 19:54:23,108 | INFO | ------------------------------
2026-01-28 19:54:23,108 | INFO | Conf ester Done!
2026-01-28 19:54:58,637 | INFO | Chunk: 0 | WER=18.823529 | S=10 D=3 I=3
2026-01-28 19:54:58,645 | INFO | Chunk: 1 | WER=26.666667 | S=16 D=2 I=2
2026-01-28 19:54:58,679 | INFO | File: Rhap-M0013.wav | WER=22.500000 | S=26 D=5 I=5
2026-01-28 19:54:58,679 | INFO | ------------------------------
2026-01-28 19:54:58,679 | INFO | hmm_tdnn Done!
2026-01-28 19:54:58,979 | INFO | ==================================Rhap-M0014.wav=========================================
2026-01-28 19:54:59,247 | INFO | Using rVAD model
2026-01-28 19:55:01,880 | INFO | Chunk: 0 | WER=29.032258 | S=1 D=4 I=4
2026-01-28 19:55:01,882 | INFO | Chunk: 1 | WER=40.909091 | S=8 D=6 I=4
2026-01-28 19:55:01,890 | INFO | File: Rhap-M0014.wav | WER=28.000000 | S=9 D=7 I=5
2026-01-28 19:55:01,890 | INFO | ------------------------------
2026-01-28 19:55:01,890 | INFO | w2vec vad chunk Done!
2026-01-28 19:55:05,947 | INFO | Chunk: 0 | WER=16.129032 | S=1 D=0 I=4
2026-01-28 19:55:05,948 | INFO | Chunk: 1 | WER=52.272727 | S=1 D=22 I=0
2026-01-28 19:55:05,955 | INFO | File: Rhap-M0014.wav | WER=29.333333 | S=2 D=19 I=1
2026-01-28 19:55:05,955 | INFO | ------------------------------
2026-01-28 19:55:05,955 | INFO | whisper med Done!
2026-01-28 19:55:12,464 | INFO | Chunk: 0 | WER=32.258065 | S=3 D=2 I=5
2026-01-28 19:55:12,466 | INFO | Chunk: 1 | WER=38.636364 | S=3 D=6 I=8
2026-01-28 19:55:12,472 | INFO | File: Rhap-M0014.wav | WER=28.000000 | S=6 D=5 I=10
2026-01-28 19:55:12,472 | INFO | ------------------------------
2026-01-28 19:55:12,472 | INFO | whisper large Done!
2026-01-28 19:55:12,679 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 19:55:12,718 | INFO | Vocabulary size: 350
2026-01-28 19:55:14,008 | INFO | Gradient checkpoint layers: []
2026-01-28 19:55:14,881 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:55:14,887 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:55:14,887 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:55:14,888 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 19:55:14,888 | INFO | speech length: 185760
2026-01-28 19:55:14,945 | INFO | decoder input length: 289
2026-01-28 19:55:14,945 | INFO | max output length: 289
2026-01-28 19:55:14,945 | INFO | min output length: 28
2026-01-28 19:55:21,236 | INFO | end detected at 81
2026-01-28 19:55:21,237 | INFO | -53.94 * 0.5 = -26.97 for decoder
2026-01-28 19:55:21,238 | INFO | -25.29 * 0.5 = -12.65 for ctc
2026-01-28 19:55:21,238 | INFO | total log probability: -39.62
2026-01-28 19:55:21,238 | INFO | normalized log probability: -0.53
2026-01-28 19:55:21,238 | INFO | total number of ended hypotheses: 160
2026-01-28 19:55:21,239 | INFO | best hypo: ▁alors▁en▁partant▁de▁la▁place▁paul▁vally▁pour▁la▁place▁notre▁dame▁alors▁champrunte▁la▁rue▁de▁strasbourg▁je▁passe▁par▁la▁place▁vaucançon▁je▁prends▁en▁direction▁de▁maison▁du▁tourisme

2026-01-28 19:55:21,244 | INFO | speech length: 221600
2026-01-28 19:55:21,297 | INFO | decoder input length: 345
2026-01-28 19:55:21,297 | INFO | max output length: 345
2026-01-28 19:55:21,297 | INFO | min output length: 34
2026-01-28 19:55:28,809 | INFO | end detected at 99
2026-01-28 19:55:28,811 | INFO | -92.67 * 0.5 = -46.33 for decoder
2026-01-28 19:55:28,811 | INFO | -40.50 * 0.5 = -20.25 for ctc
2026-01-28 19:55:28,811 | INFO | total log probability: -66.58
2026-01-28 19:55:28,811 | INFO | normalized log probability: -0.71
2026-01-28 19:55:28,811 | INFO | total number of ended hypotheses: 164
2026-01-28 19:55:28,813 | INFO | best hypo: ▁à▁la▁maison▁du▁tourisme▁je▁compte▁tourne▁laval▁je▁prends▁la▁rue▁de▁la▁république▁en▁remontant▁la▁rue▁de▁la▁république▁je▁tombe▁sur▁la▁place▁sainte▁clerc▁on▁va▁à▁dire▁la▁rue▁à▁la▁halle▁et▁j'arriver▁à▁la▁place▁notre▁dame

2026-01-28 19:55:28,823 | INFO | Chunk: 0 | WER=35.483871 | S=3 D=3 I=5
2026-01-28 19:55:28,825 | INFO | Chunk: 1 | WER=40.909091 | S=7 D=4 I=7
2026-01-28 19:55:28,830 | INFO | File: Rhap-M0014.wav | WER=30.666667 | S=10 D=4 I=9
2026-01-28 19:55:28,831 | INFO | ------------------------------
2026-01-28 19:55:28,831 | INFO | Conf cv Done!
2026-01-28 19:55:29,081 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 19:55:29,104 | INFO | Vocabulary size: 47
2026-01-28 19:55:29,976 | INFO | Gradient checkpoint layers: []
2026-01-28 19:55:30,707 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:55:30,711 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:55:30,712 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:55:30,712 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 19:55:30,715 | INFO | speech length: 185760
2026-01-28 19:55:30,760 | INFO | decoder input length: 289
2026-01-28 19:55:30,760 | INFO | max output length: 289
2026-01-28 19:55:30,760 | INFO | min output length: 28
2026-01-28 19:55:42,328 | INFO | end detected at 191
2026-01-28 19:55:42,330 | INFO | -17.38 * 0.5 =  -8.69 for decoder
2026-01-28 19:55:42,330 | INFO | -21.53 * 0.5 = -10.77 for ctc
2026-01-28 19:55:42,330 | INFO | total log probability: -19.46
2026-01-28 19:55:42,330 | INFO | normalized log probability: -0.11
2026-01-28 19:55:42,331 | INFO | total number of ended hypotheses: 211
2026-01-28 19:55:42,333 | INFO | best hypo: alors<space>en<space>partant<space>de<space>la<space>place<space>paul<space>vailly<space>pour<space>aller<space>à<space>place<space>notre<space>dame<space>alors<space>j'emprunte<space>par<space>une<space>strasbourg<space>je<space>passe<space>par<space>la<space>place<space>vos<space>cansons<space>je<space>prends<space>direction<space>de<space>maison<space>du<space>tourisme

2026-01-28 19:55:42,336 | INFO | speech length: 221600
2026-01-28 19:55:42,374 | INFO | decoder input length: 345
2026-01-28 19:55:42,374 | INFO | max output length: 345
2026-01-28 19:55:42,374 | INFO | min output length: 34
2026-01-28 19:55:56,932 | INFO | end detected at 223
2026-01-28 19:55:56,935 | INFO | -21.65 * 0.5 = -10.82 for decoder
2026-01-28 19:55:56,935 | INFO | -13.20 * 0.5 =  -6.60 for ctc
2026-01-28 19:55:56,935 | INFO | total log probability: -17.43
2026-01-28 19:55:56,935 | INFO | normalized log probability: -0.08
2026-01-28 19:55:56,935 | INFO | total number of ended hypotheses: 193
2026-01-28 19:55:56,938 | INFO | best hypo: à<space>la<space>maison<space>du<space>tourisme<space>je<space>contourne<space>enfin<space>je<space>prends<space>la<space>rue<space>de<space>la<space>république<space>en<space>remontant<space>la<space>rue<space>de<space>la<space>république<space>je<space>tombe<space>sur<space>la<space>place<space>cinq<space>claire<space>on<space>va<space>dire<space>là<space>où<space>y<space>a<space>la<space>halle<space>et<space>j'arrivais<space>à<space>la<space>place<space>notre<space>dame

2026-01-28 19:55:56,946 | INFO | Chunk: 0 | WER=35.483871 | S=4 D=2 I=5
2026-01-28 19:55:56,947 | INFO | Chunk: 1 | WER=36.363636 | S=2 D=6 I=8
2026-01-28 19:55:56,953 | INFO | File: Rhap-M0014.wav | WER=28.000000 | S=6 D=5 I=10
2026-01-28 19:55:56,953 | INFO | ------------------------------
2026-01-28 19:55:56,953 | INFO | Conf ester Done!
2026-01-28 19:56:18,772 | INFO | Chunk: 0 | WER=48.387097 | S=6 D=4 I=5
2026-01-28 19:56:18,774 | INFO | Chunk: 1 | WER=38.636364 | S=9 D=3 I=5
2026-01-28 19:56:18,780 | INFO | File: Rhap-M0014.wav | WER=34.666667 | S=15 D=4 I=7
2026-01-28 19:56:18,781 | INFO | ------------------------------
2026-01-28 19:56:18,781 | INFO | hmm_tdnn Done!
2026-01-28 19:56:19,002 | INFO | ==================================Rhap-M0015.wav=========================================
2026-01-28 19:56:19,199 | INFO | Using rVAD model
2026-01-28 19:56:21,042 | INFO | Chunk: 0 | WER=18.965517 | S=5 D=6 I=0
2026-01-28 19:56:21,043 | INFO | Chunk: 1 | WER=33.333333 | S=1 D=2 I=2
2026-01-28 19:56:21,048 | INFO | File: Rhap-M0015.wav | WER=21.917808 | S=6 D=8 I=2
2026-01-28 19:56:21,048 | INFO | ------------------------------
2026-01-28 19:56:21,048 | INFO | w2vec vad chunk Done!
2026-01-28 19:56:25,281 | INFO | Chunk: 0 | WER=43.103448 | S=7 D=18 I=0
2026-01-28 19:56:25,282 | INFO | Chunk: 1 | WER=13.333333 | S=2 D=0 I=0
2026-01-28 19:56:25,289 | INFO | File: Rhap-M0015.wav | WER=36.986301 | S=9 D=18 I=0
2026-01-28 19:56:25,289 | INFO | ------------------------------
2026-01-28 19:56:25,289 | INFO | whisper med Done!
2026-01-28 19:56:30,957 | INFO | Chunk: 0 | WER=39.655172 | S=9 D=14 I=0
2026-01-28 19:56:30,958 | INFO | Chunk: 1 | WER=40.000000 | S=3 D=1 I=2
2026-01-28 19:56:30,965 | INFO | File: Rhap-M0015.wav | WER=38.356164 | S=15 D=13 I=0
2026-01-28 19:56:30,965 | INFO | ------------------------------
2026-01-28 19:56:30,965 | INFO | whisper large Done!
2026-01-28 19:56:31,246 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 19:56:31,295 | INFO | Vocabulary size: 350
2026-01-28 19:56:32,304 | INFO | Gradient checkpoint layers: []
2026-01-28 19:56:33,009 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:56:33,014 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:56:33,014 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:56:33,015 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 19:56:33,015 | INFO | speech length: 349920
2026-01-28 19:56:33,067 | INFO | decoder input length: 546
2026-01-28 19:56:33,067 | INFO | max output length: 546
2026-01-28 19:56:33,067 | INFO | min output length: 54
2026-01-28 19:56:49,352 | INFO | end detected at 156
2026-01-28 19:56:49,354 | INFO | -212.65 * 0.5 = -106.33 for decoder
2026-01-28 19:56:49,354 | INFO | -93.09 * 0.5 = -46.55 for ctc
2026-01-28 19:56:49,354 | INFO | total log probability: -152.87
2026-01-28 19:56:49,354 | INFO | normalized log probability: -1.01
2026-01-28 19:56:49,354 | INFO | total number of ended hypotheses: 144
2026-01-28 19:56:49,356 | INFO | best hypo: ▁heu▁je▁d'ici▁du▁cc▁j▁je▁remonte▁la▁viande▁à▁lorenne▁jusqu'à▁la▁place▁victor▁hugo▁je▁traverse▁l'avenue▁vic▁la▁place▁victor▁hugo▁en▁diagonale▁et▁jusqu'à▁arrivé▁au▁boulevard▁gute▁samba▁je▁remonte▁le▁boulevard▁à▁gute▁samba▁et▁donc▁vers▁le▁sud▁à▁tonaia▁dire▁et▁déjà▁je▁passe▁devant▁la▁grand▁poste

2026-01-28 19:56:49,362 | INFO | speech length: 51360
2026-01-28 19:56:49,412 | INFO | decoder input length: 79
2026-01-28 19:56:49,412 | INFO | max output length: 79
2026-01-28 19:56:49,413 | INFO | min output length: 7
2026-01-28 19:56:51,273 | INFO | end detected at 43
2026-01-28 19:56:51,274 | INFO | -11.45 * 0.5 =  -5.72 for decoder
2026-01-28 19:56:51,274 | INFO | -12.62 * 0.5 =  -6.31 for ctc
2026-01-28 19:56:51,274 | INFO | total log probability: -12.03
2026-01-28 19:56:51,274 | INFO | normalized log probability: -0.35
2026-01-28 19:56:51,274 | INFO | total number of ended hypotheses: 181
2026-01-28 19:56:51,275 | INFO | best hypo: ▁après▁de▁passer▁devant▁ma▁grande▁poste▁et▁bien▁j'arrive▁au▁cinéma▁à▁neuf▁chars

2026-01-28 19:56:51,286 | INFO | Chunk: 0 | WER=39.655172 | S=16 D=2 I=5
2026-01-28 19:56:51,286 | INFO | Chunk: 1 | WER=60.000000 | S=6 D=1 I=2
2026-01-28 19:56:51,292 | INFO | File: Rhap-M0015.wav | WER=43.835616 | S=22 D=3 I=7
2026-01-28 19:56:51,292 | INFO | ------------------------------
2026-01-28 19:56:51,292 | INFO | Conf cv Done!
2026-01-28 19:56:51,542 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 19:56:51,566 | INFO | Vocabulary size: 47
2026-01-28 19:56:52,661 | INFO | Gradient checkpoint layers: []
2026-01-28 19:56:53,325 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:56:53,329 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:56:53,330 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:56:53,330 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 19:56:53,334 | INFO | speech length: 349920
2026-01-28 19:56:53,377 | INFO | decoder input length: 546
2026-01-28 19:56:53,377 | INFO | max output length: 546
2026-01-28 19:56:53,377 | INFO | min output length: 54
2026-01-28 19:57:20,716 | INFO | end detected at 303
2026-01-28 19:57:20,717 | INFO | -63.54 * 0.5 = -31.77 for decoder
2026-01-28 19:57:20,717 | INFO | -42.05 * 0.5 = -21.02 for ctc
2026-01-28 19:57:20,717 | INFO | total log probability: -52.79
2026-01-28 19:57:20,717 | INFO | normalized log probability: -0.18
2026-01-28 19:57:20,718 | INFO | total number of ended hypotheses: 178
2026-01-28 19:57:20,721 | INFO | best hypo: euh<space>je<space>d'ici<space>du<space>csij<space>je<space>remonte<space>la<space>violence<space>forelle<space>jusqu'à<space>la<space>place<space>victor<space>hugo<space>je<space>traverse<space>la<space>venue<space>vite<space>le<space>la<space>place<space>victor<space>hugo<space>en<space>diagonal<space>jusqu'à<space>arriver<space>au<space>au<space>boulevard<space>du<space>de<space>samba<space>je<space>remonte<space>le<space>boulevard<space>agoutte<space>samba<space>donc<space>vers<space>le<space>sud<space>on<space>va<space>dire<space>et<space>j'ar<space>je<space>passe<space>devant<space>la<space>grande<space>poste

2026-01-28 19:57:20,724 | INFO | speech length: 51360
2026-01-28 19:57:20,771 | INFO | decoder input length: 79
2026-01-28 19:57:20,771 | INFO | max output length: 79
2026-01-28 19:57:20,771 | INFO | min output length: 7
2026-01-28 19:57:24,092 | INFO | adding <eos> in the last position in the loop
2026-01-28 19:57:24,101 | INFO | no hypothesis. Finish decoding.
2026-01-28 19:57:24,102 | INFO | -28.54 * 0.5 = -14.27 for decoder
2026-01-28 19:57:24,102 | INFO | -56.77 * 0.5 = -28.38 for ctc
2026-01-28 19:57:24,102 | INFO | total log probability: -42.65
2026-01-28 19:57:24,102 | INFO | normalized log probability: -0.55
2026-01-28 19:57:24,102 | INFO | total number of ended hypotheses: 85
2026-01-28 19:57:24,103 | INFO | best hypo: et<space>après<space>de<space>pser<space>devant<space>la<space>grande<space>pote<space>eh<space>bien<space>j'arrive<space>au<space>cinéma<space>la<space>necher

2026-01-28 19:57:24,113 | INFO | Chunk: 0 | WER=27.586207 | S=12 D=1 I=3
2026-01-28 19:57:24,113 | INFO | Chunk: 1 | WER=46.666667 | S=4 D=1 I=2
2026-01-28 19:57:24,121 | INFO | File: Rhap-M0015.wav | WER=31.506849 | S=16 D=2 I=5
2026-01-28 19:57:24,121 | INFO | ------------------------------
2026-01-28 19:57:24,121 | INFO | Conf ester Done!
2026-01-28 19:57:46,841 | INFO | Chunk: 0 | WER=29.310345 | S=14 D=2 I=1
2026-01-28 19:57:46,841 | INFO | Chunk: 1 | WER=33.333333 | S=2 D=1 I=2
2026-01-28 19:57:46,849 | INFO | File: Rhap-M0015.wav | WER=30.136986 | S=16 D=3 I=3
2026-01-28 19:57:46,849 | INFO | ------------------------------
2026-01-28 19:57:46,849 | INFO | hmm_tdnn Done!
2026-01-28 19:57:47,023 | INFO | ==================================Rhap-M0016.wav=========================================
2026-01-28 19:57:47,191 | INFO | Using rVAD model
2026-01-28 19:57:52,476 | INFO | Chunk: 0 | WER=21.052632 | S=5 D=5 I=2
2026-01-28 19:57:52,479 | INFO | Chunk: 1 | WER=34.666667 | S=19 D=7 I=0
2026-01-28 19:57:52,483 | INFO | Chunk: 2 | WER=15.384615 | S=7 D=2 I=3
2026-01-28 19:57:52,484 | INFO | Chunk: 3 | WER=60.000000 | S=1 D=0 I=2
2026-01-28 19:57:52,509 | INFO | File: Rhap-M0016.wav | WER=24.651163 | S=32 D=14 I=7
2026-01-28 19:57:52,509 | INFO | ------------------------------
2026-01-28 19:57:52,509 | INFO | w2vec vad chunk Done!
2026-01-28 19:58:02,483 | INFO | Chunk: 0 | WER=38.596491 | S=15 D=4 I=3
2026-01-28 19:58:02,486 | INFO | Chunk: 1 | WER=60.000000 | S=30 D=13 I=2
2026-01-28 19:58:02,489 | INFO | Chunk: 2 | WER=44.871795 | S=9 D=25 I=1
2026-01-28 19:58:02,490 | INFO | Chunk: 3 | WER=60.000000 | S=1 D=0 I=2
2026-01-28 19:58:02,512 | INFO | File: Rhap-M0016.wav | WER=48.837209 | S=57 D=41 I=7
2026-01-28 19:58:02,512 | INFO | ------------------------------
2026-01-28 19:58:02,512 | INFO | whisper med Done!
2026-01-28 19:58:12,448 | INFO | Chunk: 0 | WER=33.333333 | S=3 D=16 I=0
2026-01-28 19:58:12,450 | INFO | Chunk: 1 | WER=57.333333 | S=1 D=41 I=1
2026-01-28 19:58:12,453 | INFO | Chunk: 2 | WER=51.282051 | S=8 D=27 I=5
2026-01-28 19:58:12,453 | INFO | Chunk: 3 | WER=60.000000 | S=1 D=0 I=2
2026-01-28 19:58:12,471 | INFO | File: Rhap-M0016.wav | WER=48.837209 | S=13 D=84 I=8
2026-01-28 19:58:12,471 | INFO | ------------------------------
2026-01-28 19:58:12,471 | INFO | whisper large Done!
2026-01-28 19:58:12,612 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 19:58:12,650 | INFO | Vocabulary size: 350
2026-01-28 19:58:13,567 | INFO | Gradient checkpoint layers: []
2026-01-28 19:58:14,244 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:58:14,248 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:58:14,248 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:58:14,249 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 19:58:14,249 | INFO | speech length: 426880
2026-01-28 19:58:14,303 | INFO | decoder input length: 666
2026-01-28 19:58:14,303 | INFO | max output length: 666
2026-01-28 19:58:14,303 | INFO | min output length: 66
2026-01-28 19:58:30,222 | INFO | end detected at 129
2026-01-28 19:58:30,224 | INFO | -272.51 * 0.5 = -136.25 for decoder
2026-01-28 19:58:30,224 | INFO | -70.67 * 0.5 = -35.33 for ctc
2026-01-28 19:58:30,224 | INFO | total log probability: -171.59
2026-01-28 19:58:30,224 | INFO | normalized log probability: -1.38
2026-01-28 19:58:30,224 | INFO | total number of ended hypotheses: 156
2026-01-28 19:58:30,226 | INFO | best hypo: ▁mon▁pour▁aller▁du▁serd'épée▁à▁la▁gare▁de▁grenoble▁je▁me▁sors▁déjà▁du▁cr▁d'épée▁je▁remonte▁l'avenue▁général▁champon▁je▁traverse▁face▁à▁la▁mdieh▁et▁je▁je▁continue▁je▁continue▁jusque'à▁une▁place▁'▁face▁de▁la▁grande▁de▁poste▁où▁la▁chambre▁de▁commerce▁et▁d'industrie

2026-01-28 19:58:30,229 | INFO | speech length: 395840
2026-01-28 19:58:30,276 | INFO | decoder input length: 618
2026-01-28 19:58:30,276 | INFO | max output length: 618
2026-01-28 19:58:30,276 | INFO | min output length: 61
2026-01-28 19:58:51,037 | INFO | end detected at 165
2026-01-28 19:58:51,039 | INFO | -393.79 * 0.5 = -196.89 for decoder
2026-01-28 19:58:51,039 | INFO | -111.68 * 0.5 = -55.84 for ctc
2026-01-28 19:58:51,039 | INFO | total log probability: -252.73
2026-01-28 19:58:51,039 | INFO | normalized log probability: -1.57
2026-01-28 19:58:51,039 | INFO | total number of ended hypotheses: 91
2026-01-28 19:58:51,042 | INFO | best hypo: ▁et▁je▁reprends▁je▁crois▁le▁non▁c'est▁pas▁le▁boulevard▁gambetta▁je▁ne▁sais▁pas▁le▁qu'elle▁sait▁c'est▁l'autre▁et▁donc▁je▁va▁jusqu'squ'à▁la▁place▁victor▁hugo▁et▁l'âge▁me▁retrouve▁en▁effetp▁près▁du▁ras▁du▁tram▁et▁de▁çail▁du▁tram▁et▁jer▁je▁l'ai▁longer▁je▁vai▁vers▁le▁boulevard▁ga▁métas▁cette▁fois▁c'est▁le▁boulevard▁ga▁betta

2026-01-28 19:58:51,045 | INFO | speech length: 441760
2026-01-28 19:58:51,175 | INFO | decoder input length: 689
2026-01-28 19:58:51,175 | INFO | max output length: 689
2026-01-28 19:58:51,175 | INFO | min output length: 68
2026-01-28 19:59:12,408 | INFO | end detected at 173
2026-01-28 19:59:12,410 | INFO | -476.37 * 0.5 = -238.19 for decoder
2026-01-28 19:59:12,410 | INFO | -128.41 * 0.5 = -64.21 for ctc
2026-01-28 19:59:12,410 | INFO | total log probability: -302.39
2026-01-28 19:59:12,410 | INFO | normalized log probability: -1.80
2026-01-28 19:59:12,410 | INFO | total number of ended hypotheses: 160
2026-01-28 19:59:12,412 | INFO | best hypo: ▁ensuite▁je▁vais▁prendre▁je▁froxer▁l'avenue▁alsace▁lorraine▁que▁je▁vais▁remonté▁remonter▁et▁je▁vais▁traverser▁le▁coursjean▁jaurès▁si▁je▁me▁souviens▁bien▁et▁je▁vais▁je▁vais▁toujours▁g▁continuer▁cette▁avenue▁alace▁de▁lorraine▁et▁e▁valà▁j'arrive▁du▁niveau▁de▁la▁grande▁place▁et▁de▁la▁gare▁bien▁à▁tous▁les▁trames▁tous▁les▁bus▁'on▁pas▁tous▁les▁bus▁qui▁sont▁pas▁de▁ce▁côtés▁là▁voilà

2026-01-28 19:59:12,415 | INFO | speech length: 36320
2026-01-28 19:59:12,469 | INFO | decoder input length: 56
2026-01-28 19:59:12,469 | INFO | max output length: 56
2026-01-28 19:59:12,469 | INFO | min output length: 5
2026-01-28 19:59:13,462 | INFO | end detected at 19
2026-01-28 19:59:13,464 | INFO |  -1.56 * 0.5 =  -0.78 for decoder
2026-01-28 19:59:13,464 | INFO |  -0.39 * 0.5 =  -0.20 for ctc
2026-01-28 19:59:13,464 | INFO | total log probability: -0.98
2026-01-28 19:59:13,464 | INFO | normalized log probability: -0.07
2026-01-28 19:59:13,464 | INFO | total number of ended hypotheses: 149
2026-01-28 19:59:13,464 | INFO | best hypo: ▁et▁je▁suis▁arrivé▁sur▁la▁gare

2026-01-28 19:59:13,472 | INFO | Chunk: 0 | WER=29.824561 | S=7 D=6 I=4
2026-01-28 19:59:13,476 | INFO | Chunk: 1 | WER=34.666667 | S=21 D=3 I=2
2026-01-28 19:59:13,480 | INFO | Chunk: 2 | WER=29.487179 | S=13 D=6 I=4
2026-01-28 19:59:13,480 | INFO | Chunk: 3 | WER=60.000000 | S=1 D=0 I=2
2026-01-28 19:59:13,505 | INFO | File: Rhap-M0016.wav | WER=32.093023 | S=42 D=15 I=12
2026-01-28 19:59:13,505 | INFO | ------------------------------
2026-01-28 19:59:13,505 | INFO | Conf cv Done!
2026-01-28 19:59:13,684 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 19:59:13,707 | INFO | Vocabulary size: 47
2026-01-28 19:59:14,887 | INFO | Gradient checkpoint layers: []
2026-01-28 19:59:15,675 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 19:59:15,681 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 19:59:15,681 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 19:59:15,681 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 19:59:15,684 | INFO | speech length: 426880
2026-01-28 19:59:15,742 | INFO | decoder input length: 666
2026-01-28 19:59:15,742 | INFO | max output length: 666
2026-01-28 19:59:15,742 | INFO | min output length: 66
2026-01-28 19:59:48,522 | INFO | end detected at 288
2026-01-28 19:59:48,524 | INFO | -70.49 * 0.5 = -35.25 for decoder
2026-01-28 19:59:48,524 | INFO |  -8.39 * 0.5 =  -4.20 for ctc
2026-01-28 19:59:48,524 | INFO | total log probability: -39.44
2026-01-28 19:59:48,524 | INFO | normalized log probability: -0.14
2026-01-28 19:59:48,524 | INFO | total number of ended hypotheses: 198
2026-01-28 19:59:48,528 | INFO | best hypo: mon<space>pour<space>aller<space>du<space>crdp<space>à<space>la<space>gare<space>euh<space>de<space>grenoble<space>je<space>euh<space>mais<space>je<space>sors<space>déjà<space>du<space>crdt<space>je<space>remonte<space>l'avenue<space>général<space>champon<space>je<space>traverse<space>euh<space>face<space>à<space>la<space>mnde<space>et<space>je<space>euh<space>je<space>continue<space>je<space>continue<space>jusqu'à<space>une<space>place<space>qui<space>est<space>face<space>à<space>la<space>grande<space>poste<space>où<space>il<space>y<space>a<space>la<space>chambre<space>de<space>commerce<space>et<space>d'industrie

2026-01-28 19:59:48,530 | INFO | speech length: 395840
2026-01-28 19:59:48,578 | INFO | decoder input length: 618
2026-01-28 19:59:48,578 | INFO | max output length: 618
2026-01-28 19:59:48,578 | INFO | min output length: 61
2026-01-28 20:00:25,261 | INFO | end detected at 386
2026-01-28 20:00:25,263 | INFO | -152.45 * 0.5 = -76.22 for decoder
2026-01-28 20:00:25,263 | INFO | -40.07 * 0.5 = -20.03 for ctc
2026-01-28 20:00:25,263 | INFO | total log probability: -96.26
2026-01-28 20:00:25,263 | INFO | normalized log probability: -0.25
2026-01-28 20:00:25,263 | INFO | total number of ended hypotheses: 196
2026-01-28 20:00:25,268 | INFO | best hypo: et<space>je<space>reprends<space>je<space>crois<space>le<space>non<space>c'est<space>pas<space>le<space>boulevard<space>grand<space>métal<space>je<space>ne<space>sais<space>pas<space>le<space>qu'elle<space>fait<space>euh<space>c'est<space>l'autre<space>et<space>euh<space>donc<space>je<space>vais<space>jusqu'au<space>jusqu'à<space>la<space>place<space>victor<space>hugo<space>et<space>là<space>je<space>me<space>retrouve<space>en<space>effet<space>euh<space>s<space>près<space>des<space>rails<space>du<space>drame<space>et<space>euh<space>de<space>ses<space>rails<space>de<space>traves<space>eh<space>ben<space>j'ai<space>j'ai<space>l'ai<space>longé<space>je<space>l'ai<space>traversé<space>euh<space>le<space>boulevard<space>en<space>métat<space>ça<space>cette<space>fois<space>c'est<space>le<space>boulevard<space>d'en<space>métat

2026-01-28 20:00:25,271 | INFO | speech length: 441760
2026-01-28 20:00:25,322 | INFO | decoder input length: 689
2026-01-28 20:00:25,322 | INFO | max output length: 689
2026-01-28 20:00:25,322 | INFO | min output length: 68
2026-01-28 20:01:11,498 | INFO | end detected at 402
2026-01-28 20:01:11,500 | INFO | -168.79 * 0.5 = -84.40 for decoder
2026-01-28 20:01:11,500 | INFO | -33.22 * 0.5 = -16.61 for ctc
2026-01-28 20:01:11,500 | INFO | total log probability: -101.01
2026-01-28 20:01:11,500 | INFO | normalized log probability: -0.25
2026-01-28 20:01:11,500 | INFO | total number of ended hypotheses: 160
2026-01-28 20:01:11,506 | INFO | best hypo: ensuite<space>euh<space>je<space>vais<space>euh<space>prendre<space>je<space>crois<space>que<space>c'est<space>l'avenir<space>de<space>la<space>floraine<space>que<space>je<space>vais<space>remonter<space>remonter<space>je<space>vais<space>traverser<space>le<space>courant<space>jaurès<space>et<space>je<space>me<space>souviens<space>bien<space>et<space>je<space>vais<space>euh<space>je<space>vais<space>toujours<space>continuer<space>cette<space>amie<space>alzas<space>lorenne<space>et<space>euh<space>eh<space>ben<space>voilà<space>j'arrive<space>au<space>niveau<space>de<space>la<space>grande<space>place<space>de<space>hagard<space>il<space>y<space>a<space>tous<space>les<space>tramains<space>tous<space>les<space>bus<space>n'ont<space>pas<space>tous<space>les<space>bus<space>qui<space>sont<space>pas<space>de<space>ce<space>côté<space>là<space>et<space>voilà

2026-01-28 20:01:11,509 | INFO | speech length: 36320
2026-01-28 20:01:11,562 | INFO | decoder input length: 56
2026-01-28 20:01:11,562 | INFO | max output length: 56
2026-01-28 20:01:11,562 | INFO | min output length: 5
2026-01-28 20:01:13,050 | INFO | end detected at 40
2026-01-28 20:01:13,052 | INFO |  -3.48 * 0.5 =  -1.74 for decoder
2026-01-28 20:01:13,053 | INFO |  -2.00 * 0.5 =  -1.00 for ctc
2026-01-28 20:01:13,053 | INFO | total log probability: -2.74
2026-01-28 20:01:13,053 | INFO | normalized log probability: -0.08
2026-01-28 20:01:13,053 | INFO | total number of ended hypotheses: 189
2026-01-28 20:01:13,053 | INFO | best hypo: et<space>je<space>suis<space>arrivé<space>un<space>surveillard

2026-01-28 20:01:13,062 | INFO | Chunk: 0 | WER=14.035088 | S=4 D=0 I=4
2026-01-28 20:01:13,066 | INFO | Chunk: 1 | WER=44.000000 | S=19 D=0 I=14
2026-01-28 20:01:13,070 | INFO | Chunk: 2 | WER=30.769231 | S=13 D=3 I=8
2026-01-28 20:01:13,071 | INFO | Chunk: 3 | WER=60.000000 | S=2 D=0 I=1
2026-01-28 20:01:13,099 | INFO | File: Rhap-M0016.wav | WER=31.627907 | S=38 D=3 I=27
2026-01-28 20:01:13,099 | INFO | ------------------------------
2026-01-28 20:01:13,099 | INFO | Conf ester Done!
2026-01-28 20:02:20,999 | INFO | Chunk: 0 | WER=21.052632 | S=6 D=3 I=3
2026-01-28 20:02:21,007 | INFO | Chunk: 1 | WER=22.666667 | S=8 D=9 I=0
2026-01-28 20:02:21,016 | INFO | Chunk: 2 | WER=24.358974 | S=12 D=2 I=5
2026-01-28 20:02:21,016 | INFO | Chunk: 3 | WER=60.000000 | S=1 D=0 I=2
2026-01-28 20:02:21,074 | INFO | File: Rhap-M0016.wav | WER=23.720930 | S=27 D=14 I=10
2026-01-28 20:02:21,074 | INFO | ------------------------------
2026-01-28 20:02:21,075 | INFO | hmm_tdnn Done!
2026-01-28 20:02:21,291 | INFO | ==================================Rhap-M0018.wav=========================================
2026-01-28 20:02:21,528 | INFO | Using rVAD model
2026-01-28 20:02:29,802 | INFO | Chunk: 0 | WER=27.272727 | S=2 D=0 I=1
2026-01-28 20:02:29,806 | INFO | Chunk: 1 | WER=5.194805 | S=2 D=2 I=0
2026-01-28 20:02:29,807 | INFO | Chunk: 2 | WER=21.875000 | S=5 D=2 I=0
2026-01-28 20:02:29,807 | INFO | Chunk: 3 | WER=10.000000 | S=0 D=0 I=1
2026-01-28 20:02:29,809 | INFO | Chunk: 4 | WER=26.086957 | S=9 D=2 I=1
2026-01-28 20:02:29,809 | INFO | Chunk: 5 | WER=16.666667 | S=2 D=0 I=0
2026-01-28 20:02:29,810 | INFO | Chunk: 6 | WER=8.695652 | S=0 D=2 I=0
2026-01-28 20:02:29,810 | INFO | Chunk: 7 | WER=50.000000 | S=4 D=3 I=3
2026-01-28 20:02:29,810 | INFO | Chunk: 8 | WER=500.000000 | S=1 D=0 I=4
2026-01-28 20:02:29,840 | INFO | File: Rhap-M0018.wav | WER=19.827586 | S=25 D=11 I=10
2026-01-28 20:02:29,840 | INFO | ------------------------------
2026-01-28 20:02:29,840 | INFO | w2vec vad chunk Done!
2026-01-28 20:02:41,741 | INFO | Chunk: 0 | WER=36.363636 | S=3 D=0 I=1
2026-01-28 20:02:41,744 | INFO | Chunk: 1 | WER=55.844156 | S=3 D=40 I=0
2026-01-28 20:02:41,745 | INFO | Chunk: 2 | WER=18.750000 | S=4 D=2 I=0
2026-01-28 20:02:41,745 | INFO | Chunk: 3 | WER=20.000000 | S=0 D=1 I=1
2026-01-28 20:02:41,747 | INFO | Chunk: 4 | WER=8.695652 | S=1 D=2 I=1
2026-01-28 20:02:41,747 | INFO | Chunk: 5 | WER=25.000000 | S=3 D=0 I=0
2026-01-28 20:02:41,748 | INFO | Chunk: 6 | WER=8.695652 | S=1 D=1 I=0
2026-01-28 20:02:41,748 | INFO | Chunk: 7 | WER=30.000000 | S=1 D=2 I=3
2026-01-28 20:02:41,748 | INFO | Chunk: 8 | WER=500.000000 | S=1 D=0 I=4
2026-01-28 20:02:41,774 | INFO | File: Rhap-M0018.wav | WER=32.327586 | S=17 D=48 I=10
2026-01-28 20:02:41,774 | INFO | ------------------------------
2026-01-28 20:02:41,774 | INFO | whisper med Done!
2026-01-28 20:02:58,232 | INFO | Chunk: 0 | WER=27.272727 | S=2 D=0 I=1
2026-01-28 20:02:58,235 | INFO | Chunk: 1 | WER=32.467532 | S=7 D=18 I=0
2026-01-28 20:02:58,236 | INFO | Chunk: 2 | WER=6.250000 | S=2 D=0 I=0
2026-01-28 20:02:58,237 | INFO | Chunk: 3 | WER=20.000000 | S=0 D=1 I=1
2026-01-28 20:02:58,238 | INFO | Chunk: 4 | WER=32.608696 | S=4 D=11 I=0
2026-01-28 20:02:58,238 | INFO | Chunk: 5 | WER=25.000000 | S=3 D=0 I=0
2026-01-28 20:02:58,239 | INFO | Chunk: 6 | WER=8.695652 | S=0 D=2 I=0
2026-01-28 20:02:58,239 | INFO | Chunk: 7 | WER=50.000000 | S=3 D=4 I=3
2026-01-28 20:02:58,240 | INFO | Chunk: 8 | WER=100.000000 | S=1 D=0 I=0
2026-01-28 20:02:58,266 | INFO | File: Rhap-M0018.wav | WER=26.724138 | S=23 D=35 I=4
2026-01-28 20:02:58,266 | INFO | ------------------------------
2026-01-28 20:02:58,266 | INFO | whisper large Done!
2026-01-28 20:02:58,399 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 20:02:58,449 | INFO | Vocabulary size: 350
2026-01-28 20:02:59,470 | INFO | Gradient checkpoint layers: []
2026-01-28 20:03:00,156 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 20:03:00,160 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 20:03:00,161 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 20:03:00,161 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 20:03:00,161 | INFO | speech length: 60480
2026-01-28 20:03:00,201 | INFO | decoder input length: 94
2026-01-28 20:03:00,202 | INFO | max output length: 94
2026-01-28 20:03:00,202 | INFO | min output length: 9
2026-01-28 20:03:01,873 | INFO | end detected at 36
2026-01-28 20:03:01,876 | INFO |  -3.15 * 0.5 =  -1.57 for decoder
2026-01-28 20:03:01,876 | INFO |  -1.72 * 0.5 =  -0.86 for ctc
2026-01-28 20:03:01,876 | INFO | total log probability: -2.43
2026-01-28 20:03:01,876 | INFO | normalized log probability: -0.08
2026-01-28 20:03:01,876 | INFO | total number of ended hypotheses: 147
2026-01-28 20:03:01,876 | INFO | best hypo: ▁il▁s'enchaîne▁sur▁une▁seconde▁scène▁où▁on▁voit▁une▁femme

2026-01-28 20:03:01,880 | INFO | speech length: 372480
2026-01-28 20:03:01,925 | INFO | decoder input length: 581
2026-01-28 20:03:01,926 | INFO | max output length: 581
2026-01-28 20:03:01,926 | INFO | min output length: 58
2026-01-28 20:03:19,357 | INFO | end detected at 161
2026-01-28 20:03:19,359 | INFO | -310.37 * 0.5 = -155.19 for decoder
2026-01-28 20:03:19,359 | INFO | -52.52 * 0.5 = -26.26 for ctc
2026-01-28 20:03:19,359 | INFO | total log probability: -181.45
2026-01-28 20:03:19,359 | INFO | normalized log probability: -1.17
2026-01-28 20:03:19,359 | INFO | total number of ended hypotheses: 158
2026-01-28 20:03:19,361 | INFO | best hypo: ▁qui▁a▁priorié▁pauvre▁et▁qui▁passe▁à▁l'endroit▁où▁un▁boulanger▁vient▁de▁se▁stationner▁avec▁des▁baguettes▁de▁pain▁qui▁il▁prend▁du▁pain▁un▁traverse▁qu'on▁sort▁de▁sa▁voiture▁rendent▁dans▁une▁boutique▁et▁au▁moment▁où▁la▁fille▁y▁passe▁et▁qu'elle▁voit▁le▁manque▁d'attentend▁du▁boulanger▁elle▁prend▁un▁bou▁de▁pain▁elle▁se▁sauve▁en▁courant▁mais▁à▁ce▁moment▁là▁elle▁est▁vue▁par▁une▁dame

2026-01-28 20:03:19,364 | INFO | speech length: 214880
2026-01-28 20:03:19,429 | INFO | decoder input length: 335
2026-01-28 20:03:19,429 | INFO | max output length: 335
2026-01-28 20:03:19,429 | INFO | min output length: 33
2026-01-28 20:03:26,767 | INFO | end detected at 87
2026-01-28 20:03:26,769 | INFO | -88.99 * 0.5 = -44.50 for decoder
2026-01-28 20:03:26,769 | INFO | -30.03 * 0.5 = -15.02 for ctc
2026-01-28 20:03:26,769 | INFO | total log probability: -59.51
2026-01-28 20:03:26,769 | INFO | normalized log probability: -0.73
2026-01-28 20:03:26,769 | INFO | total number of ended hypotheses: 167
2026-01-28 20:03:26,771 | INFO | best hypo: ▁plutôt▁âgé▁bonne▁bourgeoise▁voilà▁tout▁le▁conflit▁social▁dans▁cette▁petite▁année▁d'astumeran▁et▁qui▁bien▁évidemment▁délate▁le▁vol▁au▁boulanger▁lors▁qu'il▁sort▁de▁la▁boutique

2026-01-28 20:03:26,773 | INFO | speech length: 94400
2026-01-28 20:03:26,827 | INFO | decoder input length: 147
2026-01-28 20:03:26,828 | INFO | max output length: 147
2026-01-28 20:03:26,828 | INFO | min output length: 14
2026-01-28 20:03:30,292 | INFO | end detected at 35
2026-01-28 20:03:30,295 | INFO |  -4.28 * 0.5 =  -2.14 for decoder
2026-01-28 20:03:30,295 | INFO |  -6.62 * 0.5 =  -3.31 for ctc
2026-01-28 20:03:30,295 | INFO | total log probability: -5.45
2026-01-28 20:03:30,295 | INFO | normalized log probability: -0.19
2026-01-28 20:03:30,295 | INFO | total number of ended hypotheses: 190
2026-01-28 20:03:30,296 | INFO | best hypo: ▁et▁ha▁la▁réaction▁du▁boulanger▁pendant▁que▁ça▁se▁passait▁ça

2026-01-28 20:03:30,300 | INFO | speech length: 238720
2026-01-28 20:03:30,367 | INFO | decoder input length: 372
2026-01-28 20:03:30,368 | INFO | max output length: 372
2026-01-28 20:03:30,368 | INFO | min output length: 37
2026-01-28 20:03:45,841 | INFO | end detected at 101
2026-01-28 20:03:45,843 | INFO | -129.69 * 0.5 = -64.84 for decoder
2026-01-28 20:03:45,843 | INFO | -51.32 * 0.5 = -25.66 for ctc
2026-01-28 20:03:45,843 | INFO | total log probability: -90.50
2026-01-28 20:03:45,843 | INFO | normalized log probability: -0.94
2026-01-28 20:03:45,843 | INFO | total number of ended hypotheses: 172
2026-01-28 20:03:45,846 | INFO | best hypo: ▁j'ai▁préfé▁dans▁l'ordre▁on▁voit▁la▁vigue▁non▁qui▁part▁avec▁son▁bout▁de▁pain▁au▁moment▁où▁charlie▁chaplin▁arrive▁sur▁le▁trottoir▁où▁il▁se▁heurte▁sauvagement▁et▁il▁tombe▁en▁terre▁dans▁les▁bras▁l'un▁de▁l'autre▁et

2026-01-28 20:03:45,850 | INFO | speech length: 47040
2026-01-28 20:03:45,904 | INFO | decoder input length: 73
2026-01-28 20:03:45,904 | INFO | max output length: 73
2026-01-28 20:03:45,904 | INFO | min output length: 7
2026-01-28 20:03:47,918 | INFO | end detected at 26
2026-01-28 20:03:47,921 | INFO |  -3.28 * 0.5 =  -1.64 for decoder
2026-01-28 20:03:47,921 | INFO |  -1.81 * 0.5 =  -0.90 for ctc
2026-01-28 20:03:47,921 | INFO | total log probability: -2.54
2026-01-28 20:03:47,921 | INFO | normalized log probability: -0.12
2026-01-28 20:03:47,921 | INFO | total number of ended hypotheses: 169
2026-01-28 20:03:47,922 | INFO | best hypo: ▁et▁voilà▁et▁bien▁à▁la▁fin▁je▁peux▁déconcentrer

2026-01-28 20:03:47,926 | INFO | speech length: 132000
2026-01-28 20:03:47,979 | INFO | decoder input length: 205
2026-01-28 20:03:47,979 | INFO | max output length: 205
2026-01-28 20:03:47,979 | INFO | min output length: 20
2026-01-28 20:03:51,822 | INFO | end detected at 64
2026-01-28 20:03:51,824 | INFO | -13.11 * 0.5 =  -6.56 for decoder
2026-01-28 20:03:51,824 | INFO |  -5.76 * 0.5 =  -2.88 for ctc
2026-01-28 20:03:51,824 | INFO | total log probability: -9.44
2026-01-28 20:03:51,824 | INFO | normalized log probability: -0.16
2026-01-28 20:03:51,824 | INFO | total number of ended hypotheses: 182
2026-01-28 20:03:51,825 | INFO | best hypo: ▁mais▁ça▁se▁termine▁un▁peu▁en▁coupure▁où▁le▁boulanger▁la▁vieille▁dame▁rejoint▁la▁bourgeoise▁rejoint▁le▁boulanger

2026-01-28 20:03:51,828 | INFO | speech length: 102400
2026-01-28 20:03:51,891 | INFO | decoder input length: 159
2026-01-28 20:03:51,891 | INFO | max output length: 159
2026-01-28 20:03:51,891 | INFO | min output length: 15
2026-01-28 20:03:57,841 | INFO | end detected at 61
2026-01-28 20:03:57,844 | INFO | -20.09 * 0.5 = -10.05 for decoder
2026-01-28 20:03:57,845 | INFO | -14.14 * 0.5 =  -7.07 for ctc
2026-01-28 20:03:57,845 | INFO | total log probability: -17.12
2026-01-28 20:03:57,845 | INFO | normalized log probability: -0.34
2026-01-28 20:03:57,845 | INFO | total number of ended hypotheses: 215
2026-01-28 20:03:57,846 | INFO | best hypo: ▁après▁que▁la▁fille▁soit▁par▁terre▁et▁charlie▁nous▁gère▁l'échapune▁soit▁par▁terre▁et▁la▁fille▁soit▁partie▁je▁vois▁pue

2026-01-28 20:03:57,850 | INFO | speech length: 14400
2026-01-28 20:03:57,906 | INFO | decoder input length: 22
2026-01-28 20:03:57,906 | INFO | max output length: 22
2026-01-28 20:03:57,906 | INFO | min output length: 2
2026-01-28 20:03:58,970 | INFO | end detected at 16
2026-01-28 20:03:58,973 | INFO |  -3.27 * 0.5 =  -1.64 for decoder
2026-01-28 20:03:58,973 | INFO | -12.76 * 0.5 =  -6.38 for ctc
2026-01-28 20:03:58,973 | INFO | total log probability: -8.02
2026-01-28 20:03:58,973 | INFO | normalized log probability: -1.34
2026-01-28 20:03:58,973 | INFO | total number of ended hypotheses: 200
2026-01-28 20:03:58,974 | INFO | best hypo: ▁mon▁grand▁set

2026-01-28 20:03:58,986 | INFO | Chunk: 0 | WER=27.272727 | S=2 D=0 I=1
2026-01-28 20:03:58,994 | INFO | Chunk: 1 | WER=15.584416 | S=6 D=2 I=4
2026-01-28 20:03:58,996 | INFO | Chunk: 2 | WER=31.250000 | S=7 D=2 I=1
2026-01-28 20:03:58,997 | INFO | Chunk: 3 | WER=30.000000 | S=1 D=0 I=2
2026-01-28 20:03:59,001 | INFO | Chunk: 4 | WER=23.913043 | S=9 D=1 I=1
2026-01-28 20:03:59,002 | INFO | Chunk: 5 | WER=41.666667 | S=3 D=2 I=0
2026-01-28 20:03:59,003 | INFO | Chunk: 6 | WER=13.043478 | S=0 D=3 I=0
2026-01-28 20:03:59,005 | INFO | Chunk: 7 | WER=35.000000 | S=3 D=0 I=4
2026-01-28 20:03:59,005 | INFO | Chunk: 8 | WER=300.000000 | S=1 D=0 I=2
2026-01-28 20:03:59,069 | INFO | File: Rhap-M0018.wav | WER=24.568966 | S=32 D=10 I=15
2026-01-28 20:03:59,070 | INFO | ------------------------------
2026-01-28 20:03:59,070 | INFO | Conf cv Done!
2026-01-28 20:03:59,324 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 20:03:59,372 | INFO | Vocabulary size: 47
2026-01-28 20:04:00,270 | INFO | Gradient checkpoint layers: []
2026-01-28 20:04:00,974 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 20:04:00,979 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 20:04:00,979 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 20:04:00,979 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 20:04:00,982 | INFO | speech length: 60480
2026-01-28 20:04:01,021 | INFO | decoder input length: 94
2026-01-28 20:04:01,022 | INFO | max output length: 94
2026-01-28 20:04:01,022 | INFO | min output length: 9
2026-01-28 20:04:03,728 | INFO | end detected at 67
2026-01-28 20:04:03,729 | INFO |  -6.12 * 0.5 =  -3.06 for decoder
2026-01-28 20:04:03,730 | INFO |  -4.51 * 0.5 =  -2.25 for ctc
2026-01-28 20:04:03,730 | INFO | total log probability: -5.31
2026-01-28 20:04:03,730 | INFO | normalized log probability: -0.09
2026-01-28 20:04:03,730 | INFO | total number of ended hypotheses: 200
2026-01-28 20:04:03,731 | INFO | best hypo: et<space>s'enchaîne<space>sur<space>une<space>seconde<space>scène<space>où<space>on<space>voit<space>une<space>femme

2026-01-28 20:04:03,733 | INFO | speech length: 372480
2026-01-28 20:04:03,767 | INFO | decoder input length: 581
2026-01-28 20:04:03,768 | INFO | max output length: 581
2026-01-28 20:04:03,768 | INFO | min output length: 58
2026-01-28 20:04:42,894 | INFO | end detected at 386
2026-01-28 20:04:42,897 | INFO | -211.45 * 0.5 = -105.72 for decoder
2026-01-28 20:04:42,897 | INFO | -35.53 * 0.5 = -17.77 for ctc
2026-01-28 20:04:42,897 | INFO | total log probability: -123.49
2026-01-28 20:04:42,897 | INFO | normalized log probability: -0.33
2026-01-28 20:04:42,897 | INFO | total number of ended hypotheses: 182
2026-01-28 20:04:42,903 | INFO | best hypo: qui<space>a<space>priori<space>est<space>pauvre<space>euh<space>et<space>qui<space>passe<space>à<space>l'endroit<space>où<space>un<space>boulanger<space>vient<space>de<space>se<space>stationner<space>avec<space>des<space>baillettes<space>de<space>pain<space>qui<space>prend<space>du<space>pain<space>traverse<space>son<space>sort<space>de<space>sa<space>voiture<space>rentre<space>dans<space>une<space>boutique<space>et<space>au<space>moment<space>où<space>la<space>fille<space>passe<space>et<space>qu'elle<space>voit<space>le<space>manque<space>d'attention<space>du<space>boulanger<space>elle<space>prend<space>en<space>bout<space>le<space>pain<space>et<space>elle<space>st<space>sauve<space>aun<space>courant<space>mais<space>à<space>ce<space>moment<space>là<space>elle<space>est<space>vu<space>par<space>une<space>dame

2026-01-28 20:04:42,906 | INFO | speech length: 214880
2026-01-28 20:04:42,949 | INFO | decoder input length: 335
2026-01-28 20:04:42,949 | INFO | max output length: 335
2026-01-28 20:04:42,949 | INFO | min output length: 33
2026-01-28 20:04:55,279 | INFO | end detected at 188
2026-01-28 20:04:55,281 | INFO | -21.17 * 0.5 = -10.59 for decoder
2026-01-28 20:04:55,281 | INFO | -14.74 * 0.5 =  -7.37 for ctc
2026-01-28 20:04:55,281 | INFO | total log probability: -17.96
2026-01-28 20:04:55,281 | INFO | normalized log probability: -0.10
2026-01-28 20:04:55,281 | INFO | total number of ended hypotheses: 181
2026-01-28 20:04:55,283 | INFO | best hypo: plutôt<space>agée<space>bonne<space>bourgeoise<space>euh<space>voilà<space>tout<space>le<space>conflit<space>social<space>dans<space>cette<space>petite<space>anecdotement<space>et<space>euh<space>qui<space>bien<space>évidemment<space>délate<space>le<space>vol<space>au<space>boulanger<space>lorsqu'il<space>ressort<space>de<space>la<space>boutique

2026-01-28 20:04:55,286 | INFO | speech length: 94400
2026-01-28 20:04:55,322 | INFO | decoder input length: 147
2026-01-28 20:04:55,323 | INFO | max output length: 147
2026-01-28 20:04:55,323 | INFO | min output length: 14
2026-01-28 20:04:59,345 | INFO | end detected at 87
2026-01-28 20:04:59,346 | INFO |  -7.07 * 0.5 =  -3.54 for decoder
2026-01-28 20:04:59,347 | INFO |  -4.48 * 0.5 =  -2.24 for ctc
2026-01-28 20:04:59,347 | INFO | total log probability: -5.78
2026-01-28 20:04:59,347 | INFO | normalized log probability: -0.07
2026-01-28 20:04:59,347 | INFO | total number of ended hypotheses: 183
2026-01-28 20:04:59,348 | INFO | best hypo: et<space>pour<space>cent<space>hesitation<space>voilà<space>réaction<space>du<space>boulanger<space>pendant<space>que<space>ça<space>se<space>passait<space>ça

2026-01-28 20:04:59,350 | INFO | speech length: 238720
2026-01-28 20:04:59,391 | INFO | decoder input length: 372
2026-01-28 20:04:59,392 | INFO | max output length: 372
2026-01-28 20:04:59,392 | INFO | min output length: 37
2026-01-28 20:05:18,648 | INFO | end detected at 246
2026-01-28 20:05:18,651 | INFO | -34.87 * 0.5 = -17.44 for decoder
2026-01-28 20:05:18,651 | INFO | -38.23 * 0.5 = -19.11 for ctc
2026-01-28 20:05:18,651 | INFO | total log probability: -36.55
2026-01-28 20:05:18,651 | INFO | normalized log probability: -0.15
2026-01-28 20:05:18,651 | INFO | total number of ended hypotheses: 209
2026-01-28 20:05:18,654 | INFO | best hypo: j'ai<space>pas<space>fait<space>dans<space>l'ordre<space>on<space>voit<space>la<space>vigue<space>donc<space>qui<space>part<space>euh<space>avec<space>son<space>boupin<space>au<space>moment<space>où<space>charlie<space>chaklin<space>arrive<space>sur<space>le<space>trottoir<space>où<space>il<space>se<space>heurte<space>euh<space>sauvagement<space>et<space>tombent<space>par<space>terre<space>dans<space>les<space>bralins<space>de<space>l'autre<space>et<space>pour<space>cent<space>hesitation

2026-01-28 20:05:18,656 | INFO | speech length: 47040
2026-01-28 20:05:18,698 | INFO | decoder input length: 73
2026-01-28 20:05:18,698 | INFO | max output length: 73
2026-01-28 20:05:18,698 | INFO | min output length: 7
2026-01-28 20:05:20,951 | INFO | end detected at 60
2026-01-28 20:05:20,954 | INFO |  -4.62 * 0.5 =  -2.31 for decoder
2026-01-28 20:05:20,954 | INFO |  -9.16 * 0.5 =  -4.58 for ctc
2026-01-28 20:05:20,954 | INFO | total log probability: -6.89
2026-01-28 20:05:20,954 | INFO | normalized log probability: -0.14
2026-01-28 20:05:20,954 | INFO | total number of ended hypotheses: 199
2026-01-28 20:05:20,955 | INFO | best hypo: et<space>voilà<space>et<space>puis<space>à<space>la<space>fin<space>je<space>peux<space>déconcentrer

2026-01-28 20:05:20,957 | INFO | speech length: 132000
2026-01-28 20:05:21,000 | INFO | decoder input length: 205
2026-01-28 20:05:21,000 | INFO | max output length: 205
2026-01-28 20:05:21,000 | INFO | min output length: 20
2026-01-28 20:05:28,505 | INFO | end detected at 150
2026-01-28 20:05:28,507 | INFO | -11.86 * 0.5 =  -5.93 for decoder
2026-01-28 20:05:28,508 | INFO |  -2.86 * 0.5 =  -1.43 for ctc
2026-01-28 20:05:28,508 | INFO | total log probability: -7.36
2026-01-28 20:05:28,508 | INFO | normalized log probability: -0.05
2026-01-28 20:05:28,508 | INFO | total number of ended hypotheses: 247
2026-01-28 20:05:28,510 | INFO | best hypo: mais<space>euh<space>et<space>ça<space>se<space>termine<space>un<space>peu<space>en<space>coupure<space>ou<space>euh<space>le<space>boulanger<space>fin<space>la<space>la<space>vieille<space>dame<space>rejoint<space>euh<space>la<space>bourgeoise<space>rejoint<space>le<space>boulanger<space>euh

2026-01-28 20:05:28,512 | INFO | speech length: 102400
2026-01-28 20:05:28,554 | INFO | decoder input length: 159
2026-01-28 20:05:28,554 | INFO | max output length: 159
2026-01-28 20:05:28,554 | INFO | min output length: 15
2026-01-28 20:05:34,625 | INFO | end detected at 136
2026-01-28 20:05:34,627 | INFO | -17.82 * 0.5 =  -8.91 for decoder
2026-01-28 20:05:34,627 | INFO | -12.62 * 0.5 =  -6.31 for ctc
2026-01-28 20:05:34,627 | INFO | total log probability: -15.22
2026-01-28 20:05:34,627 | INFO | normalized log probability: -0.12
2026-01-28 20:05:34,627 | INFO | total number of ended hypotheses: 185
2026-01-28 20:05:34,629 | INFO | best hypo: après<space>que<space>euh<space>la<space>fille<space>soit<space>par<space>terre<space>et<space>charlie<space>mouchère<space>les<space>chefs<space>qui<space>ne<space>soient<space>par<space>terre<space>à<space>la<space>fille<space>soit<space>partie<space>je<space>vois<space>plus

2026-01-28 20:05:34,631 | INFO | speech length: 14400
2026-01-28 20:05:34,686 | INFO | decoder input length: 22
2026-01-28 20:05:34,686 | INFO | max output length: 22
2026-01-28 20:05:34,686 | INFO | min output length: 2
2026-01-28 20:05:35,917 | INFO | adding <eos> in the last position in the loop
2026-01-28 20:05:35,931 | INFO | no hypothesis. Finish decoding.
2026-01-28 20:05:35,933 | INFO |  -3.76 * 0.5 =  -1.88 for decoder
2026-01-28 20:05:35,933 | INFO |  -6.96 * 0.5 =  -3.48 for ctc
2026-01-28 20:05:35,933 | INFO | total log probability: -5.36
2026-01-28 20:05:35,934 | INFO | normalized log probability: -0.28
2026-01-28 20:05:35,934 | INFO | total number of ended hypotheses: 128
2026-01-28 20:05:35,934 | INFO | best hypo: bon<space>en<space>gros<space>c'est

2026-01-28 20:05:35,945 | INFO | Chunk: 0 | WER=18.181818 | S=1 D=0 I=1
2026-01-28 20:05:35,953 | INFO | Chunk: 1 | WER=11.688312 | S=8 D=0 I=1
2026-01-28 20:05:35,955 | INFO | Chunk: 2 | WER=25.000000 | S=2 D=4 I=2
2026-01-28 20:05:35,956 | INFO | Chunk: 3 | WER=40.000000 | S=0 D=0 I=4
2026-01-28 20:05:35,960 | INFO | Chunk: 4 | WER=39.130435 | S=7 D=5 I=6
2026-01-28 20:05:35,960 | INFO | Chunk: 5 | WER=33.333333 | S=2 D=2 I=0
2026-01-28 20:05:35,962 | INFO | Chunk: 6 | WER=21.739130 | S=1 D=0 I=4
2026-01-28 20:05:35,963 | INFO | Chunk: 7 | WER=55.000000 | S=5 D=0 I=6
2026-01-28 20:05:35,964 | INFO | Chunk: 8 | WER=400.000000 | S=0 D=0 I=4
2026-01-28 20:05:36,030 | INFO | File: Rhap-M0018.wav | WER=28.017241 | S=26 D=11 I=28
2026-01-28 20:05:36,030 | INFO | ------------------------------
2026-01-28 20:05:36,030 | INFO | Conf ester Done!
2026-01-28 20:07:00,773 | INFO | Chunk: 0 | WER=27.272727 | S=2 D=0 I=1
2026-01-28 20:07:00,781 | INFO | Chunk: 1 | WER=19.480519 | S=10 D=5 I=0
2026-01-28 20:07:00,783 | INFO | Chunk: 2 | WER=18.750000 | S=2 D=3 I=1
2026-01-28 20:07:00,784 | INFO | Chunk: 3 | WER=40.000000 | S=2 D=0 I=2
2026-01-28 20:07:00,788 | INFO | Chunk: 4 | WER=19.565217 | S=7 D=1 I=1
2026-01-28 20:07:00,788 | INFO | Chunk: 5 | WER=58.333333 | S=3 D=4 I=0
2026-01-28 20:07:00,790 | INFO | Chunk: 6 | WER=21.739130 | S=1 D=3 I=1
2026-01-28 20:07:00,791 | INFO | Chunk: 7 | WER=40.000000 | S=3 D=2 I=3
2026-01-28 20:07:00,791 | INFO | Chunk: 8 | WER=300.000000 | S=1 D=0 I=2
2026-01-28 20:07:00,852 | INFO | File: Rhap-M0018.wav | WER=25.862069 | S=31 D=18 I=11
2026-01-28 20:07:00,852 | INFO | ------------------------------
2026-01-28 20:07:00,852 | INFO | hmm_tdnn Done!
2026-01-28 20:07:01,065 | INFO | ==================================Rhap-M0019.wav=========================================
2026-01-28 20:07:01,260 | INFO | Using rVAD model
2026-01-28 20:07:08,276 | INFO | Chunk: 0 | WER=15.584416 | S=4 D=8 I=0
2026-01-28 20:07:08,279 | INFO | Chunk: 1 | WER=9.090909 | S=1 D=1 I=1
2026-01-28 20:07:08,283 | INFO | Chunk: 2 | WER=40.000000 | S=9 D=11 I=2
2026-01-28 20:07:08,283 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:07:08,316 | INFO | File: Rhap-M0019.wav | WER=22.023810 | S=14 D=20 I=3
2026-01-28 20:07:08,316 | INFO | ------------------------------
2026-01-28 20:07:08,316 | INFO | w2vec vad chunk Done!
2026-01-28 20:07:17,249 | INFO | Chunk: 0 | WER=51.948052 | S=21 D=18 I=1
2026-01-28 20:07:17,251 | INFO | Chunk: 1 | WER=27.272727 | S=5 D=2 I=2
2026-01-28 20:07:17,252 | INFO | Chunk: 2 | WER=56.363636 | S=3 D=25 I=3
2026-01-28 20:07:17,252 | INFO | Chunk: 3 | WER=66.666667 | S=1 D=0 I=1
2026-01-28 20:07:17,266 | INFO | File: Rhap-M0019.wav | WER=47.619048 | S=30 D=44 I=6
2026-01-28 20:07:17,266 | INFO | ------------------------------
2026-01-28 20:07:17,267 | INFO | whisper med Done!
2026-01-28 20:07:25,829 | INFO | Chunk: 0 | WER=42.857143 | S=4 D=29 I=0
2026-01-28 20:07:25,830 | INFO | Chunk: 1 | WER=15.151515 | S=2 D=2 I=1
2026-01-28 20:07:25,831 | INFO | Chunk: 2 | WER=70.909091 | S=3 D=36 I=0
2026-01-28 20:07:25,831 | INFO | Chunk: 3 | WER=66.666667 | S=1 D=1 I=0
2026-01-28 20:07:25,843 | INFO | File: Rhap-M0019.wav | WER=46.428571 | S=9 D=68 I=1
2026-01-28 20:07:25,843 | INFO | ------------------------------
2026-01-28 20:07:25,843 | INFO | whisper large Done!
2026-01-28 20:07:26,018 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 20:07:26,056 | INFO | Vocabulary size: 350
2026-01-28 20:07:26,974 | INFO | Gradient checkpoint layers: []
2026-01-28 20:07:27,642 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 20:07:27,646 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 20:07:27,646 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 20:07:27,647 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 20:07:27,647 | INFO | speech length: 427840
2026-01-28 20:07:27,696 | INFO | decoder input length: 668
2026-01-28 20:07:27,696 | INFO | max output length: 668
2026-01-28 20:07:27,696 | INFO | min output length: 66
2026-01-28 20:07:45,237 | INFO | end detected at 147
2026-01-28 20:07:45,239 | INFO | -313.80 * 0.5 = -156.90 for decoder
2026-01-28 20:07:45,239 | INFO | -85.82 * 0.5 = -42.91 for ctc
2026-01-28 20:07:45,239 | INFO | total log probability: -199.81
2026-01-28 20:07:45,239 | INFO | normalized log probability: -1.42
2026-01-28 20:07:45,239 | INFO | total number of ended hypotheses: 150
2026-01-28 20:07:45,241 | INFO | best hypo: ▁ensuite▁on▁change▁de▁séquence▁et▁on▁se▁retrouve▁dans▁la▁rue▁avec▁une▁jeune▁fille▁qui▁visiblement▁regarde▁des▁vitrines▁avec▁chaussures▁en▁train▁de▁formolage▁à▁l'air▁triste▁à▁côté▁d'allier▁une▁voiture▁avec▁une▁camionnette▁avec▁un▁monsieur▁en▁train▁'charger▁de▁saler▁d'être▁un▁iteur▁en▁train'écharger▁des▁aliment▁et▁elle▁vole▁une▁baguette▁de▁pain

2026-01-28 20:07:45,244 | INFO | speech length: 164000
2026-01-28 20:07:45,288 | INFO | decoder input length: 255
2026-01-28 20:07:45,288 | INFO | max output length: 255
2026-01-28 20:07:45,288 | INFO | min output length: 25
2026-01-28 20:07:50,149 | INFO | end detected at 75
2026-01-28 20:07:50,151 | INFO | -12.81 * 0.5 =  -6.40 for decoder
2026-01-28 20:07:50,151 | INFO | -11.32 * 0.5 =  -5.66 for ctc
2026-01-28 20:07:50,151 | INFO | total log probability: -12.06
2026-01-28 20:07:50,151 | INFO | normalized log probability: -0.17
2026-01-28 20:07:50,151 | INFO | total number of ended hypotheses: 177
2026-01-28 20:07:50,152 | INFO | best hypo: ▁elle▁est▁prise▁en▁flagrant▁délit▁par▁une▁autre▁dame▁qui▁à▁côté▁et▁elle▁s'apprête▁à▁courir▁avec▁sa▁baguette▁et▁elle▁rentre▁dans▁chary▁chaplin▁ensuite▁le▁traiteur▁revient

2026-01-28 20:07:50,154 | INFO | speech length: 273600
2026-01-28 20:07:50,193 | INFO | decoder input length: 427
2026-01-28 20:07:50,194 | INFO | max output length: 427
2026-01-28 20:07:50,194 | INFO | min output length: 42
2026-01-28 20:07:59,465 | INFO | end detected at 107
2026-01-28 20:07:59,466 | INFO | -222.21 * 0.5 = -111.11 for decoder
2026-01-28 20:07:59,466 | INFO | -133.48 * 0.5 = -66.74 for ctc
2026-01-28 20:07:59,466 | INFO | total log probability: -177.84
2026-01-28 20:07:59,466 | INFO | normalized log probability: -1.76
2026-01-28 20:07:59,466 | INFO | total number of ended hypotheses: 165
2026-01-28 20:07:59,468 | INFO | best hypo: ▁ah▁et▁le▁on▁voit▁expliquer▁un▁mot▁après▁macho▁j'ai▁pas▁compris▁ce▁qui▁est▁écrit▁je▁ne▁le▁disais▁pas▁et▁on▁le▁le▁voit▁madame▁expliquer▁qu'il▁voulé▁à▁la▁baguette▁de▁pin▁et▁à▁une▁explication▁qu'il▁rence▁et▁la▁foup▁qui▁sa▁trompée

2026-01-28 20:07:59,470 | INFO | speech length: 12480
2026-01-28 20:07:59,510 | INFO | decoder input length: 19
2026-01-28 20:07:59,510 | INFO | max output length: 19
2026-01-28 20:07:59,510 | INFO | min output length: 1
2026-01-28 20:08:00,003 | INFO | end detected at 13
2026-01-28 20:08:00,004 | INFO |  -0.97 * 0.5 =  -0.48 for decoder
2026-01-28 20:08:00,005 | INFO |  -2.40 * 0.5 =  -1.20 for ctc
2026-01-28 20:08:00,005 | INFO | total log probability: -1.68
2026-01-28 20:08:00,005 | INFO | normalized log probability: -0.21
2026-01-28 20:08:00,005 | INFO | total number of ended hypotheses: 166
2026-01-28 20:08:00,005 | INFO | best hypo: ▁des▁policiers

2026-01-28 20:08:00,014 | INFO | Chunk: 0 | WER=25.974026 | S=7 D=13 I=0
2026-01-28 20:08:00,015 | INFO | Chunk: 1 | WER=12.121212 | S=1 D=2 I=1
2026-01-28 20:08:00,017 | INFO | Chunk: 2 | WER=54.545455 | S=14 D=10 I=6
2026-01-28 20:08:00,017 | INFO | Chunk: 3 | WER=100.000000 | S=2 D=1 I=0
2026-01-28 20:08:00,033 | INFO | File: Rhap-M0019.wav | WER=33.928571 | S=24 D=26 I=7
2026-01-28 20:08:00,033 | INFO | ------------------------------
2026-01-28 20:08:00,033 | INFO | Conf cv Done!
2026-01-28 20:08:00,212 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 20:08:00,234 | INFO | Vocabulary size: 47
2026-01-28 20:08:01,144 | INFO | Gradient checkpoint layers: []
2026-01-28 20:08:01,798 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 20:08:01,802 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 20:08:01,802 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 20:08:01,803 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 20:08:01,805 | INFO | speech length: 427840
2026-01-28 20:08:01,848 | INFO | decoder input length: 668
2026-01-28 20:08:01,848 | INFO | max output length: 668
2026-01-28 20:08:01,848 | INFO | min output length: 66
2026-01-28 20:08:47,260 | INFO | end detected at 474
2026-01-28 20:08:47,262 | INFO | -316.50 * 0.5 = -158.25 for decoder
2026-01-28 20:08:47,262 | INFO | -46.86 * 0.5 = -23.43 for ctc
2026-01-28 20:08:47,262 | INFO | total log probability: -181.68
2026-01-28 20:08:47,262 | INFO | normalized log probability: -0.39
2026-01-28 20:08:47,262 | INFO | total number of ended hypotheses: 179
2026-01-28 20:08:47,268 | INFO | best hypo: juste<space>on<space>change<space>de<space>séquence<space>et<space>on<space>se<space>retrouve<space>dans<space>la<space>rue<space>avec<space>une<space>jeune<space>fille<space>pour<space>cent<space>hesitation<space>qui<space>visiblement<space>regarde<space>des<space>vitrines<space>pour<space>cent<space>hesitation<space>avec<space>des<space>chaussures<space>en<space>train<space>de<space>pour<space>cent<space>hesitation<space>enfin<space>voilà<space>la<space>l'air<space>triste<space>pour<space>cent<space>hesitation<space>à<space>côté<space>d'elle<space>il<space>y<space>a<space>une<space>voiture<space>avec<space>une<space>camionnette<space>en<space>avec<space>euh<space>une<space>je<space>suis<space>en<space>train<space>de<space>de<space>décharger<space>euh<space>de<space>salaire<space>d'être<space>un<space>traiteur<space>en<space>train<space>de<space>cherger<space>des<space>alimant<space>e<space>et<space>elle<space>vol<space>e<space>une<space>baguette<space>de<space>pant

2026-01-28 20:08:47,271 | INFO | speech length: 164000
2026-01-28 20:08:47,312 | INFO | decoder input length: 255
2026-01-28 20:08:47,312 | INFO | max output length: 255
2026-01-28 20:08:47,312 | INFO | min output length: 25
2026-01-28 20:08:58,116 | INFO | end detected at 197
2026-01-28 20:08:58,117 | INFO | -19.43 * 0.5 =  -9.71 for decoder
2026-01-28 20:08:58,117 | INFO | -13.75 * 0.5 =  -6.88 for ctc
2026-01-28 20:08:58,117 | INFO | total log probability: -16.59
2026-01-28 20:08:58,118 | INFO | normalized log probability: -0.09
2026-01-28 20:08:58,118 | INFO | total number of ended hypotheses: 193
2026-01-28 20:08:58,120 | INFO | best hypo: elle<space>est<space>euh<space>prise<space>en<space>flagrant<space>délit<space>par<space>une<space>une<space>autre<space>dame<space>euh<space>qui<space>est<space>à<space>côté<space>et<space>elle<space>elle<space>s'apprête<space>à<space>courir<space>avec<space>sa<space>baguette<space>et<space>elle<space>rentre<space>dans<space>chahi<space>chapi<space>ensuite<space>le<space>traiteur<space>revient

2026-01-28 20:08:58,122 | INFO | speech length: 273600
2026-01-28 20:08:58,160 | INFO | decoder input length: 427
2026-01-28 20:08:58,160 | INFO | max output length: 427
2026-01-28 20:08:58,160 | INFO | min output length: 42
2026-01-28 20:09:18,648 | INFO | end detected at 273
2026-01-28 20:09:18,651 | INFO | -39.32 * 0.5 = -19.66 for decoder
2026-01-28 20:09:18,651 | INFO | -23.02 * 0.5 = -11.51 for ctc
2026-01-28 20:09:18,651 | INFO | total log probability: -31.17
2026-01-28 20:09:18,651 | INFO | normalized log probability: -0.12
2026-01-28 20:09:18,652 | INFO | total number of ended hypotheses: 244
2026-01-28 20:09:18,655 | INFO | best hypo: euh<space>et<space>le<space>le<space>on<space>voit<space>expliquer<space>euh<space>euh<space>bon<space>après<space>moi<space>je<space>j'ai<space>pas<space>compris<space>c'est<space>ce<space>qui<space>était<space>écrit<space>euh<space>je<space>le<space>disais<space>pas<space>euh<space>et<space>on<space>le<space>le<space>voit<space>la<space>dame<space>a<space>expliqué<space>qu'à<space>la<space>volait<space>la<space>vaguette<space>de<space>pain<space>et<space>une<space>explication<space>qui<space>commence<space>avec<space>euh<space>la<space>foule<space>qui<space>s'a<space>troupée

2026-01-28 20:09:18,658 | INFO | speech length: 12480
2026-01-28 20:09:18,696 | INFO | decoder input length: 19
2026-01-28 20:09:18,696 | INFO | max output length: 19
2026-01-28 20:09:18,696 | INFO | min output length: 1
2026-01-28 20:09:19,290 | INFO | adding <eos> in the last position in the loop
2026-01-28 20:09:19,299 | INFO | no hypothesis. Finish decoding.
2026-01-28 20:09:19,300 | INFO |  -1.41 * 0.5 =  -0.70 for decoder
2026-01-28 20:09:19,300 | INFO |  -1.24 * 0.5 =  -0.62 for ctc
2026-01-28 20:09:19,300 | INFO | total log probability: -1.32
2026-01-28 20:09:19,300 | INFO | normalized log probability: -0.08
2026-01-28 20:09:19,300 | INFO | total number of ended hypotheses: 130
2026-01-28 20:09:19,301 | INFO | best hypo: et<space>le<space>policier

2026-01-28 20:09:19,310 | INFO | Chunk: 0 | WER=38.961039 | S=9 D=4 I=17
2026-01-28 20:09:19,312 | INFO | Chunk: 1 | WER=18.181818 | S=2 D=0 I=4
2026-01-28 20:09:19,314 | INFO | Chunk: 2 | WER=38.181818 | S=7 D=5 I=9
2026-01-28 20:09:19,314 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:09:19,333 | INFO | File: Rhap-M0019.wav | WER=33.928571 | S=18 D=9 I=30
2026-01-28 20:09:19,333 | INFO | ------------------------------
2026-01-28 20:09:19,333 | INFO | Conf ester Done!
2026-01-28 20:10:06,526 | INFO | Chunk: 0 | WER=15.584416 | S=6 D=6 I=0
2026-01-28 20:10:06,528 | INFO | Chunk: 1 | WER=9.090909 | S=1 D=2 I=0
2026-01-28 20:10:06,531 | INFO | Chunk: 2 | WER=29.090909 | S=6 D=7 I=3
2026-01-28 20:10:06,531 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:10:06,553 | INFO | File: Rhap-M0019.wav | WER=18.452381 | S=13 D=15 I=3
2026-01-28 20:10:06,553 | INFO | ------------------------------
2026-01-28 20:10:06,553 | INFO | hmm_tdnn Done!
2026-01-28 20:10:06,743 | INFO | ==================================Rhap-M0021.wav=========================================
2026-01-28 20:10:06,927 | INFO | Using rVAD model
2026-01-28 20:10:16,958 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:10:16,959 | INFO | Chunk: 1 | WER=12.500000 | S=1 D=0 I=0
2026-01-28 20:10:16,959 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:10:16,959 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:10:16,959 | INFO | Chunk: 4 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:10:16,960 | INFO | Chunk: 5 | WER=20.000000 | S=1 D=0 I=1
2026-01-28 20:10:16,960 | INFO | Chunk: 6 | WER=43.750000 | S=4 D=1 I=2
2026-01-28 20:10:16,960 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:10:16,960 | INFO | Chunk: 8 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:10:16,961 | INFO | Chunk: 9 | WER=15.789474 | S=2 D=0 I=1
2026-01-28 20:10:16,961 | INFO | Chunk: 10 | WER=26.666667 | S=0 D=0 I=4
2026-01-28 20:10:16,962 | INFO | Chunk: 11 | WER=8.695652 | S=0 D=2 I=0
2026-01-28 20:10:16,964 | INFO | Chunk: 12 | WER=24.390244 | S=8 D=1 I=1
2026-01-28 20:10:16,964 | INFO | Chunk: 13 | WER=41.666667 | S=3 D=1 I=1
2026-01-28 20:10:16,981 | INFO | File: Rhap-M0021.wav | WER=20.858896 | S=21 D=4 I=9
2026-01-28 20:10:16,981 | INFO | ------------------------------
2026-01-28 20:10:16,981 | INFO | w2vec vad chunk Done!
2026-01-28 20:10:28,127 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:10:28,127 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:10:28,127 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:10:28,128 | INFO | Chunk: 3 | WER=500.000000 | S=1 D=0 I=4
2026-01-28 20:10:28,128 | INFO | Chunk: 4 | WER=40.000000 | S=0 D=2 I=0
2026-01-28 20:10:28,128 | INFO | Chunk: 5 | WER=10.000000 | S=0 D=0 I=1
2026-01-28 20:10:28,129 | INFO | Chunk: 6 | WER=37.500000 | S=4 D=2 I=0
2026-01-28 20:10:28,129 | INFO | Chunk: 7 | WER=40.000000 | S=2 D=0 I=0
2026-01-28 20:10:28,129 | INFO | Chunk: 8 | WER=25.000000 | S=0 D=1 I=0
2026-01-28 20:10:28,129 | INFO | Chunk: 9 | WER=31.578947 | S=1 D=4 I=1
2026-01-28 20:10:28,130 | INFO | Chunk: 10 | WER=26.666667 | S=1 D=0 I=3
2026-01-28 20:10:28,131 | INFO | Chunk: 11 | WER=8.695652 | S=0 D=2 I=0
2026-01-28 20:10:28,132 | INFO | Chunk: 12 | WER=31.707317 | S=7 D=4 I=2
2026-01-28 20:10:28,132 | INFO | Chunk: 13 | WER=8.333333 | S=0 D=0 I=1
2026-01-28 20:10:28,149 | INFO | File: Rhap-M0021.wav | WER=26.380368 | S=20 D=13 I=10
2026-01-28 20:10:28,149 | INFO | ------------------------------
2026-01-28 20:10:28,149 | INFO | whisper med Done!
2026-01-28 20:10:42,836 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:10:42,836 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:10:42,836 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:10:42,836 | INFO | Chunk: 3 | WER=200.000000 | S=1 D=0 I=1
2026-01-28 20:10:42,837 | INFO | Chunk: 4 | WER=40.000000 | S=0 D=2 I=0
2026-01-28 20:10:42,837 | INFO | Chunk: 5 | WER=10.000000 | S=0 D=0 I=1
2026-01-28 20:10:42,837 | INFO | Chunk: 6 | WER=43.750000 | S=4 D=2 I=1
2026-01-28 20:10:42,838 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:10:42,838 | INFO | Chunk: 8 | WER=25.000000 | S=0 D=1 I=0
2026-01-28 20:10:42,838 | INFO | Chunk: 9 | WER=21.052632 | S=2 D=1 I=1
2026-01-28 20:10:42,839 | INFO | Chunk: 10 | WER=26.666667 | S=1 D=0 I=3
2026-01-28 20:10:42,839 | INFO | Chunk: 11 | WER=8.695652 | S=0 D=2 I=0
2026-01-28 20:10:42,841 | INFO | Chunk: 12 | WER=17.073171 | S=1 D=5 I=1
2026-01-28 20:10:42,841 | INFO | Chunk: 13 | WER=25.000000 | S=2 D=0 I=1
2026-01-28 20:10:42,857 | INFO | File: Rhap-M0021.wav | WER=20.245399 | S=11 D=13 I=9
2026-01-28 20:10:42,857 | INFO | ------------------------------
2026-01-28 20:10:42,857 | INFO | whisper large Done!
2026-01-28 20:10:42,991 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 20:10:43,029 | INFO | Vocabulary size: 350
2026-01-28 20:10:44,226 | INFO | Gradient checkpoint layers: []
2026-01-28 20:10:45,018 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 20:10:45,024 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 20:10:45,024 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 20:10:45,025 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 20:10:45,025 | INFO | speech length: 18240
2026-01-28 20:10:45,073 | INFO | decoder input length: 28
2026-01-28 20:10:45,073 | INFO | max output length: 28
2026-01-28 20:10:45,073 | INFO | min output length: 2
2026-01-28 20:10:45,678 | INFO | end detected at 13
2026-01-28 20:10:45,679 | INFO |  -0.76 * 0.5 =  -0.38 for decoder
2026-01-28 20:10:45,679 | INFO |  -0.04 * 0.5 =  -0.02 for ctc
2026-01-28 20:10:45,679 | INFO | total log probability: -0.40
2026-01-28 20:10:45,679 | INFO | normalized log probability: -0.04
2026-01-28 20:10:45,679 | INFO | total number of ended hypotheses: 137
2026-01-28 20:10:45,679 | INFO | best hypo: ▁la▁deuxième▁scène

2026-01-28 20:10:45,683 | INFO | speech length: 49280
2026-01-28 20:10:45,732 | INFO | decoder input length: 76
2026-01-28 20:10:45,733 | INFO | max output length: 76
2026-01-28 20:10:45,733 | INFO | min output length: 7
2026-01-28 20:10:46,888 | INFO | end detected at 22
2026-01-28 20:10:46,890 | INFO |  -1.28 * 0.5 =  -0.64 for decoder
2026-01-28 20:10:46,890 | INFO |  -0.04 * 0.5 =  -0.02 for ctc
2026-01-28 20:10:46,890 | INFO | total log probability: -0.66
2026-01-28 20:10:46,890 | INFO | normalized log probability: -0.04
2026-01-28 20:10:46,890 | INFO | total number of ended hypotheses: 143
2026-01-28 20:10:46,890 | INFO | best hypo: ▁c'est▁une▁jeune▁fille▁pauvre▁et▁affamée

2026-01-28 20:10:46,893 | INFO | speech length: 12320
2026-01-28 20:10:46,931 | INFO | decoder input length: 18
2026-01-28 20:10:46,931 | INFO | max output length: 18
2026-01-28 20:10:46,932 | INFO | min output length: 1
2026-01-28 20:10:47,296 | INFO | end detected at 7
2026-01-28 20:10:47,297 | INFO |  -0.14 * 0.5 =  -0.07 for decoder
2026-01-28 20:10:47,297 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-28 20:10:47,297 | INFO | total log probability: -0.08
2026-01-28 20:10:47,297 | INFO | normalized log probability: -0.03
2026-01-28 20:10:47,297 | INFO | total number of ended hypotheses: 125
2026-01-28 20:10:47,297 | INFO | best hypo: ▁qui

2026-01-28 20:10:47,299 | INFO | speech length: 28640
2026-01-28 20:10:47,338 | INFO | decoder input length: 44
2026-01-28 20:10:47,338 | INFO | max output length: 44
2026-01-28 20:10:47,339 | INFO | min output length: 4
2026-01-28 20:10:47,850 | INFO | end detected at 12
2026-01-28 20:10:47,852 | INFO |  -0.56 * 0.5 =  -0.28 for decoder
2026-01-28 20:10:47,852 | INFO |  -1.81 * 0.5 =  -0.91 for ctc
2026-01-28 20:10:47,852 | INFO | total log probability: -1.19
2026-01-28 20:10:47,852 | INFO | normalized log probability: -0.17
2026-01-28 20:10:47,852 | INFO | total number of ended hypotheses: 160
2026-01-28 20:10:47,852 | INFO | best hypo: ▁l'ornière

2026-01-28 20:10:47,854 | INFO | speech length: 38560
2026-01-28 20:10:47,889 | INFO | decoder input length: 59
2026-01-28 20:10:47,889 | INFO | max output length: 59
2026-01-28 20:10:47,889 | INFO | min output length: 5
2026-01-28 20:10:48,943 | INFO | end detected at 20
2026-01-28 20:10:48,945 | INFO |  -2.72 * 0.5 =  -1.36 for decoder
2026-01-28 20:10:48,945 | INFO |  -2.93 * 0.5 =  -1.47 for ctc
2026-01-28 20:10:48,945 | INFO | total log probability: -2.83
2026-01-28 20:10:48,945 | INFO | normalized log probability: -0.20
2026-01-28 20:10:48,945 | INFO | total number of ended hypotheses: 164
2026-01-28 20:10:48,945 | INFO | best hypo: ▁la▁vitrine▁a▁pâtissier

2026-01-28 20:10:48,947 | INFO | speech length: 92800
2026-01-28 20:10:48,988 | INFO | decoder input length: 144
2026-01-28 20:10:48,988 | INFO | max output length: 144
2026-01-28 20:10:48,988 | INFO | min output length: 14
2026-01-28 20:10:51,232 | INFO | end detected at 42
2026-01-28 20:10:51,235 | INFO |  -4.50 * 0.5 =  -2.25 for decoder
2026-01-28 20:10:51,235 | INFO |  -4.95 * 0.5 =  -2.47 for ctc
2026-01-28 20:10:51,235 | INFO | total log probability: -4.73
2026-01-28 20:10:51,235 | INFO | normalized log probability: -0.14
2026-01-28 20:10:51,235 | INFO | total number of ended hypotheses: 195
2026-01-28 20:10:51,236 | INFO | best hypo: ▁elle▁voit▁le▁garçon▁pâtissier▁qui▁transporte▁des▁plateaux▁chargés▁d'eux

2026-01-28 20:10:51,238 | INFO | speech length: 172800
2026-01-28 20:10:51,287 | INFO | decoder input length: 269
2026-01-28 20:10:51,287 | INFO | max output length: 269
2026-01-28 20:10:51,287 | INFO | min output length: 26
2026-01-28 20:10:54,497 | INFO | end detected at 46
2026-01-28 20:10:54,500 | INFO |  -8.61 * 0.5 =  -4.30 for decoder
2026-01-28 20:10:54,500 | INFO | -18.96 * 0.5 =  -9.48 for ctc
2026-01-28 20:10:54,500 | INFO | total log probability: -13.78
2026-01-28 20:10:54,501 | INFO | normalized log probability: -0.37
2026-01-28 20:10:54,501 | INFO | total number of ended hypotheses: 212
2026-01-28 20:10:54,501 | INFO | best hypo: ▁friandise▁gâteau▁etc▁et▁donc▁elle▁vole▁elle▁emprunte▁dans▁le▁camion▁de▁livraison▁en▁une

2026-01-28 20:10:54,504 | INFO | speech length: 32000
2026-01-28 20:10:54,558 | INFO | decoder input length: 49
2026-01-28 20:10:54,558 | INFO | max output length: 49
2026-01-28 20:10:54,558 | INFO | min output length: 4
2026-01-28 20:10:55,463 | INFO | end detected at 18
2026-01-28 20:10:55,466 | INFO |  -1.08 * 0.5 =  -0.54 for decoder
2026-01-28 20:10:55,481 | INFO |  -1.31 * 0.5 =  -0.66 for ctc
2026-01-28 20:10:55,481 | INFO | total log probability: -1.20
2026-01-28 20:10:55,481 | INFO | normalized log probability: -0.09
2026-01-28 20:10:55,481 | INFO | total number of ended hypotheses: 145
2026-01-28 20:10:55,481 | INFO | best hypo: ▁un▁sandwich▁ou▁une▁baguette

2026-01-28 20:10:55,484 | INFO | speech length: 50240
2026-01-28 20:10:55,564 | INFO | decoder input length: 78
2026-01-28 20:10:55,564 | INFO | max output length: 78
2026-01-28 20:10:55,564 | INFO | min output length: 7
2026-01-28 20:10:56,275 | INFO | end detected at 15
2026-01-28 20:10:56,277 | INFO |  -0.73 * 0.5 =  -0.37 for decoder
2026-01-28 20:10:56,277 | INFO |  -0.11 * 0.5 =  -0.05 for ctc
2026-01-28 20:10:56,277 | INFO | total log probability: -0.42
2026-01-28 20:10:56,277 | INFO | normalized log probability: -0.04
2026-01-28 20:10:56,277 | INFO | total number of ended hypotheses: 161
2026-01-28 20:10:56,277 | INFO | best hypo: ▁elle▁elle▁s'enfuit

2026-01-28 20:10:56,279 | INFO | speech length: 116480
2026-01-28 20:10:56,333 | INFO | decoder input length: 181
2026-01-28 20:10:56,333 | INFO | max output length: 181
2026-01-28 20:10:56,333 | INFO | min output length: 18
2026-01-28 20:11:00,059 | INFO | end detected at 56
2026-01-28 20:11:00,061 | INFO |  -5.45 * 0.5 =  -2.73 for decoder
2026-01-28 20:11:00,061 | INFO |  -2.09 * 0.5 =  -1.04 for ctc
2026-01-28 20:11:00,061 | INFO | total log probability: -3.77
2026-01-28 20:11:00,061 | INFO | normalized log probability: -0.07
2026-01-28 20:11:00,061 | INFO | total number of ended hypotheses: 163
2026-01-28 20:11:00,062 | INFO | best hypo: ▁mais▁malheureusement▁heureusement▁il▁se▁heurte▁à▁personnage▁de▁charlot▁qui▁apparaît▁au▁coin▁de▁la▁rue▁il▁tombe

2026-01-28 20:11:00,065 | INFO | speech length: 130720
2026-01-28 20:11:00,110 | INFO | decoder input length: 203
2026-01-28 20:11:00,111 | INFO | max output length: 203
2026-01-28 20:11:00,111 | INFO | min output length: 20
2026-01-28 20:11:04,027 | INFO | end detected at 53
2026-01-28 20:11:04,029 | INFO |  -5.08 * 0.5 =  -2.54 for decoder
2026-01-28 20:11:04,029 | INFO |  -2.81 * 0.5 =  -1.41 for ctc
2026-01-28 20:11:04,029 | INFO | total log probability: -3.95
2026-01-28 20:11:04,029 | INFO | normalized log probability: -0.08
2026-01-28 20:11:04,029 | INFO | total number of ended hypotheses: 171
2026-01-28 20:11:04,030 | INFO | best hypo: ▁une▁dame▁vertueuse▁et▁bien▁pensante▁de▁bonne▁bourgeoisie▁dénonce▁le▁larsin▁au▁solide▁gaillard▁qui▁est▁le

2026-01-28 20:11:04,032 | INFO | speech length: 128960
2026-01-28 20:11:04,083 | INFO | decoder input length: 201
2026-01-28 20:11:04,083 | INFO | max output length: 201
2026-01-28 20:11:04,084 | INFO | min output length: 20
2026-01-28 20:11:07,138 | INFO | end detected at 51
2026-01-28 20:11:07,139 | INFO |  -3.54 * 0.5 =  -1.77 for decoder
2026-01-28 20:11:07,139 | INFO |  -0.27 * 0.5 =  -0.13 for ctc
2026-01-28 20:11:07,139 | INFO | total log probability: -1.90
2026-01-28 20:11:07,139 | INFO | normalized log probability: -0.04
2026-01-28 20:11:07,139 | INFO | total number of ended hypotheses: 140
2026-01-28 20:11:07,140 | INFO | best hypo: ▁qui▁est▁donc▁l'employé▁de▁la▁pâtisserie▁qui▁se▁précipite▁pour▁rétablir▁l'ordre▁au▁même▁moment▁arrive▁un▁policier

2026-01-28 20:11:07,141 | INFO | speech length: 230560
2026-01-28 20:11:07,208 | INFO | decoder input length: 359
2026-01-28 20:11:07,208 | INFO | max output length: 359
2026-01-28 20:11:07,209 | INFO | min output length: 35
2026-01-28 20:11:17,198 | INFO | end detected at 98
2026-01-28 20:11:17,199 | INFO | -165.41 * 0.5 = -82.71 for decoder
2026-01-28 20:11:17,199 | INFO | -55.82 * 0.5 = -27.91 for ctc
2026-01-28 20:11:17,199 | INFO | total log probability: -110.62
2026-01-28 20:11:17,200 | INFO | normalized log probability: -1.19
2026-01-28 20:11:17,200 | INFO | total number of ended hypotheses: 160
2026-01-28 20:11:17,201 | INFO | best hypo: ▁et▁après▁quelques▁explications▁on▁comprend▁que▁charlot▁prend▁la▁faute▁ou▁le▁l'arcin▁sur▁lui▁et▁s'en▁met▁et▁est▁emmené▁par▁le▁policier▁et▁de▁la▁jeune▁fille▁elle▁est▁toute▁ébahis▁et▁se▁retrouvent▁seul▁dans▁le▁trottoir▁mais▁libre

2026-01-28 20:11:17,203 | INFO | speech length: 42400
2026-01-28 20:11:17,253 | INFO | decoder input length: 65
2026-01-28 20:11:17,253 | INFO | max output length: 65
2026-01-28 20:11:17,253 | INFO | min output length: 6
2026-01-28 20:11:18,552 | INFO | end detected at 31
2026-01-28 20:11:18,555 | INFO |  -3.09 * 0.5 =  -1.54 for decoder
2026-01-28 20:11:18,555 | INFO |  -7.77 * 0.5 =  -3.89 for ctc
2026-01-28 20:11:18,555 | INFO | total log probability: -5.43
2026-01-28 20:11:18,555 | INFO | normalized log probability: -0.22
2026-01-28 20:11:18,555 | INFO | total number of ended hypotheses: 159
2026-01-28 20:11:18,555 | INFO | best hypo: ▁je▁crois▁que▁c'est▁à▁peu▁près▁tout▁cette▁scène▁là

2026-01-28 20:11:18,562 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:11:18,562 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:11:18,562 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:11:18,562 | INFO | Chunk: 3 | WER=200.000000 | S=1 D=0 I=1
2026-01-28 20:11:18,562 | INFO | Chunk: 4 | WER=40.000000 | S=1 D=1 I=0
2026-01-28 20:11:18,563 | INFO | Chunk: 5 | WER=20.000000 | S=0 D=0 I=2
2026-01-28 20:11:18,563 | INFO | Chunk: 6 | WER=37.500000 | S=2 D=2 I=2
2026-01-28 20:11:18,563 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:11:18,564 | INFO | Chunk: 8 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:11:18,564 | INFO | Chunk: 9 | WER=26.315789 | S=3 D=1 I=1
2026-01-28 20:11:18,565 | INFO | Chunk: 10 | WER=26.666667 | S=1 D=0 I=3
2026-01-28 20:11:18,565 | INFO | Chunk: 11 | WER=8.695652 | S=0 D=2 I=0
2026-01-28 20:11:18,567 | INFO | Chunk: 12 | WER=34.146341 | S=8 D=1 I=5
2026-01-28 20:11:18,567 | INFO | Chunk: 13 | WER=16.666667 | S=0 D=1 I=1
2026-01-28 20:11:18,584 | INFO | File: Rhap-M0021.wav | WER=23.926380 | S=18 D=7 I=14
2026-01-28 20:11:18,584 | INFO | ------------------------------
2026-01-28 20:11:18,584 | INFO | Conf cv Done!
2026-01-28 20:11:18,764 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 20:11:18,787 | INFO | Vocabulary size: 47
2026-01-28 20:11:19,818 | INFO | Gradient checkpoint layers: []
2026-01-28 20:11:20,502 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 20:11:20,506 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 20:11:20,506 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 20:11:20,507 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 20:11:20,509 | INFO | speech length: 18240
2026-01-28 20:11:20,548 | INFO | decoder input length: 28
2026-01-28 20:11:20,548 | INFO | max output length: 28
2026-01-28 20:11:20,548 | INFO | min output length: 2
2026-01-28 20:11:21,488 | INFO | end detected at 24
2026-01-28 20:11:21,489 | INFO |  -1.42 * 0.5 =  -0.71 for decoder
2026-01-28 20:11:21,490 | INFO |  -0.24 * 0.5 =  -0.12 for ctc
2026-01-28 20:11:21,490 | INFO | total log probability: -0.83
2026-01-28 20:11:21,490 | INFO | normalized log probability: -0.04
2026-01-28 20:11:21,490 | INFO | total number of ended hypotheses: 167
2026-01-28 20:11:21,490 | INFO | best hypo: la<space>deuxième<space>scène

2026-01-28 20:11:21,509 | INFO | speech length: 49280
2026-01-28 20:11:21,607 | INFO | decoder input length: 76
2026-01-28 20:11:21,607 | INFO | max output length: 76
2026-01-28 20:11:21,607 | INFO | min output length: 7
2026-01-28 20:11:23,457 | INFO | end detected at 46
2026-01-28 20:11:23,458 | INFO |  -3.32 * 0.5 =  -1.66 for decoder
2026-01-28 20:11:23,458 | INFO |  -2.16 * 0.5 =  -1.08 for ctc
2026-01-28 20:11:23,458 | INFO | total log probability: -2.74
2026-01-28 20:11:23,459 | INFO | normalized log probability: -0.07
2026-01-28 20:11:23,459 | INFO | total number of ended hypotheses: 169
2026-01-28 20:11:23,459 | INFO | best hypo: c'est<space>une<space>jeune<space>fille<space>pauvre<space>et<space>affamée

2026-01-28 20:11:23,461 | INFO | speech length: 12320
2026-01-28 20:11:23,496 | INFO | decoder input length: 18
2026-01-28 20:11:23,497 | INFO | max output length: 18
2026-01-28 20:11:23,497 | INFO | min output length: 1
2026-01-28 20:11:23,828 | INFO | end detected at 9
2026-01-28 20:11:23,829 | INFO |  -0.32 * 0.5 =  -0.16 for decoder
2026-01-28 20:11:23,829 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 20:11:23,829 | INFO | total log probability: -0.16
2026-01-28 20:11:23,829 | INFO | normalized log probability: -0.03
2026-01-28 20:11:23,829 | INFO | total number of ended hypotheses: 126
2026-01-28 20:11:23,830 | INFO | best hypo: qui

2026-01-28 20:11:23,831 | INFO | speech length: 28640
2026-01-28 20:11:23,866 | INFO | decoder input length: 44
2026-01-28 20:11:23,866 | INFO | max output length: 44
2026-01-28 20:11:23,866 | INFO | min output length: 4
2026-01-28 20:11:25,090 | INFO | end detected at 33
2026-01-28 20:11:25,092 | INFO |  -3.06 * 0.5 =  -1.53 for decoder
2026-01-28 20:11:25,092 | INFO | -11.11 * 0.5 =  -5.55 for ctc
2026-01-28 20:11:25,092 | INFO | total log probability: -7.08
2026-01-28 20:11:25,092 | INFO | normalized log probability: -0.54
2026-01-28 20:11:25,092 | INFO | total number of ended hypotheses: 173
2026-01-28 20:11:25,092 | INFO | best hypo: l'or<space>ni<space>euh

2026-01-28 20:11:25,094 | INFO | speech length: 38560
2026-01-28 20:11:25,129 | INFO | decoder input length: 59
2026-01-28 20:11:25,129 | INFO | max output length: 59
2026-01-28 20:11:25,129 | INFO | min output length: 5
2026-01-28 20:11:26,394 | INFO | end detected at 33
2026-01-28 20:11:26,396 | INFO |  -2.17 * 0.5 =  -1.08 for decoder
2026-01-28 20:11:26,396 | INFO |  -0.57 * 0.5 =  -0.29 for ctc
2026-01-28 20:11:26,396 | INFO | total log probability: -1.37
2026-01-28 20:11:26,396 | INFO | normalized log probability: -0.05
2026-01-28 20:11:26,396 | INFO | total number of ended hypotheses: 177
2026-01-28 20:11:26,396 | INFO | best hypo: la<space>vitrine<space>d'un<space>pâtissier

2026-01-28 20:11:26,398 | INFO | speech length: 92800
2026-01-28 20:11:26,434 | INFO | decoder input length: 144
2026-01-28 20:11:26,434 | INFO | max output length: 144
2026-01-28 20:11:26,434 | INFO | min output length: 14
2026-01-28 20:11:30,121 | INFO | end detected at 79
2026-01-28 20:11:30,123 | INFO |  -5.87 * 0.5 =  -2.94 for decoder
2026-01-28 20:11:30,123 | INFO |  -2.37 * 0.5 =  -1.18 for ctc
2026-01-28 20:11:30,123 | INFO | total log probability: -4.12
2026-01-28 20:11:30,123 | INFO | normalized log probability: -0.06
2026-01-28 20:11:30,123 | INFO | total number of ended hypotheses: 177
2026-01-28 20:11:30,124 | INFO | best hypo: elle<space>voit<space>le<space>garçon<space>pâtissier<space>euh<space>qui<space>transporte<space>des<space>plateaux<space>chargés<space>de

2026-01-28 20:11:30,127 | INFO | speech length: 172800
2026-01-28 20:11:30,168 | INFO | decoder input length: 269
2026-01-28 20:11:30,168 | INFO | max output length: 269
2026-01-28 20:11:30,168 | INFO | min output length: 26
2026-01-28 20:11:38,697 | INFO | end detected at 143
2026-01-28 20:11:38,700 | INFO | -27.03 * 0.5 = -13.51 for decoder
2026-01-28 20:11:38,700 | INFO | -14.51 * 0.5 =  -7.26 for ctc
2026-01-28 20:11:38,700 | INFO | total log probability: -20.77
2026-01-28 20:11:38,700 | INFO | normalized log probability: -0.15
2026-01-28 20:11:38,700 | INFO | total number of ended hypotheses: 150
2026-01-28 20:11:38,703 | INFO | best hypo: friandises<space>gatto<space>etc<space>et<space>donc<space>elles<space>pour<space>cent<space>hesitation<space>elles<space>volent<space>elle<space>empreinte<space>dans<space>le<space>camion<space>de<space>livraisons<space>une<space>pou<space>cent<space>hesitation

2026-01-28 20:11:38,706 | INFO | speech length: 32000
2026-01-28 20:11:38,813 | INFO | decoder input length: 49
2026-01-28 20:11:38,814 | INFO | max output length: 49
2026-01-28 20:11:38,814 | INFO | min output length: 4
2026-01-28 20:11:40,372 | INFO | end detected at 41
2026-01-28 20:11:40,374 | INFO |  -6.67 * 0.5 =  -3.34 for decoder
2026-01-28 20:11:40,374 | INFO | -10.34 * 0.5 =  -5.17 for ctc
2026-01-28 20:11:40,374 | INFO | total log probability: -8.51
2026-01-28 20:11:40,374 | INFO | normalized log probability: -0.27
2026-01-28 20:11:40,374 | INFO | total number of ended hypotheses: 212
2026-01-28 20:11:40,375 | INFO | best hypo: un<space>sandomi<space>chouer<space>une<space>baguette

2026-01-28 20:11:40,377 | INFO | speech length: 50240
2026-01-28 20:11:40,446 | INFO | decoder input length: 78
2026-01-28 20:11:40,446 | INFO | max output length: 78
2026-01-28 20:11:40,446 | INFO | min output length: 7
2026-01-28 20:11:43,768 | INFO | end detected at 47
2026-01-28 20:11:43,773 | INFO |  -4.39 * 0.5 =  -2.20 for decoder
2026-01-28 20:11:43,773 | INFO | -10.60 * 0.5 =  -5.30 for ctc
2026-01-28 20:11:43,773 | INFO | total log probability: -7.50
2026-01-28 20:11:43,773 | INFO | normalized log probability: -0.18
2026-01-28 20:11:43,773 | INFO | total number of ended hypotheses: 233
2026-01-28 20:11:43,774 | INFO | best hypo: elle<space>pour<space>cent<space>hesitation<space>elle<space>s'enfuie

2026-01-28 20:11:43,779 | INFO | speech length: 116480
2026-01-28 20:11:43,835 | INFO | decoder input length: 181
2026-01-28 20:11:43,835 | INFO | max output length: 181
2026-01-28 20:11:43,835 | INFO | min output length: 18
2026-01-28 20:11:50,985 | INFO | end detected at 126
2026-01-28 20:11:50,989 | INFO | -11.55 * 0.5 =  -5.78 for decoder
2026-01-28 20:11:50,989 | INFO |  -3.80 * 0.5 =  -1.90 for ctc
2026-01-28 20:11:50,989 | INFO | total log probability: -7.68
2026-01-28 20:11:50,989 | INFO | normalized log probability: -0.07
2026-01-28 20:11:50,989 | INFO | total number of ended hypotheses: 244
2026-01-28 20:11:50,991 | INFO | best hypo: et<space>malheureusement<space>ou<space>heureusement<space>elle<space>se<space>heurte<space>à<space>un<space>personnage<space>de<space>charlot<space>qui<space>apparaît<space>au<space>coin<space>de<space>la<space>rue<space>il<space>tombe

2026-01-28 20:11:50,994 | INFO | speech length: 130720
2026-01-28 20:11:51,052 | INFO | decoder input length: 203
2026-01-28 20:11:51,053 | INFO | max output length: 203
2026-01-28 20:11:51,053 | INFO | min output length: 20
2026-01-28 20:11:59,435 | INFO | end detected at 118
2026-01-28 20:11:59,437 | INFO | -11.84 * 0.5 =  -5.92 for decoder
2026-01-28 20:11:59,437 | INFO |  -4.12 * 0.5 =  -2.06 for ctc
2026-01-28 20:11:59,437 | INFO | total log probability: -7.98
2026-01-28 20:11:59,437 | INFO | normalized log probability: -0.07
2026-01-28 20:11:59,437 | INFO | total number of ended hypotheses: 197
2026-01-28 20:11:59,439 | INFO | best hypo: une<space>dame<space>vertueuse<space>et<space>bien<space>pensante<space>de<space>bonne<space>bourgeoisie<space>dénonce<space>le<space>larsin<space>euh<space>au<space>solide<space>gaillard<space>qui<space>est<space>le

2026-01-28 20:11:59,441 | INFO | speech length: 128960
2026-01-28 20:11:59,481 | INFO | decoder input length: 201
2026-01-28 20:11:59,481 | INFO | max output length: 201
2026-01-28 20:11:59,481 | INFO | min output length: 20
2026-01-28 20:12:07,136 | INFO | end detected at 124
2026-01-28 20:12:07,138 | INFO |  -9.94 * 0.5 =  -4.97 for decoder
2026-01-28 20:12:07,138 | INFO |  -0.94 * 0.5 =  -0.47 for ctc
2026-01-28 20:12:07,139 | INFO | total log probability: -5.44
2026-01-28 20:12:07,139 | INFO | normalized log probability: -0.05
2026-01-28 20:12:07,139 | INFO | total number of ended hypotheses: 179
2026-01-28 20:12:07,141 | INFO | best hypo: qui<space>est<space>donc<space>le<space>l'employé<space>de<space>la<space>pâtisserie<space>qui<space>se<space>précipite<space>pour<space>rétablir<space>l'ordre<space>au<space>même<space>moment<space>arrive<space>à<space>un<space>policier

2026-01-28 20:12:07,143 | INFO | speech length: 230560
2026-01-28 20:12:07,184 | INFO | decoder input length: 359
2026-01-28 20:12:07,184 | INFO | max output length: 359
2026-01-28 20:12:07,184 | INFO | min output length: 35
2026-01-28 20:12:24,518 | INFO | end detected at 267
2026-01-28 20:12:24,520 | INFO | -40.59 * 0.5 = -20.30 for decoder
2026-01-28 20:12:24,520 | INFO | -25.50 * 0.5 = -12.75 for ctc
2026-01-28 20:12:24,520 | INFO | total log probability: -33.05
2026-01-28 20:12:24,520 | INFO | normalized log probability: -0.13
2026-01-28 20:12:24,520 | INFO | total number of ended hypotheses: 228
2026-01-28 20:12:24,524 | INFO | best hypo: et<space>pour<space>cent<space>hesitation<space>après<space>quelques<space>explications<space>on<space>comprend<space>que<space>charlot<space>prend<space>la<space>la<space>faute<space>ou<space>le<space>l'arsain<space>sur<space>lui<space>et<space>sans<space>met<space>et<space>est<space>emmené<space>par<space>le<space>policier<space>euh<space>et<space>que<space>la<space>jeune<space>fille<space>euh<space>tout<space>est<space>tout<space>est<space>bailli<space>se<space>retrouve<space>seul<space>sur<space>le<space>trottoir<space>mais<space>libre

2026-01-28 20:12:24,527 | INFO | speech length: 42400
2026-01-28 20:12:24,572 | INFO | decoder input length: 65
2026-01-28 20:12:24,572 | INFO | max output length: 65
2026-01-28 20:12:24,572 | INFO | min output length: 6
2026-01-28 20:12:26,811 | INFO | end detected at 61
2026-01-28 20:12:26,813 | INFO |  -6.31 * 0.5 =  -3.16 for decoder
2026-01-28 20:12:26,813 | INFO |  -8.59 * 0.5 =  -4.29 for ctc
2026-01-28 20:12:26,813 | INFO | total log probability: -7.45
2026-01-28 20:12:26,813 | INFO | normalized log probability: -0.16
2026-01-28 20:12:26,813 | INFO | total number of ended hypotheses: 211
2026-01-28 20:12:26,814 | INFO | best hypo: je<space>crois<space>que<space>c'est<space>à<space>peu<space>près<space>et<space>tout<space>c'est<space>là

2026-01-28 20:12:26,821 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:12:26,821 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:12:26,821 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:12:26,821 | INFO | Chunk: 3 | WER=400.000000 | S=1 D=0 I=3
2026-01-28 20:12:26,821 | INFO | Chunk: 4 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:12:26,822 | INFO | Chunk: 5 | WER=20.000000 | S=0 D=0 I=2
2026-01-28 20:12:26,822 | INFO | Chunk: 6 | WER=100.000000 | S=8 D=1 I=7
2026-01-28 20:12:26,823 | INFO | Chunk: 7 | WER=40.000000 | S=2 D=0 I=0
2026-01-28 20:12:26,823 | INFO | Chunk: 8 | WER=100.000000 | S=1 D=0 I=3
2026-01-28 20:12:26,823 | INFO | Chunk: 9 | WER=15.789474 | S=1 D=0 I=2
2026-01-28 20:12:26,824 | INFO | Chunk: 10 | WER=33.333333 | S=1 D=0 I=4
2026-01-28 20:12:26,824 | INFO | Chunk: 11 | WER=4.347826 | S=1 D=0 I=0
2026-01-28 20:12:26,826 | INFO | Chunk: 12 | WER=41.463415 | S=8 D=0 I=9
2026-01-28 20:12:26,826 | INFO | Chunk: 13 | WER=33.333333 | S=3 D=0 I=1
2026-01-28 20:12:26,845 | INFO | File: Rhap-M0021.wav | WER=34.969325 | S=27 D=0 I=30
2026-01-28 20:12:26,845 | INFO | ------------------------------
2026-01-28 20:12:26,845 | INFO | Conf ester Done!
2026-01-28 20:13:58,404 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:13:58,404 | INFO | Chunk: 1 | WER=12.500000 | S=1 D=0 I=0
2026-01-28 20:13:58,405 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:13:58,405 | INFO | Chunk: 3 | WER=200.000000 | S=1 D=0 I=1
2026-01-28 20:13:58,405 | INFO | Chunk: 4 | WER=40.000000 | S=0 D=2 I=0
2026-01-28 20:13:58,406 | INFO | Chunk: 5 | WER=20.000000 | S=1 D=0 I=1
2026-01-28 20:13:58,407 | INFO | Chunk: 6 | WER=62.500000 | S=7 D=1 I=2
2026-01-28 20:13:58,408 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:13:58,408 | INFO | Chunk: 8 | WER=75.000000 | S=2 D=0 I=1
2026-01-28 20:13:58,409 | INFO | Chunk: 9 | WER=31.578947 | S=4 D=0 I=2
2026-01-28 20:13:58,410 | INFO | Chunk: 10 | WER=60.000000 | S=6 D=0 I=3
2026-01-28 20:13:58,412 | INFO | Chunk: 11 | WER=8.695652 | S=1 D=1 I=0
2026-01-28 20:13:58,415 | INFO | Chunk: 12 | WER=41.463415 | S=13 D=2 I=2
2026-01-28 20:13:58,415 | INFO | Chunk: 13 | WER=25.000000 | S=0 D=2 I=1
2026-01-28 20:13:58,451 | INFO | File: Rhap-M0021.wav | WER=34.969325 | S=36 D=8 I=13
2026-01-28 20:13:58,451 | INFO | ------------------------------
2026-01-28 20:13:58,451 | INFO | hmm_tdnn Done!
2026-01-28 20:13:58,629 | INFO | ==================================Rhap-M0022.wav=========================================
2026-01-28 20:13:58,777 | INFO | Using rVAD model
2026-01-28 20:14:09,198 | INFO | Chunk: 0 | WER=200.000000 | S=1 D=0 I=1
2026-01-28 20:14:09,201 | INFO | Chunk: 1 | WER=22.222222 | S=5 D=5 I=2
2026-01-28 20:14:09,201 | INFO | Chunk: 2 | WER=100.000000 | S=0 D=2 I=1
2026-01-28 20:14:09,201 | INFO | Chunk: 3 | WER=14.285714 | S=1 D=0 I=1
2026-01-28 20:14:09,202 | INFO | Chunk: 4 | WER=7.692308 | S=1 D=0 I=0
2026-01-28 20:14:09,203 | INFO | Chunk: 5 | WER=22.448980 | S=3 D=8 I=0
2026-01-28 20:14:09,206 | INFO | Chunk: 6 | WER=4.687500 | S=1 D=2 I=0
2026-01-28 20:14:09,206 | INFO | Chunk: 7 | WER=60.000000 | S=2 D=1 I=0
2026-01-28 20:14:09,228 | INFO | File: Rhap-M0022.wav | WER=17.241379 | S=14 D=17 I=4
2026-01-28 20:14:09,228 | INFO | ------------------------------
2026-01-28 20:14:09,229 | INFO | w2vec vad chunk Done!
2026-01-28 20:14:19,724 | INFO | Chunk: 0 | WER=100.000000 | S=0 D=0 I=1
2026-01-28 20:14:19,727 | INFO | Chunk: 1 | WER=37.037037 | S=2 D=18 I=0
2026-01-28 20:14:19,727 | INFO | Chunk: 2 | WER=66.666667 | S=0 D=2 I=0
2026-01-28 20:14:19,727 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:14:19,728 | INFO | Chunk: 4 | WER=30.769231 | S=4 D=0 I=0
2026-01-28 20:14:19,730 | INFO | Chunk: 5 | WER=36.734694 | S=6 D=12 I=0
2026-01-28 20:14:19,733 | INFO | Chunk: 6 | WER=29.687500 | S=2 D=17 I=0
2026-01-28 20:14:19,733 | INFO | Chunk: 7 | WER=60.000000 | S=2 D=1 I=0
2026-01-28 20:14:19,758 | INFO | File: Rhap-M0022.wav | WER=33.004926 | S=16 D=50 I=1
2026-01-28 20:14:19,758 | INFO | ------------------------------
2026-01-28 20:14:19,759 | INFO | whisper med Done!
2026-01-28 20:14:34,566 | INFO | Chunk: 0 | WER=100.000000 | S=1 D=0 I=0
2026-01-28 20:14:34,568 | INFO | Chunk: 1 | WER=35.185185 | S=1 D=18 I=0
2026-01-28 20:14:34,568 | INFO | Chunk: 2 | WER=100.000000 | S=2 D=1 I=0
2026-01-28 20:14:34,568 | INFO | Chunk: 3 | WER=14.285714 | S=1 D=1 I=0
2026-01-28 20:14:34,569 | INFO | Chunk: 4 | WER=7.692308 | S=1 D=0 I=0
2026-01-28 20:14:34,570 | INFO | Chunk: 5 | WER=24.489796 | S=2 D=9 I=1
2026-01-28 20:14:34,573 | INFO | Chunk: 6 | WER=31.250000 | S=7 D=12 I=1
2026-01-28 20:14:34,573 | INFO | Chunk: 7 | WER=60.000000 | S=2 D=0 I=1
2026-01-28 20:14:34,593 | INFO | File: Rhap-M0022.wav | WER=30.049261 | S=17 D=41 I=3
2026-01-28 20:14:34,593 | INFO | ------------------------------
2026-01-28 20:14:34,593 | INFO | whisper large Done!
2026-01-28 20:14:34,767 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 20:14:34,805 | INFO | Vocabulary size: 350
2026-01-28 20:14:35,960 | INFO | Gradient checkpoint layers: []
2026-01-28 20:14:36,667 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 20:14:36,671 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 20:14:36,671 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 20:14:36,672 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 20:14:36,672 | INFO | speech length: 34240
2026-01-28 20:14:36,721 | INFO | decoder input length: 53
2026-01-28 20:14:36,722 | INFO | max output length: 53
2026-01-28 20:14:36,722 | INFO | min output length: 5
2026-01-28 20:14:37,214 | INFO | end detected at 11
2026-01-28 20:14:37,215 | INFO |  -3.71 * 0.5 =  -1.85 for decoder
2026-01-28 20:14:37,215 | INFO |  -6.53 * 0.5 =  -3.27 for ctc
2026-01-28 20:14:37,215 | INFO | total log probability: -5.12
2026-01-28 20:14:37,215 | INFO | normalized log probability: -0.73
2026-01-28 20:14:37,215 | INFO | total number of ended hypotheses: 130
2026-01-28 20:14:37,215 | INFO | best hypo: ▁ah▁doncques

2026-01-28 20:14:37,218 | INFO | speech length: 462720
2026-01-28 20:14:37,256 | INFO | decoder input length: 722
2026-01-28 20:14:37,256 | INFO | max output length: 722
2026-01-28 20:14:37,256 | INFO | min output length: 72
2026-01-28 20:14:53,256 | INFO | end detected at 122
2026-01-28 20:14:53,259 | INFO | -185.37 * 0.5 = -92.69 for decoder
2026-01-28 20:14:53,259 | INFO | -50.11 * 0.5 = -25.06 for ctc
2026-01-28 20:14:53,259 | INFO | total log probability: -117.74
2026-01-28 20:14:53,259 | INFO | normalized log probability: -1.02
2026-01-28 20:14:53,259 | INFO | total number of ended hypotheses: 188
2026-01-28 20:14:53,260 | INFO | best hypo: ▁on▁voit▁chaplin▁s'habiller▁pour▁sortir▁son▁usine▁ensuite▁on▁voit▁une▁jeune▁fille▁habillée▁en▁noir▁qui▁regarde▁une▁devanture▁de▁magasins▁qui▁profitent▁d'un▁aller▁retour▁de▁boulanger▁pour▁piquer▁une▁baguette▁sur▁ce▁une▁vieille▁femme▁le▁voit▁le▁vol▁vient▁boulanger▁la▁jeune

2026-01-28 20:14:53,263 | INFO | speech length: 29600
2026-01-28 20:14:53,316 | INFO | decoder input length: 45
2026-01-28 20:14:53,316 | INFO | max output length: 45
2026-01-28 20:14:53,316 | INFO | min output length: 4
2026-01-28 20:14:53,997 | INFO | end detected at 11
2026-01-28 20:14:53,998 | INFO |  -5.00 * 0.5 =  -2.50 for decoder
2026-01-28 20:14:53,998 | INFO | -10.75 * 0.5 =  -5.38 for ctc
2026-01-28 20:14:53,998 | INFO | total log probability: -7.88
2026-01-28 20:14:53,998 | INFO | normalized log probability: -1.31
2026-01-28 20:14:53,998 | INFO | total number of ended hypotheses: 160
2026-01-28 20:14:53,999 | INFO | best hypo: ▁fi▁hey

2026-01-28 20:14:54,001 | INFO | speech length: 138400
2026-01-28 20:14:54,056 | INFO | decoder input length: 215
2026-01-28 20:14:54,057 | INFO | max output length: 215
2026-01-28 20:14:54,057 | INFO | min output length: 21
2026-01-28 20:14:56,712 | INFO | end detected at 41
2026-01-28 20:14:56,714 | INFO |  -3.28 * 0.5 =  -1.64 for decoder
2026-01-28 20:14:56,714 | INFO |  -3.19 * 0.5 =  -1.60 for ctc
2026-01-28 20:14:56,714 | INFO | total log probability: -3.24
2026-01-28 20:14:56,714 | INFO | normalized log probability: -0.09
2026-01-28 20:14:56,714 | INFO | total number of ended hypotheses: 164
2026-01-28 20:14:56,715 | INFO | best hypo: ▁il▁se▁retrouve▁par▁terre▁avec▁chaplin▁au▁sol▁le▁boulanger▁arrive▁à▁la▁rattraper

2026-01-28 20:14:56,717 | INFO | speech length: 144480
2026-01-28 20:14:56,763 | INFO | decoder input length: 225
2026-01-28 20:14:56,763 | INFO | max output length: 225
2026-01-28 20:14:56,763 | INFO | min output length: 22
2026-01-28 20:14:59,415 | INFO | end detected at 41
2026-01-28 20:14:59,417 | INFO |  -9.58 * 0.5 =  -4.79 for decoder
2026-01-28 20:14:59,417 | INFO |  -6.49 * 0.5 =  -3.25 for ctc
2026-01-28 20:14:59,417 | INFO | total log probability: -8.03
2026-01-28 20:14:59,417 | INFO | normalized log probability: -0.24
2026-01-28 20:14:59,417 | INFO | total number of ended hypotheses: 199
2026-01-28 20:14:59,418 | INFO | best hypo: ▁à▁travers▁la▁foule▁le▁policier▁arrive▁demande▁des▁explications▁de▁ce▁broat

2026-01-28 20:14:59,421 | INFO | speech length: 308320
2026-01-28 20:14:59,466 | INFO | decoder input length: 481
2026-01-28 20:14:59,467 | INFO | max output length: 481
2026-01-28 20:14:59,467 | INFO | min output length: 48
2026-01-28 20:15:09,376 | INFO | end detected at 101
2026-01-28 20:15:09,379 | INFO | -229.25 * 0.5 = -114.62 for decoder
2026-01-28 20:15:09,379 | INFO | -81.45 * 0.5 = -40.73 for ctc
2026-01-28 20:15:09,379 | INFO | total log probability: -155.35
2026-01-28 20:15:09,379 | INFO | normalized log probability: -1.65
2026-01-28 20:15:09,379 | INFO | total number of ended hypotheses: 179
2026-01-28 20:15:09,380 | INFO | best hypo: ▁le▁boulanger▁est▁très▁bien▁très▁énervé▁contre▁la▁fille▁explique▁que▁le▁vol▁le▁chapelet▁grand▁seigneur▁explique▁que▁séduit▁qu'à▁la▁baguette▁est▁donc▁la▁pâtte▁de▁vol▁uf▁que▁pour▁policier▁ses▁colon▁du▁vol▁dont▁il▁embarque

2026-01-28 20:15:09,383 | INFO | speech length: 343680
2026-01-28 20:15:09,432 | INFO | decoder input length: 536
2026-01-28 20:15:09,432 | INFO | max output length: 536
2026-01-28 20:15:09,432 | INFO | min output length: 53
2026-01-28 20:15:25,275 | INFO | end detected at 140
2026-01-28 20:15:25,277 | INFO | -391.76 * 0.5 = -195.88 for decoder
2026-01-28 20:15:25,277 | INFO | -111.47 * 0.5 = -55.73 for ctc
2026-01-28 20:15:25,277 | INFO | total log probability: -251.61
2026-01-28 20:15:25,277 | INFO | normalized log probability: -1.86
2026-01-28 20:15:25,277 | INFO | total number of ended hypotheses: 151
2026-01-28 20:15:25,279 | INFO | best hypo: ▁le▁chapeline▁pour▁en▁suite▁on▁se▁retrouve▁donc▁tout▁le▁monde▁s'en▁va▁y▁compris▁la▁foule▁puisque▁la▁foulée▁intéressée▁par▁les▁flics▁on▁se▁retrouve▁b▁avec▁le▁boulanger▁d▁la▁jeune▁fille▁au▁sols▁et▁la▁vieille▁dame▁s▁qui▁revient▁vers▁boulanger▁le▁pour▁lui▁dire'▁non▁'est▁pas▁le▁monsieur▁mais▁c'est▁la▁jeune▁fille▁effectivement

2026-01-28 20:15:25,281 | INFO | speech length: 31520
2026-01-28 20:15:25,320 | INFO | decoder input length: 48
2026-01-28 20:15:25,320 | INFO | max output length: 48
2026-01-28 20:15:25,320 | INFO | min output length: 4
2026-01-28 20:15:25,919 | INFO | end detected at 14
2026-01-28 20:15:25,920 | INFO |  -1.92 * 0.5 =  -0.96 for decoder
2026-01-28 20:15:25,920 | INFO |  -6.63 * 0.5 =  -3.31 for ctc
2026-01-28 20:15:25,920 | INFO | total log probability: -4.27
2026-01-28 20:15:25,920 | INFO | normalized log probability: -0.53
2026-01-28 20:15:25,920 | INFO | total number of ended hypotheses: 177
2026-01-28 20:15:25,920 | INFO | best hypo: ▁et▁il▁était▁revenu

2026-01-28 20:15:25,927 | INFO | Chunk: 0 | WER=200.000000 | S=1 D=0 I=1
2026-01-28 20:15:25,929 | INFO | Chunk: 1 | WER=25.925926 | S=5 D=7 I=2
2026-01-28 20:15:25,929 | INFO | Chunk: 2 | WER=100.000000 | S=2 D=1 I=0
2026-01-28 20:15:25,929 | INFO | Chunk: 3 | WER=7.142857 | S=0 D=0 I=1
2026-01-28 20:15:25,930 | INFO | Chunk: 4 | WER=7.692308 | S=1 D=0 I=0
2026-01-28 20:15:25,932 | INFO | Chunk: 5 | WER=44.897959 | S=11 D=9 I=2
2026-01-28 20:15:25,934 | INFO | Chunk: 6 | WER=20.312500 | S=5 D=4 I=4
2026-01-28 20:15:25,935 | INFO | Chunk: 7 | WER=80.000000 | S=3 D=1 I=0
2026-01-28 20:15:25,957 | INFO | File: Rhap-M0022.wav | WER=29.064039 | S=27 D=22 I=10
2026-01-28 20:15:25,957 | INFO | ------------------------------
2026-01-28 20:15:25,957 | INFO | Conf cv Done!
2026-01-28 20:15:26,134 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 20:15:26,157 | INFO | Vocabulary size: 47
2026-01-28 20:15:27,455 | INFO | Gradient checkpoint layers: []
2026-01-28 20:15:28,158 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 20:15:28,162 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 20:15:28,162 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 20:15:28,163 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 20:15:28,166 | INFO | speech length: 34240
2026-01-28 20:15:28,220 | INFO | decoder input length: 53
2026-01-28 20:15:28,221 | INFO | max output length: 53
2026-01-28 20:15:28,221 | INFO | min output length: 5
2026-01-28 20:15:29,144 | INFO | end detected at 23
2026-01-28 20:15:29,147 | INFO |  -5.68 * 0.5 =  -2.84 for decoder
2026-01-28 20:15:29,147 | INFO |  -5.22 * 0.5 =  -2.61 for ctc
2026-01-28 20:15:29,147 | INFO | total log probability: -5.45
2026-01-28 20:15:29,147 | INFO | normalized log probability: -0.30
2026-01-28 20:15:29,147 | INFO | total number of ended hypotheses: 208
2026-01-28 20:15:29,147 | INFO | best hypo: euh<space>donc<space>qui<space>euh

2026-01-28 20:15:29,150 | INFO | speech length: 462720
2026-01-28 20:15:29,192 | INFO | decoder input length: 722
2026-01-28 20:15:29,192 | INFO | max output length: 722
2026-01-28 20:15:29,192 | INFO | min output length: 72
2026-01-28 20:16:06,561 | INFO | end detected at 327
2026-01-28 20:16:06,562 | INFO | -182.95 * 0.5 = -91.47 for decoder
2026-01-28 20:16:06,562 | INFO | -18.09 * 0.5 =  -9.04 for ctc
2026-01-28 20:16:06,562 | INFO | total log probability: -100.52
2026-01-28 20:16:06,562 | INFO | normalized log probability: -0.31
2026-01-28 20:16:06,562 | INFO | total number of ended hypotheses: 177
2026-01-28 20:16:06,566 | INFO | best hypo: on<space>voit<space>chaqueline<space>euh<space>s'habiller<space>pour<space>sortir<space>de<space>son<space>usine<space>ensuite<space>euh<space>on<space>voit<space>une<space>jeune<space>fille<space>habillée<space>en<space>noir<space>qui<space>regarde<space>une<space>devanture<space>de<space>magasin<space>qui<space>profite<space>d'un<space>aller<space>retour<space>du<space>boulanger<space>on<space>va<space>dire<space>pour<space>piquer<space>une<space>baquette<space>euh<space>sur<space>ce<space>une<space>vieille<space>femme<space>pour<space>cent<space>hesitation<space>voit<space>le<space>vol<space>très<space>vient<space>boulanger<space>la<space>jeune

2026-01-28 20:16:06,569 | INFO | speech length: 29600
2026-01-28 20:16:06,607 | INFO | decoder input length: 45
2026-01-28 20:16:06,607 | INFO | max output length: 45
2026-01-28 20:16:06,607 | INFO | min output length: 4
2026-01-28 20:16:07,214 | INFO | end detected at 16
2026-01-28 20:16:07,216 | INFO |  -1.07 * 0.5 =  -0.54 for decoder
2026-01-28 20:16:07,216 | INFO |  -0.37 * 0.5 =  -0.19 for ctc
2026-01-28 20:16:07,216 | INFO | total log probability: -0.72
2026-01-28 20:16:07,216 | INFO | normalized log probability: -0.07
2026-01-28 20:16:07,216 | INFO | total number of ended hypotheses: 167
2026-01-28 20:16:07,216 | INFO | best hypo: fille<space>euh

2026-01-28 20:16:07,219 | INFO | speech length: 138400
2026-01-28 20:16:07,254 | INFO | decoder input length: 215
2026-01-28 20:16:07,255 | INFO | max output length: 215
2026-01-28 20:16:07,255 | INFO | min output length: 21
2026-01-28 20:16:12,684 | INFO | end detected at 89
2026-01-28 20:16:12,687 | INFO |  -9.63 * 0.5 =  -4.82 for decoder
2026-01-28 20:16:12,687 | INFO | -11.76 * 0.5 =  -5.88 for ctc
2026-01-28 20:16:12,687 | INFO | total log probability: -10.70
2026-01-28 20:16:12,687 | INFO | normalized log probability: -0.13
2026-01-28 20:16:12,687 | INFO | total number of ended hypotheses: 175
2026-01-28 20:16:12,688 | INFO | best hypo: se<space>retrouve<space>par<space>terre<space>avec<space>sa<space>pline<space>euh<space>au<space>sol<space>le<space>boulanger<space>arrive<space>à<space>la<space>rattraper

2026-01-28 20:16:12,692 | INFO | speech length: 144480
2026-01-28 20:16:12,742 | INFO | decoder input length: 225
2026-01-28 20:16:12,742 | INFO | max output length: 225
2026-01-28 20:16:12,742 | INFO | min output length: 22
2026-01-28 20:16:18,330 | INFO | end detected at 87
2026-01-28 20:16:18,332 | INFO |  -7.35 * 0.5 =  -3.67 for decoder
2026-01-28 20:16:18,332 | INFO |  -6.50 * 0.5 =  -3.25 for ctc
2026-01-28 20:16:18,332 | INFO | total log probability: -6.92
2026-01-28 20:16:18,332 | INFO | normalized log probability: -0.09
2026-01-28 20:16:18,332 | INFO | total number of ended hypotheses: 204
2026-01-28 20:16:18,334 | INFO | best hypo: euh<space>à<space>travers<space>la<space>foule<space>le<space>policier<space>arrive<space>demande<space>des<space>explications<space>de<space>ce<space>proie

2026-01-28 20:16:18,336 | INFO | speech length: 308320
2026-01-28 20:16:18,377 | INFO | decoder input length: 481
2026-01-28 20:16:18,377 | INFO | max output length: 481
2026-01-28 20:16:18,377 | INFO | min output length: 48
2026-01-28 20:16:41,175 | INFO | end detected at 270
2026-01-28 20:16:41,178 | INFO | -47.44 * 0.5 = -23.72 for decoder
2026-01-28 20:16:41,178 | INFO | -35.94 * 0.5 = -17.97 for ctc
2026-01-28 20:16:41,178 | INFO | total log probability: -41.69
2026-01-28 20:16:41,178 | INFO | normalized log probability: -0.16
2026-01-28 20:16:41,178 | INFO | total number of ended hypotheses: 229
2026-01-28 20:16:41,181 | INFO | best hypo: euh<space>le<space>boulanger<space>très<space>très<space>énervé<space>contre<space>la<space>fille<space>euh<space>explique<space>le<space>vol<space>chapely<space>grand<space>seigneur<space>explique<space>que<space>euh<space>euh<space>c'est<space>duit<space>à<space>la<space>baguette<space>et<space>que<space>donc<space>y<space>a<space>pas<space>de<space>vol<space>euh<space>sauf<space>que<space>pour<space>le<space>policier<space>c'est<space>quand<space>même<space>du<space>vol<space>donc<space>il<space>embarque<space>pour<space>cent<space>hesitation

2026-01-28 20:16:41,184 | INFO | speech length: 343680
2026-01-28 20:16:41,226 | INFO | decoder input length: 536
2026-01-28 20:16:41,226 | INFO | max output length: 536
2026-01-28 20:16:41,226 | INFO | min output length: 53
2026-01-28 20:17:10,532 | INFO | end detected at 337
2026-01-28 20:17:10,533 | INFO | -132.09 * 0.5 = -66.04 for decoder
2026-01-28 20:17:10,534 | INFO |  -5.20 * 0.5 =  -2.60 for ctc
2026-01-28 20:17:10,534 | INFO | total log probability: -68.64
2026-01-28 20:17:10,534 | INFO | normalized log probability: -0.21
2026-01-28 20:17:10,534 | INFO | total number of ended hypotheses: 191
2026-01-28 20:17:10,538 | INFO | best hypo: le<space>chaqueline<space>pour<space>en<space>suite<space>euh<space>on<space>se<space>retrouve<space>donc<space>tout<space>le<space>monde<space>s'en<space>va<space>y<space>compris<space>la<space>foule<space>puisque<space>la<space>foulée<space>intéressée<space>par<space>les<space>flics<space>euh<space>on<space>se<space>retrouve<space>donc<space>avec<space>le<space>boulanger<space>la<space>jeune<space>fille<space>au<space>sol<space>et<space>la<space>vieille<space>dame<space>qui<space>revient<space>vers<space>boulanger<space>pour<space>lui<space>dire<space>non<space>ce<space>n'est<space>pas<space>le<space>monsieur<space>mais<space>c'est<space>la<space>jeune<space>fille<space>effectivement

2026-01-28 20:17:10,541 | INFO | speech length: 31520
2026-01-28 20:17:10,577 | INFO | decoder input length: 48
2026-01-28 20:17:10,577 | INFO | max output length: 48
2026-01-28 20:17:10,577 | INFO | min output length: 4
2026-01-28 20:17:11,710 | INFO | end detected at 31
2026-01-28 20:17:11,713 | INFO |  -6.40 * 0.5 =  -3.20 for decoder
2026-01-28 20:17:11,713 | INFO |  -5.69 * 0.5 =  -2.85 for ctc
2026-01-28 20:17:11,713 | INFO | total log probability: -6.05
2026-01-28 20:17:11,713 | INFO | normalized log probability: -0.27
2026-01-28 20:17:11,713 | INFO | total number of ended hypotheses: 226
2026-01-28 20:17:11,714 | INFO | best hypo: et<space>là<space>te<space>est<space>revenue

2026-01-28 20:17:11,720 | INFO | Chunk: 0 | WER=300.000000 | S=0 D=0 I=3
2026-01-28 20:17:11,723 | INFO | Chunk: 1 | WER=25.925926 | S=5 D=2 I=7
2026-01-28 20:17:11,723 | INFO | Chunk: 2 | WER=100.000000 | S=0 D=2 I=1
2026-01-28 20:17:11,723 | INFO | Chunk: 3 | WER=21.428571 | S=1 D=0 I=2
2026-01-28 20:17:11,724 | INFO | Chunk: 4 | WER=15.384615 | S=1 D=0 I=1
2026-01-28 20:17:11,726 | INFO | Chunk: 5 | WER=30.612245 | S=2 D=5 I=8
2026-01-28 20:17:11,729 | INFO | Chunk: 6 | WER=12.500000 | S=3 D=2 I=3
2026-01-28 20:17:11,729 | INFO | Chunk: 7 | WER=40.000000 | S=2 D=0 I=0
2026-01-28 20:17:11,753 | INFO | File: Rhap-M0022.wav | WER=23.645320 | S=14 D=10 I=24
2026-01-28 20:17:11,753 | INFO | ------------------------------
2026-01-28 20:17:11,754 | INFO | Conf ester Done!
2026-01-28 20:18:24,452 | INFO | Chunk: 0 | WER=300.000000 | S=1 D=0 I=2
2026-01-28 20:18:24,456 | INFO | Chunk: 1 | WER=18.518519 | S=3 D=6 I=1
2026-01-28 20:18:24,457 | INFO | Chunk: 2 | WER=100.000000 | S=2 D=1 I=0
2026-01-28 20:18:24,458 | INFO | Chunk: 3 | WER=50.000000 | S=4 D=1 I=2
2026-01-28 20:18:24,459 | INFO | Chunk: 4 | WER=38.461538 | S=2 D=1 I=2
2026-01-28 20:18:24,462 | INFO | Chunk: 5 | WER=36.734694 | S=6 D=11 I=1
2026-01-28 20:18:24,468 | INFO | Chunk: 6 | WER=6.250000 | S=3 D=1 I=0
2026-01-28 20:18:24,469 | INFO | Chunk: 7 | WER=100.000000 | S=3 D=2 I=0
2026-01-28 20:18:24,521 | INFO | File: Rhap-M0022.wav | WER=26.600985 | S=23 D=23 I=8
2026-01-28 20:18:24,521 | INFO | ------------------------------
2026-01-28 20:18:24,521 | INFO | hmm_tdnn Done!
2026-01-28 20:18:24,683 | INFO | ==================================Rhap-M0023.wav=========================================
2026-01-28 20:18:24,834 | INFO | Using rVAD model
2026-01-28 20:18:30,474 | INFO | Chunk: 0 | WER=40.229885 | S=11 D=24 I=0
2026-01-28 20:18:30,476 | INFO | Chunk: 1 | WER=24.390244 | S=4 D=5 I=1
2026-01-28 20:18:30,479 | INFO | Chunk: 2 | WER=62.000000 | S=8 D=22 I=1
2026-01-28 20:18:30,484 | INFO | Chunk: 3 | WER=21.126761 | S=4 D=11 I=0
2026-01-28 20:18:30,487 | INFO | Chunk: 4 | WER=10.526316 | S=1 D=3 I=0
2026-01-28 20:18:30,487 | INFO | Chunk: 5 | WER=15.384615 | S=2 D=0 I=0
2026-01-28 20:18:30,563 | INFO | File: Rhap-M0023.wav | WER=32.333333 | S=30 D=65 I=2
2026-01-28 20:18:30,563 | INFO | ------------------------------
2026-01-28 20:18:30,563 | INFO | w2vec vad chunk Done!
2026-01-28 20:18:41,867 | INFO | Chunk: 0 | WER=63.218391 | S=11 D=44 I=0
2026-01-28 20:18:41,868 | INFO | Chunk: 1 | WER=36.585366 | S=7 D=7 I=1
2026-01-28 20:18:41,869 | INFO | Chunk: 2 | WER=52.000000 | S=4 D=21 I=1
2026-01-28 20:18:41,872 | INFO | Chunk: 3 | WER=54.929577 | S=8 D=31 I=0
2026-01-28 20:18:41,873 | INFO | Chunk: 4 | WER=18.421053 | S=3 D=4 I=0
2026-01-28 20:18:41,873 | INFO | Chunk: 5 | WER=7.692308 | S=1 D=0 I=0
2026-01-28 20:18:41,905 | INFO | File: Rhap-M0023.wav | WER=47.666667 | S=34 D=107 I=2
2026-01-28 20:18:41,905 | INFO | ------------------------------
2026-01-28 20:18:41,905 | INFO | whisper med Done!
2026-01-28 20:18:56,037 | INFO | Chunk: 0 | WER=75.862069 | S=6 D=60 I=0
2026-01-28 20:18:56,038 | INFO | Chunk: 1 | WER=29.268293 | S=4 D=8 I=0
2026-01-28 20:18:56,039 | INFO | Chunk: 2 | WER=90.000000 | S=2 D=43 I=0
2026-01-28 20:18:56,042 | INFO | Chunk: 3 | WER=45.070423 | S=12 D=13 I=7
2026-01-28 20:18:56,043 | INFO | Chunk: 4 | WER=15.789474 | S=2 D=4 I=0
2026-01-28 20:18:56,043 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:18:56,073 | INFO | File: Rhap-M0023.wav | WER=53.000000 | S=24 D=128 I=7
2026-01-28 20:18:56,073 | INFO | ------------------------------
2026-01-28 20:18:56,073 | INFO | whisper large Done!
2026-01-28 20:18:56,247 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 20:18:56,285 | INFO | Vocabulary size: 350
2026-01-28 20:18:57,204 | INFO | Gradient checkpoint layers: []
2026-01-28 20:18:57,919 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 20:18:57,923 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 20:18:57,924 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 20:18:57,924 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 20:18:57,924 | INFO | speech length: 395520
2026-01-28 20:18:57,973 | INFO | decoder input length: 617
2026-01-28 20:18:57,973 | INFO | max output length: 617
2026-01-28 20:18:57,973 | INFO | min output length: 61
2026-01-28 20:19:13,862 | INFO | end detected at 139
2026-01-28 20:19:13,865 | INFO | -359.82 * 0.5 = -179.91 for decoder
2026-01-28 20:19:13,865 | INFO | -149.47 * 0.5 = -74.74 for ctc
2026-01-28 20:19:13,865 | INFO | total log probability: -254.64
2026-01-28 20:19:13,865 | INFO | normalized log probability: -1.91
2026-01-28 20:19:13,865 | INFO | total number of ended hypotheses: 186
2026-01-28 20:19:13,867 | INFO | best hypo: ▁alors▁il▁y▁a▁une▁jeune▁fille▁qui▁en▁aborde▁j'ai▁commencé▁est▁d'abord▁une▁voiture▁qui▁à▁une▁voiture▁avec▁des▁marins▁des▁marchandises▁qui▁arrive▁et▁en▁fait▁c'est▁pour▁un▁boulanger▁ou▁comme▁on▁les▁voi▁sortir▁et▁du▁pain▁avec▁des▁brioges▁trueux▁comme▁ça▁mais▁en▁fait▁'▁marioyage▁la▁jeune▁fille▁la▁jeune▁fille▁qui▁à▁l'air▁pauvre▁elle▁a▁juste

2026-01-28 20:19:13,870 | INFO | speech length: 228960
2026-01-28 20:19:13,911 | INFO | decoder input length: 357
2026-01-28 20:19:13,911 | INFO | max output length: 357
2026-01-28 20:19:13,911 | INFO | min output length: 35
2026-01-28 20:19:21,057 | INFO | end detected at 85
2026-01-28 20:19:21,059 | INFO | -72.17 * 0.5 = -36.08 for decoder
2026-01-28 20:19:21,059 | INFO | -45.68 * 0.5 = -22.84 for ctc
2026-01-28 20:19:21,059 | INFO | total log probability: -58.93
2026-01-28 20:19:21,059 | INFO | normalized log probability: -0.74
2026-01-28 20:19:21,059 | INFO | total number of ended hypotheses: 158
2026-01-28 20:19:21,060 | INFO | best hypo: ▁justin▁un▁morceau▁de▁tissu▁comme▁habit▁et▁qui▁wa▁qui▁voit▁sa▁passée▁est▁donc▁elle▁se▁dire▁et▁peut▁récupérer▁et▁pourra▁récupérer▁quelque▁chose▁non▁qu'elle▁va▁voler▁en▁fait▁un▁une▁baguette

2026-01-28 20:19:21,063 | INFO | speech length: 177120
2026-01-28 20:19:21,106 | INFO | decoder input length: 276
2026-01-28 20:19:21,106 | INFO | max output length: 276
2026-01-28 20:19:21,106 | INFO | min output length: 27
2026-01-28 20:19:25,519 | INFO | end detected at 62
2026-01-28 20:19:25,520 | INFO | -39.35 * 0.5 = -19.68 for decoder
2026-01-28 20:19:25,520 | INFO | -20.49 * 0.5 = -10.24 for ctc
2026-01-28 20:19:25,520 | INFO | total log probability: -29.92
2026-01-28 20:19:25,520 | INFO | normalized log probability: -0.53
2026-01-28 20:19:25,520 | INFO | total number of ended hypotheses: 170
2026-01-28 20:19:25,521 | INFO | best hypo: ▁et▁donc▁elle▁va▁courir▁va▁courir▁et▁fatyach▁et▁charlotte▁arrive▁à▁ce▁moment▁là▁et▁sa▁rencontre▁et▁une▁enquête▁marie▁un▁accident▁coaisseur

2026-01-28 20:19:25,523 | INFO | speech length: 327680
2026-01-28 20:19:25,568 | INFO | decoder input length: 511
2026-01-28 20:19:25,568 | INFO | max output length: 511
2026-01-28 20:19:25,569 | INFO | min output length: 51
2026-01-28 20:19:39,732 | INFO | end detected at 146
2026-01-28 20:19:39,734 | INFO | -246.38 * 0.5 = -123.19 for decoder
2026-01-28 20:19:39,734 | INFO | -118.05 * 0.5 = -59.03 for ctc
2026-01-28 20:19:39,734 | INFO | total log probability: -182.22
2026-01-28 20:19:39,734 | INFO | normalized log probability: -1.29
2026-01-28 20:19:39,734 | INFO | total number of ended hypotheses: 160
2026-01-28 20:19:39,736 | INFO | best hypo: ▁ils▁tombent▁tous▁les▁deux▁et▁c'est▁à▁ce▁moment▁là▁que▁le▁blonger▁voit▁que▁la▁baguette▁a▁disparu▁et▁se▁dit▁que▁ronge▁pense▁qu'un▁faite▁ils▁voient▁d'abord▁la▁jeune▁fille▁partir▁et▁ont▁elles▁l'appelle▁la▁police▁et▁en▁faite▁'ins▁est▁forcément▁chars▁▁ils▁se▁rencontaient▁tous▁les▁deux▁carlot▁et▁récupéré▁la▁la▁baguette

2026-01-28 20:19:39,738 | INFO | speech length: 164160
2026-01-28 20:19:39,787 | INFO | decoder input length: 256
2026-01-28 20:19:39,787 | INFO | max output length: 256
2026-01-28 20:19:39,787 | INFO | min output length: 25
2026-01-28 20:19:44,272 | INFO | end detected at 68
2026-01-28 20:19:44,274 | INFO | -25.20 * 0.5 = -12.60 for decoder
2026-01-28 20:19:44,274 | INFO | -35.18 * 0.5 = -17.59 for ctc
2026-01-28 20:19:44,274 | INFO | total log probability: -30.19
2026-01-28 20:19:44,274 | INFO | normalized log probability: -0.51
2026-01-28 20:19:44,274 | INFO | total number of ended hypotheses: 194
2026-01-28 20:19:44,275 | INFO | best hypo: ▁et▁le▁policier▁va▁voir▁le▁boulanger▁dit▁au▁policier▁qu'en▁fait▁c'est▁la▁fille▁mais▁en▁fait▁que▁charlot▁va▁dire▁en▁fait▁que▁c'est▁lui▁qu'il▁a▁pris

2026-01-28 20:19:44,278 | INFO | speech length: 43520
2026-01-28 20:19:44,330 | INFO | decoder input length: 67
2026-01-28 20:19:44,330 | INFO | max output length: 67
2026-01-28 20:19:44,330 | INFO | min output length: 6
2026-01-28 20:19:46,020 | INFO | end detected at 34
2026-01-28 20:19:46,022 | INFO |  -2.22 * 0.5 =  -1.11 for decoder
2026-01-28 20:19:46,022 | INFO |  -9.04 * 0.5 =  -4.52 for ctc
2026-01-28 20:19:46,022 | INFO | total log probability: -5.63
2026-01-28 20:19:46,022 | INFO | normalized log probability: -0.19
2026-01-28 20:19:46,022 | INFO | total number of ended hypotheses: 143
2026-01-28 20:19:46,023 | INFO | best hypo: ▁ce▁qu'il▁a▁dans▁la▁main▁et▁finalement▁les▁policiers▁vont▁l'arrêter

2026-01-28 20:19:46,036 | INFO | Chunk: 0 | WER=43.678161 | S=17 D=19 I=2
2026-01-28 20:19:46,038 | INFO | Chunk: 1 | WER=34.146341 | S=8 D=5 I=1
2026-01-28 20:19:46,039 | INFO | Chunk: 2 | WER=66.000000 | S=9 D=24 I=0
2026-01-28 20:19:46,044 | INFO | Chunk: 3 | WER=33.802817 | S=15 D=8 I=1
2026-01-28 20:19:46,045 | INFO | Chunk: 4 | WER=23.684211 | S=3 D=5 I=1
2026-01-28 20:19:46,046 | INFO | Chunk: 5 | WER=15.384615 | S=1 D=0 I=1
2026-01-28 20:19:46,099 | INFO | File: Rhap-M0023.wav | WER=40.000000 | S=53 D=61 I=6
2026-01-28 20:19:46,099 | INFO | ------------------------------
2026-01-28 20:19:46,099 | INFO | Conf cv Done!
2026-01-28 20:19:46,306 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 20:19:46,335 | INFO | Vocabulary size: 47
2026-01-28 20:19:47,460 | INFO | Gradient checkpoint layers: []
2026-01-28 20:19:48,103 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 20:19:48,108 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 20:19:48,108 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 20:19:48,108 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 20:19:48,111 | INFO | speech length: 395520
2026-01-28 20:19:48,152 | INFO | decoder input length: 617
2026-01-28 20:19:48,152 | INFO | max output length: 617
2026-01-28 20:19:48,152 | INFO | min output length: 61
2026-01-28 20:20:25,453 | INFO | end detected at 407
2026-01-28 20:20:25,456 | INFO | -158.20 * 0.5 = -79.10 for decoder
2026-01-28 20:20:25,456 | INFO | -75.07 * 0.5 = -37.53 for ctc
2026-01-28 20:20:25,456 | INFO | total log probability: -116.63
2026-01-28 20:20:25,456 | INFO | normalized log probability: -0.29
2026-01-28 20:20:25,456 | INFO | total number of ended hypotheses: 183
2026-01-28 20:20:25,461 | INFO | best hypo: alors<space>euh<space>y<space>a<space>une<space>jeune<space>fille<space>pour<space>cent<space>hesitation<space>qui<space>en<space>abord<space>j'ai<space>commencé<space>et<space>d'abord<space>une<space>voiture<space>qui<space>est<space>ouais<space>c'est<space>un<space>une<space>voiture<space>avec<space>des<space>ma<space>des<space>marchandises<space>qui<space>arrivent<space>et<space>en<space>fait<space>c'est<space>pour<space>un<space>boulanger<space>donc<space>on<space>vont<space>on<space>les<space>voit<space>sortir<space>pour<space>cent<space>hesitation<space>du<space>pain<space>des<space>brillons<space>des<space>trucs<space>comme<space>ça<space>et<space>en<space>fait<space>moi<space>il<space>y<space>a<space>jeune<space>fille<space>y<space>une<space>jeune<space>fille<space>qui<space>est<space>qui<space>est<space>l'art<space>pauvre<space>qui<space>a<space>juste

2026-01-28 20:20:25,464 | INFO | speech length: 228960
2026-01-28 20:20:25,500 | INFO | decoder input length: 357
2026-01-28 20:20:25,501 | INFO | max output length: 357
2026-01-28 20:20:25,501 | INFO | min output length: 35
2026-01-28 20:20:41,563 | INFO | end detected at 246
2026-01-28 20:20:41,564 | INFO | -46.98 * 0.5 = -23.49 for decoder
2026-01-28 20:20:41,565 | INFO | -42.27 * 0.5 = -21.13 for ctc
2026-01-28 20:20:41,565 | INFO | total log probability: -44.63
2026-01-28 20:20:41,565 | INFO | normalized log probability: -0.19
2026-01-28 20:20:41,565 | INFO | total number of ended hypotheses: 192
2026-01-28 20:20:41,568 | INFO | best hypo: juste<space>un<space>un<space>morceau<space>de<space>tissu<space>comme<space>un<space>bi<space>et<space>qui<space>pour<space>cent<space>hesitation<space>qui<space>oit<space>pour<space>cent<space>hesitation<space>qui<space>voit<space>s'attasser<space>et<space>donc<space>elle<space>se<space>dit<space>ben<space>elle<space>peut<space>récupérer<space>et<space>pour<space>récupérer<space>quelque<space>chose<space>donc<space>elle<space>va<space>voler<space>en<space>fait<space>un<space>une<space>baguette

2026-01-28 20:20:41,570 | INFO | speech length: 177120
2026-01-28 20:20:41,606 | INFO | decoder input length: 276
2026-01-28 20:20:41,606 | INFO | max output length: 276
2026-01-28 20:20:41,606 | INFO | min output length: 27
2026-01-28 20:20:52,935 | INFO | end detected at 194
2026-01-28 20:20:52,937 | INFO | -28.92 * 0.5 = -14.46 for decoder
2026-01-28 20:20:52,937 | INFO | -31.42 * 0.5 = -15.71 for ctc
2026-01-28 20:20:52,937 | INFO | total log probability: -30.17
2026-01-28 20:20:52,937 | INFO | normalized log probability: -0.17
2026-01-28 20:20:52,937 | INFO | total number of ended hypotheses: 202
2026-01-28 20:20:52,940 | INFO | best hypo: et<space>donc<space>elle<space>va<space>courir<space>euh<space>va<space>courir<space>et<space>euh<space>en<space>fait<space>il<space>y<space>a<space>charlot<space>qui<space>arrive<space>à<space>ce<space>moment<space>là<space>et<space>euh<space>ils<space>se<space>se<space>rencontrent<space>et<space>donc<space>elles<space>tous<space>euh<space>moi<space>il<space>y<space>a<space>un<space>accident<space>quoi<space>il<space>se

2026-01-28 20:20:52,942 | INFO | speech length: 327680
2026-01-28 20:20:52,983 | INFO | decoder input length: 511
2026-01-28 20:20:52,984 | INFO | max output length: 511
2026-01-28 20:20:52,984 | INFO | min output length: 51
2026-01-28 20:21:21,622 | INFO | end detected at 353
2026-01-28 20:21:21,625 | INFO | -54.93 * 0.5 = -27.46 for decoder
2026-01-28 20:21:21,626 | INFO | -38.21 * 0.5 = -19.10 for ctc
2026-01-28 20:21:21,626 | INFO | total log probability: -46.57
2026-01-28 20:21:21,626 | INFO | normalized log probability: -0.14
2026-01-28 20:21:21,626 | INFO | total number of ended hypotheses: 206
2026-01-28 20:21:21,632 | INFO | best hypo: ils<space>tombent<space>tous<space>les<space>deux<space>et<space>c'est<space>à<space>ce<space>moment<space>là<space>que<space>le<space>le<space>blonger<space>voit<space>que<space>la<space>baquette<space>disparue<space>et<space>il<space>se<space>dit<space>que<space>bon<space>donc<space>je<space>pense<space>qu'en<space>fait<space>il<space>voit<space>d'abord<space>la<space>la<space>jeune<space>fille<space>partir<space>donc<space>il<space>appelle<space>la<space>police<space>et<space>pour<space>cent<space>hesitation<space>en<space>fait<space>euh<space>bon<space>forcément<space>charles<space>comme<space>si<space>on<space>rencontrait<space>tous<space>les<space>deux<space>charlot<space>a<space>récupéré<space>la<space>la<space>baguette

2026-01-28 20:21:21,636 | INFO | speech length: 164160
2026-01-28 20:21:21,721 | INFO | decoder input length: 256
2026-01-28 20:21:21,722 | INFO | max output length: 256
2026-01-28 20:21:21,732 | INFO | min output length: 25
2026-01-28 20:21:32,677 | INFO | end detected at 191
2026-01-28 20:21:32,680 | INFO | -30.29 * 0.5 = -15.15 for decoder
2026-01-28 20:21:32,680 | INFO | -59.74 * 0.5 = -29.87 for ctc
2026-01-28 20:21:32,680 | INFO | total log probability: -45.02
2026-01-28 20:21:32,680 | INFO | normalized log probability: -0.25
2026-01-28 20:21:32,680 | INFO | total number of ended hypotheses: 266
2026-01-28 20:21:32,683 | INFO | best hypo: et<space>euh<space>le<space>policier<space>euh<space>va<space>voir<space>le<space>le<space>boulanger<space>pour<space>cent<space>hesitation<space>au<space>policier<space>qu'en<space>fait<space>c'est<space>la<space>fille<space>mais<space>en<space>fait<space>charlot<space>va<space>dire<space>en<space>fait<space>que<space>c'est<space>lui<space>qu'il<space>qu'il<space>a<space>pris

2026-01-28 20:21:32,686 | INFO | speech length: 43520
2026-01-28 20:21:32,729 | INFO | decoder input length: 67
2026-01-28 20:21:32,729 | INFO | max output length: 67
2026-01-28 20:21:32,729 | INFO | min output length: 6
2026-01-28 20:21:35,116 | INFO | adding <eos> in the last position in the loop
2026-01-28 20:21:35,124 | INFO | no hypothesis. Finish decoding.
2026-01-28 20:21:35,125 | INFO | -11.66 * 0.5 =  -5.83 for decoder
2026-01-28 20:21:35,125 | INFO | -17.52 * 0.5 =  -8.76 for ctc
2026-01-28 20:21:35,125 | INFO | total log probability: -14.59
2026-01-28 20:21:35,125 | INFO | normalized log probability: -0.22
2026-01-28 20:21:35,125 | INFO | total number of ended hypotheses: 82
2026-01-28 20:21:35,126 | INFO | best hypo: ce<space>qu'il<space>a<space>dans<space>lamain<space>et<space>finalement<space>les<space>policiers<space>vont<space>l'arrêter

2026-01-28 20:21:35,136 | INFO | Chunk: 0 | WER=29.885057 | S=12 D=8 I=6
2026-01-28 20:21:35,138 | INFO | Chunk: 1 | WER=36.585366 | S=8 D=1 I=6
2026-01-28 20:21:35,139 | INFO | Chunk: 2 | WER=34.000000 | S=7 D=9 I=1
2026-01-28 20:21:35,143 | INFO | Chunk: 3 | WER=25.352113 | S=9 D=5 I=4
2026-01-28 20:21:35,144 | INFO | Chunk: 4 | WER=28.947368 | S=7 D=1 I=3
2026-01-28 20:21:35,144 | INFO | Chunk: 5 | WER=30.769231 | S=2 D=1 I=1
2026-01-28 20:21:35,191 | INFO | File: Rhap-M0023.wav | WER=30.333333 | S=45 D=25 I=21
2026-01-28 20:21:35,191 | INFO | ------------------------------
2026-01-28 20:21:35,191 | INFO | Conf ester Done!
2026-01-28 20:23:00,660 | INFO | Chunk: 0 | WER=43.678161 | S=18 D=18 I=2
2026-01-28 20:23:00,662 | INFO | Chunk: 1 | WER=31.707317 | S=8 D=5 I=0
2026-01-28 20:23:00,664 | INFO | Chunk: 2 | WER=58.000000 | S=9 D=20 I=0
2026-01-28 20:23:00,668 | INFO | Chunk: 3 | WER=30.985915 | S=12 D=9 I=1
2026-01-28 20:23:00,669 | INFO | Chunk: 4 | WER=26.315789 | S=6 D=4 I=0
2026-01-28 20:23:00,670 | INFO | Chunk: 5 | WER=15.384615 | S=1 D=0 I=1
2026-01-28 20:23:00,723 | INFO | File: Rhap-M0023.wav | WER=37.666667 | S=53 D=56 I=4
2026-01-28 20:23:00,723 | INFO | ------------------------------
2026-01-28 20:23:00,724 | INFO | hmm_tdnn Done!
2026-01-28 20:23:00,897 | INFO | ==================================Rhap-M0024.wav=========================================
2026-01-28 20:23:01,068 | INFO | Using rVAD model
2026-01-28 20:23:04,914 | INFO | Chunk: 0 | WER=14.925373 | S=7 D=3 I=0
2026-01-28 20:23:04,917 | INFO | Chunk: 1 | WER=20.370370 | S=4 D=6 I=1
2026-01-28 20:23:04,930 | INFO | File: Rhap-M0024.wav | WER=17.355372 | S=11 D=9 I=1
2026-01-28 20:23:04,931 | INFO | ------------------------------
2026-01-28 20:23:04,931 | INFO | w2vec vad chunk Done!
2026-01-28 20:23:10,242 | INFO | Chunk: 0 | WER=44.776119 | S=1 D=29 I=0
2026-01-28 20:23:10,243 | INFO | Chunk: 1 | WER=40.740741 | S=1 D=20 I=1
2026-01-28 20:23:10,251 | INFO | File: Rhap-M0024.wav | WER=42.975207 | S=2 D=49 I=1
2026-01-28 20:23:10,251 | INFO | ------------------------------
2026-01-28 20:23:10,251 | INFO | whisper med Done!
2026-01-28 20:23:17,437 | INFO | Chunk: 0 | WER=35.820896 | S=21 D=3 I=0
2026-01-28 20:23:17,438 | INFO | Chunk: 1 | WER=55.555556 | S=0 D=30 I=0
2026-01-28 20:23:17,446 | INFO | File: Rhap-M0024.wav | WER=44.628099 | S=21 D=33 I=0
2026-01-28 20:23:17,446 | INFO | ------------------------------
2026-01-28 20:23:17,447 | INFO | whisper large Done!
2026-01-28 20:23:17,578 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 20:23:17,616 | INFO | Vocabulary size: 350
2026-01-28 20:23:18,352 | INFO | Gradient checkpoint layers: []
2026-01-28 20:23:19,033 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 20:23:19,037 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 20:23:19,037 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 20:23:19,038 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 20:23:19,038 | INFO | speech length: 425600
2026-01-28 20:23:19,083 | INFO | decoder input length: 664
2026-01-28 20:23:19,083 | INFO | max output length: 664
2026-01-28 20:23:19,083 | INFO | min output length: 66
2026-01-28 20:23:38,033 | INFO | end detected at 152
2026-01-28 20:23:38,034 | INFO | -514.56 * 0.5 = -257.28 for decoder
2026-01-28 20:23:38,034 | INFO | -150.69 * 0.5 = -75.35 for ctc
2026-01-28 20:23:38,035 | INFO | total log probability: -332.63
2026-01-28 20:23:38,035 | INFO | normalized log probability: -2.25
2026-01-28 20:23:38,035 | INFO | total number of ended hypotheses: 143
2026-01-28 20:23:38,037 | INFO | best hypo: ▁il▁y▁avait▁donc▁une▁jeune▁fille▁qui▁regardait▁dans▁une▁boutique▁apparemment▁une▁pâtisserie▁qui▁semblait▁avoir▁faim▁qui▁il▁a▁profité▁de▁ce▁que▁leur▁livreur▁s'éloigne▁y▁volerait▁à▁une▁baguette▁rencontrer▁dans▁charlot▁à▁sa▁moment▁là▁à▁lui▁est▁rentrée▁de▁'an▁dans▁la▁infusion▁d'une▁f▁passante▁à▁dénoncer▁la▁jeune▁fille▁de▁livreur▁qui▁accourue▁après▁la▁jeune▁fille

2026-01-28 20:23:38,040 | INFO | speech length: 277120
2026-01-28 20:23:38,087 | INFO | decoder input length: 432
2026-01-28 20:23:38,088 | INFO | max output length: 432
2026-01-28 20:23:38,088 | INFO | min output length: 43
2026-01-28 20:23:48,817 | INFO | end detected at 124
2026-01-28 20:23:48,819 | INFO | -176.78 * 0.5 = -88.39 for decoder
2026-01-28 20:23:48,819 | INFO | -48.61 * 0.5 = -24.31 for ctc
2026-01-28 20:23:48,819 | INFO | total log probability: -112.70
2026-01-28 20:23:48,819 | INFO | normalized log probability: -0.96
2026-01-28 20:23:48,819 | INFO | total number of ended hypotheses: 172
2026-01-28 20:23:48,820 | INFO | best hypo: ▁les▁policiers▁sont▁arrivés▁en▁raison▁du▁duvakhamj▁pense▁et▁charlot▁s'est▁accusé▁plutôt▁que▁de▁laisser▁la▁jeune▁fille▁s'accuser▁etie▁ça▁s'arrête▁lorsque▁lapsante▁indique▁aux▁livreur▁que▁c'était▁bien▁la▁jeune▁fille▁qui▁avait▁volé▁le▁pain▁et▁pas▁le▁jeune

2026-01-28 20:23:48,830 | INFO | Chunk: 0 | WER=29.850746 | S=13 D=3 I=4
2026-01-28 20:23:48,832 | INFO | Chunk: 1 | WER=18.518519 | S=4 D=6 I=0
2026-01-28 20:23:48,842 | INFO | File: Rhap-M0024.wav | WER=24.793388 | S=17 D=9 I=4
2026-01-28 20:23:48,842 | INFO | ------------------------------
2026-01-28 20:23:48,842 | INFO | Conf cv Done!
2026-01-28 20:23:49,022 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 20:23:49,045 | INFO | Vocabulary size: 47
2026-01-28 20:23:50,010 | INFO | Gradient checkpoint layers: []
2026-01-28 20:23:50,774 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 20:23:50,780 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 20:23:50,780 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 20:23:50,781 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 20:23:50,784 | INFO | speech length: 425600
2026-01-28 20:23:50,837 | INFO | decoder input length: 664
2026-01-28 20:23:50,837 | INFO | max output length: 664
2026-01-28 20:23:50,837 | INFO | min output length: 66
2026-01-28 20:24:36,845 | INFO | end detected at 434
2026-01-28 20:24:36,847 | INFO | -333.95 * 0.5 = -166.97 for decoder
2026-01-28 20:24:36,847 | INFO | -68.87 * 0.5 = -34.44 for ctc
2026-01-28 20:24:36,847 | INFO | total log probability: -201.41
2026-01-28 20:24:36,847 | INFO | normalized log probability: -0.47
2026-01-28 20:24:36,847 | INFO | total number of ended hypotheses: 170
2026-01-28 20:24:36,852 | INFO | best hypo: euh<space>il<space>y<space>avait<space>donc<space>une<space>jeune<space>fille<space>qui<space>regardait<space>dans<space>une<space>boutique<space>apparemment<space>une<space>pâtisserie<space>qui<space>semblait<space>avoir<space>faim<space>qui<space>a<space>profité<space>de<space>ce<space>que<space>le<space>livreur<space>s'éloigne<space>pour<space>voler<space>un<space>une<space>baguette<space>pour<space>cent<space>hesitation<space>a<space>rencontré<space>donc<space>charlot<space>à<space>ce<space>moment<space>là<space>lui<space>est<space>rentré<space>de<space>dans<space>pour<space>cent<space>hesitation<space>dans<space>la<space>confusion<space>'<space>qu'une<space>pour<space>cent<space>hesi<space>ation<space>une<space>euh<space>assant<space>à<space>dénonce<space>la<space>jeune<space>fille<space>a<space>livreur<space>qui<space>acourue<space>après<space>la<space>jeune<space>fille

2026-01-28 20:24:36,855 | INFO | speech length: 277120
2026-01-28 20:24:36,897 | INFO | decoder input length: 432
2026-01-28 20:24:36,897 | INFO | max output length: 432
2026-01-28 20:24:36,897 | INFO | min output length: 43
2026-01-28 20:25:00,483 | INFO | end detected at 320
2026-01-28 20:25:00,484 | INFO | -38.64 * 0.5 = -19.32 for decoder
2026-01-28 20:25:00,484 | INFO | -33.61 * 0.5 = -16.80 for ctc
2026-01-28 20:25:00,484 | INFO | total log probability: -36.12
2026-01-28 20:25:00,484 | INFO | normalized log probability: -0.12
2026-01-28 20:25:00,484 | INFO | total number of ended hypotheses: 171
2026-01-28 20:25:00,489 | INFO | best hypo: pour<space>cent<space>hesitation<space>les<space>policiers<space>sont<space>arrivés<space>en<space>raison<space>du<space>du<space>du<space>vacame<space>je<space>pense<space>et<space>euh<space>charlot<space>c'est<space>accusé<space>plutôt<space>que<space>de<space>laisser<space>la<space>jeune<space>fille<space>s'accuser<space>et<space>pour<space>cent<space>hesitation<space>ça<space>s'arrête<space>lorsque<space>la<space>passante<space>euh<space>indique<space>aux<space>livreurs<space>que<space>c'était<space>bien<space>la<space>jeune<space>fille<space>qui<space>avait<space>volé<space>le<space>pain<space>et<space>pas<space>le<space>jeune

2026-01-28 20:25:00,498 | INFO | Chunk: 0 | WER=31.343284 | S=7 D=1 I=13
2026-01-28 20:25:00,501 | INFO | Chunk: 1 | WER=25.925926 | S=4 D=2 I=8
2026-01-28 20:25:00,513 | INFO | File: Rhap-M0024.wav | WER=28.925620 | S=11 D=3 I=21
2026-01-28 20:25:00,513 | INFO | ------------------------------
2026-01-28 20:25:00,513 | INFO | Conf ester Done!
2026-01-28 20:25:30,649 | INFO | Chunk: 0 | WER=13.432836 | S=6 D=1 I=2
2026-01-28 20:25:30,664 | INFO | Chunk: 1 | WER=11.111111 | S=3 D=3 I=0
2026-01-28 20:25:30,733 | INFO | File: Rhap-M0024.wav | WER=12.396694 | S=9 D=4 I=2
2026-01-28 20:25:30,733 | INFO | ------------------------------
2026-01-28 20:25:30,733 | INFO | hmm_tdnn Done!
2026-01-28 20:25:30,998 | INFO | ==================================Rhap-M1001.wav=========================================
2026-01-28 20:25:31,243 | INFO | Using rVAD model
2026-01-28 20:25:37,121 | INFO | Chunk: 0 | WER=24.193548 | S=5 D=10 I=0
2026-01-28 20:25:37,122 | INFO | Chunk: 1 | WER=14.285714 | S=1 D=1 I=1
2026-01-28 20:25:37,124 | INFO | Chunk: 2 | WER=35.000000 | S=13 D=8 I=0
2026-01-28 20:25:37,127 | INFO | Chunk: 3 | WER=8.333333 | S=2 D=1 I=2
2026-01-28 20:25:37,131 | INFO | Chunk: 4 | WER=26.666667 | S=9 D=14 I=1
2026-01-28 20:25:37,133 | INFO | Chunk: 5 | WER=28.571429 | S=9 D=7 I=0
2026-01-28 20:25:37,194 | INFO | File: Rhap-M1001.wav | WER=23.782235 | S=38 D=41 I=4
2026-01-28 20:25:37,194 | INFO | ------------------------------
2026-01-28 20:25:37,194 | INFO | w2vec vad chunk Done!
2026-01-28 20:25:50,337 | INFO | Chunk: 0 | WER=33.870968 | S=4 D=16 I=1
2026-01-28 20:25:50,337 | INFO | Chunk: 1 | WER=19.047619 | S=4 D=0 I=0
2026-01-28 20:25:50,339 | INFO | Chunk: 2 | WER=61.666667 | S=5 D=32 I=0
2026-01-28 20:25:50,341 | INFO | Chunk: 3 | WER=30.000000 | S=1 D=17 I=0
2026-01-28 20:25:50,343 | INFO | Chunk: 4 | WER=85.555556 | S=10 D=65 I=2
2026-01-28 20:25:50,345 | INFO | Chunk: 5 | WER=33.928571 | S=9 D=10 I=0
2026-01-28 20:25:50,385 | INFO | File: Rhap-M1001.wav | WER=49.570201 | S=34 D=138 I=1
2026-01-28 20:25:50,385 | INFO | ------------------------------
2026-01-28 20:25:50,385 | INFO | whisper med Done!
2026-01-28 20:26:11,024 | INFO | Chunk: 0 | WER=33.870968 | S=1 D=20 I=0
2026-01-28 20:26:11,025 | INFO | Chunk: 1 | WER=19.047619 | S=4 D=0 I=0
2026-01-28 20:26:11,027 | INFO | Chunk: 2 | WER=26.666667 | S=5 D=9 I=2
2026-01-28 20:26:11,029 | INFO | Chunk: 3 | WER=35.000000 | S=1 D=20 I=0
2026-01-28 20:26:11,033 | INFO | Chunk: 4 | WER=46.666667 | S=12 D=28 I=2
2026-01-28 20:26:11,035 | INFO | Chunk: 5 | WER=26.785714 | S=4 D=11 I=0
2026-01-28 20:26:11,084 | INFO | File: Rhap-M1001.wav | WER=33.810888 | S=26 D=88 I=4
2026-01-28 20:26:11,084 | INFO | ------------------------------
2026-01-28 20:26:11,084 | INFO | whisper large Done!
2026-01-28 20:26:11,217 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 20:26:11,255 | INFO | Vocabulary size: 350
2026-01-28 20:26:12,499 | INFO | Gradient checkpoint layers: []
2026-01-28 20:26:13,241 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 20:26:13,245 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 20:26:13,246 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 20:26:13,246 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 20:26:13,246 | INFO | speech length: 259200
2026-01-28 20:26:13,301 | INFO | decoder input length: 404
2026-01-28 20:26:13,301 | INFO | max output length: 404
2026-01-28 20:26:13,301 | INFO | min output length: 40
2026-01-28 20:26:25,375 | INFO | end detected at 145
2026-01-28 20:26:25,376 | INFO | -240.51 * 0.5 = -120.25 for decoder
2026-01-28 20:26:25,376 | INFO | -104.09 * 0.5 = -52.05 for ctc
2026-01-28 20:26:25,376 | INFO | total log probability: -172.30
2026-01-28 20:26:25,376 | INFO | normalized log probability: -1.26
2026-01-28 20:26:25,377 | INFO | total number of ended hypotheses: 180
2026-01-28 20:26:25,378 | INFO | best hypo: ▁elle▁a▁leur▁rangement▁avec▁clarage▁et▁dix▁neuf▁ans▁j'ai▁eu▁l'obtention▁d'un▁bac▁glad▁emerner▁simbeck▁et▁sa▁messe▁donc▁technologique▁ses▁sciences▁médico▁sociales▁s'anarrêt▁d'avoir▁avec▁eux▁la▁littérature▁et▁pres▁qu'en▁faiteugène▁la▁biologie▁et▁plus▁se▁ver▁la▁biologie▁le▁sociale

2026-01-28 20:26:25,382 | INFO | speech length: 89600
2026-01-28 20:26:25,419 | INFO | decoder input length: 139
2026-01-28 20:26:25,419 | INFO | max output length: 139
2026-01-28 20:26:25,419 | INFO | min output length: 13
2026-01-28 20:26:28,203 | INFO | end detected at 56
2026-01-28 20:26:28,205 | INFO | -13.78 * 0.5 =  -6.89 for decoder
2026-01-28 20:26:28,205 | INFO | -17.12 * 0.5 =  -8.56 for ctc
2026-01-28 20:26:28,205 | INFO | total log probability: -15.45
2026-01-28 20:26:28,205 | INFO | normalized log probability: -0.32
2026-01-28 20:26:28,205 | INFO | total number of ended hypotheses: 185
2026-01-28 20:26:28,206 | INFO | best hypo: ▁le▁sens▁médico▁social▁est▁donc▁en▁fait▁ji▁choisi▁italien▁en▁deuxième▁chambre▁au▁premier▁choix▁cette▁psychologie

2026-01-28 20:26:28,208 | INFO | speech length: 194880
2026-01-28 20:26:28,248 | INFO | decoder input length: 304
2026-01-28 20:26:28,249 | INFO | max output length: 304
2026-01-28 20:26:28,249 | INFO | min output length: 30
2026-01-28 20:26:36,759 | INFO | end detected at 125
2026-01-28 20:26:36,760 | INFO | -137.50 * 0.5 = -68.75 for decoder
2026-01-28 20:26:36,760 | INFO | -81.54 * 0.5 = -40.77 for ctc
2026-01-28 20:26:36,760 | INFO | total log probability: -109.52
2026-01-28 20:26:36,760 | INFO | normalized log probability: -0.92
2026-01-28 20:26:36,760 | INFO | total number of ended hypotheses: 169
2026-01-28 20:26:36,762 | INFO | best hypo: ▁et▁en▁fait▁j'ai▁a▁été▁accepté▁pascal▁avant▁d'entretien▁aurai▁je▁le▁savais▁bain▁ou▁que▁fetterola▁c'est▁cette▁province▁stupide▁dont▁jej'ai▁pris▁tali▁un▁deuxième▁choi▁à▁ce▁que▁aussi▁j'ai▁dépai▁italien▁je▁suis▁bilin▁je▁parle▁l'italien▁je▁le▁comprends

2026-01-28 20:26:36,764 | INFO | speech length: 275360
2026-01-28 20:26:36,800 | INFO | decoder input length: 429
2026-01-28 20:26:36,801 | INFO | max output length: 429
2026-01-28 20:26:36,801 | INFO | min output length: 42
2026-01-28 20:26:48,030 | INFO | end detected at 129
2026-01-28 20:26:48,031 | INFO | -149.11 * 0.5 = -74.56 for decoder
2026-01-28 20:26:48,032 | INFO | -47.88 * 0.5 = -23.94 for ctc
2026-01-28 20:26:48,032 | INFO | total log probability: -98.50
2026-01-28 20:26:48,032 | INFO | normalized log probability: -0.80
2026-01-28 20:26:48,032 | INFO | total number of ended hypotheses: 155
2026-01-28 20:26:48,033 | INFO | best hypo: ▁et▁donc▁en▁fait▁bah▁j'aime▁beaucoup▁l'italien▁là▁et▁j'apprécie▁vraiment▁ce▁que▁je▁finis▁m'en▁faite▁je▁pense▁que▁je▁vais▁être▁me▁réorienter▁si▁à▁la▁fin▁d'année▁si▁je▁vais▁bien▁finir▁l'année▁là▁un▁an▁et▁si▁je▁réussi▁pas▁je▁pense▁que▁je▁vais▁en▁psychologie

2026-01-28 20:26:48,035 | INFO | speech length: 298560
2026-01-28 20:26:48,079 | INFO | decoder input length: 466
2026-01-28 20:26:48,079 | INFO | max output length: 466
2026-01-28 20:26:48,079 | INFO | min output length: 46
2026-01-28 20:27:04,762 | INFO | end detected at 190
2026-01-28 20:27:04,764 | INFO | -353.32 * 0.5 = -176.66 for decoder
2026-01-28 20:27:04,764 | INFO | -158.46 * 0.5 = -79.23 for ctc
2026-01-28 20:27:04,764 | INFO | total log probability: -255.89
2026-01-28 20:27:04,764 | INFO | normalized log probability: -1.39
2026-01-28 20:27:04,764 | INFO | total number of ended hypotheses: 165
2026-01-28 20:27:04,767 | INFO | best hypo: ▁est▁ce▁que▁c'est▁plus▁que▁je▁veux▁faire▁mapsco▁et▁si▁je▁passe▁l'année▁par▁constance▁ce▁que▁j'ai▁fini▁à▁ma▁licence▁si▁je▁ne▁passe▁passerai▁un▁deuxième▁annéeier▁en▁troisième▁année▁chiniam▁finirai▁ma▁discence▁pour▁avoir▁ma▁licence▁mais▁si▁j'apase▁pas▁fini▁je▁n'ai▁j'ai▁j'à▁priseaucou▁de▁chos▁et▁très▁intéressantes▁mais▁je▁pense▁ce▁que▁je▁me▁renier▁entre▁en▁psychologique▁parce▁que▁ça▁m'intéresse▁plus

2026-01-28 20:27:04,769 | INFO | speech length: 216160
2026-01-28 20:27:04,826 | INFO | decoder input length: 337
2026-01-28 20:27:04,827 | INFO | max output length: 337
2026-01-28 20:27:04,827 | INFO | min output length: 33
2026-01-28 20:27:14,793 | INFO | end detected at 117
2026-01-28 20:27:14,794 | INFO | -140.63 * 0.5 = -70.31 for decoder
2026-01-28 20:27:14,794 | INFO | -37.61 * 0.5 = -18.81 for ctc
2026-01-28 20:27:14,794 | INFO | total log probability: -89.12
2026-01-28 20:27:14,794 | INFO | normalized log probability: -0.80
2026-01-28 20:27:14,794 | INFO | total number of ended hypotheses: 169
2026-01-28 20:27:14,796 | INFO | best hypo: ▁je▁suis▁plus▁vent▁on▁dit▁que▁tu▁à▁l'écoute▁des▁personnes▁tu▁comprends▁aussi▁et▁que▁ventement▁me▁dit▁que▁je▁suis▁plus▁ce▁faite▁pour▁eux▁pour▁ça▁mettez▁là▁ces▁études▁là▁que▁pour▁l'italien▁tout▁que▁je▁pense▁que▁je▁vais▁je▁vais▁surprendre▁rientée▁en▁psychologie

2026-01-28 20:27:14,805 | INFO | Chunk: 0 | WER=62.903226 | S=24 D=12 I=3
2026-01-28 20:27:14,806 | INFO | Chunk: 1 | WER=47.619048 | S=8 D=2 I=0
2026-01-28 20:27:14,808 | INFO | Chunk: 2 | WER=51.666667 | S=20 D=10 I=1
2026-01-28 20:27:14,810 | INFO | Chunk: 3 | WER=18.333333 | S=5 D=4 I=2
2026-01-28 20:27:14,815 | INFO | Chunk: 4 | WER=46.666667 | S=22 D=14 I=6
2026-01-28 20:27:14,817 | INFO | Chunk: 5 | WER=35.714286 | S=11 D=6 I=3
2026-01-28 20:27:14,875 | INFO | File: Rhap-M1001.wav | WER=43.839542 | S=90 D=48 I=15
2026-01-28 20:27:14,875 | INFO | ------------------------------
2026-01-28 20:27:14,875 | INFO | Conf cv Done!
2026-01-28 20:27:15,054 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 20:27:15,077 | INFO | Vocabulary size: 47
2026-01-28 20:27:16,246 | INFO | Gradient checkpoint layers: []
2026-01-28 20:27:17,001 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 20:27:17,006 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 20:27:17,007 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 20:27:17,007 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 20:27:17,010 | INFO | speech length: 259200
2026-01-28 20:27:17,056 | INFO | decoder input length: 404
2026-01-28 20:27:17,056 | INFO | max output length: 404
2026-01-28 20:27:17,056 | INFO | min output length: 40
2026-01-28 20:27:41,376 | INFO | end detected at 302
2026-01-28 20:27:41,380 | INFO | -49.22 * 0.5 = -24.61 for decoder
2026-01-28 20:27:41,380 | INFO | -54.40 * 0.5 = -27.20 for ctc
2026-01-28 20:27:41,380 | INFO | total log probability: -51.81
2026-01-28 20:27:41,380 | INFO | normalized log probability: -0.18
2026-01-28 20:27:41,380 | INFO | total number of ended hypotheses: 209
2026-01-28 20:27:41,385 | INFO | best hypo: alors<space>je<space>m'appelle<space>éclarage<space>de<space>dix<space>neuf<space>ans<space>j'ai<space>eu<space>le<space>petit<space>sous<space>mon<space>bac<space>l'année<space>dernière<space>son<space>bac<space>euh<space>sms<space>donc<space>technologie<space>et<space>sciences<space>médicaux<space>sociales<space>ça<space>n'a<space>rien<space>à<space>voir<space>avec<space>la<space>littérature<space>parce<space>qu'en<space>fait<space>euh<space>j'aime<space>la<space>biologie<space>et<space>je<space>plus<space>euh<space>par<space>la<space>biologie<space>et<space>euh<space>le<space>social<space>donc<space>euh

2026-01-28 20:27:41,388 | INFO | speech length: 89600
2026-01-28 20:27:41,436 | INFO | decoder input length: 139
2026-01-28 20:27:41,436 | INFO | max output length: 139
2026-01-28 20:27:41,436 | INFO | min output length: 13
2026-01-28 20:27:47,260 | INFO | end detected at 131
2026-01-28 20:27:47,261 | INFO | -14.90 * 0.5 =  -7.45 for decoder
2026-01-28 20:27:47,261 | INFO | -15.97 * 0.5 =  -7.98 for ctc
2026-01-28 20:27:47,261 | INFO | total log probability: -15.44
2026-01-28 20:27:47,262 | INFO | normalized log probability: -0.13
2026-01-28 20:27:47,262 | INFO | total number of ended hypotheses: 196
2026-01-28 20:27:47,263 | INFO | best hypo: ouais<space>sens<space>médico<space>social<space>et<space>donc<space>en<space>fait<space>euh<space>j'ai<space>choisi<space>talie<space>en<space>deuxième<space>fois<space>mon<space>premier<space>choix<space>c'était<space>euh<space>psychologie

2026-01-28 20:27:47,266 | INFO | speech length: 194880
2026-01-28 20:27:47,302 | INFO | decoder input length: 304
2026-01-28 20:27:47,302 | INFO | max output length: 304
2026-01-28 20:27:47,302 | INFO | min output length: 30
2026-01-28 20:28:03,217 | INFO | end detected at 291
2026-01-28 20:28:03,218 | INFO | -49.03 * 0.5 = -24.52 for decoder
2026-01-28 20:28:03,219 | INFO | -99.64 * 0.5 = -49.82 for ctc
2026-01-28 20:28:03,219 | INFO | total log probability: -74.34
2026-01-28 20:28:03,219 | INFO | normalized log probability: -0.26
2026-01-28 20:28:03,219 | INFO | total number of ended hypotheses: 214
2026-01-28 20:28:03,223 | INFO | best hypo: et<space>en<space>fait<space>euh<space>j'ai<space>pas<space>été<space>accepté<space>parce<space>qu'<space>y<space>avait<space>un<space>entretien<space>oral<space>et<space>je<space>le<space>savais<space>pas<space>donc<space>euh<space>fait<space>voilà<space>c'est<space>c'est<space>trop<space>vince<space>je<space>suis<space>donc<space>j'ai<space>pris<space>palé<space>en<space>deuxième<space>fois<space>parce<space>que<space>aussi<space>euh<space>je<space>dis<space>pas<space>ce<space>lit<space>en<space>italien<space>je<space>je<space>suis<space>non<space>je<space>parle<space>euh<space>italien<space>je<space>le<space>comprends

2026-01-28 20:28:03,225 | INFO | speech length: 275360
2026-01-28 20:28:03,261 | INFO | decoder input length: 429
2026-01-28 20:28:03,261 | INFO | max output length: 429
2026-01-28 20:28:03,261 | INFO | min output length: 42
2026-01-28 20:28:26,832 | INFO | end detected at 316
2026-01-28 20:28:26,835 | INFO | -40.35 * 0.5 = -20.18 for decoder
2026-01-28 20:28:26,835 | INFO | -12.34 * 0.5 =  -6.17 for ctc
2026-01-28 20:28:26,835 | INFO | total log probability: -26.35
2026-01-28 20:28:26,835 | INFO | normalized log probability: -0.09
2026-01-28 20:28:26,835 | INFO | total number of ended hypotheses: 225
2026-01-28 20:28:26,839 | INFO | best hypo: et<space>donc<space>euh<space>en<space>fait<space>euh<space>ben<space>j'aime<space>beaucoup<space>l'italien<space>là<space>euh<space>et<space>j'apprécie<space>vraiment<space>ce<space>que<space>je<space>fais<space>mais<space>en<space>fait<space>euh<space>je<space>pense<space>que<space>je<space>vais<space>peut<space>être<space>me<space>réorienter<space>euh<space>si<space>à<space>la<space>fin<space>de<space>l'année<space>si<space>euh<space>fin<space>je<space>vais<space>finir<space>l'année<space>là<space>euh<space>en<space>un<space>an<space>et<space>euh<space>si<space>euh<space>je<space>réussis<space>pas<space>ben<space>je<space>pense<space>que<space>je<space>vais<space>en<space>psychologie

2026-01-28 20:28:26,842 | INFO | speech length: 298560
2026-01-28 20:28:26,901 | INFO | decoder input length: 466
2026-01-28 20:28:26,901 | INFO | max output length: 466
2026-01-28 20:28:26,901 | INFO | min output length: 46
2026-01-28 20:29:18,227 | INFO | end detected at 428
2026-01-28 20:29:18,229 | INFO | -138.45 * 0.5 = -69.23 for decoder
2026-01-28 20:29:18,229 | INFO | -141.03 * 0.5 = -70.51 for ctc
2026-01-28 20:29:18,229 | INFO | total log probability: -139.74
2026-01-28 20:29:18,229 | INFO | normalized log probability: -0.33
2026-01-28 20:29:18,229 | INFO | total number of ended hypotheses: 188
2026-01-28 20:29:18,234 | INFO | best hypo: parce<space>que<space>c'est<space>plus<space>euh<space>ce<space>que<space>je<space>veux<space>faire<space>ma<space>psycho<space>et<space>euh<space>si<space>je<space>si<space>je<space>passe<space>'année<space>par<space>contre<space>je<space>pense<space>que<space>je<space>fais<space>finir<space>ma<space>lescence<space>je<space>passe<space>je<space>passe<space>en<space>deuxième<space>et<space>demi<space>en<space>troisième<space>année<space>je<space>finis<space>à<space>finirais<space>ma<space>licence<space>pour<space>avoir<space>mal<space>sens<space>mais<space>si<space>je<space>passe<space>pas<space>fin<space>je<space>c'est<space>j'ai<space>beau<space>j'appris<space>beaucop<space>de<space>chose<space>et<space>euh<space>c'est<space>très<space>intéressant<space>mais<space>je<space>pense<space>que<space>je<space>me<space>rendse<space>en<space>psychologie<space>parce<space>que<space>ça<space>m'intéresse<space>plus

2026-01-28 20:29:18,237 | INFO | speech length: 216160
2026-01-28 20:29:18,303 | INFO | decoder input length: 337
2026-01-28 20:29:18,303 | INFO | max output length: 337
2026-01-28 20:29:18,303 | INFO | min output length: 33
2026-01-28 20:29:35,956 | INFO | end detected at 293
2026-01-28 20:29:35,959 | INFO | -35.64 * 0.5 = -17.82 for decoder
2026-01-28 20:29:35,959 | INFO | -26.13 * 0.5 = -13.07 for ctc
2026-01-28 20:29:35,959 | INFO | total log probability: -30.89
2026-01-28 20:29:35,959 | INFO | normalized log probability: -0.11
2026-01-28 20:29:35,959 | INFO | total number of ended hypotheses: 233
2026-01-28 20:29:35,963 | INFO | best hypo: je<space>suis<space>plus<space>euh<space>fin<space>on<space>dit<space>que<space>je<space>suis<space>à<space>l'écoute<space>des<space>personnes<space>je<space>comprends<space>aussi<space>et<space>que<space>quand<space>tout<space>le<space>monde<space>me<space>dit<space>que<space>je<space>suis<space>plus<space>euh<space>faite<space>pour<space>euh<space>pour<space>ce<space>métier<space>là<space>ces<space>études<space>là<space>que<space>pour<space>l'italien<space>donc<space>euh<space>je<space>pense<space>que<space>j'ai<space>euh<space>je<space>vais<space>sûrement<space>rereurter<space>en<space>psychologie

2026-01-28 20:29:35,973 | INFO | Chunk: 0 | WER=33.870968 | S=10 D=6 I=5
2026-01-28 20:29:35,973 | INFO | Chunk: 1 | WER=33.333333 | S=5 D=0 I=2
2026-01-28 20:29:35,976 | INFO | Chunk: 2 | WER=30.000000 | S=13 D=1 I=4
2026-01-28 20:29:35,979 | INFO | Chunk: 3 | WER=20.000000 | S=1 D=0 I=11
2026-01-28 20:29:35,984 | INFO | Chunk: 4 | WER=26.666667 | S=14 D=5 I=5
2026-01-28 20:29:35,986 | INFO | Chunk: 5 | WER=25.000000 | S=7 D=2 I=5
2026-01-28 20:29:36,054 | INFO | File: Rhap-M1001.wav | WER=27.507163 | S=50 D=14 I=32
2026-01-28 20:29:36,054 | INFO | ------------------------------
2026-01-28 20:29:36,054 | INFO | Conf ester Done!
2026-01-28 20:31:27,295 | INFO | Chunk: 0 | WER=41.935484 | S=8 D=18 I=0
2026-01-28 20:31:27,296 | INFO | Chunk: 1 | WER=33.333333 | S=5 D=2 I=0
2026-01-28 20:31:27,301 | INFO | Chunk: 2 | WER=45.000000 | S=16 D=10 I=1
2026-01-28 20:31:27,306 | INFO | Chunk: 3 | WER=18.333333 | S=7 D=3 I=1
2026-01-28 20:31:27,316 | INFO | Chunk: 4 | WER=41.111111 | S=19 D=16 I=2
2026-01-28 20:31:27,321 | INFO | Chunk: 5 | WER=35.714286 | S=14 D=4 I=2
2026-01-28 20:31:27,443 | INFO | File: Rhap-M1001.wav | WER=36.676218 | S=69 D=53 I=6
2026-01-28 20:31:27,443 | INFO | ------------------------------
2026-01-28 20:31:27,443 | INFO | hmm_tdnn Done!
2026-01-28 20:31:27,658 | INFO | ==================================Rhap-M1003.wav=========================================
2026-01-28 20:31:27,902 | INFO | Using rVAD model
2026-01-28 20:31:57,634 | INFO | Chunk: 0 | WER=8.695652 | S=5 D=3 I=0
2026-01-28 20:31:57,640 | INFO | Chunk: 1 | WER=11.111111 | S=5 D=4 I=0
2026-01-28 20:31:57,644 | INFO | Chunk: 2 | WER=18.840580 | S=7 D=5 I=1
2026-01-28 20:31:57,645 | INFO | Chunk: 3 | WER=5.263158 | S=1 D=0 I=0
2026-01-28 20:31:57,647 | INFO | Chunk: 4 | WER=7.142857 | S=1 D=2 I=0
2026-01-28 20:31:57,647 | INFO | Chunk: 5 | WER=0.000000 | S=1 D=0 I=0
2026-01-28 20:31:57,648 | INFO | Chunk: 6 | WER=17.857143 | S=3 D=2 I=0
2026-01-28 20:31:57,649 | INFO | Chunk: 7 | WER=20.000000 | S=1 D=0 I=0
2026-01-28 20:31:57,653 | INFO | Chunk: 8 | WER=21.917808 | S=7 D=9 I=0
2026-01-28 20:31:57,654 | INFO | Chunk: 9 | WER=41.666667 | S=1 D=9 I=0
2026-01-28 20:31:57,656 | INFO | Chunk: 10 | WER=1.923077 | S=1 D=0 I=0
2026-01-28 20:31:57,658 | INFO | Chunk: 11 | WER=7.407407 | S=2 D=0 I=0
2026-01-28 20:31:57,662 | INFO | Chunk: 12 | WER=19.718310 | S=6 D=8 I=0
2026-01-28 20:31:57,664 | INFO | Chunk: 13 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:31:57,668 | INFO | Chunk: 14 | WER=27.397260 | S=7 D=13 I=0
2026-01-28 20:31:57,670 | INFO | Chunk: 15 | WER=11.320755 | S=5 D=1 I=0
2026-01-28 20:31:58,052 | INFO | File: Rhap-M1003.wav | WER=14.591700 | S=53 D=55 I=1
2026-01-28 20:31:58,053 | INFO | ------------------------------
2026-01-28 20:31:58,053 | INFO | w2vec vad chunk Done!
2026-01-28 20:32:32,517 | INFO | Chunk: 0 | WER=67.391304 | S=2 D=60 I=0
2026-01-28 20:32:32,519 | INFO | Chunk: 1 | WER=76.543210 | S=0 D=62 I=0
2026-01-28 20:32:32,521 | INFO | Chunk: 2 | WER=47.826087 | S=3 D=30 I=0
2026-01-28 20:32:32,522 | INFO | Chunk: 3 | WER=15.789474 | S=2 D=1 I=0
2026-01-28 20:32:32,523 | INFO | Chunk: 4 | WER=19.047619 | S=3 D=5 I=0
2026-01-28 20:32:32,523 | INFO | Chunk: 5 | WER=0.000000 | S=1 D=0 I=0
2026-01-28 20:32:32,524 | INFO | Chunk: 6 | WER=10.714286 | S=0 D=3 I=0
2026-01-28 20:32:32,524 | INFO | Chunk: 7 | WER=40.000000 | S=1 D=1 I=0
2026-01-28 20:32:32,527 | INFO | Chunk: 8 | WER=46.575342 | S=4 D=29 I=1
2026-01-28 20:32:32,527 | INFO | Chunk: 9 | WER=37.500000 | S=2 D=7 I=0
2026-01-28 20:32:32,529 | INFO | Chunk: 10 | WER=15.384615 | S=0 D=8 I=0
2026-01-28 20:32:32,530 | INFO | Chunk: 11 | WER=11.111111 | S=2 D=1 I=0
2026-01-28 20:32:32,532 | INFO | Chunk: 12 | WER=60.563380 | S=0 D=43 I=0
2026-01-28 20:32:32,533 | INFO | Chunk: 13 | WER=7.894737 | S=1 D=0 I=2
2026-01-28 20:32:32,536 | INFO | Chunk: 14 | WER=57.534247 | S=22 D=19 I=1
2026-01-28 20:32:32,538 | INFO | Chunk: 15 | WER=15.094340 | S=4 D=1 I=3
2026-01-28 20:32:32,737 | INFO | File: Rhap-M1003.wav | WER=43.105756 | S=48 D=268 I=6
2026-01-28 20:32:32,737 | INFO | ------------------------------
2026-01-28 20:32:32,738 | INFO | whisper med Done!
2026-01-28 20:33:29,020 | INFO | Chunk: 0 | WER=65.217391 | S=0 D=60 I=0
2026-01-28 20:33:29,024 | INFO | Chunk: 1 | WER=40.740741 | S=6 D=27 I=0
2026-01-28 20:33:29,026 | INFO | Chunk: 2 | WER=49.275362 | S=0 D=34 I=0
2026-01-28 20:33:29,026 | INFO | Chunk: 3 | WER=15.789474 | S=2 D=1 I=0
2026-01-28 20:33:29,028 | INFO | Chunk: 4 | WER=9.523810 | S=1 D=3 I=0
2026-01-28 20:33:29,028 | INFO | Chunk: 5 | WER=0.000000 | S=1 D=0 I=0
2026-01-28 20:33:29,029 | INFO | Chunk: 6 | WER=3.571429 | S=1 D=0 I=0
2026-01-28 20:33:29,029 | INFO | Chunk: 7 | WER=100.000000 | S=2 D=3 I=0
2026-01-28 20:33:29,032 | INFO | Chunk: 8 | WER=42.465753 | S=6 D=23 I=2
2026-01-28 20:33:29,032 | INFO | Chunk: 9 | WER=37.500000 | S=1 D=8 I=0
2026-01-28 20:33:29,034 | INFO | Chunk: 10 | WER=28.846154 | S=2 D=13 I=0
2026-01-28 20:33:29,035 | INFO | Chunk: 11 | WER=11.111111 | S=3 D=0 I=0
2026-01-28 20:33:29,037 | INFO | Chunk: 12 | WER=32.394366 | S=11 D=12 I=0
2026-01-28 20:33:29,039 | INFO | Chunk: 13 | WER=5.263158 | S=2 D=0 I=0
2026-01-28 20:33:29,042 | INFO | Chunk: 14 | WER=36.986301 | S=14 D=12 I=1
2026-01-28 20:33:29,044 | INFO | Chunk: 15 | WER=7.547170 | S=3 D=1 I=0
2026-01-28 20:33:29,270 | INFO | File: Rhap-M1003.wav | WER=34.002677 | S=55 D=196 I=3
2026-01-28 20:33:29,270 | INFO | ------------------------------
2026-01-28 20:33:29,270 | INFO | whisper large Done!
2026-01-28 20:33:29,446 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 20:33:29,484 | INFO | Vocabulary size: 350
2026-01-28 20:33:30,700 | INFO | Gradient checkpoint layers: []
2026-01-28 20:33:31,379 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 20:33:31,383 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 20:33:31,383 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 20:33:31,384 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 20:33:31,384 | INFO | speech length: 465280
2026-01-28 20:33:31,430 | INFO | decoder input length: 726
2026-01-28 20:33:31,430 | INFO | max output length: 726
2026-01-28 20:33:31,430 | INFO | min output length: 72
2026-01-28 20:33:59,438 | INFO | end detected at 227
2026-01-28 20:33:59,440 | INFO | -741.34 * 0.5 = -370.67 for decoder
2026-01-28 20:33:59,440 | INFO | -307.06 * 0.5 = -153.53 for ctc
2026-01-28 20:33:59,440 | INFO | total log probability: -524.20
2026-01-28 20:33:59,440 | INFO | normalized log probability: -2.37
2026-01-28 20:33:59,441 | INFO | total number of ended hypotheses: 163
2026-01-28 20:33:59,443 | INFO | best hypo: ▁alors▁un▁bonjour▁à▁tous▁donc▁je▁me▁présente▁de▁façon▁brève▁je▁m'appelle▁angeline▁âge▁et▁dix▁huit▁ans▁qui▁a▁obtenu▁montacle▁l'année▁dernière▁donc▁section▁économique▁et▁sociale▁lorsque▁je▁le▁réfèree▁de▁ma▁vis▁à▁devenir▁professieur▁d'italien▁à▁savoir▁certser▁donc▁enseigner▁le▁collège▁et▁ainsi▁qu'au▁lychée▁donc▁l'enseignement▁c'est▁le▁fait▁de▁le▁tranchesmettres▁et▁ses▁compéttes▁et▁ces▁elle▁acquise▁les▁chosée▁qu'onn▁acquisme▁tout▁au▁lonogre▁de▁mes▁années▁d'étude▁également▁de▁façonersonnelle

2026-01-28 20:33:59,447 | INFO | speech length: 438720
2026-01-28 20:33:59,508 | INFO | decoder input length: 685
2026-01-28 20:33:59,508 | INFO | max output length: 685
2026-01-28 20:33:59,508 | INFO | min output length: 68
2026-01-28 20:34:23,652 | INFO | end detected at 197
2026-01-28 20:34:23,655 | INFO | -441.19 * 0.5 = -220.59 for decoder
2026-01-28 20:34:23,655 | INFO | -111.68 * 0.5 = -55.84 for ctc
2026-01-28 20:34:23,655 | INFO | total log probability: -276.43
2026-01-28 20:34:23,655 | INFO | normalized log probability: -1.45
2026-01-28 20:34:23,656 | INFO | total number of ended hypotheses: 152
2026-01-28 20:34:23,659 | INFO | best hypo: ▁et▁afin▁d'offrir▁les▁clés▁et▁les▁outils▁nécessaires▁à▁la▁réussite▁à▁une▁classe▁à▁un▁groupe▁d'élèves▁par▁ailleurs▁ça▁demande▁également▁d'être▁à▁l'écoute▁un▁car▁les▁élèves▁sont▁souvent▁complexes▁et▁également▁fragiles▁il▁faut▁également▁pour▁proposer▁des▁méthodes▁pédagogiques▁avec▁qui▁visent▁à▁la▁intéresster▁dont▁on▁auditor▁dans▁sa▁classe▁pour▁parvenir▁à▁de▁très▁bons▁rés▁seultats▁et▁également▁c'est▁le▁fait▁'ain▁de▁corger▁mais▁également▁de▁réaliser▁des▁devoirs

2026-01-28 20:34:23,663 | INFO | speech length: 296800
2026-01-28 20:34:23,725 | INFO | decoder input length: 463
2026-01-28 20:34:23,725 | INFO | max output length: 463
2026-01-28 20:34:23,725 | INFO | min output length: 46
2026-01-28 20:34:40,699 | INFO | end detected at 166
2026-01-28 20:34:40,700 | INFO | -459.00 * 0.5 = -229.50 for decoder
2026-01-28 20:34:40,700 | INFO | -167.04 * 0.5 = -83.52 for ctc
2026-01-28 20:34:40,700 | INFO | total log probability: -313.02
2026-01-28 20:34:40,700 | INFO | normalized log probability: -1.94
2026-01-28 20:34:40,700 | INFO | total number of ended hypotheses: 157
2026-01-28 20:34:40,703 | INFO | best hypo: ▁donc▁par▁ailleurs▁ce▁métier▁que▁nous▁emmenies▁nous▁amener▁également▁à▁nous▁déplacer▁donc▁lors▁de▁sorties▁qui▁peuvent▁être▁plus▁ou▁moins▁importantes▁nous▁nous▁appons▁être▁p▁la▁région▁ou▁comme▁à▁l'étnranger▁en▁particulier▁avec▁lorsqu'onceigne▁une▁langued▁ce▁métier▁s'exerce'▁ou▁d'un▁privé▁ou▁dans▁un▁public▁de▁ce▁qui▁ne▁se▁concernen▁jamais▁reneigné▁dans▁un▁établissement▁public

2026-01-28 20:34:40,705 | INFO | speech length: 130400
2026-01-28 20:34:40,751 | INFO | decoder input length: 203
2026-01-28 20:34:40,751 | INFO | max output length: 203
2026-01-28 20:34:40,751 | INFO | min output length: 20
2026-01-28 20:34:43,629 | INFO | end detected at 48
2026-01-28 20:34:43,630 | INFO |  -5.22 * 0.5 =  -2.61 for decoder
2026-01-28 20:34:43,631 | INFO |  -3.83 * 0.5 =  -1.92 for ctc
2026-01-28 20:34:43,631 | INFO | total log probability: -4.53
2026-01-28 20:34:43,631 | INFO | normalized log probability: -0.10
2026-01-28 20:34:43,631 | INFO | total number of ended hypotheses: 165
2026-01-28 20:34:43,631 | INFO | best hypo: ▁donc▁c'est▁à▁peu▁près▁donc▁un▁professeur▁certifié▁à▁de▁travail▁dix▁huit▁heures▁par▁semaine▁ou▁hebdomadaire

2026-01-28 20:34:43,634 | INFO | speech length: 204000
2026-01-28 20:34:43,713 | INFO | decoder input length: 318
2026-01-28 20:34:43,713 | INFO | max output length: 318
2026-01-28 20:34:43,713 | INFO | min output length: 31
2026-01-28 20:34:55,498 | INFO | end detected at 103
2026-01-28 20:34:55,501 | INFO | -82.36 * 0.5 = -41.18 for decoder
2026-01-28 20:34:55,501 | INFO |  -9.48 * 0.5 =  -4.74 for ctc
2026-01-28 20:34:55,501 | INFO | total log probability: -45.92
2026-01-28 20:34:55,501 | INFO | normalized log probability: -0.48
2026-01-28 20:34:55,502 | INFO | total number of ended hypotheses: 213
2026-01-28 20:34:55,503 | INFO | best hypo: ▁vous▁généralisez▁une▁enquête▁métier▁qui▁a▁mis▁fortement▁en▁valeur▁le▁fait▁que▁cette▁profession▁est▁véritablement▁réservée▁aux▁personnes▁non▁seulement▁passionnées▁par▁la▁matière▁qu'ils▁enseignent▁les▁également▁par▁l'enseignement▁parce▁qu'on▁remarque

2026-01-28 20:34:55,505 | INFO | speech length: 8480
2026-01-28 20:34:55,539 | INFO | decoder input length: 12
2026-01-28 20:34:55,539 | INFO | max output length: 12
2026-01-28 20:34:55,539 | INFO | min output length: 1
2026-01-28 20:34:55,852 | INFO | end detected at 8
2026-01-28 20:34:55,854 | INFO |  -1.44 * 0.5 =  -0.72 for decoder
2026-01-28 20:34:55,854 | INFO |  -1.98 * 0.5 =  -0.99 for ctc
2026-01-28 20:34:55,854 | INFO | total log probability: -1.71
2026-01-28 20:34:55,854 | INFO | normalized log probability: -0.57
2026-01-28 20:34:55,854 | INFO | total number of ended hypotheses: 154
2026-01-28 20:34:55,854 | INFO | best hypo: ▁un

2026-01-28 20:34:55,856 | INFO | speech length: 137920
2026-01-28 20:34:55,918 | INFO | decoder input length: 215
2026-01-28 20:34:55,918 | INFO | max output length: 215
2026-01-28 20:34:55,918 | INFO | min output length: 21
2026-01-28 20:35:00,153 | INFO | end detected at 65
2026-01-28 20:35:00,154 | INFO |  -8.30 * 0.5 =  -4.15 for decoder
2026-01-28 20:35:00,154 | INFO |  -4.21 * 0.5 =  -2.11 for ctc
2026-01-28 20:35:00,154 | INFO | total log probability: -6.26
2026-01-28 20:35:00,154 | INFO | normalized log probability: -0.10
2026-01-28 20:35:00,154 | INFO | total number of ended hypotheses: 137
2026-01-28 20:35:00,155 | INFO | best hypo: ▁on▁remarque▁bien▁que▁le▁salaire▁n'est▁ni▁en▁adéquation▁avec▁le▁nombre▁d'années▁avec▁le▁nombre▁d'études▁suivies▁ni▁avec▁le▁travail▁a▁fourni

2026-01-28 20:35:00,157 | INFO | speech length: 19840
2026-01-28 20:35:00,208 | INFO | decoder input length: 30
2026-01-28 20:35:00,208 | INFO | max output length: 30
2026-01-28 20:35:00,208 | INFO | min output length: 3
2026-01-28 20:35:01,031 | INFO | end detected at 21
2026-01-28 20:35:01,032 | INFO |  -1.14 * 0.5 =  -0.57 for decoder
2026-01-28 20:35:01,032 | INFO |  -0.93 * 0.5 =  -0.46 for ctc
2026-01-28 20:35:01,032 | INFO | total log probability: -1.04
2026-01-28 20:35:01,032 | INFO | normalized log probability: -0.06
2026-01-28 20:35:01,032 | INFO | total number of ended hypotheses: 166
2026-01-28 20:35:01,032 | INFO | best hypo: ▁personnel▁que▁l'on▁fournit

2026-01-28 20:35:01,034 | INFO | speech length: 417600
2026-01-28 20:35:01,070 | INFO | decoder input length: 652
2026-01-28 20:35:01,070 | INFO | max output length: 652
2026-01-28 20:35:01,070 | INFO | min output length: 65
2026-01-28 20:35:23,967 | INFO | end detected at 199
2026-01-28 20:35:23,970 | INFO | -706.64 * 0.5 = -353.32 for decoder
2026-01-28 20:35:23,970 | INFO | -297.36 * 0.5 = -148.68 for ctc
2026-01-28 20:35:23,970 | INFO | total log probability: -502.00
2026-01-28 20:35:23,970 | INFO | normalized log probability: -2.61
2026-01-28 20:35:23,970 | INFO | total number of ended hypotheses: 194
2026-01-28 20:35:23,973 | INFO | best hypo: ▁peuh▁d'autre▁part▁j'ai▁pu▁relever▁en▁faisant▁tous▁ces▁tests▁au▁cours▁de▁donc▁du▁semestre▁dans▁cette▁matière▁mais▁également▁j▁grâce▁à▁mon▁dossier▁hommemallah▁à▁l'éboration▁de▁mon▁projet▁comme▁je▁compreux▁no▁train▁je▁pouvais▁noter▁un▁certains▁comme▁atoupt▁indis▁pensables▁pas▁dans▁ce▁métier▁à▁avoir▁donc▁que▁je▁suis▁dynramique▁contsencieuse▁percevérante▁objectif▁et▁j▁justes▁j'aimement▁énorrmment▁m'indresct▁fe▁un▁auditoire▁et

2026-01-28 20:35:23,976 | INFO | speech length: 90240
2026-01-28 20:35:24,025 | INFO | decoder input length: 140
2026-01-28 20:35:24,026 | INFO | max output length: 140
2026-01-28 20:35:24,026 | INFO | min output length: 14
2026-01-28 20:35:26,259 | INFO | end detected at 43
2026-01-28 20:35:26,261 | INFO |  -5.61 * 0.5 =  -2.80 for decoder
2026-01-28 20:35:26,261 | INFO |  -8.19 * 0.5 =  -4.09 for ctc
2026-01-28 20:35:26,261 | INFO | total log probability: -6.90
2026-01-28 20:35:26,261 | INFO | normalized log probability: -0.18
2026-01-28 20:35:26,261 | INFO | total number of ended hypotheses: 175
2026-01-28 20:35:26,262 | INFO | best hypo: ▁et▁je▁jugeais▁je▁venger▁le▁sentiment▁de▁m'accomplir▁en▁formant▁et▁en▁aidant▁les▁autres

2026-01-28 20:35:26,264 | INFO | speech length: 254560
2026-01-28 20:35:26,308 | INFO | decoder input length: 397
2026-01-28 20:35:26,308 | INFO | max output length: 397
2026-01-28 20:35:26,308 | INFO | min output length: 39
2026-01-28 20:35:36,213 | INFO | end detected at 118
2026-01-28 20:35:36,214 | INFO | -198.29 * 0.5 = -99.14 for decoder
2026-01-28 20:35:36,214 | INFO | -90.53 * 0.5 = -45.27 for ctc
2026-01-28 20:35:36,214 | INFO | total log probability: -144.41
2026-01-28 20:35:36,215 | INFO | normalized log probability: -1.29
2026-01-28 20:35:36,215 | INFO | total number of ended hypotheses: 198
2026-01-28 20:35:36,216 | INFO | best hypo: ▁et▁donc▁on▁note▁j'ai▁le▁souci▁également▁de▁bien▁faire▁dont▁qu'on▁peut▁noter▁donc▁par▁rapport▁à▁mon▁profil▁que▁je▁mêle▁le▁côté▁conventionnel▁avec▁ce▁souci▁de▁bien▁père▁le▁fait▁d'ête▁conscienciencieuse▁et▁également▁le▁côté▁du▁soucial▁avec▁ce▁besoing▁performent▁d'changer▁avec▁autru

2026-01-28 20:35:36,219 | INFO | speech length: 130400
2026-01-28 20:35:36,274 | INFO | decoder input length: 203
2026-01-28 20:35:36,274 | INFO | max output length: 203
2026-01-28 20:35:36,274 | INFO | min output length: 20
2026-01-28 20:35:40,764 | INFO | end detected at 77
2026-01-28 20:35:40,766 | INFO |  -7.94 * 0.5 =  -3.97 for decoder
2026-01-28 20:35:40,766 | INFO |  -8.24 * 0.5 =  -4.12 for ctc
2026-01-28 20:35:40,766 | INFO | total log probability: -8.09
2026-01-28 20:35:40,766 | INFO | normalized log probability: -0.12
2026-01-28 20:35:40,766 | INFO | total number of ended hypotheses: 176
2026-01-28 20:35:40,767 | INFO | best hypo: ▁par▁ailleurs▁les▁avantages▁dans▁ce▁métier▁sont▁les▁possibilités▁d'évolution▁puisque▁on▁peut▁après▁passer▁le▁concours▁de▁lagré▁pour▁enseigner▁à▁l'université

2026-01-28 20:35:40,769 | INFO | speech length: 351680
2026-01-28 20:35:40,833 | INFO | decoder input length: 549
2026-01-28 20:35:40,833 | INFO | max output length: 549
2026-01-28 20:35:40,833 | INFO | min output length: 54
2026-01-28 20:35:57,159 | INFO | end detected at 159
2026-01-28 20:35:57,161 | INFO | -321.51 * 0.5 = -160.75 for decoder
2026-01-28 20:35:57,161 | INFO | -67.71 * 0.5 = -33.85 for ctc
2026-01-28 20:35:57,161 | INFO | total log probability: -194.61
2026-01-28 20:35:57,161 | INFO | normalized log probability: -1.27
2026-01-28 20:35:57,161 | INFO | total number of ended hypotheses: 146
2026-01-28 20:35:57,163 | INFO | best hypo: ▁d'autre▁part▁il▁ne▁faut▁pas▁se▁mentir▁les▁vacances▁sont▁nombreuses▁ce▁qui▁permet▁d'avoir▁une▁vie▁privée▁agréable▁et▁de▁construire▁également▁une▁famille▁ou▁les▁horaires▁sont▁également▁satisfaisants▁puisques▁auteures▁par▁semaine▁ce▁n'est▁pas▁le▁but▁atroces▁et▁c'▁que▁même▁si▁on▁a▁un▁programme▁suivre▁on▁les▁programmes▁sont▁établis▁ont▁ont▁ui▁d'une▁certaine▁liberté

2026-01-28 20:35:57,166 | INFO | speech length: 195680
2026-01-28 20:35:57,220 | INFO | decoder input length: 305
2026-01-28 20:35:57,221 | INFO | max output length: 305
2026-01-28 20:35:57,221 | INFO | min output length: 30
2026-01-28 20:36:04,661 | INFO | end detected at 98
2026-01-28 20:36:04,664 | INFO | -52.22 * 0.5 = -26.11 for decoder
2026-01-28 20:36:04,664 | INFO |  -7.43 * 0.5 =  -3.72 for ctc
2026-01-28 20:36:04,664 | INFO | total log probability: -29.83
2026-01-28 20:36:04,664 | INFO | normalized log probability: -0.32
2026-01-28 20:36:04,664 | INFO | total number of ended hypotheses: 185
2026-01-28 20:36:04,665 | INFO | best hypo: ▁les▁inconvénients▁sont▁par▁ailleurs▁en▁premier▁lieu▁le▁salaire▁évidemment▁qui▁n'est▁pas▁vraiment▁puisque▁un▁professeur▁certifié▁a▁commencé▁à▁mille▁quatre▁cents▁euros▁par▁mois▁et▁finira▁aux▁alentours▁de▁trois▁mille▁euros

2026-01-28 20:36:04,668 | INFO | speech length: 387040
2026-01-28 20:36:04,724 | INFO | decoder input length: 604
2026-01-28 20:36:04,724 | INFO | max output length: 604
2026-01-28 20:36:04,724 | INFO | min output length: 60
2026-01-28 20:36:23,277 | INFO | end detected at 172
2026-01-28 20:36:23,278 | INFO | -423.79 * 0.5 = -211.90 for decoder
2026-01-28 20:36:23,279 | INFO | -119.10 * 0.5 = -59.55 for ctc
2026-01-28 20:36:23,279 | INFO | total log probability: -271.44
2026-01-28 20:36:23,279 | INFO | normalized log probability: -1.65
2026-01-28 20:36:23,279 | INFO | total number of ended hypotheses: 176
2026-01-28 20:36:23,281 | INFO | best hypo: ▁en▁d'autre▁part▁le▁comportement▁des▁élèves▁semble▁au▁fil▁des▁années▁se▁dégrader▁également▁beaucoup▁moins▁concentré▁à▁mains▁de▁respect▁pour▁le▁professeur▁ce▁qui▁débalkaya▁plusieurs▁années▁le▁nombre▁de▁posttes▁diminue▁est▁également▁ce▁qui▁est▁regrettable▁d'autant▁que▁j'ai▁j'aimerais▁encoenseigner▁d'▁en▁corthe▁ou▁de▁l'amendroit▁où▁je▁suis▁né▁les▁pos▁encore▁moins▁nombreux

2026-01-28 20:36:23,284 | INFO | speech length: 194400
2026-01-28 20:36:23,341 | INFO | decoder input length: 303
2026-01-28 20:36:23,342 | INFO | max output length: 303
2026-01-28 20:36:23,342 | INFO | min output length: 30
2026-01-28 20:36:31,618 | INFO | end detected at 118
2026-01-28 20:36:31,619 | INFO | -115.25 * 0.5 = -57.62 for decoder
2026-01-28 20:36:31,619 | INFO | -26.47 * 0.5 = -13.24 for ctc
2026-01-28 20:36:31,619 | INFO | total log probability: -70.86
2026-01-28 20:36:31,619 | INFO | normalized log probability: -0.63
2026-01-28 20:36:31,620 | INFO | total number of ended hypotheses: 167
2026-01-28 20:36:31,621 | INFO | best hypo: ▁hé▁c'est▁drai▁qu'on▁est▁souvent▁éloigné▁de▁son▁domicile▁puisque▁ma▁professeur▁d'italien▁par▁exemple▁de▁l'an▁dernière▁mettait▁une▁heure▁et▁demie▁avant▁d'arriver▁d'un▁à▁son▁lycée▁et▁mamerk▁est▁également▁prof▁est▁à▁trois▁quart▁d'heure▁de▁son▁lycée▁également

2026-01-28 20:36:31,633 | INFO | Chunk: 0 | WER=41.304348 | S=26 D=7 I=5
2026-01-28 20:36:31,638 | INFO | Chunk: 1 | WER=18.518519 | S=10 D=1 I=4
2026-01-28 20:36:31,641 | INFO | Chunk: 2 | WER=33.333333 | S=18 D=3 I=2
2026-01-28 20:36:31,642 | INFO | Chunk: 3 | WER=26.315789 | S=4 D=0 I=1
2026-01-28 20:36:31,643 | INFO | Chunk: 4 | WER=14.285714 | S=3 D=3 I=0
2026-01-28 20:36:31,643 | INFO | Chunk: 5 | WER=0.000000 | S=1 D=0 I=0
2026-01-28 20:36:31,644 | INFO | Chunk: 6 | WER=3.571429 | S=1 D=0 I=0
2026-01-28 20:36:31,644 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:36:31,648 | INFO | Chunk: 8 | WER=42.465753 | S=18 D=5 I=8
2026-01-28 20:36:31,648 | INFO | Chunk: 9 | WER=45.833333 | S=4 D=7 I=0
2026-01-28 20:36:31,650 | INFO | Chunk: 10 | WER=21.153846 | S=9 D=0 I=2
2026-01-28 20:36:31,651 | INFO | Chunk: 11 | WER=7.407407 | S=1 D=1 I=0
2026-01-28 20:36:31,654 | INFO | Chunk: 12 | WER=21.126761 | S=9 D=5 I=1
2026-01-28 20:36:31,656 | INFO | Chunk: 13 | WER=7.894737 | S=2 D=1 I=0
2026-01-28 20:36:31,659 | INFO | Chunk: 14 | WER=36.986301 | S=12 D=11 I=4
2026-01-28 20:36:31,661 | INFO | Chunk: 15 | WER=18.867925 | S=6 D=3 I=1
2026-01-28 20:36:31,958 | INFO | File: Rhap-M1003.wav | WER=26.506024 | S=124 D=46 I=28
2026-01-28 20:36:31,959 | INFO | ------------------------------
2026-01-28 20:36:31,959 | INFO | Conf cv Done!
2026-01-28 20:36:32,094 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 20:36:32,117 | INFO | Vocabulary size: 47
2026-01-28 20:36:33,489 | INFO | Gradient checkpoint layers: []
2026-01-28 20:36:34,230 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 20:36:34,234 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 20:36:34,234 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 20:36:34,235 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 20:36:34,237 | INFO | speech length: 465280
2026-01-28 20:36:34,285 | INFO | decoder input length: 726
2026-01-28 20:36:34,285 | INFO | max output length: 726
2026-01-28 20:36:34,285 | INFO | min output length: 72
2026-01-28 20:37:30,597 | INFO | end detected at 532
2026-01-28 20:37:30,599 | INFO | -469.14 * 0.5 = -234.57 for decoder
2026-01-28 20:37:30,599 | INFO | -92.41 * 0.5 = -46.21 for ctc
2026-01-28 20:37:30,599 | INFO | total log probability: -280.78
2026-01-28 20:37:30,599 | INFO | normalized log probability: -0.53
2026-01-28 20:37:30,599 | INFO | total number of ended hypotheses: 161
2026-01-28 20:37:30,607 | INFO | best hypo: alors<space>bonjour<space>à<space>tous<space>donc<space>euh<space>je<space>me<space>présente<space>de<space>façon<space>brève<space>je<space>m'appelle<space>en<space>jeune<space>nage<space>et<space>dix<space>huit<space>ans<space>qu'est<space>ce<space>qui<space>me<space>m'en<space>bat<space>que<space>l'année<space>dernière<space>euh<space>donc<space>euh<space>section<space>économique<space>et<space>sociale<space>alors<space>ce<space>que<space>j'ai<space>préfère<space>de<space>ma<space>vie<space>c'est<space>de<space>manière<space>professeur<space>d'italiens<space>à<space>savoir<space>certifier<space>donc<space>euh<space>enseigner<space>au<space>collège<space>ainsi<space>qu'au<space>lycée<space>donc<space>euh<space>l'enseignement<space>euh<space>c'est<space>le<space>fait<space>de<space>transmettre<space>euh<space>ses<space>compétences<space>t<space>ses<space>acquis<space>des<space>choses<space>qu'on<space>a<space>acquis<space>t<space>tout<space>au<space>long<space>de<space>ces<space>années<space>d'étude<space>mais<space>également<space>de<space>façon<space>personnelle

2026-01-28 20:37:30,610 | INFO | speech length: 438720
2026-01-28 20:37:30,649 | INFO | decoder input length: 685
2026-01-28 20:37:30,649 | INFO | max output length: 685
2026-01-28 20:37:30,649 | INFO | min output length: 68
2026-01-28 20:38:19,059 | INFO | end detected at 489
2026-01-28 20:38:19,061 | INFO | -447.88 * 0.5 = -223.94 for decoder
2026-01-28 20:38:19,061 | INFO | -62.20 * 0.5 = -31.10 for ctc
2026-01-28 20:38:19,061 | INFO | total log probability: -255.04
2026-01-28 20:38:19,061 | INFO | normalized log probability: -0.53
2026-01-28 20:38:19,061 | INFO | total number of ended hypotheses: 166
2026-01-28 20:38:19,067 | INFO | best hypo: et<space>euh<space>afin<space>d'offrir<space>les<space>clés<space>tous<space>les<space>les<space>outils<space>nécessaires<space>à<space>la<space>réussite<space>à<space>une<space>classe<space>donc<space>à<space>un<space>groupe<space>d'élèves<space>par<space>ailleurs<space>ça<space>demande<space>également<space>d'être<space>à<space>l'écoute<space>car<space>les<space>élèves<space>sont<space>souvent<space>complexes<space>et<space>également<space>fragiles<space>euh<space>il<space>faut<space>également<space>proposer<space>des<space>méthodes<space>pédagogiques<space>qui<space>qui<space>visent<space>à<space>intéresser<space>euh<space>donc<space>au<space>monde<space>temps<space>dans<space>sa<space>classe<space>pour<space>parvenir<space>à<space>de<space>très<space>bonus<space>résultats<space>et<space>euh<space>également<space>c'est<space>le<space>fait<space>deuh<space>ça<space>de<space>corriger<space>mais<space>dégalement<space>de<space>réaliser<space>des<space>débor

2026-01-28 20:38:19,070 | INFO | speech length: 296800
2026-01-28 20:38:19,109 | INFO | decoder input length: 463
2026-01-28 20:38:19,109 | INFO | max output length: 463
2026-01-28 20:38:19,109 | INFO | min output length: 46
2026-01-28 20:38:48,192 | INFO | end detected at 395
2026-01-28 20:38:48,194 | INFO | -61.26 * 0.5 = -30.63 for decoder
2026-01-28 20:38:48,194 | INFO | -24.22 * 0.5 = -12.11 for ctc
2026-01-28 20:38:48,194 | INFO | total log probability: -42.74
2026-01-28 20:38:48,194 | INFO | normalized log probability: -0.11
2026-01-28 20:38:48,194 | INFO | total number of ended hypotheses: 194
2026-01-28 20:38:48,199 | INFO | best hypo: donc<space>euh<space>par<space>ailleurs<space>euh<space>ce<space>métier<space>qui<space>nous<space>emmene<space>nous<space>amène<space>également<space>à<space>nous<space>déplacer<space>donc<space>leurs<space>de<space>sortie<space>qui<space>peuvent<space>être<space>plus<space>ou<space>moins<space>importantes<space>donc<space>ça<space>peut<space>être<space>dans<space>la<space>région<space>comme<space>euh<space>à<space>l'étranger<space>et<space>en<space>particulier<space>lorsqu'on<space>enseigne<space>une<space>langue<space>donc<space>euh<space>ce<space>métier<space>s'exerce<space>ou<space>dans<space>le<space>privé<space>ou<space>dans<space>le<space>public<space>en<space>ce<space>qui<space>me<space>concerne<space>jamais<space>enseigné<space>dans<space>un<space>établissement<space>public

2026-01-28 20:38:48,202 | INFO | speech length: 130400
2026-01-28 20:38:48,241 | INFO | decoder input length: 203
2026-01-28 20:38:48,241 | INFO | max output length: 203
2026-01-28 20:38:48,241 | INFO | min output length: 20
2026-01-28 20:38:55,052 | INFO | end detected at 134
2026-01-28 20:38:55,055 | INFO | -14.67 * 0.5 =  -7.34 for decoder
2026-01-28 20:38:55,055 | INFO |  -6.93 * 0.5 =  -3.46 for ctc
2026-01-28 20:38:55,055 | INFO | total log probability: -10.80
2026-01-28 20:38:55,055 | INFO | normalized log probability: -0.09
2026-01-28 20:38:55,055 | INFO | total number of ended hypotheses: 189
2026-01-28 20:38:55,057 | INFO | best hypo: donc<space>euh<space>hum<space>c'est<space>à<space>peu<space>près<space>donc<space>un<space>professeur<space>certifié<space>euh<space>donc<space>travaille<space>dix<space>huit<space>heures<space>euh<space>par<space>semaine<space>au<space>hebdomadaire

2026-01-28 20:38:55,061 | INFO | speech length: 204000
2026-01-28 20:38:55,143 | INFO | decoder input length: 318
2026-01-28 20:38:55,143 | INFO | max output length: 318
2026-01-28 20:38:55,143 | INFO | min output length: 31
2026-01-28 20:39:11,352 | INFO | end detected at 272
2026-01-28 20:39:11,355 | INFO | -21.74 * 0.5 = -10.87 for decoder
2026-01-28 20:39:11,355 | INFO |  -4.60 * 0.5 =  -2.30 for ctc
2026-01-28 20:39:11,355 | INFO | total log probability: -13.17
2026-01-28 20:39:11,355 | INFO | normalized log probability: -0.05
2026-01-28 20:39:11,355 | INFO | total number of ended hypotheses: 196
2026-01-28 20:39:11,358 | INFO | best hypo: donc<space>j'ai<space>réalisé<space>une<space>enquête<space>métier<space>qui<space>a<space>mis<space>euh<space>fortement<space>en<space>valeur<space>le<space>fait<space>que<space>cette<space>profession<space>est<space>véritablement<space>réservée<space>aux<space>personnes<space>non<space>seulement<space>passionnées<space>par<space>la<space>matière<space>qu'ils<space>enseignent<space>mais<space>également<space>par<space>l'enseignement<space>parce<space>qu'on<space>remarque<space>euh<space>à<space>ce

2026-01-28 20:39:11,361 | INFO | speech length: 8480
2026-01-28 20:39:11,388 | INFO | decoder input length: 12
2026-01-28 20:39:11,388 | INFO | max output length: 12
2026-01-28 20:39:11,389 | INFO | min output length: 1
2026-01-28 20:39:11,768 | INFO | adding <eos> in the last position in the loop
2026-01-28 20:39:11,777 | INFO | end detected at 11
2026-01-28 20:39:11,779 | INFO |  -0.51 * 0.5 =  -0.25 for decoder
2026-01-28 20:39:11,779 | INFO |  -1.12 * 0.5 =  -0.56 for ctc
2026-01-28 20:39:11,779 | INFO | total log probability: -0.82
2026-01-28 20:39:11,779 | INFO | normalized log probability: -0.16
2026-01-28 20:39:11,779 | INFO | total number of ended hypotheses: 203
2026-01-28 20:39:11,779 | INFO | best hypo: oui

2026-01-28 20:39:11,781 | INFO | speech length: 137920
2026-01-28 20:39:11,823 | INFO | decoder input length: 215
2026-01-28 20:39:11,823 | INFO | max output length: 215
2026-01-28 20:39:11,823 | INFO | min output length: 21
2026-01-28 20:39:20,247 | INFO | end detected at 142
2026-01-28 20:39:20,250 | INFO | -12.65 * 0.5 =  -6.33 for decoder
2026-01-28 20:39:20,250 | INFO |  -8.42 * 0.5 =  -4.21 for ctc
2026-01-28 20:39:20,250 | INFO | total log probability: -10.54
2026-01-28 20:39:20,250 | INFO | normalized log probability: -0.08
2026-01-28 20:39:20,251 | INFO | total number of ended hypotheses: 177
2026-01-28 20:39:20,253 | INFO | best hypo: on<space>remarque<space>bien<space>que<space>le<space>salaire<space>mis<space>en<space>adéquation<space>avec<space>le<space>nombre<space>d'années<space>avec<space>le<space>nombre<space>d'études<space>suivies<space>mis<space>avec<space>le<space>travail<space>à<space>fournir

2026-01-28 20:39:20,258 | INFO | speech length: 19840
2026-01-28 20:39:20,306 | INFO | decoder input length: 30
2026-01-28 20:39:20,306 | INFO | max output length: 30
2026-01-28 20:39:20,306 | INFO | min output length: 3
2026-01-28 20:39:21,447 | INFO | adding <eos> in the last position in the loop
2026-01-28 20:39:21,457 | INFO | no hypothesis. Finish decoding.
2026-01-28 20:39:21,458 | INFO |  -3.65 * 0.5 =  -1.82 for decoder
2026-01-28 20:39:21,458 | INFO |  -7.95 * 0.5 =  -3.98 for ctc
2026-01-28 20:39:21,458 | INFO | total log probability: -5.80
2026-01-28 20:39:21,458 | INFO | normalized log probability: -0.21
2026-01-28 20:39:21,458 | INFO | total number of ended hypotheses: 111
2026-01-28 20:39:21,459 | INFO | best hypo: personnel<space>que<space>l'on<space>fournit

2026-01-28 20:39:21,460 | INFO | speech length: 417600
2026-01-28 20:39:21,515 | INFO | decoder input length: 652
2026-01-28 20:39:21,515 | INFO | max output length: 652
2026-01-28 20:39:21,515 | INFO | min output length: 65
2026-01-28 20:40:07,650 | INFO | end detected at 445
2026-01-28 20:40:07,652 | INFO | -445.04 * 0.5 = -222.52 for decoder
2026-01-28 20:40:07,652 | INFO | -39.90 * 0.5 = -19.95 for ctc
2026-01-28 20:40:07,652 | INFO | total log probability: -242.47
2026-01-28 20:40:07,652 | INFO | normalized log probability: -0.55
2026-01-28 20:40:07,653 | INFO | total number of ended hypotheses: 190
2026-01-28 20:40:07,658 | INFO | best hypo: euh<space>d'autre<space>part<space>j'ai<space>pu<space>relever<space>en<space>faisant<space>tous<space>ces<space>tests<space>au<space>cours<space>de<space>de<space>mon<space>de<space>du<space>semestre<space>dans<space>cette<space>matière<space>mais<space>également<space>grâce<space>à<space>mon<space>dossier<space>à<space>m<space>à<space>la<space>à<space>l'élaboration<space>de<space>mon<space>projet<space>que<space>euh<space>j'ai<space>on<space>pouvait<space>mon<space>pain<space>je<space>pouvais<space>monter<space>certains<space>euh<space>atouts<space>indispensables<space>euh<space>dans<space>ce<space>métier<space>à<space>savoir<space>euh<space>donc<space>euh<space>je<space>suis<space>dynamique<space>conscientieuse<space>euh<space>persévérante<space>objective<space>et<space>juste<space>j'aime<space>énormément<space>m'adresser<space>pa<space>ça<space>un<space>auditoire<space>et<space>euh

2026-01-28 20:40:07,661 | INFO | speech length: 90240
2026-01-28 20:40:07,705 | INFO | decoder input length: 140
2026-01-28 20:40:07,705 | INFO | max output length: 140
2026-01-28 20:40:07,705 | INFO | min output length: 14
2026-01-28 20:40:12,096 | INFO | end detected at 99
2026-01-28 20:40:12,099 | INFO | -13.02 * 0.5 =  -6.51 for decoder
2026-01-28 20:40:12,099 | INFO |  -6.30 * 0.5 =  -3.15 for ctc
2026-01-28 20:40:12,099 | INFO | total log probability: -9.66
2026-01-28 20:40:12,099 | INFO | normalized log probability: -0.11
2026-01-28 20:40:12,099 | INFO | total number of ended hypotheses: 209
2026-01-28 20:40:12,100 | INFO | best hypo: et<space>je<space>j<space>j'a<space>je<space>fin<space>j'ai<space>le<space>le<space>sentiment<space>de<space>m'accomplir<space>en<space>formant<space>et<space>en<space>aidant<space>les<space>autres

2026-01-28 20:40:12,103 | INFO | speech length: 254560
2026-01-28 20:40:12,163 | INFO | decoder input length: 397
2026-01-28 20:40:12,163 | INFO | max output length: 397
2026-01-28 20:40:12,164 | INFO | min output length: 39
2026-01-28 20:40:33,814 | INFO | end detected at 289
2026-01-28 20:40:33,816 | INFO | -31.08 * 0.5 = -15.54 for decoder
2026-01-28 20:40:33,816 | INFO | -12.32 * 0.5 =  -6.16 for ctc
2026-01-28 20:40:33,816 | INFO | total log probability: -21.70
2026-01-28 20:40:33,816 | INFO | normalized log probability: -0.08
2026-01-28 20:40:33,816 | INFO | total number of ended hypotheses: 181
2026-01-28 20:40:33,820 | INFO | best hypo: euh<space>et<space>donc<space>on<space>note<space>j'ai<space>le<space>souci<space>également<space>de<space>bien<space>faire<space>donc<space>on<space>peut<space>noter<space>donc<space>par<space>rapport<space>à<space>mon<space>profil<space>que<space>je<space>mêle<space>euh<space>le<space>côté<space>conventionnel<space>avec<space>ce<space>souci<space>de<space>bien<space>père<space>euh<space>le<space>fait<space>d'être<space>consciencieuse<space>et<space>également<space>le<space>côté<space>social<space>avec<space>ce<space>nous<space>en<space>permanent<space>d'échanger<space>avec<space>autre

2026-01-28 20:40:33,823 | INFO | speech length: 130400
2026-01-28 20:40:33,871 | INFO | decoder input length: 203
2026-01-28 20:40:33,871 | INFO | max output length: 203
2026-01-28 20:40:33,871 | INFO | min output length: 20
2026-01-28 20:40:43,278 | INFO | end detected at 172
2026-01-28 20:40:43,280 | INFO | -13.64 * 0.5 =  -6.82 for decoder
2026-01-28 20:40:43,280 | INFO |  -0.82 * 0.5 =  -0.41 for ctc
2026-01-28 20:40:43,280 | INFO | total log probability: -7.23
2026-01-28 20:40:43,280 | INFO | normalized log probability: -0.04
2026-01-28 20:40:43,280 | INFO | total number of ended hypotheses: 154
2026-01-28 20:40:43,282 | INFO | best hypo: euh<space>par<space>ailleurs<space>les<space>avantages<space>dans<space>ce<space>métier<space>sont<space>euh<space>les<space>possibilités<space>d'évolution<space>puisque<space>on<space>peut<space>après<space>passer<space>le<space>concours<space>de<space>la<space>grève<space>pour<space>enseigner<space>à<space>l'université

2026-01-28 20:40:43,285 | INFO | speech length: 351680
2026-01-28 20:40:43,332 | INFO | decoder input length: 549
2026-01-28 20:40:43,333 | INFO | max output length: 549
2026-01-28 20:40:43,333 | INFO | min output length: 54
2026-01-28 20:41:25,139 | INFO | end detected at 404
2026-01-28 20:41:25,142 | INFO | -44.94 * 0.5 = -22.47 for decoder
2026-01-28 20:41:25,142 | INFO | -18.12 * 0.5 =  -9.06 for ctc
2026-01-28 20:41:25,142 | INFO | total log probability: -31.53
2026-01-28 20:41:25,142 | INFO | normalized log probability: -0.08
2026-01-28 20:41:25,142 | INFO | total number of ended hypotheses: 182
2026-01-28 20:41:25,147 | INFO | best hypo: euh<space>d'autre<space>part<space>il<space>ne<space>faut<space>pas<space>se<space>mentir<space>les<space>vacances<space>sont<space>nombreuses<space>ce<space>qui<space>permet<space>euh<space>d'avoir<space>une<space>vie<space>privée<space>agréable<space>et<space>de<space>construire<space>également<space>une<space>famille<space>euh<space>les<space>horaires<space>sont<space>également<space>satisfaisants<space>puisque<space>dix<space>heures<space>par<space>semaine<space>ce<space>n'est<space>pas<space>même<space>vu<space>atroce<space>et<space>euh<space>hum<space>c'est<space>vrai<space>que<space>même<space>si<space>on<space>a<space>un<space>programme<space>à<space>suivre<space>donc<space>les<space>programmes<space>sont<space>établis<space>euh<space>on<space>on<space>jouit<space>une<space>certaine<space>liberté

2026-01-28 20:41:25,150 | INFO | speech length: 195680
2026-01-28 20:41:25,189 | INFO | decoder input length: 305
2026-01-28 20:41:25,189 | INFO | max output length: 305
2026-01-28 20:41:25,189 | INFO | min output length: 30
2026-01-28 20:41:39,175 | INFO | end detected at 237
2026-01-28 20:41:39,177 | INFO | -18.83 * 0.5 =  -9.42 for decoder
2026-01-28 20:41:39,177 | INFO |  -5.15 * 0.5 =  -2.57 for ctc
2026-01-28 20:41:39,177 | INFO | total log probability: -11.99
2026-01-28 20:41:39,177 | INFO | normalized log probability: -0.05
2026-01-28 20:41:39,177 | INFO | total number of ended hypotheses: 188
2026-01-28 20:41:39,180 | INFO | best hypo: les<space>inconvénients<space>sont<space>en<space>par<space>ailleurs<space>en<space>premier<space>lieu<space>le<space>salaire<space>évidemment<space>qui<space>n'est<space>pas<space>vraiment<space>haut<space>puisque<space>un<space>professeur<space>certifié<space>va<space>commencer<space>à<space>mille<space>quatre<space>cents<space>euros<space>par<space>mois<space>et<space>finira<space>aux<space>alentours<space>de<space>trois<space>mille<space>euros

2026-01-28 20:41:39,183 | INFO | speech length: 387040
2026-01-28 20:41:39,219 | INFO | decoder input length: 604
2026-01-28 20:41:39,219 | INFO | max output length: 604
2026-01-28 20:41:39,219 | INFO | min output length: 60
2026-01-28 20:42:16,089 | INFO | end detected at 410
2026-01-28 20:42:16,090 | INFO | -97.37 * 0.5 = -48.69 for decoder
2026-01-28 20:42:16,090 | INFO | -24.42 * 0.5 = -12.21 for ctc
2026-01-28 20:42:16,090 | INFO | total log probability: -60.90
2026-01-28 20:42:16,090 | INFO | normalized log probability: -0.15
2026-01-28 20:42:16,090 | INFO | total number of ended hypotheses: 67
2026-01-28 20:42:16,095 | INFO | best hypo: euh<space>d'autre<space>part<space>le<space>comportement<space>des<space>élèves<space>semble<space>au<space>fil<space>des<space>années<space>se<space>dégrader<space>également<space>donc<space>beaucoup<space>moins<space>concentré<space>euh<space>mon<space>respect<space>pour<space>le<space>professeur<space>ce<space>qui<space>n'était<space>pas<space>le<space>cas<space>y<space>a<space>plusieurs<space>années<space>euh<space>le<space>nombre<space>de<space>postes<space>diminue<space>également<space>ce<space>qui<space>est<space>regrettable<space>euh<space>d'autant<space>plus<space>que<space>j'ai<space>j'aimerais<space>enseigner<space>donc<space>euh<space>en<space>corse<space>l'endroit<space>où<space>je<space>suis<space>né<space>euh<space>les<space>les<space>postes<space>donc<space>en<space>moins<space>nombreux

2026-01-28 20:42:16,097 | INFO | speech length: 194400
2026-01-28 20:42:16,142 | INFO | decoder input length: 303
2026-01-28 20:42:16,142 | INFO | max output length: 303
2026-01-28 20:42:16,142 | INFO | min output length: 30
2026-01-28 20:42:31,898 | INFO | end detected at 288
2026-01-28 20:42:31,900 | INFO | -35.24 * 0.5 = -17.62 for decoder
2026-01-28 20:42:31,900 | INFO | -42.64 * 0.5 = -21.32 for ctc
2026-01-28 20:42:31,900 | INFO | total log probability: -38.94
2026-01-28 20:42:31,901 | INFO | normalized log probability: -0.14
2026-01-28 20:42:31,901 | INFO | total number of ended hypotheses: 199
2026-01-28 20:42:31,904 | INFO | best hypo: et<space>euh<space>c'est<space>vrai<space>qu'on<space>est<space>souvent<space>et<space>l'année<space>de<space>son<space>domicile<space>puisque<space>ma<space>professeur<space>d'italien<space>par<space>exemple<space>l'année<space>dernière<space>euh<space>mettait<space>une<space>heure<space>et<space>demie<space>avant<space>l'arrivée<space>donc<space>à<space>son<space>lycée<space>et<space>ma<space>mère<space>qui<space>est<space>également<space>le<space>prof<space>est<space>à<space>trois<space>quart<space>d'heure<space>de<space>de<space>son<space>idée<space>également

2026-01-28 20:42:31,919 | INFO | Chunk: 0 | WER=34.782609 | S=18 D=0 I=14
2026-01-28 20:42:31,924 | INFO | Chunk: 1 | WER=17.283951 | S=7 D=0 I=7
2026-01-28 20:42:31,927 | INFO | Chunk: 2 | WER=17.391304 | S=7 D=1 I=4
2026-01-28 20:42:31,928 | INFO | Chunk: 3 | WER=31.578947 | S=2 D=0 I=4
2026-01-28 20:42:31,929 | INFO | Chunk: 4 | WER=9.523810 | S=1 D=0 I=3
2026-01-28 20:42:31,929 | INFO | Chunk: 5 | WER=0.000000 | S=1 D=0 I=0
2026-01-28 20:42:31,930 | INFO | Chunk: 6 | WER=14.285714 | S=2 D=2 I=0
2026-01-28 20:42:31,930 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:42:31,934 | INFO | Chunk: 8 | WER=26.027397 | S=8 D=0 I=11
2026-01-28 20:42:31,935 | INFO | Chunk: 9 | WER=25.000000 | S=4 D=2 I=0
2026-01-28 20:42:31,937 | INFO | Chunk: 10 | WER=13.461538 | S=3 D=0 I=4
2026-01-28 20:42:31,938 | INFO | Chunk: 11 | WER=14.814815 | S=2 D=0 I=2
2026-01-28 20:42:31,941 | INFO | Chunk: 12 | WER=14.084507 | S=2 D=2 I=6
2026-01-28 20:42:31,943 | INFO | Chunk: 13 | WER=2.631579 | S=0 D=0 I=1
2026-01-28 20:42:31,946 | INFO | Chunk: 14 | WER=19.178082 | S=7 D=2 I=5
2026-01-28 20:42:31,948 | INFO | Chunk: 15 | WER=20.754717 | S=5 D=1 I=5
2026-01-28 20:42:32,276 | INFO | File: Rhap-M1003.wav | WER=19.410977 | S=68 D=10 I=67
2026-01-28 20:42:32,277 | INFO | ------------------------------
2026-01-28 20:42:32,277 | INFO | Conf ester Done!
2026-01-28 20:45:46,224 | INFO | Chunk: 0 | WER=30.434783 | S=17 D=8 I=3
2026-01-28 20:45:46,230 | INFO | Chunk: 1 | WER=19.753086 | S=6 D=9 I=1
2026-01-28 20:45:46,234 | INFO | Chunk: 2 | WER=31.884058 | S=16 D=4 I=2
2026-01-28 20:45:46,235 | INFO | Chunk: 3 | WER=26.315789 | S=4 D=0 I=1
2026-01-28 20:45:46,237 | INFO | Chunk: 4 | WER=16.666667 | S=5 D=1 I=1
2026-01-28 20:45:46,237 | INFO | Chunk: 5 | WER=0.000000 | S=1 D=0 I=0
2026-01-28 20:45:46,238 | INFO | Chunk: 6 | WER=25.000000 | S=4 D=3 I=0
2026-01-28 20:45:46,238 | INFO | Chunk: 7 | WER=80.000000 | S=3 D=1 I=0
2026-01-28 20:45:46,243 | INFO | Chunk: 8 | WER=27.397260 | S=13 D=6 I=1
2026-01-28 20:45:46,244 | INFO | Chunk: 9 | WER=25.000000 | S=1 D=5 I=0
2026-01-28 20:45:46,247 | INFO | Chunk: 10 | WER=28.846154 | S=6 D=4 I=5
2026-01-28 20:45:46,248 | INFO | Chunk: 11 | WER=14.814815 | S=4 D=0 I=0
2026-01-28 20:45:46,252 | INFO | Chunk: 12 | WER=19.718310 | S=6 D=8 I=0
2026-01-28 20:45:46,254 | INFO | Chunk: 13 | WER=13.157895 | S=3 D=0 I=2
2026-01-28 20:45:46,258 | INFO | Chunk: 14 | WER=42.465753 | S=15 D=14 I=2
2026-01-28 20:45:46,261 | INFO | Chunk: 15 | WER=22.641509 | S=7 D=4 I=1
2026-01-28 20:45:46,657 | INFO | File: Rhap-M1003.wav | WER=26.372155 | S=110 D=67 I=20
2026-01-28 20:45:46,657 | INFO | ------------------------------
2026-01-28 20:45:46,657 | INFO | hmm_tdnn Done!
2026-01-28 20:45:46,930 | INFO | ==================================Rhap-M2001.wav=========================================
2026-01-28 20:45:47,206 | INFO | Using rVAD model
2026-01-28 20:46:02,145 | INFO | Chunk: 0 | WER=33.333333 | S=1 D=0 I=0
2026-01-28 20:46:02,146 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:02,146 | INFO | Chunk: 2 | WER=33.333333 | S=2 D=2 I=0
2026-01-28 20:46:02,146 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:02,146 | INFO | Chunk: 4 | WER=37.500000 | S=2 D=1 I=0
2026-01-28 20:46:02,147 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:02,148 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:02,148 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:02,148 | INFO | Chunk: 8 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:02,148 | INFO | Chunk: 9 | WER=14.285714 | S=1 D=0 I=0
2026-01-28 20:46:02,149 | INFO | Chunk: 10 | WER=16.666667 | S=1 D=0 I=0
2026-01-28 20:46:02,149 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:02,149 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:02,149 | INFO | Chunk: 13 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:02,150 | INFO | Chunk: 14 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:02,151 | INFO | Chunk: 15 | WER=5.000000 | S=0 D=0 I=1
2026-01-28 20:46:02,151 | INFO | Chunk: 16 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:02,151 | INFO | Chunk: 17 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:02,152 | INFO | Chunk: 18 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:02,152 | INFO | Chunk: 19 | WER=57.142857 | S=2 D=2 I=0
2026-01-28 20:46:02,152 | INFO | Chunk: 20 | WER=100.000000 | S=1 D=1 I=0
2026-01-28 20:46:02,152 | INFO | Chunk: 21 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:02,153 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:02,153 | INFO | Chunk: 23 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:02,153 | INFO | Chunk: 24 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:02,154 | INFO | Chunk: 25 | WER=5.263158 | S=1 D=0 I=0
2026-01-28 20:46:02,155 | INFO | Chunk: 26 | WER=4.761905 | S=0 D=0 I=1
2026-01-28 20:46:02,155 | INFO | Chunk: 27 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:02,155 | INFO | Chunk: 28 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:02,155 | INFO | Chunk: 29 | WER=16.666667 | S=1 D=0 I=0
2026-01-28 20:46:02,155 | INFO | Chunk: 30 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:02,156 | INFO | Chunk: 31 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:02,156 | INFO | Chunk: 32 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:02,157 | INFO | Chunk: 33 | WER=7.142857 | S=1 D=0 I=0
2026-01-28 20:46:02,157 | INFO | Chunk: 34 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:02,157 | INFO | Chunk: 35 | WER=33.333333 | S=0 D=0 I=1
2026-01-28 20:46:02,158 | INFO | Chunk: 36 | WER=4.000000 | S=0 D=1 I=0
2026-01-28 20:46:02,158 | INFO | Chunk: 37 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:02,159 | INFO | Chunk: 38 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:02,159 | INFO | Chunk: 39 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:02,159 | INFO | Chunk: 40 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:02,160 | INFO | Chunk: 41 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:02,160 | INFO | Chunk: 42 | WER=5.000000 | S=1 D=0 I=0
2026-01-28 20:46:02,160 | INFO | Chunk: 43 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:02,161 | INFO | Chunk: 44 | WER=20.000000 | S=0 D=1 I=0
2026-01-28 20:46:02,161 | INFO | Chunk: 45 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:02,162 | INFO | Chunk: 46 | WER=6.666667 | S=1 D=0 I=0
2026-01-28 20:46:02,162 | INFO | Chunk: 47 | WER=12.500000 | S=1 D=0 I=0
2026-01-28 20:46:02,312 | INFO | File: Rhap-M2001.wav | WER=5.056180 | S=16 D=8 I=3
2026-01-28 20:46:02,312 | INFO | ------------------------------
2026-01-28 20:46:02,312 | INFO | w2vec vad chunk Done!
2026-01-28 20:46:39,071 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:39,071 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:39,072 | INFO | Chunk: 2 | WER=33.333333 | S=2 D=1 I=1
2026-01-28 20:46:39,072 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:39,072 | INFO | Chunk: 4 | WER=25.000000 | S=2 D=0 I=0
2026-01-28 20:46:39,073 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:39,073 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:39,074 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:39,074 | INFO | Chunk: 8 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:39,074 | INFO | Chunk: 9 | WER=14.285714 | S=1 D=0 I=0
2026-01-28 20:46:39,075 | INFO | Chunk: 10 | WER=16.666667 | S=1 D=0 I=0
2026-01-28 20:46:39,075 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:39,075 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:39,075 | INFO | Chunk: 13 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:39,076 | INFO | Chunk: 14 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:39,076 | INFO | Chunk: 15 | WER=5.000000 | S=0 D=0 I=1
2026-01-28 20:46:39,077 | INFO | Chunk: 16 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:39,077 | INFO | Chunk: 17 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:39,077 | INFO | Chunk: 18 | WER=5.882353 | S=1 D=0 I=0
2026-01-28 20:46:39,078 | INFO | Chunk: 19 | WER=28.571429 | S=1 D=1 I=0
2026-01-28 20:46:39,078 | INFO | Chunk: 20 | WER=100.000000 | S=1 D=1 I=0
2026-01-28 20:46:39,078 | INFO | Chunk: 21 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:39,078 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:39,079 | INFO | Chunk: 23 | WER=7.692308 | S=1 D=0 I=0
2026-01-28 20:46:39,079 | INFO | Chunk: 24 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:39,080 | INFO | Chunk: 25 | WER=5.263158 | S=1 D=0 I=0
2026-01-28 20:46:39,080 | INFO | Chunk: 26 | WER=4.761905 | S=0 D=0 I=1
2026-01-28 20:46:39,080 | INFO | Chunk: 27 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:39,081 | INFO | Chunk: 28 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:39,081 | INFO | Chunk: 29 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:39,081 | INFO | Chunk: 30 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:39,082 | INFO | Chunk: 31 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:39,082 | INFO | Chunk: 32 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:39,082 | INFO | Chunk: 33 | WER=7.142857 | S=1 D=0 I=0
2026-01-28 20:46:39,083 | INFO | Chunk: 34 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:39,083 | INFO | Chunk: 35 | WER=33.333333 | S=0 D=0 I=1
2026-01-28 20:46:39,083 | INFO | Chunk: 36 | WER=4.000000 | S=0 D=1 I=0
2026-01-28 20:46:39,084 | INFO | Chunk: 37 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:39,084 | INFO | Chunk: 38 | WER=4.545455 | S=0 D=1 I=0
2026-01-28 20:46:39,085 | INFO | Chunk: 39 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:39,085 | INFO | Chunk: 40 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:39,085 | INFO | Chunk: 41 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:39,086 | INFO | Chunk: 42 | WER=5.000000 | S=1 D=0 I=0
2026-01-28 20:46:39,086 | INFO | Chunk: 43 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:39,086 | INFO | Chunk: 44 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:46:39,086 | INFO | Chunk: 45 | WER=6.666667 | S=0 D=0 I=1
2026-01-28 20:46:39,087 | INFO | Chunk: 46 | WER=6.666667 | S=1 D=0 I=0
2026-01-28 20:46:39,087 | INFO | Chunk: 47 | WER=50.000000 | S=4 D=0 I=0
2026-01-28 20:46:39,239 | INFO | File: Rhap-M2001.wav | WER=5.243446 | S=18 D=5 I=5
2026-01-28 20:46:39,239 | INFO | ------------------------------
2026-01-28 20:46:39,239 | INFO | whisper med Done!
2026-01-28 20:47:27,322 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:47:27,323 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:47:27,323 | INFO | Chunk: 2 | WER=33.333333 | S=2 D=2 I=0
2026-01-28 20:47:27,323 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:47:27,323 | INFO | Chunk: 4 | WER=12.500000 | S=1 D=0 I=0
2026-01-28 20:47:27,324 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:47:27,324 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:47:27,325 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:47:27,325 | INFO | Chunk: 8 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:47:27,325 | INFO | Chunk: 9 | WER=14.285714 | S=1 D=0 I=0
2026-01-28 20:47:27,325 | INFO | Chunk: 10 | WER=16.666667 | S=1 D=0 I=0
2026-01-28 20:47:27,326 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:47:27,326 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:47:27,326 | INFO | Chunk: 13 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:47:27,327 | INFO | Chunk: 14 | WER=4.166667 | S=1 D=0 I=0
2026-01-28 20:47:27,327 | INFO | Chunk: 15 | WER=5.000000 | S=0 D=0 I=1
2026-01-28 20:47:27,328 | INFO | Chunk: 16 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:47:27,328 | INFO | Chunk: 17 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:47:27,328 | INFO | Chunk: 18 | WER=5.882353 | S=1 D=0 I=0
2026-01-28 20:47:27,329 | INFO | Chunk: 19 | WER=28.571429 | S=1 D=1 I=0
2026-01-28 20:47:27,329 | INFO | Chunk: 20 | WER=100.000000 | S=1 D=1 I=0
2026-01-28 20:47:27,329 | INFO | Chunk: 21 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:47:27,329 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:47:27,330 | INFO | Chunk: 23 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:47:27,330 | INFO | Chunk: 24 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:47:27,331 | INFO | Chunk: 25 | WER=5.263158 | S=1 D=0 I=0
2026-01-28 20:47:27,331 | INFO | Chunk: 26 | WER=4.761905 | S=0 D=0 I=1
2026-01-28 20:47:27,331 | INFO | Chunk: 27 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:47:27,332 | INFO | Chunk: 28 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:47:27,332 | INFO | Chunk: 29 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:47:27,332 | INFO | Chunk: 30 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:47:27,333 | INFO | Chunk: 31 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:47:27,333 | INFO | Chunk: 32 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:47:27,333 | INFO | Chunk: 33 | WER=7.142857 | S=1 D=0 I=0
2026-01-28 20:47:27,334 | INFO | Chunk: 34 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:47:27,334 | INFO | Chunk: 35 | WER=33.333333 | S=0 D=0 I=1
2026-01-28 20:47:27,334 | INFO | Chunk: 36 | WER=4.000000 | S=0 D=1 I=0
2026-01-28 20:47:27,335 | INFO | Chunk: 37 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:47:27,335 | INFO | Chunk: 38 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:47:27,336 | INFO | Chunk: 39 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:47:27,336 | INFO | Chunk: 40 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:47:27,336 | INFO | Chunk: 41 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:47:27,337 | INFO | Chunk: 42 | WER=5.000000 | S=1 D=0 I=0
2026-01-28 20:47:27,337 | INFO | Chunk: 43 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:47:27,337 | INFO | Chunk: 44 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:47:27,337 | INFO | Chunk: 45 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:47:27,338 | INFO | Chunk: 46 | WER=6.666667 | S=1 D=0 I=0
2026-01-28 20:47:27,338 | INFO | Chunk: 47 | WER=25.000000 | S=2 D=0 I=0
2026-01-28 20:47:27,490 | INFO | File: Rhap-M2001.wav | WER=4.307116 | S=15 D=5 I=3
2026-01-28 20:47:27,490 | INFO | ------------------------------
2026-01-28 20:47:27,490 | INFO | whisper large Done!
2026-01-28 20:47:27,664 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 20:47:27,703 | INFO | Vocabulary size: 350
2026-01-28 20:47:28,668 | INFO | Gradient checkpoint layers: []
2026-01-28 20:47:29,479 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 20:47:29,483 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 20:47:29,483 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 20:47:29,484 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 20:47:29,484 | INFO | speech length: 14880
2026-01-28 20:47:29,522 | INFO | decoder input length: 22
2026-01-28 20:47:29,522 | INFO | max output length: 22
2026-01-28 20:47:29,522 | INFO | min output length: 2
2026-01-28 20:47:30,243 | INFO | end detected at 19
2026-01-28 20:47:30,245 | INFO |  -1.10 * 0.5 =  -0.55 for decoder
2026-01-28 20:47:30,245 | INFO |  -1.39 * 0.5 =  -0.69 for ctc
2026-01-28 20:47:30,245 | INFO | total log probability: -1.24
2026-01-28 20:47:30,245 | INFO | normalized log probability: -0.09
2026-01-28 20:47:30,245 | INFO | total number of ended hypotheses: 159
2026-01-28 20:47:30,245 | INFO | best hypo: ▁mesdames▁et▁messieurs

2026-01-28 20:47:30,248 | INFO | speech length: 53600
2026-01-28 20:47:30,283 | INFO | decoder input length: 83
2026-01-28 20:47:30,283 | INFO | max output length: 83
2026-01-28 20:47:30,283 | INFO | min output length: 8
2026-01-28 20:47:31,416 | INFO | end detected at 25
2026-01-28 20:47:31,417 | INFO |  -1.37 * 0.5 =  -0.68 for decoder
2026-01-28 20:47:31,417 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 20:47:31,417 | INFO | total log probability: -0.69
2026-01-28 20:47:31,417 | INFO | normalized log probability: -0.03
2026-01-28 20:47:31,417 | INFO | total number of ended hypotheses: 141
2026-01-28 20:47:31,418 | INFO | best hypo: ▁je▁suis▁heureux▁de▁me▁retrouver▁ce▁soir▁parmi▁vous

2026-01-28 20:47:31,419 | INFO | speech length: 53280
2026-01-28 20:47:31,454 | INFO | decoder input length: 82
2026-01-28 20:47:31,454 | INFO | max output length: 82
2026-01-28 20:47:31,454 | INFO | min output length: 8
2026-01-28 20:47:32,881 | INFO | end detected at 32
2026-01-28 20:47:32,883 | INFO |  -3.07 * 0.5 =  -1.53 for decoder
2026-01-28 20:47:32,883 | INFO |  -7.42 * 0.5 =  -3.71 for ctc
2026-01-28 20:47:32,883 | INFO | total log probability: -5.24
2026-01-28 20:47:32,883 | INFO | normalized log probability: -0.20
2026-01-28 20:47:32,883 | INFO | total number of ended hypotheses: 176
2026-01-28 20:47:32,883 | INFO | best hypo: ▁après▁ma▁visite▁landivisiau▁et▁à▁l'île▁longue▁ce▁matin

2026-01-28 20:47:32,885 | INFO | speech length: 37440
2026-01-28 20:47:32,920 | INFO | decoder input length: 58
2026-01-28 20:47:32,920 | INFO | max output length: 58
2026-01-28 20:47:32,920 | INFO | min output length: 5
2026-01-28 20:47:33,696 | INFO | end detected at 18
2026-01-28 20:47:33,697 | INFO |  -0.98 * 0.5 =  -0.49 for decoder
2026-01-28 20:47:33,697 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 20:47:33,697 | INFO | total log probability: -0.49
2026-01-28 20:47:33,697 | INFO | normalized log probability: -0.04
2026-01-28 20:47:33,697 | INFO | total number of ended hypotheses: 146
2026-01-28 20:47:33,697 | INFO | best hypo: ▁c'est▁donc▁une▁journée▁entière

2026-01-28 20:47:33,699 | INFO | speech length: 30400
2026-01-28 20:47:33,733 | INFO | decoder input length: 47
2026-01-28 20:47:33,733 | INFO | max output length: 47
2026-01-28 20:47:33,733 | INFO | min output length: 4
2026-01-28 20:47:34,751 | INFO | end detected at 25
2026-01-28 20:47:34,752 | INFO |  -2.21 * 0.5 =  -1.11 for decoder
2026-01-28 20:47:34,752 | INFO |  -0.31 * 0.5 =  -0.16 for ctc
2026-01-28 20:47:34,752 | INFO | total log probability: -1.26
2026-01-28 20:47:34,752 | INFO | normalized log probability: -0.06
2026-01-28 20:47:34,752 | INFO | total number of ended hypotheses: 145
2026-01-28 20:47:34,753 | INFO | best hypo: ▁que▁j'aurais▁passé▁avec▁nos▁forces▁armées

2026-01-28 20:47:34,755 | INFO | speech length: 88640
2026-01-28 20:47:34,805 | INFO | decoder input length: 138
2026-01-28 20:47:34,805 | INFO | max output length: 138
2026-01-28 20:47:34,805 | INFO | min output length: 13
2026-01-28 20:47:38,103 | INFO | end detected at 56
2026-01-28 20:47:38,105 | INFO |  -6.06 * 0.5 =  -3.03 for decoder
2026-01-28 20:47:38,105 | INFO |  -3.37 * 0.5 =  -1.68 for ctc
2026-01-28 20:47:38,105 | INFO | total log probability: -4.71
2026-01-28 20:47:38,105 | INFO | normalized log probability: -0.09
2026-01-28 20:47:38,105 | INFO | total number of ended hypotheses: 172
2026-01-28 20:47:38,106 | INFO | best hypo: ▁c'est▁bien▁sûr▁dans▁mon▁esprit▁la▁marque▁du▁lien▁direct▁qui▁une▁île▁président▁de▁la▁république▁chef▁des▁armées

2026-01-28 20:47:38,109 | INFO | speech length: 86880
2026-01-28 20:47:38,160 | INFO | decoder input length: 135
2026-01-28 20:47:38,160 | INFO | max output length: 135
2026-01-28 20:47:38,160 | INFO | min output length: 13
2026-01-28 20:47:40,888 | INFO | end detected at 46
2026-01-28 20:47:40,891 | INFO |  -3.55 * 0.5 =  -1.78 for decoder
2026-01-28 20:47:40,891 | INFO |  -2.07 * 0.5 =  -1.04 for ctc
2026-01-28 20:47:40,891 | INFO | total log probability: -2.81
2026-01-28 20:47:40,891 | INFO | normalized log probability: -0.07
2026-01-28 20:47:40,891 | INFO | total number of ended hypotheses: 179
2026-01-28 20:47:40,892 | INFO | best hypo: ▁avec▁toute▁celle▁et▁tous▁ceux▁qui▁ont▁la▁difficile▁mission▁de▁veiller▁sur▁nos▁intérêts

2026-01-28 20:47:40,895 | INFO | speech length: 31360
2026-01-28 20:47:40,943 | INFO | decoder input length: 48
2026-01-28 20:47:40,943 | INFO | max output length: 48
2026-01-28 20:47:40,943 | INFO | min output length: 4
2026-01-28 20:47:41,984 | INFO | end detected at 22
2026-01-28 20:47:41,985 | INFO |  -1.26 * 0.5 =  -0.63 for decoder
2026-01-28 20:47:41,985 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 20:47:41,985 | INFO | total log probability: -0.63
2026-01-28 20:47:41,985 | INFO | normalized log probability: -0.03
2026-01-28 20:47:41,985 | INFO | total number of ended hypotheses: 141
2026-01-28 20:47:41,986 | INFO | best hypo: ▁et▁sur▁la▁sécurité▁de▁nos▁concitoyens

2026-01-28 20:47:41,988 | INFO | speech length: 62720
2026-01-28 20:47:42,045 | INFO | decoder input length: 97
2026-01-28 20:47:42,046 | INFO | max output length: 97
2026-01-28 20:47:42,046 | INFO | min output length: 9
2026-01-28 20:47:44,071 | INFO | end detected at 38
2026-01-28 20:47:44,072 | INFO |  -2.41 * 0.5 =  -1.21 for decoder
2026-01-28 20:47:44,072 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-28 20:47:44,072 | INFO | total log probability: -1.21
2026-01-28 20:47:44,072 | INFO | normalized log probability: -0.04
2026-01-28 20:47:44,072 | INFO | total number of ended hypotheses: 149
2026-01-28 20:47:44,073 | INFO | best hypo: ▁c'est▁pour▁cela▁que▁je▁tenais▁à▁vous▁rencontrer▁la▁veille▁de▁notre▁fête▁nationale

2026-01-28 20:47:44,075 | INFO | speech length: 50080
2026-01-28 20:47:44,136 | INFO | decoder input length: 77
2026-01-28 20:47:44,136 | INFO | max output length: 77
2026-01-28 20:47:44,137 | INFO | min output length: 7
2026-01-28 20:47:46,342 | INFO | end detected at 28
2026-01-28 20:47:46,344 | INFO |  -1.87 * 0.5 =  -0.93 for decoder
2026-01-28 20:47:46,344 | INFO |  -0.51 * 0.5 =  -0.25 for ctc
2026-01-28 20:47:46,344 | INFO | total log probability: -1.19
2026-01-28 20:47:46,344 | INFO | normalized log probability: -0.05
2026-01-28 20:47:46,344 | INFO | total number of ended hypotheses: 150
2026-01-28 20:47:46,345 | INFO | best hypo: ▁le▁quatorze▁juillet▁lors▁du▁traditionnel▁défilé

2026-01-28 20:47:46,348 | INFO | speech length: 25280
2026-01-28 20:47:46,405 | INFO | decoder input length: 39
2026-01-28 20:47:46,406 | INFO | max output length: 39
2026-01-28 20:47:46,406 | INFO | min output length: 3
2026-01-28 20:47:47,542 | INFO | end detected at 16
2026-01-28 20:47:47,544 | INFO |  -0.83 * 0.5 =  -0.42 for decoder
2026-01-28 20:47:47,544 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-28 20:47:47,544 | INFO | total log probability: -0.42
2026-01-28 20:47:47,544 | INFO | normalized log probability: -0.04
2026-01-28 20:47:47,544 | INFO | total number of ended hypotheses: 140
2026-01-28 20:47:47,545 | INFO | best hypo: ▁c'est▁la▁nation▁tout▁entière

2026-01-28 20:47:47,548 | INFO | speech length: 13600
2026-01-28 20:47:47,602 | INFO | decoder input length: 20
2026-01-28 20:47:47,602 | INFO | max output length: 20
2026-01-28 20:47:47,602 | INFO | min output length: 2
2026-01-28 20:47:48,643 | INFO | end detected at 16
2026-01-28 20:47:48,644 | INFO |  -1.12 * 0.5 =  -0.56 for decoder
2026-01-28 20:47:48,644 | INFO |  -0.98 * 0.5 =  -0.49 for ctc
2026-01-28 20:47:48,645 | INFO | total log probability: -1.05
2026-01-28 20:47:48,645 | INFO | normalized log probability: -0.09
2026-01-28 20:47:48,645 | INFO | total number of ended hypotheses: 141
2026-01-28 20:47:48,645 | INFO | best hypo: ▁qui▁vous▁rend▁hommage

2026-01-28 20:47:48,648 | INFO | speech length: 21920
2026-01-28 20:47:48,701 | INFO | decoder input length: 33
2026-01-28 20:47:48,702 | INFO | max output length: 33
2026-01-28 20:47:48,702 | INFO | min output length: 3
2026-01-28 20:47:49,605 | INFO | end detected at 13
2026-01-28 20:47:49,606 | INFO |  -0.61 * 0.5 =  -0.31 for decoder
2026-01-28 20:47:49,606 | INFO |  -0.05 * 0.5 =  -0.02 for ctc
2026-01-28 20:47:49,606 | INFO | total log probability: -0.33
2026-01-28 20:47:49,607 | INFO | normalized log probability: -0.04
2026-01-28 20:47:49,607 | INFO | total number of ended hypotheses: 140
2026-01-28 20:47:49,607 | INFO | best hypo: ▁elle▁salue▁le▁courage

2026-01-28 20:47:49,610 | INFO | speech length: 48480
2026-01-28 20:47:49,655 | INFO | decoder input length: 75
2026-01-28 20:47:49,655 | INFO | max output length: 75
2026-01-28 20:47:49,655 | INFO | min output length: 7
2026-01-28 20:47:50,956 | INFO | end detected at 25
2026-01-28 20:47:50,957 | INFO |  -1.42 * 0.5 =  -0.71 for decoder
2026-01-28 20:47:50,957 | INFO |  -0.10 * 0.5 =  -0.05 for ctc
2026-01-28 20:47:50,957 | INFO | total log probability: -0.76
2026-01-28 20:47:50,957 | INFO | normalized log probability: -0.04
2026-01-28 20:47:50,957 | INFO | total number of ended hypotheses: 140
2026-01-28 20:47:50,957 | INFO | best hypo: ▁elle▁salue▁la▁loyauté▁elle▁salue▁le▁dévouement

2026-01-28 20:47:50,960 | INFO | speech length: 147840
2026-01-28 20:47:51,005 | INFO | decoder input length: 230
2026-01-28 20:47:51,005 | INFO | max output length: 230
2026-01-28 20:47:51,005 | INFO | min output length: 23
2026-01-28 20:47:55,788 | INFO | end detected at 63
2026-01-28 20:47:55,789 | INFO |  -4.72 * 0.5 =  -2.36 for decoder
2026-01-28 20:47:55,789 | INFO |  -0.38 * 0.5 =  -0.19 for ctc
2026-01-28 20:47:55,789 | INFO | total log probability: -2.55
2026-01-28 20:47:55,789 | INFO | normalized log probability: -0.04
2026-01-28 20:47:55,789 | INFO | total number of ended hypotheses: 165
2026-01-28 20:47:55,790 | INFO | best hypo: ▁elle▁salue▁l'esprit▁de▁sacrifice▁de▁ceux▁qui▁ont▁choisi▁de▁servir▁sa▁défense▁ses▁intérêts▁et▁ses▁valeurs▁à▁travers▁le▁monde

2026-01-28 20:47:55,793 | INFO | speech length: 95200
2026-01-28 20:47:55,839 | INFO | decoder input length: 148
2026-01-28 20:47:55,839 | INFO | max output length: 148
2026-01-28 20:47:55,839 | INFO | min output length: 14
2026-01-28 20:47:58,480 | INFO | end detected at 50
2026-01-28 20:47:58,483 | INFO |  -5.53 * 0.5 =  -2.76 for decoder
2026-01-28 20:47:58,483 | INFO |  -3.11 * 0.5 =  -1.56 for ctc
2026-01-28 20:47:58,483 | INFO | total log probability: -4.32
2026-01-28 20:47:58,483 | INFO | normalized log probability: -0.10
2026-01-28 20:47:58,483 | INFO | total number of ended hypotheses: 172
2026-01-28 20:47:58,484 | INFO | best hypo: ▁et▁c'est▁la▁raison▁pour▁laquelle▁au▁nom▁de▁la▁république▁j'honorerai▁certaines▁d'entre▁vous▁dans▁quelques▁uns

2026-01-28 20:47:58,487 | INFO | speech length: 69920
2026-01-28 20:47:58,545 | INFO | decoder input length: 108
2026-01-28 20:47:58,545 | INFO | max output length: 108
2026-01-28 20:47:58,545 | INFO | min output length: 10
2026-01-28 20:48:00,929 | INFO | end detected at 43
2026-01-28 20:48:00,931 | INFO |  -5.10 * 0.5 =  -2.55 for decoder
2026-01-28 20:48:00,931 | INFO |  -0.40 * 0.5 =  -0.20 for ctc
2026-01-28 20:48:00,931 | INFO | total log probability: -2.75
2026-01-28 20:48:00,931 | INFO | normalized log probability: -0.07
2026-01-28 20:48:00,932 | INFO | total number of ended hypotheses: 150
2026-01-28 20:48:00,932 | INFO | best hypo: ▁cette▁année▁j'ai▁tenue▁à▁ce▁que▁cette▁fête▁nationale▁soit▁aussi▁celle▁de▁l'europe

2026-01-28 20:48:00,935 | INFO | speech length: 12160
2026-01-28 20:48:00,975 | INFO | decoder input length: 18
2026-01-28 20:48:00,975 | INFO | max output length: 18
2026-01-28 20:48:00,975 | INFO | min output length: 1
2026-01-28 20:48:01,435 | INFO | end detected at 10
2026-01-28 20:48:01,438 | INFO |  -0.50 * 0.5 =  -0.25 for decoder
2026-01-28 20:48:01,438 | INFO |  -2.31 * 0.5 =  -1.16 for ctc
2026-01-28 20:48:01,438 | INFO | total log probability: -1.40
2026-01-28 20:48:01,438 | INFO | normalized log probability: -0.28
2026-01-28 20:48:01,438 | INFO | total number of ended hypotheses: 154
2026-01-28 20:48:01,438 | INFO | best hypo: ▁demain

2026-01-28 20:48:01,440 | INFO | speech length: 76000
2026-01-28 20:48:01,486 | INFO | decoder input length: 118
2026-01-28 20:48:01,486 | INFO | max output length: 118
2026-01-28 20:48:01,486 | INFO | min output length: 11
2026-01-28 20:48:04,148 | INFO | end detected at 48
2026-01-28 20:48:04,149 | INFO |  -3.08 * 0.5 =  -1.54 for decoder
2026-01-28 20:48:04,149 | INFO |  -0.56 * 0.5 =  -0.28 for ctc
2026-01-28 20:48:04,149 | INFO | total log probability: -1.82
2026-01-28 20:48:04,149 | INFO | normalized log probability: -0.04
2026-01-28 20:48:04,149 | INFO | total number of ended hypotheses: 150
2026-01-28 20:48:04,150 | INFO | best hypo: ▁ce▁sont▁les▁vingt▁six▁drapeaux▁de▁nos▁partenaires▁européens▁qui▁défileront▁aux▁côtés▁de▁nos▁armées

2026-01-28 20:48:04,152 | INFO | speech length: 29120
2026-01-28 20:48:04,198 | INFO | decoder input length: 45
2026-01-28 20:48:04,198 | INFO | max output length: 45
2026-01-28 20:48:04,198 | INFO | min output length: 4
2026-01-28 20:48:05,254 | INFO | end detected at 25
2026-01-28 20:48:05,255 | INFO |  -2.69 * 0.5 =  -1.35 for decoder
2026-01-28 20:48:05,255 | INFO |  -5.62 * 0.5 =  -2.81 for ctc
2026-01-28 20:48:05,255 | INFO | total log probability: -4.16
2026-01-28 20:48:05,255 | INFO | normalized log probability: -0.20
2026-01-28 20:48:05,255 | INFO | total number of ended hypotheses: 161
2026-01-28 20:48:05,256 | INFO | best hypo: ▁je▁remercie▁d'ailleurs▁chez▁hervé

2026-01-28 20:48:05,258 | INFO | speech length: 9600
2026-01-28 20:48:05,289 | INFO | decoder input length: 14
2026-01-28 20:48:05,290 | INFO | max output length: 14
2026-01-28 20:48:05,290 | INFO | min output length: 1
2026-01-28 20:48:05,819 | INFO | end detected at 12
2026-01-28 20:48:05,821 | INFO |  -0.60 * 0.5 =  -0.30 for decoder
2026-01-28 20:48:05,821 | INFO |  -1.86 * 0.5 =  -0.93 for ctc
2026-01-28 20:48:05,821 | INFO | total log probability: -1.23
2026-01-28 20:48:05,821 | INFO | normalized log probability: -0.18
2026-01-28 20:48:05,821 | INFO | total number of ended hypotheses: 164
2026-01-28 20:48:05,821 | INFO | best hypo: ▁cher▁alain

2026-01-28 20:48:05,824 | INFO | speech length: 65920
2026-01-28 20:48:05,859 | INFO | decoder input length: 102
2026-01-28 20:48:05,859 | INFO | max output length: 102
2026-01-28 20:48:05,860 | INFO | min output length: 10
2026-01-28 20:48:07,738 | INFO | end detected at 35
2026-01-28 20:48:07,740 | INFO |  -2.27 * 0.5 =  -1.13 for decoder
2026-01-28 20:48:07,741 | INFO |  -0.85 * 0.5 =  -0.42 for ctc
2026-01-28 20:48:07,741 | INFO | total log probability: -1.56
2026-01-28 20:48:07,741 | INFO | normalized log probability: -0.05
2026-01-28 20:48:07,741 | INFO | total number of ended hypotheses: 162
2026-01-28 20:48:07,741 | INFO | best hypo: ▁tout▁spécialement▁les▁ministres▁de▁la▁défense▁qui▁sont▁parmi▁nous▁ce▁soir

2026-01-28 20:48:07,744 | INFO | speech length: 38560
2026-01-28 20:48:07,797 | INFO | decoder input length: 59
2026-01-28 20:48:07,797 | INFO | max output length: 59
2026-01-28 20:48:07,797 | INFO | min output length: 5
2026-01-28 20:48:09,513 | INFO | end detected at 23
2026-01-28 20:48:09,514 | INFO |  -1.30 * 0.5 =  -0.65 for decoder
2026-01-28 20:48:09,514 | INFO |  -0.06 * 0.5 =  -0.03 for ctc
2026-01-28 20:48:09,514 | INFO | total log probability: -0.68
2026-01-28 20:48:09,514 | INFO | normalized log probability: -0.04
2026-01-28 20:48:09,514 | INFO | total number of ended hypotheses: 146
2026-01-28 20:48:09,515 | INFO | best hypo: ▁je▁leur▁adresse▁un▁salut▁très▁amical

2026-01-28 20:48:09,518 | INFO | speech length: 40960
2026-01-28 20:48:09,576 | INFO | decoder input length: 63
2026-01-28 20:48:09,576 | INFO | max output length: 63
2026-01-28 20:48:09,576 | INFO | min output length: 6
2026-01-28 20:48:12,004 | INFO | end detected at 31
2026-01-28 20:48:12,005 | INFO |  -1.87 * 0.5 =  -0.94 for decoder
2026-01-28 20:48:12,005 | INFO |  -4.31 * 0.5 =  -2.16 for ctc
2026-01-28 20:48:12,006 | INFO | total log probability: -3.09
2026-01-28 20:48:12,006 | INFO | normalized log probability: -0.12
2026-01-28 20:48:12,006 | INFO | total number of ended hypotheses: 170
2026-01-28 20:48:12,006 | INFO | best hypo: ▁c'est▁l'image▁d'une▁robe▁qui▁repart▁d'un▁même▁pas

2026-01-28 20:48:12,008 | INFO | speech length: 74880
2026-01-28 20:48:12,060 | INFO | decoder input length: 116
2026-01-28 20:48:12,060 | INFO | max output length: 116
2026-01-28 20:48:12,060 | INFO | min output length: 11
2026-01-28 20:48:13,987 | INFO | end detected at 39
2026-01-28 20:48:13,988 | INFO |  -3.31 * 0.5 =  -1.66 for decoder
2026-01-28 20:48:13,988 | INFO |  -8.18 * 0.5 =  -4.09 for ctc
2026-01-28 20:48:13,988 | INFO | total log probability: -5.75
2026-01-28 20:48:13,988 | INFO | normalized log probability: -0.17
2026-01-28 20:48:13,988 | INFO | total number of ended hypotheses: 176
2026-01-28 20:48:13,989 | INFO | best hypo: ▁une▁robe▁résolue▁à▁se▁remettre▁en▁mouvement▁après▁deux▁années▁d'immobilisme

2026-01-28 20:48:13,991 | INFO | speech length: 95840
2026-01-28 20:48:14,032 | INFO | decoder input length: 149
2026-01-28 20:48:14,032 | INFO | max output length: 149
2026-01-28 20:48:14,032 | INFO | min output length: 14
2026-01-28 20:48:16,991 | INFO | end detected at 57
2026-01-28 20:48:16,993 | INFO |  -4.33 * 0.5 =  -2.16 for decoder
2026-01-28 20:48:16,993 | INFO |  -0.58 * 0.5 =  -0.29 for ctc
2026-01-28 20:48:16,993 | INFO | total log probability: -2.45
2026-01-28 20:48:16,993 | INFO | normalized log probability: -0.05
2026-01-28 20:48:16,993 | INFO | total number of ended hypotheses: 162
2026-01-28 20:48:16,994 | INFO | best hypo: ▁une▁robe▁vigilante▁qui▁n'oublie▁pas▁l'impératif▁de▁protection▁dans▁un▁monde▁plus▁instable▁et▁moins▁prévisible

2026-01-28 20:48:16,996 | INFO | speech length: 114560
2026-01-28 20:48:17,040 | INFO | decoder input length: 178
2026-01-28 20:48:17,041 | INFO | max output length: 178
2026-01-28 20:48:17,041 | INFO | min output length: 17
2026-01-28 20:48:20,509 | INFO | end detected at 63
2026-01-28 20:48:20,510 | INFO |  -5.58 * 0.5 =  -2.79 for decoder
2026-01-28 20:48:20,510 | INFO |  -4.77 * 0.5 =  -2.39 for ctc
2026-01-28 20:48:20,510 | INFO | total log probability: -5.18
2026-01-28 20:48:20,511 | INFO | normalized log probability: -0.09
2026-01-28 20:48:20,511 | INFO | total number of ended hypotheses: 158
2026-01-28 20:48:20,511 | INFO | best hypo: ▁une▁europe▁prête▁à▁assumer▁ses▁responsabilités▁et▁au▁sein▁de▁laquelle▁la▁france▁a▁bien▁l'intention▁de▁tenir▁son▁rang

2026-01-28 20:48:20,513 | INFO | speech length: 33920
2026-01-28 20:48:20,556 | INFO | decoder input length: 52
2026-01-28 20:48:20,556 | INFO | max output length: 52
2026-01-28 20:48:20,557 | INFO | min output length: 5
2026-01-28 20:48:21,655 | INFO | end detected at 27
2026-01-28 20:48:21,657 | INFO |  -1.60 * 0.5 =  -0.80 for decoder
2026-01-28 20:48:21,657 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-28 20:48:21,657 | INFO | total log probability: -0.81
2026-01-28 20:48:21,657 | INFO | normalized log probability: -0.04
2026-01-28 20:48:21,657 | INFO | total number of ended hypotheses: 146
2026-01-28 20:48:21,658 | INFO | best hypo: ▁les▁bases▁d'une▁défense▁européenne▁existent

2026-01-28 20:48:21,659 | INFO | speech length: 21120
2026-01-28 20:48:21,714 | INFO | decoder input length: 32
2026-01-28 20:48:21,714 | INFO | max output length: 32
2026-01-28 20:48:21,714 | INFO | min output length: 3
2026-01-28 20:48:22,357 | INFO | end detected at 15
2026-01-28 20:48:22,358 | INFO |  -0.83 * 0.5 =  -0.42 for decoder
2026-01-28 20:48:22,358 | INFO |  -0.06 * 0.5 =  -0.03 for ctc
2026-01-28 20:48:22,358 | INFO | total log probability: -0.45
2026-01-28 20:48:22,358 | INFO | normalized log probability: -0.04
2026-01-28 20:48:22,358 | INFO | total number of ended hypotheses: 140
2026-01-28 20:48:22,358 | INFO | best hypo: ▁il▁faut▁les▁faire▁grandir

2026-01-28 20:48:22,360 | INFO | speech length: 30880
2026-01-28 20:48:22,404 | INFO | decoder input length: 47
2026-01-28 20:48:22,404 | INFO | max output length: 47
2026-01-28 20:48:22,404 | INFO | min output length: 4
2026-01-28 20:48:23,224 | INFO | end detected at 20
2026-01-28 20:48:23,226 | INFO |  -1.66 * 0.5 =  -0.83 for decoder
2026-01-28 20:48:23,226 | INFO |  -1.60 * 0.5 =  -0.80 for ctc
2026-01-28 20:48:23,226 | INFO | total log probability: -1.63
2026-01-28 20:48:23,226 | INFO | normalized log probability: -0.11
2026-01-28 20:48:23,226 | INFO | total number of ended hypotheses: 163
2026-01-28 20:48:23,226 | INFO | best hypo: ▁en▁quittant▁le▁terrain▁des▁maux

2026-01-28 20:48:23,228 | INFO | speech length: 25920
2026-01-28 20:48:23,267 | INFO | decoder input length: 40
2026-01-28 20:48:23,267 | INFO | max output length: 40
2026-01-28 20:48:23,267 | INFO | min output length: 4
2026-01-28 20:48:24,095 | INFO | end detected at 19
2026-01-28 20:48:24,096 | INFO |  -1.05 * 0.5 =  -0.53 for decoder
2026-01-28 20:48:24,096 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 20:48:24,096 | INFO | total log probability: -0.53
2026-01-28 20:48:24,096 | INFO | normalized log probability: -0.04
2026-01-28 20:48:24,096 | INFO | total number of ended hypotheses: 144
2026-01-28 20:48:24,096 | INFO | best hypo: ▁pour▁gagner▁celui▁de▁l'action

2026-01-28 20:48:24,098 | INFO | speech length: 134240
2026-01-28 20:48:24,142 | INFO | decoder input length: 209
2026-01-28 20:48:24,142 | INFO | max output length: 209
2026-01-28 20:48:24,142 | INFO | min output length: 20
2026-01-28 20:48:28,677 | INFO | end detected at 66
2026-01-28 20:48:28,678 | INFO |  -4.35 * 0.5 =  -2.17 for decoder
2026-01-28 20:48:28,678 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-28 20:48:28,678 | INFO | total log probability: -2.18
2026-01-28 20:48:28,678 | INFO | normalized log probability: -0.04
2026-01-28 20:48:28,678 | INFO | total number of ended hypotheses: 143
2026-01-28 20:48:28,679 | INFO | best hypo: ▁demain▁davantage▁qu'aujourd'hui▁je▁souhaite▁que▁l'europe▁soit▁capable▁d'assurer▁sa▁sécurité▁de▁façon▁autonome

2026-01-28 20:48:28,681 | INFO | speech length: 46080
2026-01-28 20:48:28,728 | INFO | decoder input length: 71
2026-01-28 20:48:28,728 | INFO | max output length: 71
2026-01-28 20:48:28,728 | INFO | min output length: 7
2026-01-28 20:48:30,579 | INFO | end detected at 37
2026-01-28 20:48:30,581 | INFO |  -2.90 * 0.5 =  -1.45 for decoder
2026-01-28 20:48:30,581 | INFO |  -0.51 * 0.5 =  -0.25 for ctc
2026-01-28 20:48:30,581 | INFO | total log probability: -1.70
2026-01-28 20:48:30,581 | INFO | normalized log probability: -0.05
2026-01-28 20:48:30,581 | INFO | total number of ended hypotheses: 163
2026-01-28 20:48:30,581 | INFO | best hypo: ▁alors▁si▁la▁france▁pèse▁aujourd'hui▁sur▁la▁scène▁internationale

2026-01-28 20:48:30,583 | INFO | speech length: 80320
2026-01-28 20:48:30,634 | INFO | decoder input length: 125
2026-01-28 20:48:30,634 | INFO | max output length: 125
2026-01-28 20:48:30,634 | INFO | min output length: 12
2026-01-28 20:48:32,508 | INFO | end detected at 37
2026-01-28 20:48:32,510 | INFO |  -2.35 * 0.5 =  -1.18 for decoder
2026-01-28 20:48:32,510 | INFO |  -0.14 * 0.5 =  -0.07 for ctc
2026-01-28 20:48:32,510 | INFO | total log probability: -1.25
2026-01-28 20:48:32,510 | INFO | normalized log probability: -0.04
2026-01-28 20:48:32,510 | INFO | total number of ended hypotheses: 145
2026-01-28 20:48:32,511 | INFO | best hypo: ▁il▁lui▁faut▁un▁outil▁de▁défense▁adapté▁à▁la▁hauteur▁de▁ses▁ambitions

2026-01-28 20:48:32,513 | INFO | speech length: 55840
2026-01-28 20:48:32,558 | INFO | decoder input length: 86
2026-01-28 20:48:32,559 | INFO | max output length: 86
2026-01-28 20:48:32,559 | INFO | min output length: 8
2026-01-28 20:48:34,019 | INFO | end detected at 32
2026-01-28 20:48:34,020 | INFO |  -2.01 * 0.5 =  -1.00 for decoder
2026-01-28 20:48:34,020 | INFO |  -0.06 * 0.5 =  -0.03 for ctc
2026-01-28 20:48:34,020 | INFO | total log probability: -1.03
2026-01-28 20:48:34,021 | INFO | normalized log probability: -0.04
2026-01-28 20:48:34,021 | INFO | total number of ended hypotheses: 146
2026-01-28 20:48:34,021 | INFO | best hypo: ▁c'est▁aussi▁là▁l'intérêt▁fondamental▁de▁l'europe

2026-01-28 20:48:34,023 | INFO | speech length: 10400
2026-01-28 20:48:34,065 | INFO | decoder input length: 15
2026-01-28 20:48:34,065 | INFO | max output length: 15
2026-01-28 20:48:34,065 | INFO | min output length: 1
2026-01-28 20:48:34,560 | INFO | end detected at 13
2026-01-28 20:48:34,561 | INFO |  -1.28 * 0.5 =  -0.64 for decoder
2026-01-28 20:48:34,561 | INFO |  -1.60 * 0.5 =  -0.80 for ctc
2026-01-28 20:48:34,562 | INFO | total log probability: -1.44
2026-01-28 20:48:34,562 | INFO | normalized log probability: -0.21
2026-01-28 20:48:34,562 | INFO | total number of ended hypotheses: 166
2026-01-28 20:48:34,562 | INFO | best hypo: ▁et▁de▁nos▁pas

2026-01-28 20:48:34,564 | INFO | speech length: 88320
2026-01-28 20:48:34,603 | INFO | decoder input length: 137
2026-01-28 20:48:34,603 | INFO | max output length: 137
2026-01-28 20:48:34,603 | INFO | min output length: 13
2026-01-28 20:48:37,517 | INFO | end detected at 57
2026-01-28 20:48:37,519 | INFO |  -6.78 * 0.5 =  -3.39 for decoder
2026-01-28 20:48:37,519 | INFO |  -1.98 * 0.5 =  -0.99 for ctc
2026-01-28 20:48:37,519 | INFO | total log probability: -4.38
2026-01-28 20:48:37,519 | INFO | normalized log probability: -0.08
2026-01-28 20:48:37,520 | INFO | total number of ended hypotheses: 182
2026-01-28 20:48:37,521 | INFO | best hypo: ▁aujourd'hui▁plus▁jamais▁vous▁ne▁savez▁mieux▁que▁personne▁c'est▁sur▁le▁terrain▁que▁se▁gagne▁où▁se▁perd▁le▁combat

2026-01-28 20:48:37,524 | INFO | speech length: 20320
2026-01-28 20:48:37,611 | INFO | decoder input length: 31
2026-01-28 20:48:37,611 | INFO | max output length: 31
2026-01-28 20:48:37,611 | INFO | min output length: 3
2026-01-28 20:48:38,316 | INFO | end detected at 15
2026-01-28 20:48:38,318 | INFO |  -0.81 * 0.5 =  -0.41 for decoder
2026-01-28 20:48:38,318 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 20:48:38,318 | INFO | total log probability: -0.41
2026-01-28 20:48:38,318 | INFO | normalized log probability: -0.04
2026-01-28 20:48:38,318 | INFO | total number of ended hypotheses: 144
2026-01-28 20:48:38,318 | INFO | best hypo: ▁c'est▁sur▁le▁terrain

2026-01-28 20:48:38,320 | INFO | speech length: 127680
2026-01-28 20:48:38,372 | INFO | decoder input length: 199
2026-01-28 20:48:38,372 | INFO | max output length: 199
2026-01-28 20:48:38,373 | INFO | min output length: 19
2026-01-28 20:48:41,592 | INFO | end detected at 54
2026-01-28 20:48:41,593 | INFO |  -9.22 * 0.5 =  -4.61 for decoder
2026-01-28 20:48:41,593 | INFO |  -0.50 * 0.5 =  -0.25 for ctc
2026-01-28 20:48:41,594 | INFO | total log probability: -4.86
2026-01-28 20:48:41,594 | INFO | normalized log probability: -0.10
2026-01-28 20:48:41,594 | INFO | total number of ended hypotheses: 148
2026-01-28 20:48:41,594 | INFO | best hypo: ▁qu'une▁nation▁affirme▁son▁influence▁qu'elle▁pèse▁dans▁une▁coalition▁au▁travers▁des▁forces▁qu'elle▁est▁capable▁d'engager

2026-01-28 20:48:41,597 | INFO | speech length: 79200
2026-01-28 20:48:41,641 | INFO | decoder input length: 123
2026-01-28 20:48:41,641 | INFO | max output length: 123
2026-01-28 20:48:41,641 | INFO | min output length: 12
2026-01-28 20:48:43,586 | INFO | end detected at 39
2026-01-28 20:48:43,588 | INFO |  -2.53 * 0.5 =  -1.26 for decoder
2026-01-28 20:48:43,588 | INFO |  -0.04 * 0.5 =  -0.02 for ctc
2026-01-28 20:48:43,588 | INFO | total log probability: -1.28
2026-01-28 20:48:43,588 | INFO | normalized log probability: -0.04
2026-01-28 20:48:43,588 | INFO | total number of ended hypotheses: 143
2026-01-28 20:48:43,588 | INFO | best hypo: ▁cet▁engagement▁sur▁le▁terrain▁est▁pourvu▁de▁plus▁en▁plus▁difficile▁et▁de▁plus▁en▁plus▁dangereux

2026-01-28 20:48:43,590 | INFO | speech length: 17280
2026-01-28 20:48:43,633 | INFO | decoder input length: 26
2026-01-28 20:48:43,633 | INFO | max output length: 26
2026-01-28 20:48:43,633 | INFO | min output length: 2
2026-01-28 20:48:44,260 | INFO | end detected at 16
2026-01-28 20:48:44,261 | INFO |  -0.85 * 0.5 =  -0.43 for decoder
2026-01-28 20:48:44,261 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 20:48:44,261 | INFO | total log probability: -0.43
2026-01-28 20:48:44,261 | INFO | normalized log probability: -0.04
2026-01-28 20:48:44,261 | INFO | total number of ended hypotheses: 133
2026-01-28 20:48:44,262 | INFO | best hypo: ▁l'afghanistan

2026-01-28 20:48:44,263 | INFO | speech length: 16000
2026-01-28 20:48:44,307 | INFO | decoder input length: 24
2026-01-28 20:48:44,307 | INFO | max output length: 24
2026-01-28 20:48:44,307 | INFO | min output length: 2
2026-01-28 20:48:44,822 | INFO | end detected at 13
2026-01-28 20:48:44,823 | INFO |  -0.70 * 0.5 =  -0.35 for decoder
2026-01-28 20:48:44,823 | INFO |  -0.41 * 0.5 =  -0.21 for ctc
2026-01-28 20:48:44,823 | INFO | total log probability: -0.55
2026-01-28 20:48:44,823 | INFO | normalized log probability: -0.06
2026-01-28 20:48:44,823 | INFO | total number of ended hypotheses: 149
2026-01-28 20:48:44,823 | INFO | best hypo: ▁le▁proche▁orient

2026-01-28 20:48:44,825 | INFO | speech length: 106240
2026-01-28 20:48:44,867 | INFO | decoder input length: 165
2026-01-28 20:48:44,867 | INFO | max output length: 165
2026-01-28 20:48:44,867 | INFO | min output length: 16
2026-01-28 20:48:47,865 | INFO | end detected at 55
2026-01-28 20:48:47,867 | INFO |  -3.60 * 0.5 =  -1.80 for decoder
2026-01-28 20:48:47,867 | INFO |  -0.66 * 0.5 =  -0.33 for ctc
2026-01-28 20:48:47,867 | INFO | total log probability: -2.13
2026-01-28 20:48:47,867 | INFO | normalized log probability: -0.04
2026-01-28 20:48:47,867 | INFO | total number of ended hypotheses: 166
2026-01-28 20:48:47,868 | INFO | best hypo: ▁je▁connais▁la▁somme▁de▁courage▁et▁d'abnégation▁que▁requiert▁l'accomplissement▁de▁vos▁missions▁dans▁un▁tel▁contexte

2026-01-28 20:48:47,870 | INFO | speech length: 45920
2026-01-28 20:48:47,910 | INFO | decoder input length: 71
2026-01-28 20:48:47,910 | INFO | max output length: 71
2026-01-28 20:48:47,910 | INFO | min output length: 7
2026-01-28 20:48:49,062 | INFO | end detected at 26
2026-01-28 20:48:49,065 | INFO |  -1.51 * 0.5 =  -0.75 for decoder
2026-01-28 20:48:49,065 | INFO |  -0.04 * 0.5 =  -0.02 for ctc
2026-01-28 20:48:49,065 | INFO | total log probability: -0.77
2026-01-28 20:48:49,065 | INFO | normalized log probability: -0.04
2026-01-28 20:48:49,065 | INFO | total number of ended hypotheses: 166
2026-01-28 20:48:49,065 | INFO | best hypo: ▁je▁sais▁également▁ce▁que▁cela▁signifie▁pour▁vos▁familles

2026-01-28 20:48:49,067 | INFO | speech length: 28480
2026-01-28 20:48:49,110 | INFO | decoder input length: 44
2026-01-28 20:48:49,110 | INFO | max output length: 44
2026-01-28 20:48:49,110 | INFO | min output length: 4
2026-01-28 20:48:49,861 | INFO | end detected at 18
2026-01-28 20:48:49,862 | INFO |  -2.38 * 0.5 =  -1.19 for decoder
2026-01-28 20:48:49,863 | INFO |  -1.34 * 0.5 =  -0.67 for ctc
2026-01-28 20:48:49,863 | INFO | total log probability: -1.86
2026-01-28 20:48:49,863 | INFO | normalized log probability: -0.14
2026-01-28 20:48:49,863 | INFO | total number of ended hypotheses: 163
2026-01-28 20:48:49,863 | INFO | best hypo: ▁que▁je▁me▁saluais▁particulièrement

2026-01-28 20:48:49,865 | INFO | speech length: 82240
2026-01-28 20:48:49,920 | INFO | decoder input length: 128
2026-01-28 20:48:49,920 | INFO | max output length: 128
2026-01-28 20:48:49,920 | INFO | min output length: 12
2026-01-28 20:48:52,237 | INFO | end detected at 46
2026-01-28 20:48:52,238 | INFO |  -3.21 * 0.5 =  -1.61 for decoder
2026-01-28 20:48:52,238 | INFO |  -3.13 * 0.5 =  -1.57 for ctc
2026-01-28 20:48:52,239 | INFO | total log probability: -3.17
2026-01-28 20:48:52,239 | INFO | normalized log probability: -0.08
2026-01-28 20:48:52,239 | INFO | total number of ended hypotheses: 171
2026-01-28 20:48:52,239 | INFO | best hypo: ▁dont▁j'imagine▁qu'elles▁sont▁souvent▁confrontées▁à▁l'absence▁et▁parfois▁à▁l'angoisse

2026-01-28 20:48:52,241 | INFO | speech length: 62880
2026-01-28 20:48:52,287 | INFO | decoder input length: 97
2026-01-28 20:48:52,288 | INFO | max output length: 97
2026-01-28 20:48:52,288 | INFO | min output length: 9
2026-01-28 20:48:54,272 | INFO | end detected at 43
2026-01-28 20:48:54,273 | INFO |  -2.71 * 0.5 =  -1.35 for decoder
2026-01-28 20:48:54,273 | INFO |  -4.41 * 0.5 =  -2.21 for ctc
2026-01-28 20:48:54,273 | INFO | total log probability: -3.56
2026-01-28 20:48:54,274 | INFO | normalized log probability: -0.09
2026-01-28 20:48:54,274 | INFO | total number of ended hypotheses: 174
2026-01-28 20:48:54,274 | INFO | best hypo: ▁je▁sais▁aussi▁hélas▁le▁lourd▁tribut▁payé▁par▁certains▁de▁vos▁compagnons▁d'armes

2026-01-28 20:48:54,276 | INFO | speech length: 47840
2026-01-28 20:48:54,322 | INFO | decoder input length: 74
2026-01-28 20:48:54,323 | INFO | max output length: 74
2026-01-28 20:48:54,323 | INFO | min output length: 7
2026-01-28 20:48:55,868 | INFO | end detected at 30
2026-01-28 20:48:55,869 | INFO |  -2.81 * 0.5 =  -1.41 for decoder
2026-01-28 20:48:55,869 | INFO |  -3.78 * 0.5 =  -1.89 for ctc
2026-01-28 20:48:55,870 | INFO | total log probability: -3.29
2026-01-28 20:48:55,870 | INFO | normalized log probability: -0.13
2026-01-28 20:48:55,870 | INFO | total number of ended hypotheses: 163
2026-01-28 20:48:55,870 | INFO | best hypo: ▁tribut▁qui▁peut▁aller▁jusqu'au▁sacrifice▁ulté

2026-01-28 20:48:55,879 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:48:55,879 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:48:55,880 | INFO | Chunk: 2 | WER=8.333333 | S=0 D=1 I=0
2026-01-28 20:48:55,880 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:48:55,880 | INFO | Chunk: 4 | WER=25.000000 | S=2 D=0 I=0
2026-01-28 20:48:55,881 | INFO | Chunk: 5 | WER=9.090909 | S=2 D=0 I=0
2026-01-28 20:48:55,881 | INFO | Chunk: 6 | WER=12.500000 | S=2 D=0 I=0
2026-01-28 20:48:55,882 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:48:55,882 | INFO | Chunk: 8 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:48:55,882 | INFO | Chunk: 9 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:48:55,883 | INFO | Chunk: 10 | WER=16.666667 | S=1 D=0 I=0
2026-01-28 20:48:55,883 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:48:55,883 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:48:55,883 | INFO | Chunk: 13 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:48:55,884 | INFO | Chunk: 14 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:48:55,885 | INFO | Chunk: 15 | WER=10.000000 | S=1 D=0 I=1
2026-01-28 20:48:55,885 | INFO | Chunk: 16 | WER=5.882353 | S=1 D=0 I=0
2026-01-28 20:48:55,886 | INFO | Chunk: 17 | WER=50.000000 | S=0 D=1 I=0
2026-01-28 20:48:55,886 | INFO | Chunk: 18 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:48:55,886 | INFO | Chunk: 19 | WER=42.857143 | S=2 D=1 I=0
2026-01-28 20:48:55,887 | INFO | Chunk: 20 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:48:55,887 | INFO | Chunk: 21 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:48:55,887 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:48:55,888 | INFO | Chunk: 23 | WER=7.692308 | S=1 D=0 I=0
2026-01-28 20:48:55,888 | INFO | Chunk: 24 | WER=7.692308 | S=1 D=0 I=0
2026-01-28 20:48:55,889 | INFO | Chunk: 25 | WER=10.526316 | S=2 D=0 I=0
2026-01-28 20:48:55,889 | INFO | Chunk: 26 | WER=4.761905 | S=0 D=0 I=1
2026-01-28 20:48:55,890 | INFO | Chunk: 27 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:48:55,890 | INFO | Chunk: 28 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:48:55,890 | INFO | Chunk: 29 | WER=16.666667 | S=1 D=0 I=0
2026-01-28 20:48:55,890 | INFO | Chunk: 30 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:48:55,891 | INFO | Chunk: 31 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:48:55,891 | INFO | Chunk: 32 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:48:55,892 | INFO | Chunk: 33 | WER=7.142857 | S=1 D=0 I=0
2026-01-28 20:48:55,892 | INFO | Chunk: 34 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:48:55,892 | INFO | Chunk: 35 | WER=33.333333 | S=0 D=0 I=1
2026-01-28 20:48:55,893 | INFO | Chunk: 36 | WER=12.000000 | S=1 D=2 I=0
2026-01-28 20:48:55,893 | INFO | Chunk: 37 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:48:55,894 | INFO | Chunk: 38 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:48:55,895 | INFO | Chunk: 39 | WER=10.526316 | S=1 D=1 I=0
2026-01-28 20:48:55,895 | INFO | Chunk: 40 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:48:55,895 | INFO | Chunk: 41 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:48:55,896 | INFO | Chunk: 42 | WER=5.000000 | S=1 D=0 I=0
2026-01-28 20:48:55,896 | INFO | Chunk: 43 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:48:55,896 | INFO | Chunk: 44 | WER=40.000000 | S=2 D=0 I=0
2026-01-28 20:48:55,897 | INFO | Chunk: 45 | WER=6.666667 | S=0 D=0 I=1
2026-01-28 20:48:55,897 | INFO | Chunk: 46 | WER=6.666667 | S=1 D=0 I=0
2026-01-28 20:48:55,898 | INFO | Chunk: 47 | WER=12.500000 | S=1 D=0 I=0
2026-01-28 20:48:56,051 | INFO | File: Rhap-M2001.wav | WER=6.367041 | S=24 D=6 I=4
2026-01-28 20:48:56,051 | INFO | ------------------------------
2026-01-28 20:48:56,051 | INFO | Conf cv Done!
2026-01-28 20:48:56,232 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 20:48:56,255 | INFO | Vocabulary size: 47
2026-01-28 20:48:57,142 | INFO | Gradient checkpoint layers: []
2026-01-28 20:48:57,930 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 20:48:57,935 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 20:48:57,935 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 20:48:57,935 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 20:48:57,939 | INFO | speech length: 14880
2026-01-28 20:48:57,978 | INFO | decoder input length: 22
2026-01-28 20:48:57,978 | INFO | max output length: 22
2026-01-28 20:48:57,978 | INFO | min output length: 2
2026-01-28 20:48:58,680 | INFO | adding <eos> in the last position in the loop
2026-01-28 20:48:58,688 | INFO | no hypothesis. Finish decoding.
2026-01-28 20:48:58,688 | INFO |  -1.81 * 0.5 =  -0.90 for decoder
2026-01-28 20:48:58,688 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-28 20:48:58,688 | INFO | total log probability: -0.91
2026-01-28 20:48:58,688 | INFO | normalized log probability: -0.04
2026-01-28 20:48:58,688 | INFO | total number of ended hypotheses: 56
2026-01-28 20:48:58,689 | INFO | best hypo: mesdames<space>et<space>messieurs<sos/eos>

2026-01-28 20:48:58,689 | WARNING | best hypo length: 22 == max output length: 22
2026-01-28 20:48:58,689 | WARNING | decoding may be stopped by the max output length limitation, please consider to increase the maxlenratio.
2026-01-28 20:48:58,690 | INFO | speech length: 53600
2026-01-28 20:48:58,745 | INFO | decoder input length: 83
2026-01-28 20:48:58,746 | INFO | max output length: 83
2026-01-28 20:48:58,746 | INFO | min output length: 8
2026-01-28 20:49:01,030 | INFO | end detected at 57
2026-01-28 20:49:01,031 | INFO |  -4.09 * 0.5 =  -2.04 for decoder
2026-01-28 20:49:01,032 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 20:49:01,032 | INFO | total log probability: -2.04
2026-01-28 20:49:01,032 | INFO | normalized log probability: -0.04
2026-01-28 20:49:01,032 | INFO | total number of ended hypotheses: 169
2026-01-28 20:49:01,032 | INFO | best hypo: je<space>suis<space>heureux<space>de<space>me<space>retrouver<space>ce<space>soir<space>parmi<space>vous

2026-01-28 20:49:01,034 | INFO | speech length: 53280
2026-01-28 20:49:01,071 | INFO | decoder input length: 82
2026-01-28 20:49:01,071 | INFO | max output length: 82
2026-01-28 20:49:01,071 | INFO | min output length: 8
2026-01-28 20:49:03,512 | INFO | end detected at 63
2026-01-28 20:49:03,513 | INFO |  -5.49 * 0.5 =  -2.74 for decoder
2026-01-28 20:49:03,513 | INFO |  -1.78 * 0.5 =  -0.89 for ctc
2026-01-28 20:49:03,513 | INFO | total log probability: -3.63
2026-01-28 20:49:03,513 | INFO | normalized log probability: -0.06
2026-01-28 20:49:03,513 | INFO | total number of ended hypotheses: 159
2026-01-28 20:49:03,514 | INFO | best hypo: après<space>ma<space>visite<space>à<space>l'endivision<space>et<space>à<space>l'île<space>longue<space>ce<space>matin

2026-01-28 20:49:03,516 | INFO | speech length: 37440
2026-01-28 20:49:03,565 | INFO | decoder input length: 58
2026-01-28 20:49:03,565 | INFO | max output length: 58
2026-01-28 20:49:03,565 | INFO | min output length: 5
2026-01-28 20:49:04,956 | INFO | end detected at 37
2026-01-28 20:49:04,958 | INFO |  -2.48 * 0.5 =  -1.24 for decoder
2026-01-28 20:49:04,958 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-28 20:49:04,958 | INFO | total log probability: -1.24
2026-01-28 20:49:04,958 | INFO | normalized log probability: -0.04
2026-01-28 20:49:04,958 | INFO | total number of ended hypotheses: 171
2026-01-28 20:49:04,959 | INFO | best hypo: c'est<space>donc<space>une<space>journée<space>entière

2026-01-28 20:49:04,961 | INFO | speech length: 30400
2026-01-28 20:49:05,006 | INFO | decoder input length: 47
2026-01-28 20:49:05,006 | INFO | max output length: 47
2026-01-28 20:49:05,006 | INFO | min output length: 4
2026-01-28 20:49:06,627 | INFO | adding <eos> in the last position in the loop
2026-01-28 20:49:06,636 | INFO | no hypothesis. Finish decoding.
2026-01-28 20:49:06,638 | INFO |  -4.16 * 0.5 =  -2.08 for decoder
2026-01-28 20:49:06,638 | INFO |  -4.26 * 0.5 =  -2.13 for ctc
2026-01-28 20:49:06,638 | INFO | total log probability: -4.21
2026-01-28 20:49:06,638 | INFO | normalized log probability: -0.10
2026-01-28 20:49:06,638 | INFO | total number of ended hypotheses: 163
2026-01-28 20:49:06,639 | INFO | best hypo: que<space>j'aurais<space>passé<space>avec<space>nos<space>forces<space>armées

2026-01-28 20:49:06,640 | INFO | speech length: 88640
2026-01-28 20:49:06,702 | INFO | decoder input length: 138
2026-01-28 20:49:06,703 | INFO | max output length: 138
2026-01-28 20:49:06,703 | INFO | min output length: 13
2026-01-28 20:49:15,676 | INFO | end detected at 116
2026-01-28 20:49:15,678 | INFO |  -8.84 * 0.5 =  -4.42 for decoder
2026-01-28 20:49:15,678 | INFO |  -1.07 * 0.5 =  -0.54 for ctc
2026-01-28 20:49:15,678 | INFO | total log probability: -4.95
2026-01-28 20:49:15,678 | INFO | normalized log probability: -0.04
2026-01-28 20:49:15,678 | INFO | total number of ended hypotheses: 147
2026-01-28 20:49:15,681 | INFO | best hypo: c'est<space>bien<space>sûr<space>dans<space>mon<space>esprit<space>la<space>marque<space>du<space>lien<space>direct<space>qui<space>unit<space>le<space>président<space>de<space>la<space>république<space>chef<space>des<space>armées

2026-01-28 20:49:15,684 | INFO | speech length: 86880
2026-01-28 20:49:15,739 | INFO | decoder input length: 135
2026-01-28 20:49:15,740 | INFO | max output length: 135
2026-01-28 20:49:15,740 | INFO | min output length: 13
2026-01-28 20:49:20,723 | INFO | end detected at 97
2026-01-28 20:49:20,725 | INFO |  -7.48 * 0.5 =  -3.74 for decoder
2026-01-28 20:49:20,726 | INFO |  -1.41 * 0.5 =  -0.71 for ctc
2026-01-28 20:49:20,726 | INFO | total log probability: -4.45
2026-01-28 20:49:20,726 | INFO | normalized log probability: -0.05
2026-01-28 20:49:20,726 | INFO | total number of ended hypotheses: 183
2026-01-28 20:49:20,727 | INFO | best hypo: avec<space>toutes<space>celles<space>et<space>tous<space>ceux<space>qui<space>ont<space>la<space>difficile<space>mission<space>de<space>veiller<space>sur<space>nos<space>intérêts

2026-01-28 20:49:20,730 | INFO | speech length: 31360
2026-01-28 20:49:20,791 | INFO | decoder input length: 48
2026-01-28 20:49:20,791 | INFO | max output length: 48
2026-01-28 20:49:20,791 | INFO | min output length: 4
2026-01-28 20:49:23,399 | INFO | end detected at 43
2026-01-28 20:49:23,401 | INFO |  -3.07 * 0.5 =  -1.53 for decoder
2026-01-28 20:49:23,401 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 20:49:23,401 | INFO | total log probability: -1.54
2026-01-28 20:49:23,401 | INFO | normalized log probability: -0.04
2026-01-28 20:49:23,401 | INFO | total number of ended hypotheses: 152
2026-01-28 20:49:23,402 | INFO | best hypo: et<space>sur<space>la<space>sécurité<space>de<space>nos<space>concitoyens

2026-01-28 20:49:23,406 | INFO | speech length: 62720
2026-01-28 20:49:23,473 | INFO | decoder input length: 97
2026-01-28 20:49:23,473 | INFO | max output length: 97
2026-01-28 20:49:23,473 | INFO | min output length: 9
2026-01-28 20:49:29,418 | INFO | end detected at 87
2026-01-28 20:49:29,420 | INFO |  -6.55 * 0.5 =  -3.28 for decoder
2026-01-28 20:49:29,421 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-28 20:49:29,421 | INFO | total log probability: -3.29
2026-01-28 20:49:29,421 | INFO | normalized log probability: -0.04
2026-01-28 20:49:29,421 | INFO | total number of ended hypotheses: 160
2026-01-28 20:49:29,423 | INFO | best hypo: c'est<space>pour<space>cela<space>que<space>je<space>tenais<space>à<space>vous<space>rencontrer<space>la<space>veille<space>de<space>notre<space>fête<space>nationale

2026-01-28 20:49:29,426 | INFO | speech length: 50080
2026-01-28 20:49:29,486 | INFO | decoder input length: 77
2026-01-28 20:49:29,486 | INFO | max output length: 77
2026-01-28 20:49:29,486 | INFO | min output length: 7
2026-01-28 20:49:31,919 | INFO | end detected at 54
2026-01-28 20:49:31,920 | INFO |  -6.47 * 0.5 =  -3.23 for decoder
2026-01-28 20:49:31,920 | INFO |  -0.15 * 0.5 =  -0.07 for ctc
2026-01-28 20:49:31,920 | INFO | total log probability: -3.31
2026-01-28 20:49:31,920 | INFO | normalized log probability: -0.07
2026-01-28 20:49:31,920 | INFO | total number of ended hypotheses: 162
2026-01-28 20:49:31,921 | INFO | best hypo: le<space>quatorze<space>juillet<space>lors<space>du<space>traditionnel<space>définé

2026-01-28 20:49:31,924 | INFO | speech length: 25280
2026-01-28 20:49:31,983 | INFO | decoder input length: 39
2026-01-28 20:49:31,984 | INFO | max output length: 39
2026-01-28 20:49:31,984 | INFO | min output length: 3
2026-01-28 20:49:34,094 | INFO | end detected at 35
2026-01-28 20:49:34,096 | INFO |  -2.41 * 0.5 =  -1.20 for decoder
2026-01-28 20:49:34,097 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 20:49:34,097 | INFO | total log probability: -1.20
2026-01-28 20:49:34,097 | INFO | normalized log probability: -0.04
2026-01-28 20:49:34,097 | INFO | total number of ended hypotheses: 149
2026-01-28 20:49:34,098 | INFO | best hypo: c'est<space>la<space>nation<space>toute<space>entière

2026-01-28 20:49:34,101 | INFO | speech length: 13600
2026-01-28 20:49:34,156 | INFO | decoder input length: 20
2026-01-28 20:49:34,156 | INFO | max output length: 20
2026-01-28 20:49:34,156 | INFO | min output length: 2
2026-01-28 20:49:34,887 | INFO | adding <eos> in the last position in the loop
2026-01-28 20:49:34,895 | INFO | no hypothesis. Finish decoding.
2026-01-28 20:49:34,895 | INFO | -14.08 * 0.5 =  -7.04 for decoder
2026-01-28 20:49:34,895 | INFO |  -3.48 * 0.5 =  -1.74 for ctc
2026-01-28 20:49:34,896 | INFO | total log probability: -8.78
2026-01-28 20:49:34,896 | INFO | normalized log probability: -0.40
2026-01-28 20:49:34,896 | INFO | total number of ended hypotheses: 55
2026-01-28 20:49:34,896 | INFO | best hypo: qu<space>vous<space>rend<space>homage<sos/eos>

2026-01-28 20:49:34,896 | WARNING | best hypo length: 20 == max output length: 20
2026-01-28 20:49:34,896 | WARNING | decoding may be stopped by the max output length limitation, please consider to increase the maxlenratio.
2026-01-28 20:49:34,897 | INFO | speech length: 21920
2026-01-28 20:49:34,950 | INFO | decoder input length: 33
2026-01-28 20:49:34,951 | INFO | max output length: 33
2026-01-28 20:49:34,951 | INFO | min output length: 3
2026-01-28 20:49:36,039 | INFO | end detected at 27
2026-01-28 20:49:36,040 | INFO |  -1.81 * 0.5 =  -0.90 for decoder
2026-01-28 20:49:36,040 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 20:49:36,040 | INFO | total log probability: -0.91
2026-01-28 20:49:36,040 | INFO | normalized log probability: -0.04
2026-01-28 20:49:36,040 | INFO | total number of ended hypotheses: 129
2026-01-28 20:49:36,041 | INFO | best hypo: elle<space>salue<space>le<space>courage

2026-01-28 20:49:36,043 | INFO | speech length: 48480
2026-01-28 20:49:36,090 | INFO | decoder input length: 75
2026-01-28 20:49:36,090 | INFO | max output length: 75
2026-01-28 20:49:36,090 | INFO | min output length: 7
2026-01-28 20:49:38,671 | INFO | end detected at 53
2026-01-28 20:49:38,673 | INFO |  -3.79 * 0.5 =  -1.90 for decoder
2026-01-28 20:49:38,673 | INFO |  -0.03 * 0.5 =  -0.02 for ctc
2026-01-28 20:49:38,673 | INFO | total log probability: -1.91
2026-01-28 20:49:38,673 | INFO | normalized log probability: -0.04
2026-01-28 20:49:38,673 | INFO | total number of ended hypotheses: 161
2026-01-28 20:49:38,674 | INFO | best hypo: elle<space>salue<space>la<space>loyauté<space>elle<space>salue<space>le<space>dévouement

2026-01-28 20:49:38,677 | INFO | speech length: 147840
2026-01-28 20:49:38,730 | INFO | decoder input length: 230
2026-01-28 20:49:38,730 | INFO | max output length: 230
2026-01-28 20:49:38,731 | INFO | min output length: 23
2026-01-28 20:49:46,802 | INFO | end detected at 130
2026-01-28 20:49:46,803 | INFO | -10.00 * 0.5 =  -5.00 for decoder
2026-01-28 20:49:46,803 | INFO |  -0.36 * 0.5 =  -0.18 for ctc
2026-01-28 20:49:46,803 | INFO | total log probability: -5.18
2026-01-28 20:49:46,803 | INFO | normalized log probability: -0.04
2026-01-28 20:49:46,803 | INFO | total number of ended hypotheses: 166
2026-01-28 20:49:46,805 | INFO | best hypo: elle<space>salue<space>l'esprit<space>de<space>sacrifice<space>de<space>ceux<space>qui<space>ont<space>choisi<space>de<space>servir<space>sa<space>défense<space>ses<space>intérêts<space>et<space>ses<space>valeurs<space>à<space>travers<space>le<space>monde

2026-01-28 20:49:46,808 | INFO | speech length: 95200
2026-01-28 20:49:46,853 | INFO | decoder input length: 148
2026-01-28 20:49:46,853 | INFO | max output length: 148
2026-01-28 20:49:46,853 | INFO | min output length: 14
2026-01-28 20:49:52,191 | INFO | end detected at 119
2026-01-28 20:49:52,192 | INFO | -13.13 * 0.5 =  -6.56 for decoder
2026-01-28 20:49:52,193 | INFO |  -5.33 * 0.5 =  -2.67 for ctc
2026-01-28 20:49:52,193 | INFO | total log probability: -9.23
2026-01-28 20:49:52,193 | INFO | normalized log probability: -0.08
2026-01-28 20:49:52,193 | INFO | total number of ended hypotheses: 171
2026-01-28 20:49:52,194 | INFO | best hypo: et<space>c'est<space>la<space>raison<space>pour<space>laquelle<space>au<space>nom<space>de<space>la<space>république<space>je<space>norerai<space>certains<space>d'entre<space>vous<space>dans<space>quelques<space>instants

2026-01-28 20:49:52,196 | INFO | speech length: 69920
2026-01-28 20:49:52,256 | INFO | decoder input length: 108
2026-01-28 20:49:52,256 | INFO | max output length: 108
2026-01-28 20:49:52,256 | INFO | min output length: 10
2026-01-28 20:49:55,924 | INFO | end detected at 88
2026-01-28 20:49:55,925 | INFO |  -6.51 * 0.5 =  -3.25 for decoder
2026-01-28 20:49:55,925 | INFO |  -1.22 * 0.5 =  -0.61 for ctc
2026-01-28 20:49:55,925 | INFO | total log probability: -3.86
2026-01-28 20:49:55,925 | INFO | normalized log probability: -0.05
2026-01-28 20:49:55,926 | INFO | total number of ended hypotheses: 178
2026-01-28 20:49:55,927 | INFO | best hypo: cette<space>année<space>j'ai<space>tenu<space>à<space>ce<space>que<space>cette<space>fête<space>nationale<space>soit<space>aussi<space>celle<space>de<space>l'europe

2026-01-28 20:49:55,929 | INFO | speech length: 12160
2026-01-28 20:49:55,963 | INFO | decoder input length: 18
2026-01-28 20:49:55,964 | INFO | max output length: 18
2026-01-28 20:49:55,964 | INFO | min output length: 1
2026-01-28 20:49:56,488 | INFO | end detected at 15
2026-01-28 20:49:56,489 | INFO |  -0.82 * 0.5 =  -0.41 for decoder
2026-01-28 20:49:56,489 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 20:49:56,489 | INFO | total log probability: -0.41
2026-01-28 20:49:56,489 | INFO | normalized log probability: -0.04
2026-01-28 20:49:56,489 | INFO | total number of ended hypotheses: 139
2026-01-28 20:49:56,489 | INFO | best hypo: et<space>demain

2026-01-28 20:49:56,491 | INFO | speech length: 76000
2026-01-28 20:49:56,545 | INFO | decoder input length: 118
2026-01-28 20:49:56,545 | INFO | max output length: 118
2026-01-28 20:49:56,545 | INFO | min output length: 11
2026-01-28 20:50:02,564 | INFO | end detected at 104
2026-01-28 20:50:02,566 | INFO |  -8.86 * 0.5 =  -4.43 for decoder
2026-01-28 20:50:02,566 | INFO |  -0.99 * 0.5 =  -0.50 for ctc
2026-01-28 20:50:02,566 | INFO | total log probability: -4.92
2026-01-28 20:50:02,566 | INFO | normalized log probability: -0.05
2026-01-28 20:50:02,566 | INFO | total number of ended hypotheses: 180
2026-01-28 20:50:02,567 | INFO | best hypo: ce<space>sont<space>les<space>vingt<space>six<space>drapeaux<space>de<space>nos<space>partenaires<space>européens<space>qui<space>défileront<space>au<space>côté<space>de<space>nos<space>armées

2026-01-28 20:50:02,569 | INFO | speech length: 29120
2026-01-28 20:50:02,609 | INFO | decoder input length: 45
2026-01-28 20:50:02,609 | INFO | max output length: 45
2026-01-28 20:50:02,609 | INFO | min output length: 4
2026-01-28 20:50:04,146 | INFO | adding <eos> in the last position in the loop
2026-01-28 20:50:04,156 | INFO | no hypothesis. Finish decoding.
2026-01-28 20:50:04,157 | INFO |  -7.56 * 0.5 =  -3.78 for decoder
2026-01-28 20:50:04,158 | INFO |  -4.15 * 0.5 =  -2.07 for ctc
2026-01-28 20:50:04,158 | INFO | total log probability: -5.85
2026-01-28 20:50:04,158 | INFO | normalized log probability: -0.15
2026-01-28 20:50:04,158 | INFO | total number of ended hypotheses: 173
2026-01-28 20:50:04,158 | INFO | best hypo: je<space>vous<space>remercie<space>d'ailleurs<space>cher<space>hervé

2026-01-28 20:50:04,160 | INFO | speech length: 9600
2026-01-28 20:50:04,188 | INFO | decoder input length: 14
2026-01-28 20:50:04,188 | INFO | max output length: 14
2026-01-28 20:50:04,188 | INFO | min output length: 1
2026-01-28 20:50:04,629 | INFO | adding <eos> in the last position in the loop
2026-01-28 20:50:04,637 | INFO | no hypothesis. Finish decoding.
2026-01-28 20:50:04,638 | INFO |  -2.36 * 0.5 =  -1.18 for decoder
2026-01-28 20:50:04,638 | INFO |  -2.97 * 0.5 =  -1.48 for ctc
2026-01-28 20:50:04,638 | INFO | total log probability: -2.66
2026-01-28 20:50:04,638 | INFO | normalized log probability: -0.22
2026-01-28 20:50:04,638 | INFO | total number of ended hypotheses: 115
2026-01-28 20:50:04,639 | INFO | best hypo: chez<space>allin

2026-01-28 20:50:04,640 | INFO | speech length: 65920
2026-01-28 20:50:04,675 | INFO | decoder input length: 102
2026-01-28 20:50:04,675 | INFO | max output length: 102
2026-01-28 20:50:04,675 | INFO | min output length: 10
2026-01-28 20:50:07,916 | INFO | end detected at 80
2026-01-28 20:50:07,918 | INFO |  -5.92 * 0.5 =  -2.96 for decoder
2026-01-28 20:50:07,918 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 20:50:07,918 | INFO | total log probability: -2.96
2026-01-28 20:50:07,918 | INFO | normalized log probability: -0.04
2026-01-28 20:50:07,918 | INFO | total number of ended hypotheses: 182
2026-01-28 20:50:07,919 | INFO | best hypo: tout<space>spécialement<space>les<space>ministres<space>de<space>la<space>défense<space>qui<space>sont<space>parmi<space>nous<space>ce<space>soir

2026-01-28 20:50:07,921 | INFO | speech length: 38560
2026-01-28 20:50:07,956 | INFO | decoder input length: 59
2026-01-28 20:50:07,956 | INFO | max output length: 59
2026-01-28 20:50:07,956 | INFO | min output length: 5
2026-01-28 20:50:09,564 | INFO | end detected at 43
2026-01-28 20:50:09,566 | INFO |  -2.96 * 0.5 =  -1.48 for decoder
2026-01-28 20:50:09,566 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-28 20:50:09,566 | INFO | total log probability: -1.49
2026-01-28 20:50:09,566 | INFO | normalized log probability: -0.04
2026-01-28 20:50:09,566 | INFO | total number of ended hypotheses: 172
2026-01-28 20:50:09,567 | INFO | best hypo: je<space>leur<space>adresse<space>un<space>salut<space>très<space>amical

2026-01-28 20:50:09,568 | INFO | speech length: 40960
2026-01-28 20:50:09,604 | INFO | decoder input length: 63
2026-01-28 20:50:09,604 | INFO | max output length: 63
2026-01-28 20:50:09,604 | INFO | min output length: 6
2026-01-28 20:50:11,688 | INFO | end detected at 57
2026-01-28 20:50:11,689 | INFO |  -4.16 * 0.5 =  -2.08 for decoder
2026-01-28 20:50:11,690 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-28 20:50:11,690 | INFO | total log probability: -2.08
2026-01-28 20:50:11,690 | INFO | normalized log probability: -0.04
2026-01-28 20:50:11,690 | INFO | total number of ended hypotheses: 151
2026-01-28 20:50:11,690 | INFO | best hypo: c'est<space>l'image<space>d'une<space>europe<space>qui<space>repart<space>d'un<space>même<space>pas

2026-01-28 20:50:11,692 | INFO | speech length: 74880
2026-01-28 20:50:11,729 | INFO | decoder input length: 116
2026-01-28 20:50:11,730 | INFO | max output length: 116
2026-01-28 20:50:11,730 | INFO | min output length: 11
2026-01-28 20:50:15,299 | INFO | end detected at 84
2026-01-28 20:50:15,300 | INFO |  -6.55 * 0.5 =  -3.28 for decoder
2026-01-28 20:50:15,300 | INFO |  -0.30 * 0.5 =  -0.15 for ctc
2026-01-28 20:50:15,301 | INFO | total log probability: -3.43
2026-01-28 20:50:15,301 | INFO | normalized log probability: -0.04
2026-01-28 20:50:15,301 | INFO | total number of ended hypotheses: 155
2026-01-28 20:50:15,302 | INFO | best hypo: une<space>europe<space>résolue<space>à<space>se<space>remettre<space>au<space>mouvement<space>après<space>deux<space>années<space>d'immobilistes

2026-01-28 20:50:15,304 | INFO | speech length: 95840
2026-01-28 20:50:15,346 | INFO | decoder input length: 149
2026-01-28 20:50:15,346 | INFO | max output length: 149
2026-01-28 20:50:15,346 | INFO | min output length: 14
2026-01-28 20:50:21,061 | INFO | end detected at 117
2026-01-28 20:50:21,062 | INFO |  -8.96 * 0.5 =  -4.48 for decoder
2026-01-28 20:50:21,062 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-28 20:50:21,062 | INFO | total log probability: -4.49
2026-01-28 20:50:21,062 | INFO | normalized log probability: -0.04
2026-01-28 20:50:21,062 | INFO | total number of ended hypotheses: 147
2026-01-28 20:50:21,064 | INFO | best hypo: une<space>europe<space>vigilante<space>qui<space>n'oublie<space>pas<space>l'impératif<space>de<space>protection<space>dans<space>un<space>monde<space>plus<space>instable<space>et<space>moins<space>prévisible

2026-01-28 20:50:21,066 | INFO | speech length: 114560
2026-01-28 20:50:21,110 | INFO | decoder input length: 178
2026-01-28 20:50:21,110 | INFO | max output length: 178
2026-01-28 20:50:21,110 | INFO | min output length: 17
2026-01-28 20:50:27,042 | INFO | end detected at 123
2026-01-28 20:50:27,043 | INFO |  -9.34 * 0.5 =  -4.67 for decoder
2026-01-28 20:50:27,043 | INFO |  -0.07 * 0.5 =  -0.03 for ctc
2026-01-28 20:50:27,043 | INFO | total log probability: -4.71
2026-01-28 20:50:27,044 | INFO | normalized log probability: -0.04
2026-01-28 20:50:27,044 | INFO | total number of ended hypotheses: 182
2026-01-28 20:50:27,045 | INFO | best hypo: une<space>europe<space>prête<space>à<space>assumer<space>ses<space>responsabilités<space>et<space>au<space>sein<space>de<space>laquelle<space>la<space>france<space>a<space>bien<space>l'intention<space>de<space>tenir<space>son<space>rang

2026-01-28 20:50:27,047 | INFO | speech length: 33920
2026-01-28 20:50:27,084 | INFO | decoder input length: 52
2026-01-28 20:50:27,084 | INFO | max output length: 52
2026-01-28 20:50:27,084 | INFO | min output length: 5
2026-01-28 20:50:28,834 | INFO | end detected at 49
2026-01-28 20:50:28,835 | INFO |  -3.52 * 0.5 =  -1.76 for decoder
2026-01-28 20:50:28,835 | INFO |  -0.07 * 0.5 =  -0.04 for ctc
2026-01-28 20:50:28,835 | INFO | total log probability: -1.80
2026-01-28 20:50:28,835 | INFO | normalized log probability: -0.04
2026-01-28 20:50:28,835 | INFO | total number of ended hypotheses: 164
2026-01-28 20:50:28,836 | INFO | best hypo: les<space>bases<space>d'une<space>défense<space>européenne<space>existent

2026-01-28 20:50:28,837 | INFO | speech length: 21120
2026-01-28 20:50:28,875 | INFO | decoder input length: 32
2026-01-28 20:50:28,875 | INFO | max output length: 32
2026-01-28 20:50:28,875 | INFO | min output length: 3
2026-01-28 20:50:30,031 | INFO | adding <eos> in the last position in the loop
2026-01-28 20:50:30,041 | INFO | end detected at 31
2026-01-28 20:50:30,042 | INFO |  -2.78 * 0.5 =  -1.39 for decoder
2026-01-28 20:50:30,042 | INFO |  -0.18 * 0.5 =  -0.09 for ctc
2026-01-28 20:50:30,042 | INFO | total log probability: -1.48
2026-01-28 20:50:30,042 | INFO | normalized log probability: -0.05
2026-01-28 20:50:30,042 | INFO | total number of ended hypotheses: 176
2026-01-28 20:50:30,042 | INFO | best hypo: il<space>faut<space>les<space>faire<space>grandir

2026-01-28 20:50:30,044 | INFO | speech length: 30880
2026-01-28 20:50:30,087 | INFO | decoder input length: 47
2026-01-28 20:50:30,087 | INFO | max output length: 47
2026-01-28 20:50:30,087 | INFO | min output length: 4
2026-01-28 20:50:31,430 | INFO | end detected at 37
2026-01-28 20:50:31,431 | INFO |  -2.56 * 0.5 =  -1.28 for decoder
2026-01-28 20:50:31,431 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 20:50:31,431 | INFO | total log probability: -1.28
2026-01-28 20:50:31,431 | INFO | normalized log probability: -0.04
2026-01-28 20:50:31,432 | INFO | total number of ended hypotheses: 139
2026-01-28 20:50:31,432 | INFO | best hypo: en<space>quittant<space>le<space>terrain<space>des<space>mots

2026-01-28 20:50:31,435 | INFO | speech length: 25920
2026-01-28 20:50:31,574 | INFO | decoder input length: 40
2026-01-28 20:50:31,574 | INFO | max output length: 40
2026-01-28 20:50:31,574 | INFO | min output length: 4
2026-01-28 20:50:33,136 | INFO | end detected at 35
2026-01-28 20:50:33,138 | INFO |  -2.42 * 0.5 =  -1.21 for decoder
2026-01-28 20:50:33,138 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 20:50:33,138 | INFO | total log probability: -1.21
2026-01-28 20:50:33,138 | INFO | normalized log probability: -0.04
2026-01-28 20:50:33,138 | INFO | total number of ended hypotheses: 143
2026-01-28 20:50:33,139 | INFO | best hypo: pour<space>gagner<space>celui<space>de<space>l'action

2026-01-28 20:50:33,141 | INFO | speech length: 134240
2026-01-28 20:50:33,274 | INFO | decoder input length: 209
2026-01-28 20:50:33,275 | INFO | max output length: 209
2026-01-28 20:50:33,275 | INFO | min output length: 20
2026-01-28 20:50:39,528 | INFO | end detected at 116
2026-01-28 20:50:39,529 | INFO |  -8.85 * 0.5 =  -4.42 for decoder
2026-01-28 20:50:39,529 | INFO |  -0.08 * 0.5 =  -0.04 for ctc
2026-01-28 20:50:39,529 | INFO | total log probability: -4.46
2026-01-28 20:50:39,529 | INFO | normalized log probability: -0.04
2026-01-28 20:50:39,529 | INFO | total number of ended hypotheses: 165
2026-01-28 20:50:39,531 | INFO | best hypo: demain<space>davantage<space>qu'aujourd'hui<space>je<space>souhaite<space>que<space>l'europe<space>soit<space>capable<space>d'assurer<space>sa<space>sécurité<space>de<space>façon<space>autonome

2026-01-28 20:50:39,533 | INFO | speech length: 46080
2026-01-28 20:50:39,574 | INFO | decoder input length: 71
2026-01-28 20:50:39,574 | INFO | max output length: 71
2026-01-28 20:50:39,574 | INFO | min output length: 7
2026-01-28 20:50:42,106 | INFO | end detected at 69
2026-01-28 20:50:42,107 | INFO |  -5.09 * 0.5 =  -2.54 for decoder
2026-01-28 20:50:42,107 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-28 20:50:42,107 | INFO | total log probability: -2.55
2026-01-28 20:50:42,107 | INFO | normalized log probability: -0.04
2026-01-28 20:50:42,107 | INFO | total number of ended hypotheses: 161
2026-01-28 20:50:42,108 | INFO | best hypo: alors<space>si<space>la<space>france<space>pèse<space>aujourd'hui<space>sur<space>la<space>scène<space>internationale

2026-01-28 20:50:42,110 | INFO | speech length: 80320
2026-01-28 20:50:42,157 | INFO | decoder input length: 125
2026-01-28 20:50:42,158 | INFO | max output length: 125
2026-01-28 20:50:42,158 | INFO | min output length: 12
2026-01-28 20:50:45,501 | INFO | end detected at 75
2026-01-28 20:50:45,503 | INFO |  -5.53 * 0.5 =  -2.77 for decoder
2026-01-28 20:50:45,503 | INFO |  -0.10 * 0.5 =  -0.05 for ctc
2026-01-28 20:50:45,503 | INFO | total log probability: -2.81
2026-01-28 20:50:45,503 | INFO | normalized log probability: -0.04
2026-01-28 20:50:45,503 | INFO | total number of ended hypotheses: 164
2026-01-28 20:50:45,504 | INFO | best hypo: il<space>lui<space>faut<space>un<space>outil<space>de<space>défense<space>adapté<space>à<space>la<space>hauteur<space>de<space>ces<space>ambitions

2026-01-28 20:50:45,506 | INFO | speech length: 55840
2026-01-28 20:50:45,545 | INFO | decoder input length: 86
2026-01-28 20:50:45,545 | INFO | max output length: 86
2026-01-28 20:50:45,545 | INFO | min output length: 8
2026-01-28 20:50:47,857 | INFO | end detected at 55
2026-01-28 20:50:47,858 | INFO |  -3.91 * 0.5 =  -1.95 for decoder
2026-01-28 20:50:47,858 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-28 20:50:47,858 | INFO | total log probability: -1.96
2026-01-28 20:50:47,859 | INFO | normalized log probability: -0.04
2026-01-28 20:50:47,859 | INFO | total number of ended hypotheses: 173
2026-01-28 20:50:47,859 | INFO | best hypo: c'est<space>aussi<space>là<space>l'intérêt<space>fondamental<space>de<space>l'europe

2026-01-28 20:50:47,861 | INFO | speech length: 10400
2026-01-28 20:50:47,897 | INFO | decoder input length: 15
2026-01-28 20:50:47,897 | INFO | max output length: 15
2026-01-28 20:50:47,897 | INFO | min output length: 1
2026-01-28 20:50:48,369 | INFO | adding <eos> in the last position in the loop
2026-01-28 20:50:48,378 | INFO | no hypothesis. Finish decoding.
2026-01-28 20:50:48,378 | INFO |  -8.95 * 0.5 =  -4.47 for decoder
2026-01-28 20:50:48,378 | INFO | -10.11 * 0.5 =  -5.05 for ctc
2026-01-28 20:50:48,378 | INFO | total log probability: -9.53
2026-01-28 20:50:48,378 | INFO | normalized log probability: -0.64
2026-01-28 20:50:48,378 | INFO | total number of ended hypotheses: 51
2026-01-28 20:50:48,379 | INFO | best hypo: et<space>denous<space>par

2026-01-28 20:50:48,379 | INFO | speech length: 88320
2026-01-28 20:50:48,417 | INFO | decoder input length: 137
2026-01-28 20:50:48,417 | INFO | max output length: 137
2026-01-28 20:50:48,417 | INFO | min output length: 13
2026-01-28 20:50:53,933 | INFO | end detected at 122
2026-01-28 20:50:53,935 | INFO |  -9.32 * 0.5 =  -4.66 for decoder
2026-01-28 20:50:53,935 | INFO |  -1.04 * 0.5 =  -0.52 for ctc
2026-01-28 20:50:53,935 | INFO | total log probability: -5.18
2026-01-28 20:50:53,935 | INFO | normalized log probability: -0.04
2026-01-28 20:50:53,935 | INFO | total number of ended hypotheses: 171
2026-01-28 20:50:53,937 | INFO | best hypo: aujourd'hui<space>plus<space>que<space>jamais<space>vous<space>le<space>savez<space>mieux<space>que<space>personne<space>c'est<space>sur<space>le<space>terrain<space>que<space>se<space>gagne<space>ou<space>se<space>perd<space>le<space>combat

2026-01-28 20:50:53,939 | INFO | speech length: 20320
2026-01-28 20:50:53,978 | INFO | decoder input length: 31
2026-01-28 20:50:53,978 | INFO | max output length: 31
2026-01-28 20:50:53,978 | INFO | min output length: 3
2026-01-28 20:50:54,887 | INFO | end detected at 26
2026-01-28 20:50:54,888 | INFO |  -1.70 * 0.5 =  -0.85 for decoder
2026-01-28 20:50:54,888 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 20:50:54,888 | INFO | total log probability: -0.85
2026-01-28 20:50:54,888 | INFO | normalized log probability: -0.04
2026-01-28 20:50:54,889 | INFO | total number of ended hypotheses: 148
2026-01-28 20:50:54,889 | INFO | best hypo: c'est<space>sur<space>le<space>terrain

2026-01-28 20:50:54,891 | INFO | speech length: 127680
2026-01-28 20:50:54,945 | INFO | decoder input length: 199
2026-01-28 20:50:54,945 | INFO | max output length: 199
2026-01-28 20:50:54,945 | INFO | min output length: 19
2026-01-28 20:51:06,991 | INFO | end detected at 126
2026-01-28 20:51:06,993 | INFO |  -9.66 * 0.5 =  -4.83 for decoder
2026-01-28 20:51:06,994 | INFO |  -0.08 * 0.5 =  -0.04 for ctc
2026-01-28 20:51:06,994 | INFO | total log probability: -4.87
2026-01-28 20:51:06,994 | INFO | normalized log probability: -0.04
2026-01-28 20:51:06,994 | INFO | total number of ended hypotheses: 179
2026-01-28 20:51:06,997 | INFO | best hypo: qu'une<space>nation<space>affirme<space>son<space>influence<space>qu'elle<space>pèse<space>dans<space>une<space>coalition<space>au<space>travers<space>des<space>forces<space>qu'elle<space>est<space>capable<space>d'engager

2026-01-28 20:51:07,001 | INFO | speech length: 79200
2026-01-28 20:51:07,049 | INFO | decoder input length: 123
2026-01-28 20:51:07,049 | INFO | max output length: 123
2026-01-28 20:51:07,049 | INFO | min output length: 12
2026-01-28 20:51:12,419 | INFO | end detected at 104
2026-01-28 20:51:12,420 | INFO |  -7.92 * 0.5 =  -3.96 for decoder
2026-01-28 20:51:12,421 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-28 20:51:12,421 | INFO | total log probability: -3.97
2026-01-28 20:51:12,421 | INFO | normalized log probability: -0.04
2026-01-28 20:51:12,421 | INFO | total number of ended hypotheses: 153
2026-01-28 20:51:12,422 | INFO | best hypo: cet<space>engagement<space>sur<space>le<space>terrain<space>est<space>pour<space>vous<space>de<space>plus<space>en<space>plus<space>difficile<space>et<space>de<space>plus<space>en<space>plus<space>dangereux

2026-01-28 20:51:12,424 | INFO | speech length: 17280
2026-01-28 20:51:12,459 | INFO | decoder input length: 26
2026-01-28 20:51:12,459 | INFO | max output length: 26
2026-01-28 20:51:12,460 | INFO | min output length: 2
2026-01-28 20:51:13,130 | INFO | end detected at 19
2026-01-28 20:51:13,131 | INFO |  -1.15 * 0.5 =  -0.58 for decoder
2026-01-28 20:51:13,131 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-28 20:51:13,131 | INFO | total log probability: -0.58
2026-01-28 20:51:13,131 | INFO | normalized log probability: -0.04
2026-01-28 20:51:13,131 | INFO | total number of ended hypotheses: 137
2026-01-28 20:51:13,132 | INFO | best hypo: l'afghanistan

2026-01-28 20:51:13,133 | INFO | speech length: 16000
2026-01-28 20:51:13,174 | INFO | decoder input length: 24
2026-01-28 20:51:13,174 | INFO | max output length: 24
2026-01-28 20:51:13,174 | INFO | min output length: 2
2026-01-28 20:51:13,932 | INFO | end detected at 22
2026-01-28 20:51:13,933 | INFO |  -1.38 * 0.5 =  -0.69 for decoder
2026-01-28 20:51:13,933 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 20:51:13,933 | INFO | total log probability: -0.69
2026-01-28 20:51:13,933 | INFO | normalized log probability: -0.04
2026-01-28 20:51:13,933 | INFO | total number of ended hypotheses: 139
2026-01-28 20:51:13,934 | INFO | best hypo: le<space>proche<space>orient

2026-01-28 20:51:13,935 | INFO | speech length: 106240
2026-01-28 20:51:13,972 | INFO | decoder input length: 165
2026-01-28 20:51:13,972 | INFO | max output length: 165
2026-01-28 20:51:13,972 | INFO | min output length: 16
2026-01-28 20:51:19,717 | INFO | end detected at 123
2026-01-28 20:51:19,719 | INFO | -13.25 * 0.5 =  -6.62 for decoder
2026-01-28 20:51:19,719 | INFO |  -2.52 * 0.5 =  -1.26 for ctc
2026-01-28 20:51:19,719 | INFO | total log probability: -7.89
2026-01-28 20:51:19,719 | INFO | normalized log probability: -0.07
2026-01-28 20:51:19,719 | INFO | total number of ended hypotheses: 182
2026-01-28 20:51:19,721 | INFO | best hypo: je<space>connais<space>la<space>somme<space>de<space>courage<space>et<space>d'abnégation<space>que<space>requièrent<space>l'accomplissement<space>de<space>vos<space>missions<space>dans<space>un<space>tel<space>contexte

2026-01-28 20:51:19,723 | INFO | speech length: 45920
2026-01-28 20:51:19,763 | INFO | decoder input length: 71
2026-01-28 20:51:19,763 | INFO | max output length: 71
2026-01-28 20:51:19,763 | INFO | min output length: 7
2026-01-28 20:51:22,108 | INFO | end detected at 62
2026-01-28 20:51:22,109 | INFO |  -4.56 * 0.5 =  -2.28 for decoder
2026-01-28 20:51:22,109 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-28 20:51:22,109 | INFO | total log probability: -2.28
2026-01-28 20:51:22,109 | INFO | normalized log probability: -0.04
2026-01-28 20:51:22,109 | INFO | total number of ended hypotheses: 156
2026-01-28 20:51:22,110 | INFO | best hypo: je<space>sais<space>également<space>ce<space>que<space>cela<space>signifie<space>pour<space>vos<space>familles

2026-01-28 20:51:22,112 | INFO | speech length: 28480
2026-01-28 20:51:22,156 | INFO | decoder input length: 44
2026-01-28 20:51:22,156 | INFO | max output length: 44
2026-01-28 20:51:22,156 | INFO | min output length: 4
2026-01-28 20:51:23,607 | INFO | end detected at 41
2026-01-28 20:51:23,608 | INFO |  -2.88 * 0.5 =  -1.44 for decoder
2026-01-28 20:51:23,608 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 20:51:23,608 | INFO | total log probability: -1.44
2026-01-28 20:51:23,608 | INFO | normalized log probability: -0.04
2026-01-28 20:51:23,608 | INFO | total number of ended hypotheses: 145
2026-01-28 20:51:23,609 | INFO | best hypo: que<space>je<space>veux<space>saluer<space>particulièrement

2026-01-28 20:51:23,611 | INFO | speech length: 82240
2026-01-28 20:51:23,665 | INFO | decoder input length: 128
2026-01-28 20:51:23,665 | INFO | max output length: 128
2026-01-28 20:51:23,665 | INFO | min output length: 12
2026-01-28 20:51:30,100 | INFO | end detected at 90
2026-01-28 20:51:30,101 | INFO |  -6.99 * 0.5 =  -3.49 for decoder
2026-01-28 20:51:30,101 | INFO |  -0.17 * 0.5 =  -0.09 for ctc
2026-01-28 20:51:30,101 | INFO | total log probability: -3.58
2026-01-28 20:51:30,101 | INFO | normalized log probability: -0.04
2026-01-28 20:51:30,101 | INFO | total number of ended hypotheses: 156
2026-01-28 20:51:30,103 | INFO | best hypo: dont<space>j'imagine<space>qu'elles<space>sont<space>souvent<space>confrontées<space>à<space>l'absence<space>et<space>parfois<space>à<space>l'angoisse

2026-01-28 20:51:30,105 | INFO | speech length: 62880
2026-01-28 20:51:30,148 | INFO | decoder input length: 97
2026-01-28 20:51:30,148 | INFO | max output length: 97
2026-01-28 20:51:30,148 | INFO | min output length: 9
2026-01-28 20:51:33,503 | INFO | end detected at 85
2026-01-28 20:51:33,504 | INFO |  -6.34 * 0.5 =  -3.17 for decoder
2026-01-28 20:51:33,504 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-28 20:51:33,504 | INFO | total log probability: -3.17
2026-01-28 20:51:33,504 | INFO | normalized log probability: -0.04
2026-01-28 20:51:33,504 | INFO | total number of ended hypotheses: 161
2026-01-28 20:51:33,505 | INFO | best hypo: je<space>sais<space>aussi<space>hélas<space>le<space>lourd<space>tribut<space>payé<space>par<space>certains<space>de<space>vos<space>compagnons<space>d'armes

2026-01-28 20:51:33,507 | INFO | speech length: 47840
2026-01-28 20:51:33,548 | INFO | decoder input length: 74
2026-01-28 20:51:33,549 | INFO | max output length: 74
2026-01-28 20:51:33,549 | INFO | min output length: 7
2026-01-28 20:51:35,588 | INFO | end detected at 52
2026-01-28 20:51:35,589 | INFO |  -7.36 * 0.5 =  -3.68 for decoder
2026-01-28 20:51:35,589 | INFO |  -0.77 * 0.5 =  -0.39 for ctc
2026-01-28 20:51:35,589 | INFO | total log probability: -4.07
2026-01-28 20:51:35,589 | INFO | normalized log probability: -0.09
2026-01-28 20:51:35,589 | INFO | total number of ended hypotheses: 165
2026-01-28 20:51:35,590 | INFO | best hypo: tribut<space>qui<space>peut<space>aller<space>jusqu'au<space>sacrifice<space>ulté

2026-01-28 20:51:35,599 | INFO | Chunk: 0 | WER=66.666667 | S=0 D=0 I=2
2026-01-28 20:51:35,599 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:51:35,600 | INFO | Chunk: 2 | WER=16.666667 | S=1 D=0 I=1
2026-01-28 20:51:35,600 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:51:35,600 | INFO | Chunk: 4 | WER=25.000000 | S=2 D=0 I=0
2026-01-28 20:51:35,601 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:51:35,601 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:51:35,601 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:51:35,602 | INFO | Chunk: 8 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:51:35,602 | INFO | Chunk: 9 | WER=14.285714 | S=1 D=0 I=0
2026-01-28 20:51:35,602 | INFO | Chunk: 10 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:51:35,602 | INFO | Chunk: 11 | WER=100.000000 | S=2 D=0 I=2
2026-01-28 20:51:35,603 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:51:35,603 | INFO | Chunk: 13 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:51:35,604 | INFO | Chunk: 14 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:51:35,604 | INFO | Chunk: 15 | WER=15.000000 | S=2 D=0 I=1
2026-01-28 20:51:35,605 | INFO | Chunk: 16 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:51:35,605 | INFO | Chunk: 17 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:51:35,605 | INFO | Chunk: 18 | WER=11.764706 | S=2 D=0 I=0
2026-01-28 20:51:35,605 | INFO | Chunk: 19 | WER=28.571429 | S=2 D=0 I=0
2026-01-28 20:51:35,606 | INFO | Chunk: 20 | WER=100.000000 | S=2 D=0 I=0
2026-01-28 20:51:35,606 | INFO | Chunk: 21 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:51:35,606 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:51:35,607 | INFO | Chunk: 23 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:51:35,607 | INFO | Chunk: 24 | WER=15.384615 | S=2 D=0 I=0
2026-01-28 20:51:35,607 | INFO | Chunk: 25 | WER=5.263158 | S=1 D=0 I=0
2026-01-28 20:51:35,608 | INFO | Chunk: 26 | WER=4.761905 | S=0 D=0 I=1
2026-01-28 20:51:35,608 | INFO | Chunk: 27 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:51:35,609 | INFO | Chunk: 28 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:51:35,609 | INFO | Chunk: 29 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:51:35,609 | INFO | Chunk: 30 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:51:35,609 | INFO | Chunk: 31 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:51:35,610 | INFO | Chunk: 32 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:51:35,610 | INFO | Chunk: 33 | WER=14.285714 | S=2 D=0 I=0
2026-01-28 20:51:35,610 | INFO | Chunk: 34 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:51:35,611 | INFO | Chunk: 35 | WER=66.666667 | S=2 D=0 I=0
2026-01-28 20:51:35,611 | INFO | Chunk: 36 | WER=4.000000 | S=0 D=1 I=0
2026-01-28 20:51:35,611 | INFO | Chunk: 37 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:51:35,612 | INFO | Chunk: 38 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:51:35,613 | INFO | Chunk: 39 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:51:35,613 | INFO | Chunk: 40 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:51:35,613 | INFO | Chunk: 41 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:51:35,613 | INFO | Chunk: 42 | WER=5.000000 | S=1 D=0 I=0
2026-01-28 20:51:35,614 | INFO | Chunk: 43 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:51:35,614 | INFO | Chunk: 44 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:51:35,614 | INFO | Chunk: 45 | WER=6.666667 | S=0 D=0 I=1
2026-01-28 20:51:35,615 | INFO | Chunk: 46 | WER=6.666667 | S=1 D=0 I=0
2026-01-28 20:51:35,615 | INFO | Chunk: 47 | WER=12.500000 | S=1 D=0 I=0
2026-01-28 20:51:35,770 | INFO | File: Rhap-M2001.wav | WER=6.179775 | S=24 D=1 I=8
2026-01-28 20:51:35,770 | INFO | ------------------------------
2026-01-28 20:51:35,770 | INFO | Conf ester Done!
2026-01-28 20:55:48,321 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:55:48,322 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:55:48,322 | INFO | Chunk: 2 | WER=33.333333 | S=3 D=0 I=1
2026-01-28 20:55:48,323 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:55:48,323 | INFO | Chunk: 4 | WER=25.000000 | S=2 D=0 I=0
2026-01-28 20:55:48,324 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:55:48,325 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:55:48,325 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:55:48,325 | INFO | Chunk: 8 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:55:48,326 | INFO | Chunk: 9 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:55:48,326 | INFO | Chunk: 10 | WER=16.666667 | S=1 D=0 I=0
2026-01-28 20:55:48,326 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:55:48,327 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:55:48,327 | INFO | Chunk: 13 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:55:48,328 | INFO | Chunk: 14 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:55:48,329 | INFO | Chunk: 15 | WER=30.000000 | S=3 D=0 I=3
2026-01-28 20:55:48,329 | INFO | Chunk: 16 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:55:48,329 | INFO | Chunk: 17 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:55:48,330 | INFO | Chunk: 18 | WER=11.764706 | S=1 D=1 I=0
2026-01-28 20:55:48,330 | INFO | Chunk: 19 | WER=57.142857 | S=3 D=1 I=0
2026-01-28 20:55:48,331 | INFO | Chunk: 20 | WER=100.000000 | S=2 D=0 I=0
2026-01-28 20:55:48,331 | INFO | Chunk: 21 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:55:48,331 | INFO | Chunk: 22 | WER=14.285714 | S=1 D=0 I=0
2026-01-28 20:55:48,332 | INFO | Chunk: 23 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:55:48,332 | INFO | Chunk: 24 | WER=7.692308 | S=1 D=0 I=0
2026-01-28 20:55:48,333 | INFO | Chunk: 25 | WER=5.263158 | S=1 D=0 I=0
2026-01-28 20:55:48,334 | INFO | Chunk: 26 | WER=4.761905 | S=0 D=0 I=1
2026-01-28 20:55:48,334 | INFO | Chunk: 27 | WER=14.285714 | S=1 D=0 I=0
2026-01-28 20:55:48,334 | INFO | Chunk: 28 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:55:48,335 | INFO | Chunk: 29 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:55:48,335 | INFO | Chunk: 30 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:55:48,336 | INFO | Chunk: 31 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:55:48,336 | INFO | Chunk: 32 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:55:48,337 | INFO | Chunk: 33 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:55:48,337 | INFO | Chunk: 34 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:55:48,337 | INFO | Chunk: 35 | WER=33.333333 | S=0 D=0 I=1
2026-01-28 20:55:48,338 | INFO | Chunk: 36 | WER=8.000000 | S=1 D=1 I=0
2026-01-28 20:55:48,338 | INFO | Chunk: 37 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:55:48,339 | INFO | Chunk: 38 | WER=13.636364 | S=2 D=1 I=0
2026-01-28 20:55:48,340 | INFO | Chunk: 39 | WER=10.526316 | S=2 D=0 I=0
2026-01-28 20:55:48,340 | INFO | Chunk: 40 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:55:48,340 | INFO | Chunk: 41 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:55:48,341 | INFO | Chunk: 42 | WER=5.000000 | S=1 D=0 I=0
2026-01-28 20:55:48,341 | INFO | Chunk: 43 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:55:48,342 | INFO | Chunk: 44 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 20:55:48,342 | INFO | Chunk: 45 | WER=6.666667 | S=1 D=0 I=0
2026-01-28 20:55:48,343 | INFO | Chunk: 46 | WER=6.666667 | S=1 D=0 I=0
2026-01-28 20:55:48,343 | INFO | Chunk: 47 | WER=12.500000 | S=1 D=0 I=0
2026-01-28 20:55:48,555 | INFO | File: Rhap-M2001.wav | WER=7.116105 | S=28 D=4 I=6
2026-01-28 20:55:48,555 | INFO | ------------------------------
2026-01-28 20:55:48,555 | INFO | hmm_tdnn Done!
2026-01-28 20:55:48,773 | INFO | ==================================Rhap-M2002.wav=========================================
2026-01-28 20:55:49,040 | INFO | Using rVAD model
2026-01-28 20:56:36,574 | INFO | Chunk: 0 | WER=13.157895 | S=2 D=3 I=0
2026-01-28 20:56:36,580 | INFO | Chunk: 1 | WER=12.371134 | S=6 D=5 I=1
2026-01-28 20:56:36,584 | INFO | Chunk: 2 | WER=22.471910 | S=4 D=16 I=0
2026-01-28 20:56:36,589 | INFO | Chunk: 3 | WER=10.344828 | S=4 D=5 I=0
2026-01-28 20:56:36,592 | INFO | Chunk: 4 | WER=36.363636 | S=14 D=18 I=0
2026-01-28 20:56:36,595 | INFO | Chunk: 5 | WER=10.169492 | S=3 D=3 I=0
2026-01-28 20:56:36,601 | INFO | Chunk: 6 | WER=13.207547 | S=7 D=6 I=1
2026-01-28 20:56:36,605 | INFO | Chunk: 7 | WER=18.421053 | S=7 D=7 I=0
2026-01-28 20:56:36,609 | INFO | Chunk: 8 | WER=4.597701 | S=3 D=1 I=0
2026-01-28 20:56:36,615 | INFO | Chunk: 9 | WER=17.757009 | S=6 D=13 I=0
2026-01-28 20:56:36,620 | INFO | Chunk: 10 | WER=10.869565 | S=7 D=2 I=1
2026-01-28 20:56:36,624 | INFO | Chunk: 11 | WER=19.480519 | S=6 D=7 I=2
2026-01-28 20:56:36,628 | INFO | Chunk: 12 | WER=7.317073 | S=3 D=3 I=0
2026-01-28 20:56:36,631 | INFO | Chunk: 13 | WER=16.438356 | S=4 D=8 I=0
2026-01-28 20:56:36,635 | INFO | Chunk: 14 | WER=18.181818 | S=6 D=7 I=1
2026-01-28 20:56:37,410 | INFO | File: Rhap-M2002.wav | WER=15.546559 | S=82 D=104 I=6
2026-01-28 20:56:37,412 | INFO | ------------------------------
2026-01-28 20:56:37,412 | INFO | w2vec vad chunk Done!
2026-01-28 20:57:18,852 | INFO | Chunk: 0 | WER=39.473684 | S=0 D=15 I=0
2026-01-28 20:57:18,856 | INFO | Chunk: 1 | WER=61.855670 | S=13 D=47 I=0
2026-01-28 20:57:18,860 | INFO | Chunk: 2 | WER=57.303371 | S=1 D=50 I=0
2026-01-28 20:57:18,862 | INFO | Chunk: 3 | WER=68.965517 | S=0 D=60 I=0
2026-01-28 20:57:18,867 | INFO | Chunk: 4 | WER=60.227273 | S=22 D=31 I=0
2026-01-28 20:57:18,870 | INFO | Chunk: 5 | WER=35.593220 | S=3 D=18 I=0
2026-01-28 20:57:18,874 | INFO | Chunk: 6 | WER=64.150943 | S=11 D=57 I=0
2026-01-28 20:57:18,877 | INFO | Chunk: 7 | WER=57.894737 | S=4 D=40 I=0
2026-01-28 20:57:18,879 | INFO | Chunk: 8 | WER=83.908046 | S=2 D=71 I=0
2026-01-28 20:57:18,885 | INFO | Chunk: 9 | WER=54.205607 | S=12 D=46 I=0
2026-01-28 20:57:18,888 | INFO | Chunk: 10 | WER=70.652174 | S=0 D=65 I=0
2026-01-28 20:57:18,891 | INFO | Chunk: 11 | WER=57.142857 | S=5 D=38 I=1
2026-01-28 20:57:18,895 | INFO | Chunk: 12 | WER=62.195122 | S=19 D=32 I=0
2026-01-28 20:57:18,898 | INFO | Chunk: 13 | WER=50.684932 | S=1 D=36 I=0
2026-01-28 20:57:18,901 | INFO | Chunk: 14 | WER=55.844156 | S=0 D=43 I=0
2026-01-28 20:57:19,449 | INFO | File: Rhap-M2002.wav | WER=60.080972 | S=94 D=648 I=0
2026-01-28 20:57:19,450 | INFO | ------------------------------
2026-01-28 20:57:19,450 | INFO | whisper med Done!
2026-01-28 20:58:09,335 | INFO | Chunk: 0 | WER=7.894737 | S=0 D=3 I=0
2026-01-28 20:58:09,338 | INFO | Chunk: 1 | WER=67.010309 | S=1 D=64 I=0
2026-01-28 20:58:09,341 | INFO | Chunk: 2 | WER=48.314607 | S=3 D=40 I=0
2026-01-28 20:58:09,343 | INFO | Chunk: 3 | WER=68.965517 | S=0 D=60 I=0
2026-01-28 20:58:09,345 | INFO | Chunk: 4 | WER=68.181818 | S=3 D=57 I=0
2026-01-28 20:58:09,347 | INFO | Chunk: 5 | WER=35.593220 | S=2 D=19 I=0
2026-01-28 20:58:09,351 | INFO | Chunk: 6 | WER=50.943396 | S=15 D=39 I=0
2026-01-28 20:58:09,354 | INFO | Chunk: 7 | WER=38.157895 | S=3 D=26 I=0
2026-01-28 20:58:09,355 | INFO | Chunk: 8 | WER=85.057471 | S=3 D=71 I=0
2026-01-28 20:58:09,359 | INFO | Chunk: 9 | WER=62.616822 | S=4 D=63 I=0
2026-01-28 20:58:09,361 | INFO | Chunk: 10 | WER=71.739130 | S=1 D=65 I=0
2026-01-28 20:58:09,363 | INFO | Chunk: 11 | WER=62.337662 | S=5 D=41 I=2
2026-01-28 20:58:09,365 | INFO | Chunk: 12 | WER=65.853659 | S=7 D=47 I=0
2026-01-28 20:58:09,368 | INFO | Chunk: 13 | WER=50.684932 | S=19 D=18 I=0
2026-01-28 20:58:09,370 | INFO | Chunk: 14 | WER=62.337662 | S=1 D=47 I=0
2026-01-28 20:58:09,766 | INFO | File: Rhap-M2002.wav | WER=58.947368 | S=68 D=659 I=1
2026-01-28 20:58:09,766 | INFO | ------------------------------
2026-01-28 20:58:09,766 | INFO | whisper large Done!
2026-01-28 20:58:09,945 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 20:58:09,983 | INFO | Vocabulary size: 350
2026-01-28 20:58:10,940 | INFO | Gradient checkpoint layers: []
2026-01-28 20:58:11,718 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 20:58:11,723 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 20:58:11,724 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 20:58:11,724 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 20:58:11,724 | INFO | speech length: 179360
2026-01-28 20:58:11,780 | INFO | decoder input length: 279
2026-01-28 20:58:11,780 | INFO | max output length: 279
2026-01-28 20:58:11,780 | INFO | min output length: 27
2026-01-28 20:58:18,292 | INFO | end detected at 90
2026-01-28 20:58:18,294 | INFO | -32.84 * 0.5 = -16.42 for decoder
2026-01-28 20:58:18,295 | INFO |  -6.22 * 0.5 =  -3.11 for ctc
2026-01-28 20:58:18,295 | INFO | total log probability: -19.53
2026-01-28 20:58:18,295 | INFO | normalized log probability: -0.24
2026-01-28 20:58:18,295 | INFO | total number of ended hypotheses: 215
2026-01-28 20:58:18,296 | INFO | best hypo: ▁c'est▁une▁conférence▁de▁de▁philosophie▁entre▁guillemet▁et▁c'est▁aussi▁une▁conférence▁d'histoire▁de▁l'art▁conférence▁d'esthétique▁parce▁que▁comme▁vous▁verrez▁chez▁une▁approche▁de

2026-01-28 20:58:18,300 | INFO | speech length: 447840
2026-01-28 20:58:18,351 | INFO | decoder input length: 699
2026-01-28 20:58:18,351 | INFO | max output length: 699
2026-01-28 20:58:18,351 | INFO | min output length: 69
2026-01-28 20:58:44,594 | INFO | end detected at 211
2026-01-28 20:58:44,597 | INFO | -539.48 * 0.5 = -269.74 for decoder
2026-01-28 20:58:44,597 | INFO | -158.07 * 0.5 = -79.03 for ctc
2026-01-28 20:58:44,597 | INFO | total log probability: -348.77
2026-01-28 20:58:44,597 | INFO | normalized log probability: -1.71
2026-01-28 20:58:44,597 | INFO | total number of ended hypotheses: 167
2026-01-28 20:58:44,600 | INFO | best hypo: ▁qu'est▁d'assez▁assez▁généraliste▁et▁qui▁mêle▁plusieurs▁points▁de▁vue▁et▁donc▁pas▁seulement▁le▁point▁de▁vue▁de▁la▁philosophie▁mais▁aussi▁celui▁de▁l'histoire▁celui▁de▁la▁sociologie▁etc▁etc▁vous▁j'avais▁traitement▁mon▁sujet▁injuieux▁comme▁je▁suis▁quelqu'un▁de▁la▁disciplinée▁j'vais▁traités▁mon▁sujet▁l'artr▁est▁il▁le▁refle▁de▁la▁société▁et▁la▁réponseu▁questions▁réponse▁que▁vous▁allez▁voir▁pas▁tout▁de▁suite▁'est▁hui▁là▁refle▁de▁la▁société▁mais▁pas▁tout▁le▁temps

2026-01-28 20:58:44,602 | INFO | speech length: 356320
2026-01-28 20:58:44,649 | INFO | decoder input length: 556
2026-01-28 20:58:44,649 | INFO | max output length: 556
2026-01-28 20:58:44,649 | INFO | min output length: 55
2026-01-28 20:59:01,508 | INFO | end detected at 164
2026-01-28 20:59:01,510 | INFO | -234.60 * 0.5 = -117.30 for decoder
2026-01-28 20:59:01,510 | INFO | -72.10 * 0.5 = -36.05 for ctc
2026-01-28 20:59:01,510 | INFO | total log probability: -153.35
2026-01-28 20:59:01,510 | INFO | normalized log probability: -0.98
2026-01-28 20:59:01,510 | INFO | total number of ended hypotheses: 169
2026-01-28 20:59:01,512 | INFO | best hypo: ▁eh▁pas▁et▁pas▁sous▁tous▁les▁aspects▁lire▁c'est▁une▁réponse▁un▁peu▁de▁à▁peu▁trop▁nuancé▁mais▁il▁n'est▁pas▁tout▁le▁temps▁et▁vous▁verrez▁qu'à▁des▁formes▁d'art▁qui▁ne▁sont▁pas▁le▁reflet▁de▁la▁société▁et▁tout▁cas▁même▁dans▁l'art▁est▁le▁reflet▁de▁la▁société▁il▁n'est▁pas▁sou▁toutes▁ces▁dimensions▁mais▁sur▁tous▁ces▁apects▁je▁commencerais▁par▁une▁remarque▁préalable

2026-01-28 20:59:01,515 | INFO | speech length: 424800
2026-01-28 20:59:01,565 | INFO | decoder input length: 663
2026-01-28 20:59:01,565 | INFO | max output length: 663
2026-01-28 20:59:01,565 | INFO | min output length: 66
2026-01-28 20:59:24,924 | INFO | end detected at 200
2026-01-28 20:59:24,926 | INFO | -571.54 * 0.5 = -285.77 for decoder
2026-01-28 20:59:24,926 | INFO | -134.12 * 0.5 = -67.06 for ctc
2026-01-28 20:59:24,926 | INFO | total log probability: -352.83
2026-01-28 20:59:24,926 | INFO | normalized log probability: -1.83
2026-01-28 20:59:24,926 | INFO | total number of ended hypotheses: 158
2026-01-28 20:59:24,929 | INFO | best hypo: ▁qui▁est▁très▁importante▁mais▁que▁les▁philosophes▁ont▁beaucoup▁de▁mal▁à▁faire▁c'est▁d'admettre▁que▁la▁notion▁d'art▁est▁une▁notion▁floue▁de▁dire▁que▁elle▁est▁informe▁indique▁que▁'on▁peut▁dire▁'importe▁quoire▁mais▁c'est▁un▁concept▁c'est▁ce▁qu'on▁apptelle▁un▁concept▁flo▁c'est▁un▁philophe▁américain'▁disciple▁du▁philosophe▁anglo▁autrichien▁wit▁guinenstein▁qui▁a▁avancé▁cette▁idée▁de▁l'artau▁comme▁concept▁est▁de▁dire

2026-01-28 20:59:24,931 | INFO | speech length: 420960
2026-01-28 20:59:24,986 | INFO | decoder input length: 657
2026-01-28 20:59:24,987 | INFO | max output length: 657
2026-01-28 20:59:24,987 | INFO | min output length: 65
2026-01-28 20:59:48,232 | INFO | end detected at 180
2026-01-28 20:59:48,234 | INFO | -402.77 * 0.5 = -201.38 for decoder
2026-01-28 20:59:48,234 | INFO | -110.12 * 0.5 = -55.06 for ctc
2026-01-28 20:59:48,234 | INFO | total log probability: -256.45
2026-01-28 20:59:48,234 | INFO | normalized log probability: -1.47
2026-01-28 20:59:48,234 | INFO | total number of ended hypotheses: 157
2026-01-28 20:59:48,237 | INFO | best hypo: ▁un▁art▁dont▁les▁l'art▁est▁un▁concept▁dont▁les▁contours▁sont▁changeants▁s'échangeant▁selon▁les▁sociétés▁s'échangeant▁selon▁selon▁les▁formes▁d'art▁considérées▁et▁'▁il▁faut▁faut▁apprendre▁à▁s'i▁'▁diresse▁se▁fairent▁à▁ces▁ris▁et▁il▁est▁évident▁qui▁appartent▁des▁formes▁d'art▁qui▁sont▁très▁liés▁à▁la▁connaiss▁par▁l'art▁de▁la▁renaiss▁l'art▁de▁la▁renaissance▁est▁vu▁comme▁un

2026-01-28 20:59:48,239 | INFO | speech length: 319840
2026-01-28 20:59:48,276 | INFO | decoder input length: 499
2026-01-28 20:59:48,276 | INFO | max output length: 499
2026-01-28 20:59:48,276 | INFO | min output length: 49
2026-01-28 21:00:01,016 | INFO | end detected at 134
2026-01-28 21:00:01,018 | INFO | -231.26 * 0.5 = -115.63 for decoder
2026-01-28 21:00:01,018 | INFO | -45.88 * 0.5 = -22.94 for ctc
2026-01-28 21:00:01,019 | INFO | total log probability: -138.57
2026-01-28 21:00:01,019 | INFO | normalized log probability: -1.07
2026-01-28 21:00:01,019 | INFO | total number of ended hypotheses: 172
2026-01-28 21:00:01,020 | INFO | best hypo: ▁un▁art▁descriptif▁qui▁doit▁nous▁représenter▁les▁choses▁telles▁qu'elles▁sont▁elles▁sont▁correctement▁vues▁et▁puis▁à▁des▁formes▁d'art▁qui▁sont▁beaucoup▁plus▁édonistes▁et▁qui▁se▁préoccupent▁très▁peu▁d'eux▁de▁de▁la▁connaissance▁l'ar▁est▁un▁concept▁flou▁et▁il▁faut▁faire▁un▁certain▁nombre▁de▁distinctions

2026-01-28 21:00:01,023 | INFO | speech length: 474240
2026-01-28 21:00:01,071 | INFO | decoder input length: 740
2026-01-28 21:00:01,071 | INFO | max output length: 740
2026-01-28 21:00:01,071 | INFO | min output length: 74
2026-01-28 21:00:34,414 | INFO | end detected at 228
2026-01-28 21:00:34,415 | INFO | -641.24 * 0.5 = -320.62 for decoder
2026-01-28 21:00:34,415 | INFO | -173.82 * 0.5 = -86.91 for ctc
2026-01-28 21:00:34,415 | INFO | total log probability: -407.53
2026-01-28 21:00:34,416 | INFO | normalized log probability: -1.83
2026-01-28 21:00:34,416 | INFO | total number of ended hypotheses: 154
2026-01-28 21:00:34,419 | INFO | best hypo: ▁ce▁sera▁juste▁des▁distinctions▁préliminaires▁d'abord▁une▁grande▁distinction▁que▁l'on▁oublie▁alors▁que▁tout▁le▁monde▁la▁connaît▁et▁tout▁le▁monde▁c'est▁de▁quoi▁il▁s'agit▁pour▁la▁distinction▁entre▁l'art▁en▁tant▁qu'on▁le▁fabrique▁qu'on▁le▁produit▁qu'on▁le▁pratique▁depuis▁l'art▁à▁tant▁qu'on▁le▁reçoit▁qu'on▁l'expérimente▁et▁qu'on▁et▁qu'on▁l'éprouve▁c'est▁à▁une▁dist'inction▁qui▁dait▁depuis▁le▁la▁plus▁haute▁et▁anticité▁et▁tout▁le▁cas▁est▁depuis▁platon▁c'est▁qu'est▁à▁un▁art▁qui▁cons▁à▁fabriquer▁per▁six▁faire

2026-01-28 21:00:34,422 | INFO | speech length: 464480
2026-01-28 21:00:34,519 | INFO | decoder input length: 725
2026-01-28 21:00:34,519 | INFO | max output length: 725
2026-01-28 21:00:34,519 | INFO | min output length: 72
2026-01-28 21:00:58,510 | INFO | end detected at 187
2026-01-28 21:00:58,512 | INFO | -409.41 * 0.5 = -204.70 for decoder
2026-01-28 21:00:58,512 | INFO | -109.39 * 0.5 = -54.69 for ctc
2026-01-28 21:00:58,512 | INFO | total log probability: -259.40
2026-01-28 21:00:58,512 | INFO | normalized log probability: -1.43
2026-01-28 21:00:58,512 | INFO | total number of ended hypotheses: 164
2026-01-28 21:00:58,514 | INFO | best hypo: ▁un▁de▁la▁vient▁en▁termes▁de▁poésie▁aussi▁curieux▁que▁s'a▁puisse▁paraître▁et▁puis▁il▁a▁un▁autre▁aspect▁qui▁consiste▁à▁éprouver▁l'art▁le▁ressentir▁à▁en▁faire▁l'expérience▁avoir▁et▁sorte▁de▁vécu▁de▁l'expérience▁artisique▁et▁c'est▁ce▁qu'on▁appeelle▁de▁l'aïstéss▁ishésis▁ça▁veut▁dire▁sensation▁en▁grec▁et▁çara▁ça▁à▁cesser▁le▁phénomène▁de▁de▁recevoir▁sans▁ressentir

2026-01-28 21:00:58,517 | INFO | speech length: 424160
2026-01-28 21:00:58,589 | INFO | decoder input length: 662
2026-01-28 21:00:58,589 | INFO | max output length: 662
2026-01-28 21:00:58,589 | INFO | min output length: 66
2026-01-28 21:01:27,478 | INFO | end detected at 220
2026-01-28 21:01:27,480 | INFO | -482.60 * 0.5 = -241.30 for decoder
2026-01-28 21:01:27,480 | INFO | -144.83 * 0.5 = -72.42 for ctc
2026-01-28 21:01:27,480 | INFO | total log probability: -313.71
2026-01-28 21:01:27,480 | INFO | normalized log probability: -1.47
2026-01-28 21:01:27,480 | INFO | total number of ended hypotheses: 182
2026-01-28 21:01:27,483 | INFO | best hypo: ▁et▁et▁ça▁a▁donné▁sipoïésis▁a▁donné▁le▁mot▁poésie▁aïstécis▁a▁donné▁le▁terme▁esthétique▁et▁ce▁qui▁est▁intéressant▁à▁souligner▁eux▁si▁vous▁poursuivez▁des▁études▁d'histoire▁de▁l'art▁par▁exemple▁oule▁verrez▁très▁bien▁c'est▁pendant▁très▁l'gtemps▁n'a▁a▁pas▁beaucoup▁parlée▁d'esthétique▁on▁n'a▁a▁pas▁beaucoup▁parlé▁de▁l'expériencee▁restentie▁de▁l'art▁et▁on▁a▁parlé▁de▁surtout▁de▁l'art▁en▁tant▁de▁fabrication▁parce▁que▁c'était▁le▁une▁afaire▁de▁spécialiste

2026-01-28 21:01:27,485 | INFO | speech length: 425760
2026-01-28 21:01:27,543 | INFO | decoder input length: 664
2026-01-28 21:01:27,543 | INFO | max output length: 664
2026-01-28 21:01:27,543 | INFO | min output length: 66
2026-01-28 21:01:53,824 | INFO | end detected at 231
2026-01-28 21:01:53,826 | INFO | -427.14 * 0.5 = -213.57 for decoder
2026-01-28 21:01:53,826 | INFO | -127.73 * 0.5 = -63.86 for ctc
2026-01-28 21:01:53,826 | INFO | total log probability: -277.43
2026-01-28 21:01:53,826 | INFO | normalized log probability: -1.23
2026-01-28 21:01:53,826 | INFO | total number of ended hypotheses: 169
2026-01-28 21:01:53,829 | INFO | best hypo: ▁c'était▁une▁affaire▁aussi▁de▁commanditaire▁les▁qui▁ce▁qui▁s'est▁qui▁commandaient▁les▁oeuvres▁nains▁et▁qui▁les▁achetaient▁en▁quelque▁sorte▁mais▁que▁le▁point▁de▁vueut▁du▁spectateur▁n'avait▁pas▁beaucoupe▁d'importance▁eux▁jusqu'à▁la▁renaissant▁et▁y▁compris▁un▁peulont▁jusqu'au▁vie▁siècle▁le▁point▁de▁vue▁de▁la▁fabriction▁le▁point▁de▁vue▁de▁la▁production▁de▁l'art▁et▁plus▁important▁que▁le▁point▁de▁vue▁de▁l'expris▁de▁la▁réception▁de▁l'art▁le▁point▁de▁vue▁du▁spectateur▁point▁de▁vue▁du▁spectateur▁point▁de▁vue▁esthétique

2026-01-28 21:01:53,832 | INFO | speech length: 451360
2026-01-28 21:01:53,895 | INFO | decoder input length: 704
2026-01-28 21:01:53,895 | INFO | max output length: 704
2026-01-28 21:01:53,895 | INFO | min output length: 70
2026-01-28 21:02:19,248 | INFO | end detected at 210
2026-01-28 21:02:19,250 | INFO | -437.81 * 0.5 = -218.90 for decoder
2026-01-28 21:02:19,250 | INFO | -109.12 * 0.5 = -54.56 for ctc
2026-01-28 21:02:19,250 | INFO | total log probability: -273.47
2026-01-28 21:02:19,250 | INFO | normalized log probability: -1.35
2026-01-28 21:02:19,250 | INFO | total number of ended hypotheses: 182
2026-01-28 21:02:19,253 | INFO | best hypo: ▁se▁dégage▁à▁la▁fin▁du▁dix▁septième▁siècle▁au▁xviiième▁siècle▁et▁puis▁va▁c'est▁quelque▁chose▁que▁nous▁sommes▁que▁nous▁avons▁bien▁que▁nous▁connaissons▁bien▁mais▁faites▁vous▁même▁à▁différence▁entre▁votre▁expérience▁pour▁exemple▁si▁vous▁pratiquez▁la▁musique▁ou▁votre▁expérience▁en▁tant▁que▁pratiquant▁la▁musique'▁et▁votre▁expérience▁en▁tant▁que▁connommateurs▁de▁la▁musiques▁qui▁suffit▁que▁vous▁ayez▁un▁baladeur▁pour▁vous▁connaissiez▁un▁expérience▁de▁consmateur▁de▁musique▁comme▁expérience▁de▁la▁réception▁et▁si▁vous▁faites▁de▁la▁musique

2026-01-28 21:02:19,256 | INFO | speech length: 436160
2026-01-28 21:02:19,324 | INFO | decoder input length: 681
2026-01-28 21:02:19,324 | INFO | max output length: 681
2026-01-28 21:02:19,324 | INFO | min output length: 68
2026-01-28 21:02:42,680 | INFO | end detected at 197
2026-01-28 21:02:42,682 | INFO | -392.12 * 0.5 = -196.06 for decoder
2026-01-28 21:02:42,682 | INFO | -114.35 * 0.5 = -57.17 for ctc
2026-01-28 21:02:42,683 | INFO | total log probability: -253.23
2026-01-28 21:02:42,683 | INFO | normalized log probability: -1.33
2026-01-28 21:02:42,683 | INFO | total number of ended hypotheses: 181
2026-01-28 21:02:42,685 | INFO | best hypo: ▁eh▁bien▁vous▁avez▁l'expérience▁de▁la▁poétise▁de▁de▁la▁production▁musicale▁une▁première▁distinction▁fondamentale▁parce▁que▁lire▁selon▁les▁sociétés▁je▁dirait▁que▁là▁la▁la▁composante▁de▁production▁ou▁la▁composante▁de▁réception▁e▁varie▁beaucoup▁la▁deuxième▁distinction▁'h▁bien▁et▁faut▁voir▁et▁c'est▁en▁sens▁que▁l'art▁est▁un▁concept▁de▁flo▁c'est▁que▁l'art▁a▁des▁fonnction▁extrêmement▁diverses▁selon▁les▁époque▁et▁selon▁les▁sociétés

2026-01-28 21:02:42,688 | INFO | speech length: 435520
2026-01-28 21:02:42,747 | INFO | decoder input length: 680
2026-01-28 21:02:42,747 | INFO | max output length: 680
2026-01-28 21:02:42,747 | INFO | min output length: 68
2026-01-28 21:03:09,568 | INFO | end detected at 226
2026-01-28 21:03:09,570 | INFO | -488.78 * 0.5 = -244.39 for decoder
2026-01-28 21:03:09,570 | INFO | -136.20 * 0.5 = -68.10 for ctc
2026-01-28 21:03:09,570 | INFO | total log probability: -312.49
2026-01-28 21:03:09,570 | INFO | normalized log probability: -1.42
2026-01-28 21:03:09,570 | INFO | total number of ended hypotheses: 175
2026-01-28 21:03:09,573 | INFO | best hypo: ▁mais▁il▁a▁des▁fonctions▁religieuses▁et▁des▁fonctions▁presque▁sacrées▁quelquefois▁l'art▁est▁utilisé▁dans▁les▁cérémonies▁et▁il▁ne▁doit▁pas▁être▁utilisé▁en▁dehors▁des▁cérémonies▁c'est▁vraix▁encore▁pour▁pas▁mal▁de▁masques▁africains▁par▁exemple▁eux▁bien▁évidemment▁s'est▁vrai▁pour▁la▁peinte▁religieuse'▁en▁occcent▁et▁la▁peinture▁de▁la▁renaissance▁jusqu'au'▁jusqu'au▁sizième▁siècle▁la▁pein▁de▁la▁renaissance▁et▁une▁peinture▁éminemment▁religieuse▁avec▁des▁fonction▁religieuse▁ou▁en▁pas▁vraie

2026-01-28 21:03:09,576 | INFO | speech length: 412160
2026-01-28 21:03:09,638 | INFO | decoder input length: 643
2026-01-28 21:03:09,638 | INFO | max output length: 643
2026-01-28 21:03:09,639 | INFO | min output length: 64
2026-01-28 21:03:28,318 | INFO | end detected at 163
2026-01-28 21:03:28,319 | INFO | -412.31 * 0.5 = -206.15 for decoder
2026-01-28 21:03:28,319 | INFO | -173.54 * 0.5 = -86.77 for ctc
2026-01-28 21:03:28,320 | INFO | total log probability: -292.92
2026-01-28 21:03:28,320 | INFO | normalized log probability: -1.88
2026-01-28 21:03:28,320 | INFO | total number of ended hypotheses: 165
2026-01-28 21:03:28,322 | INFO | best hypo: ▁et▁puis▁il▁peut▁avoir▁des▁fonctions▁politiques▁aussi▁larpeur▁des▁fonctions▁politiques▁et▁peut▁peut▁avoir▁des▁fonctions▁décoratives▁aussi▁penser▁aux▁rôles▁de▁la▁de▁la▁décoration▁par▁exemple▁eux▁quand▁vous▁regardez▁les▁es▁stations▁de▁métro▁qui▁sont▁décorés▁par▁des▁oeuvres▁d'arts▁ou▁avec▁des▁'▁et▁les▁travaux▁et▁des▁'▁il▁peut'avoir▁des▁l'arts▁peu▁avoir▁des▁fonctions▁de▁renaissions▁aussi

2026-01-28 21:03:28,324 | INFO | speech length: 377600
2026-01-28 21:03:28,379 | INFO | decoder input length: 589
2026-01-28 21:03:28,379 | INFO | max output length: 589
2026-01-28 21:03:28,379 | INFO | min output length: 58
2026-01-28 21:03:50,243 | INFO | end detected at 199
2026-01-28 21:03:50,244 | INFO | -313.32 * 0.5 = -156.66 for decoder
2026-01-28 21:03:50,244 | INFO | -37.42 * 0.5 = -18.71 for ctc
2026-01-28 21:03:50,244 | INFO | total log probability: -175.37
2026-01-28 21:03:50,244 | INFO | normalized log probability: -0.90
2026-01-28 21:03:50,244 | INFO | total number of ended hypotheses: 160
2026-01-28 21:03:50,247 | INFO | best hypo: ▁les▁fonctions▁de▁connaissance▁des▁fonctions▁de▁critiques▁y▁a▁beaucoup▁de▁fonctions▁différentes▁dans▁l'art▁et▁encore▁une▁fois▁elles▁sont▁pas▁toutes▁également▁présentes▁selon▁les▁sociétés▁et▁selon▁les▁époques▁et▁a▁des▁sociétés▁ou▁la▁dimensions▁de▁connaissances▁que▁j'aperaiss▁à▁la▁dimension▁cognitives▁et▁plus▁importantes▁y▁a▁des▁sociétés▁où▁la▁dimension▁des▁données▁édonistiques▁le▁plaisir▁que▁donnent▁l'expérience▁artisique▁sont▁plus▁importantes

2026-01-28 21:03:50,258 | INFO | Chunk: 0 | WER=23.684211 | S=3 D=6 I=0
2026-01-28 21:03:50,263 | INFO | Chunk: 1 | WER=32.989691 | S=19 D=10 I=3
2026-01-28 21:03:50,268 | INFO | Chunk: 2 | WER=26.966292 | S=13 D=11 I=0
2026-01-28 21:03:50,272 | INFO | Chunk: 3 | WER=21.839080 | S=9 D=9 I=1
2026-01-28 21:03:50,276 | INFO | Chunk: 4 | WER=32.954545 | S=15 D=14 I=0
2026-01-28 21:03:50,278 | INFO | Chunk: 5 | WER=15.254237 | S=2 D=5 I=2
2026-01-28 21:03:50,285 | INFO | Chunk: 6 | WER=24.528302 | S=12 D=5 I=9
2026-01-28 21:03:50,289 | INFO | Chunk: 7 | WER=30.263158 | S=12 D=7 I=4
2026-01-28 21:03:50,293 | INFO | Chunk: 8 | WER=22.988506 | S=10 D=3 I=7
2026-01-28 21:03:50,300 | INFO | Chunk: 9 | WER=22.429907 | S=15 D=7 I=2
2026-01-28 21:03:50,305 | INFO | Chunk: 10 | WER=14.130435 | S=8 D=2 I=3
2026-01-28 21:03:50,309 | INFO | Chunk: 11 | WER=24.675325 | S=7 D=5 I=7
2026-01-28 21:03:50,314 | INFO | Chunk: 12 | WER=25.609756 | S=13 D=3 I=5
2026-01-28 21:03:50,317 | INFO | Chunk: 13 | WER=28.767123 | S=15 D=5 I=1
2026-01-28 21:03:50,321 | INFO | Chunk: 14 | WER=25.974026 | S=14 D=5 I=1
2026-01-28 21:03:51,132 | INFO | File: Rhap-M2002.wav | WER=25.020243 | S=167 D=97 I=45
2026-01-28 21:03:51,132 | INFO | ------------------------------
2026-01-28 21:03:51,132 | INFO | Conf cv Done!
2026-01-28 21:03:51,313 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 21:03:51,336 | INFO | Vocabulary size: 47
2026-01-28 21:03:52,271 | INFO | Gradient checkpoint layers: []
2026-01-28 21:03:52,941 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 21:03:52,945 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 21:03:52,945 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 21:03:52,946 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 21:03:52,949 | INFO | speech length: 179360
2026-01-28 21:03:53,001 | INFO | decoder input length: 279
2026-01-28 21:03:53,001 | INFO | max output length: 279
2026-01-28 21:03:53,001 | INFO | min output length: 27
2026-01-28 21:04:05,242 | INFO | end detected at 208
2026-01-28 21:04:05,244 | INFO | -16.18 * 0.5 =  -8.09 for decoder
2026-01-28 21:04:05,244 | INFO |  -0.46 * 0.5 =  -0.23 for ctc
2026-01-28 21:04:05,244 | INFO | total log probability: -8.32
2026-01-28 21:04:05,244 | INFO | normalized log probability: -0.04
2026-01-28 21:04:05,244 | INFO | total number of ended hypotheses: 189
2026-01-28 21:04:05,247 | INFO | best hypo: c'est<space>une<space>conférence<space>de<space>de<space>philosophie<space>entre<space>guillemets<space>mais<space>c'est<space>aussi<space>une<space>conférence<space>de<space>d'histoire<space>de<space>l'art<space>une<space>conférence<space>d'esthétique<space>parce<space>que<space>euh<space>comme<space>vous<space>verrez<space>j'ai<space>une<space>approche<space>de<space>de<space>l'art

2026-01-28 21:04:05,249 | INFO | speech length: 447840
2026-01-28 21:04:05,284 | INFO | decoder input length: 699
2026-01-28 21:04:05,284 | INFO | max output length: 699
2026-01-28 21:04:05,284 | INFO | min output length: 69
2026-01-28 21:04:53,763 | INFO | end detected at 500
2026-01-28 21:04:53,764 | INFO | -485.43 * 0.5 = -242.72 for decoder
2026-01-28 21:04:53,764 | INFO | -30.29 * 0.5 = -15.15 for ctc
2026-01-28 21:04:53,764 | INFO | total log probability: -257.86
2026-01-28 21:04:53,764 | INFO | normalized log probability: -0.52
2026-01-28 21:04:53,764 | INFO | total number of ended hypotheses: 155
2026-01-28 21:04:53,770 | INFO | best hypo: qui<space>est<space>assez<space>assez<space>généraliste<space>et<space>qui<space>euh<space>mêle<space>plusieurs<space>euh<space>euh<space>plusieurs<space>points<space>de<space>vue<space>et<space>donc<space>pas<space>seulement<space>le<space>point<space>de<space>vue<space>de<space>la<space>philosophie<space>mais<space>aussi<space>celui<space>de<space>l'histoire<space>celui<space>de<space>la<space>sociologie<space>etc<space>etc<space>donc<space>je<space>vais<space>traider<space>mon<space>sujet<space>hein<space>je<space>veux<space>dire<space>comme<space>je<space>suis<space>quelqu'un<space>de<space>de<space>discipliné<space>jevais<space>traiter<space>euh<space>mon<space>sujet<space>là<space>arêt<space>il<space>le<space>reflet<space>de<space>la<space>société<space>et<space>euh<space>la<space>réponse<space>question<space>réponse<space>vous<space>alez<space>voir<space>tout<space>de<space>suite<space>c'est<space>ou<space>la<space>ré<space>le<space>reflet<space>de<space>la<space>société<space>mais<space>euh<space>pas<space>tout<space>le<space>temps

2026-01-28 21:04:53,773 | INFO | speech length: 356320
2026-01-28 21:04:53,831 | INFO | decoder input length: 556
2026-01-28 21:04:53,831 | INFO | max output length: 556
2026-01-28 21:04:53,831 | INFO | min output length: 55
2026-01-28 21:05:28,702 | INFO | end detected at 398
2026-01-28 21:05:28,703 | INFO | -54.85 * 0.5 = -27.42 for decoder
2026-01-28 21:05:28,704 | INFO |  -8.02 * 0.5 =  -4.01 for ctc
2026-01-28 21:05:28,704 | INFO | total log probability: -31.44
2026-01-28 21:05:28,704 | INFO | normalized log probability: -0.08
2026-01-28 21:05:28,704 | INFO | total number of ended hypotheses: 160
2026-01-28 21:05:28,709 | INFO | best hypo: et<space>pas<space>euh<space>et<space>pas<space>sous<space>tous<space>les<space>aspects<space>hein<space>c'est<space>une<space>réponse<space>un<space>peu<space>de<space>un<space>peu<space>trop<space>nuancée<space>hein<space>mais<space>il<space>est<space>pas<space>tout<space>le<space>temps<space>y<space>a<space>d<space>vous<space>verrez<space>qu'<space>y<space>a<space>des<space>formes<space>d'art<space>qui<space>ne<space>sont<space>pas<space>le<space>reflet<space>de<space>la<space>société<space>et<space>en<space>tout<space>cas<space>même<space>quand<space>l'art<space>et<space>le<space>réflet<space>de<space>la<space>société<space>il<space>n'est<space>pas<space>euh<space>sous<space>toutes<space>ses<space>dimensions<space>ni<space>sur<space>tous<space>ces<space>aspects<space>alors<space>je<space>commencerais<space>par<space>euh<space>une<space>remarque<space>préalable

2026-01-28 21:05:28,711 | INFO | speech length: 424800
2026-01-28 21:05:28,749 | INFO | decoder input length: 663
2026-01-28 21:05:28,749 | INFO | max output length: 663
2026-01-28 21:05:28,749 | INFO | min output length: 66
2026-01-28 21:06:12,534 | INFO | end detected at 469
2026-01-28 21:06:12,535 | INFO | -537.95 * 0.5 = -268.97 for decoder
2026-01-28 21:06:12,535 | INFO | -18.00 * 0.5 =  -9.00 for ctc
2026-01-28 21:06:12,535 | INFO | total log probability: -277.97
2026-01-28 21:06:12,536 | INFO | normalized log probability: -0.60
2026-01-28 21:06:12,536 | INFO | total number of ended hypotheses: 175
2026-01-28 21:06:12,541 | INFO | best hypo: qui<space>est<space>très<space>importante<space>mais<space>que<space>les<space>philosophes<space>ont<space>beaucoup<space>de<space>mal<space>à<space>faire<space>euh<space>c'est<space>d'admettre<space>que<space>la<space>notion<space>d'art<space>euh<space>est<space>une<space>notion<space>floue<space>je<space>veux<space>pas<space>lui<space>dire<space>que<space>elle<space>est<space>informe<space>hein<space>dit<space>que<space>on<space>peut<space>dire<space>n'importe<space>quoi<space>mais<space>c'est<space>un<space>concept<space>c'est<space>ce<space>qu'on<space>appelle<space>un<space>concept<space>de<space>flou<space>c'est<space>un<space>philosophe<space>euh<space>américain<space>euh<space>discible<space>du<space>philosophe<space>euh<space>anglos<space>au<space>autrichien<space>euh<space>wid<space>gunshine<space>qui<space>a<space>euh<space>avancé<space>cette<space>idée<space>de<space>l'art<space>comme<space>concept<space>flou<space>c'est<space>à<space>dire

2026-01-28 21:06:12,544 | INFO | speech length: 420960
2026-01-28 21:06:12,588 | INFO | decoder input length: 657
2026-01-28 21:06:12,588 | INFO | max output length: 657
2026-01-28 21:06:12,588 | INFO | min output length: 65
2026-01-28 21:06:54,345 | INFO | end detected at 436
2026-01-28 21:06:54,347 | INFO | -423.79 * 0.5 = -211.90 for decoder
2026-01-28 21:06:54,347 | INFO | -39.87 * 0.5 = -19.93 for ctc
2026-01-28 21:06:54,347 | INFO | total log probability: -231.83
2026-01-28 21:06:54,347 | INFO | normalized log probability: -0.54
2026-01-28 21:06:54,347 | INFO | total number of ended hypotheses: 156
2026-01-28 21:06:54,353 | INFO | best hypo: un<space>art<space>dont<space>les<space>ann<space>l'art<space>est<space>un<space>concept<space>dont<space>les<space>contours<space>sont<space>changeants<space>échangeants<space>selon<space>les<space>sociétés<space>euh<space>s'échangeant<space>selon<space>euh<space>euh<space>selon<space>les<space>formes<space>d'art<space>euh<space>considérées<space>et<space>euh<space>i<space>faut<space>i<space>faut<space>apprendre<space>à<space>à<space>s<space>je<space>dirais<space>se<space>se<space>faire<space>à<space>ces<space>variations<space>hein<space>il<space>est<space>évident<space>qu'il<space>y<space>a<space>par<space>empre<space>des<space>des<space>formes<space>d'arts<space>qui<space>sont<space>très<space>liées<space>à<space>la<space>connaissance<space>euh<space>part<space>dans<space>l'art<space>de<space>la<space>renaissance<space>l'art<space>d<space>la<space>resaissance<space>est<space>vu<space>commeun

2026-01-28 21:06:54,355 | INFO | speech length: 319840
2026-01-28 21:06:54,413 | INFO | decoder input length: 499
2026-01-28 21:06:54,413 | INFO | max output length: 499
2026-01-28 21:06:54,413 | INFO | min output length: 49
2026-01-28 21:07:47,710 | INFO | end detected at 343
2026-01-28 21:07:47,714 | INFO | -45.58 * 0.5 = -22.79 for decoder
2026-01-28 21:07:47,714 | INFO |  -6.04 * 0.5 =  -3.02 for ctc
2026-01-28 21:07:47,714 | INFO | total log probability: -25.81
2026-01-28 21:07:47,714 | INFO | normalized log probability: -0.08
2026-01-28 21:07:47,714 | INFO | total number of ended hypotheses: 209
2026-01-28 21:07:47,721 | INFO | best hypo: un<space>art<space>descriptif<space>et<space>qui<space>doit<space>nous<space>représenter<space>les<space>choses<space>telles<space>qu'elles<space>sont<space>euh<space>quand<space>elles<space>sont<space>correctement<space>vues<space>et<space>puis<space>y<space>a<space>des<space>formes<space>d'art<space>qui<space>sont<space>beaucoup<space>plus<space>édonistes<space>et<space>qui<space>se<space>préoccupent<space>très<space>peu<space>euh<space>de<space>euh<space>de<space>la<space>connaissance<space>alors<space>euh<space>là<space>est<space>un<space>concept<space>euh<space>flou<space>et<space>euh<space>i<space>faut<space>faire<space>un<space>certain<space>nombre<space>de<space>distinctions

2026-01-28 21:07:47,726 | INFO | speech length: 474240
2026-01-28 21:07:47,784 | INFO | decoder input length: 740
2026-01-28 21:07:47,784 | INFO | max output length: 740
2026-01-28 21:07:47,784 | INFO | min output length: 74
2026-01-28 21:08:56,248 | INFO | end detected at 526
2026-01-28 21:08:56,250 | INFO | -468.22 * 0.5 = -234.11 for decoder
2026-01-28 21:08:56,250 | INFO | -47.21 * 0.5 = -23.60 for ctc
2026-01-28 21:08:56,250 | INFO | total log probability: -257.71
2026-01-28 21:08:56,250 | INFO | normalized log probability: -0.50
2026-01-28 21:08:56,250 | INFO | total number of ended hypotheses: 171
2026-01-28 21:08:56,257 | INFO | best hypo: euh<space>ce<space>sera<space>juste<space>des<space>distinctions<space>préliminaires<space>d'abord<space>y<space>a<space>une<space>grande<space>distinction<space>euh<space>que<space>l'on<space>oublie<space>euh<space>alors<space>que<space>tout<space>le<space>monde<space>la<space>connaît<space>et<space>tout<space>le<space>monde<space>sait<space>de<space>quoi<space>il<space>s'agit<space>pour<space>la<space>distinction<space>entre<space>l'art<space>en<space>tant<space>qu'on<space>le<space>fabrique<space>qu'on<space>le<space>produit<space>qu'on<space>le<space>pratique<space>et<space>puis<space>là<space>en<space>tant<space>qu'on<space>euh<space>le<space>reçoit<space>qu'on<space>l'expérimente<space>et<space>qu'on<space>et<space>qu'on<space>l'éprouve<space>c'est<space>une<space>distinction<space>euh<space>qui<space>date<space>depuis<space>h<space>la<space>plus<space>haute<space>hantiquité<space>e<space>n<space>tout<space>cas<space>de<space>puis<space>platon<space>c'est<space>qui<space>y<space>a<space>un<space>art<space>qui<space>consiste<space>à<space>fabriquer<space>poy<space>ci<space>faire

2026-01-28 21:08:56,261 | INFO | speech length: 464480
2026-01-28 21:08:56,304 | INFO | decoder input length: 725
2026-01-28 21:08:56,304 | INFO | max output length: 725
2026-01-28 21:08:56,304 | INFO | min output length: 72
2026-01-28 21:09:39,168 | INFO | end detected at 403
2026-01-28 21:09:39,170 | INFO | -203.33 * 0.5 = -101.67 for decoder
2026-01-28 21:09:39,170 | INFO | -18.83 * 0.5 =  -9.41 for ctc
2026-01-28 21:09:39,170 | INFO | total log probability: -111.08
2026-01-28 21:09:39,170 | INFO | normalized log probability: -0.28
2026-01-28 21:09:39,170 | INFO | total number of ended hypotheses: 174
2026-01-28 21:09:39,175 | INFO | best hypo: euh<space>de<space>la<space>viande<space>terme<space>de<space>poésie<space>hein<space>euh<space>aussi<space>curieux<space>que<space>ça<space>puisse<space>paraître<space>et<space>puis<space>y<space>a<space>un<space>autre<space>aspect<space>qui<space>consiste<space>à<space>exp<space>éprouver<space>l'art<space>le<space>ressentir<space>euh<space>en<space>faire<space>l'expérience<space>avoir<space>une<space>sorte<space>de<space>de<space>vécu<space>de<space>l'expérience<space>artistique<space>et<space>c'est<space>ce<space>qu'on<space>appelle<space>l'aïstécis<space>euh<space>haïstécis<space>ça<space>veut<space>dire<space>sensation<space>euh<space>en<space>en<space>grec<space>euh<space>et<space>ça<space>ça<space>c'est<space>c'est<space>le<space>phénomène<space>de<space>de<space>recevoir<space>euh<space>s'en<space>ressentir

2026-01-28 21:09:39,178 | INFO | speech length: 424160
2026-01-28 21:09:39,214 | INFO | decoder input length: 662
2026-01-28 21:09:39,214 | INFO | max output length: 662
2026-01-28 21:09:39,215 | INFO | min output length: 66
2026-01-28 21:10:22,503 | INFO | end detected at 462
2026-01-28 21:10:22,504 | INFO | -380.76 * 0.5 = -190.38 for decoder
2026-01-28 21:10:22,504 | INFO | -20.83 * 0.5 = -10.42 for ctc
2026-01-28 21:10:22,504 | INFO | total log probability: -200.80
2026-01-28 21:10:22,504 | INFO | normalized log probability: -0.44
2026-01-28 21:10:22,504 | INFO | total number of ended hypotheses: 159
2026-01-28 21:10:22,510 | INFO | best hypo: et<space>ça<space>a<space>donné<space>euh<space>si<space>poyécis<space>a<space>donné<space>le<space>mot<space>poésie<space>euh<space>a<space>istécis<space>a<space>donné<space>le<space>terme<space>esthétique<space>et<space>ce<space>qui<space>est<space>intéressant<space>à<space>souligner<space>euh<space>si<space>vous<space>poursuivez<space>des<space>études<space>d'histoire<space>de<space>l'art<space>par<space>exemple<space>vous<space>le<space>verrez<space>très<space>bien<space>c'est<space>que<space>pendant<space>très<space>longtemps<space>on<space>n'a<space>pas<space>beaucoup<space>parlé<space>d'esthétique<space>on<space>n'a<space>pas<space>beaucoup<space>parlé<space>de<space>l'expérience<space>ressentie<space>de<space>l'art<space>euh<space>on<space>a<space>parlé<space>surtout<space>de<space>l'art<space>en<space>tan<space>que<space>fabrication<space>parce<space>que<space>c'était<space>une<space>affaire<space>de<space>spécialistes

2026-01-28 21:10:22,512 | INFO | speech length: 425760
2026-01-28 21:10:22,549 | INFO | decoder input length: 664
2026-01-28 21:10:22,549 | INFO | max output length: 664
2026-01-28 21:10:22,549 | INFO | min output length: 66
2026-01-28 21:11:14,119 | INFO | end detected at 540
2026-01-28 21:11:14,122 | INFO | -436.63 * 0.5 = -218.31 for decoder
2026-01-28 21:11:14,122 | INFO | -41.52 * 0.5 = -20.76 for ctc
2026-01-28 21:11:14,122 | INFO | total log probability: -239.07
2026-01-28 21:11:14,122 | INFO | normalized log probability: -0.45
2026-01-28 21:11:14,122 | INFO | total number of ended hypotheses: 204
2026-01-28 21:11:14,128 | INFO | best hypo: c'était<space>une<space>affaire<space>aussi<space>de<space>commanditaires<space>qui<space>s<space>qui<space>s'est<space>qui<space>commandait<space>les<space>oeuvres<space>d'art<space>et<space>qui<space>les<space>achetaient<space>en<space>quelque<space>sorte<space>mais<space>que<space>le<space>point<space>de<space>vue<space>du<space>spectateur<space>avait<space>pas<space>beaucoup<space>d'importance<space>euh<space>jusqu'à<space>la<space>renaissance<space>et<space>y<space>compris<space>l<space>un<space>peu<space>long<space>un<space>p<space>jusqu'au<space>dix<space>huitième<space>siècle<space>le<space>point<space>de<space>vue<space>du<space>de<space>la<space>fabrication<space>le<space>point<space>de<space>vue<space>de<space>la<space>production<space>de<space>l'art<space>est<space>plus<space>important<space>que<space>le<space>point<space>de<space>vue<space>de<space>l'expérience<space>e<space>la<space>réception<space>de<space>l'art<space>euh<space>le<space>point<space>de<space>vue<space>de<space>pectateur<space>point<space>de<space>vue<space>spectateur<space>point<space>de<space>vue<space>esthétique

2026-01-28 21:11:14,131 | INFO | speech length: 451360
2026-01-28 21:11:14,175 | INFO | decoder input length: 704
2026-01-28 21:11:14,176 | INFO | max output length: 704
2026-01-28 21:11:14,176 | INFO | min output length: 70
2026-01-28 21:12:07,938 | INFO | end detected at 568
2026-01-28 21:12:07,939 | INFO | -717.65 * 0.5 = -358.82 for decoder
2026-01-28 21:12:07,939 | INFO | -90.56 * 0.5 = -45.28 for ctc
2026-01-28 21:12:07,939 | INFO | total log probability: -404.11
2026-01-28 21:12:07,939 | INFO | normalized log probability: -0.72
2026-01-28 21:12:07,939 | INFO | total number of ended hypotheses: 167
2026-01-28 21:12:07,947 | INFO | best hypo: se<space>dégagent<space>euh<space>à<space>la<space>fin<space>du<space>dix<space>septième<space>siècle<space>et<space>au<space>dix<space>huitième<space>siècle<space>et<space>puis<space>eh<space>ben<space>c'est<space>quelque<space>chose<space>que<space>nous<space>sommes<space>euh<space>que<space>nous<space>avons<space>bien<space>euh<space>que<space>nous<space>connaissons<space>bien<space>mais<space>faites<space>vous<space>même<space>la<space>différence<space>entre<space>votre<space>expérience<space>par<space>exemple<space>si<space>vous<space>pratiquez<space>la<space>musique<space>votre<space>expérience<space>en<space>tant<space>que<space>pratiquant<space>la<space>musique<space>et<space>votre<space>expérience<space>en<space>tant<space>que<space>consommateur<space>de<space>musique<space>qu'i<space>suffut<space>que<space>vous<space>ayez<space>un<space>falla<space>deur<space>poue<space>que<space>vous<space>connaissiez<space>e<space>de<space>dexpérience<space>de<space>consommateurs<space>de<space>musique<space>cone<space>dexpérence<space>de<space>la<space>réception<space>et<space>si<space>vous<space>faites<space>de<space>la<space>musique

2026-01-28 21:12:07,949 | INFO | speech length: 436160
2026-01-28 21:12:08,006 | INFO | decoder input length: 681
2026-01-28 21:12:08,007 | INFO | max output length: 681
2026-01-28 21:12:08,007 | INFO | min output length: 68
2026-01-28 21:12:56,825 | INFO | end detected at 445
2026-01-28 21:12:56,826 | INFO | -225.96 * 0.5 = -112.98 for decoder
2026-01-28 21:12:56,826 | INFO | -14.59 * 0.5 =  -7.30 for ctc
2026-01-28 21:12:56,826 | INFO | total log probability: -120.27
2026-01-28 21:12:56,827 | INFO | normalized log probability: -0.27
2026-01-28 21:12:56,827 | INFO | total number of ended hypotheses: 151
2026-01-28 21:12:56,832 | INFO | best hypo: eh<space>bien<space>vous<space>avez<space>l'expérience<space>de<space>la<space>poésie<space>de<space>de<space>la<space>production<space>musicale<space>euh<space>première<space>distinction<space>donc<space>fondamentale<space>parce<space>que<space>dire<space>selon<space>les<space>sociétés<space>je<space>dirais<space>que<space>la<space>euh<space>la<space>composante<space>de<space>production<space>ou<space>la<space>composante<space>de<space>réception<space>euh<space>varie<space>beaucoup<space>deuxième<space>distinction<space>eh<space>bien<space>euh<space>i<space>faut<space>voir<space>et<space>c'est<space>en<space>ce<space>sens<space>que<space>là<space>est<space>un<space>concept<space>flou<space>c'est<space>que<space>l'art<space>a<space>des<space>fonctions<space>extrêmement<space>diverses<space>euh<space>selon<space>les<space>époques<space>et<space>selon<space>les<space>sociétés

2026-01-28 21:12:56,835 | INFO | speech length: 435520
2026-01-28 21:12:56,882 | INFO | decoder input length: 680
2026-01-28 21:12:56,882 | INFO | max output length: 680
2026-01-28 21:12:56,882 | INFO | min output length: 68
2026-01-28 21:13:46,374 | INFO | end detected at 523
2026-01-28 21:13:46,376 | INFO | -528.95 * 0.5 = -264.48 for decoder
2026-01-28 21:13:46,376 | INFO | -35.50 * 0.5 = -17.75 for ctc
2026-01-28 21:13:46,376 | INFO | total log probability: -282.23
2026-01-28 21:13:46,376 | INFO | normalized log probability: -0.55
2026-01-28 21:13:46,376 | INFO | total number of ended hypotheses: 204
2026-01-28 21:13:46,384 | INFO | best hypo: euh<space>il<space>y<space>a<space>des<space>fonctions<space>euh<space>religieuses<space>euh<space>des<space>fonctions<space>presque<space>sacrées<space>quelquefois<space>euh<space>l'art<space>est<space>utilisée<space>dans<space>des<space>cérémonies<space>et<space>il<space>ne<space>doit<space>pas<space>être<space>utilisée<space>en<space>dehors<space>des<space>cérémonies<space>c'est<space>vrai<space>encore<space>pour<space>pas<space>mal<space>de<space>masques<space>africains<space>par<space>exemple<space>euh<space>bien<space>évidemment<space>c'est<space>vrai<space>pour<space>la<space>peinture<space>religieuse<space>euh<space>en<space>occident<space>euh<space>la<space>peinture<space>de<space>la<space>renaissance<space>euh<space>jusqu'au<space>jusqu'au<space>seizième<space>siècle<space>la<space>peinture<space>de<space>la<space>re<space>naissance<space>est<space>une<space>painture<space>émidemment<space>religieuse<space>avec<space>des<space>fonctions<space>religieuses<space>vous<space>en<space>parait

2026-01-28 21:13:46,388 | INFO | speech length: 412160
2026-01-28 21:13:46,430 | INFO | decoder input length: 643
2026-01-28 21:13:46,430 | INFO | max output length: 643
2026-01-28 21:13:46,430 | INFO | min output length: 64
2026-01-28 21:14:29,531 | INFO | end detected at 432
2026-01-28 21:14:29,533 | INFO | -224.50 * 0.5 = -112.25 for decoder
2026-01-28 21:14:29,533 | INFO | -22.81 * 0.5 = -11.40 for ctc
2026-01-28 21:14:29,533 | INFO | total log probability: -123.66
2026-01-28 21:14:29,533 | INFO | normalized log probability: -0.29
2026-01-28 21:14:29,533 | INFO | total number of ended hypotheses: 173
2026-01-28 21:14:29,540 | INFO | best hypo: et<space>puis<space>i<space>veut<space>avoir<space>des<space>fonctions<space>politiques<space>aussi<space>l'art<space>peut<space>avoir<space>des<space>fonctions<space>politiques<space>et<space>peut<space>peut<space>avoir<space>des<space>fonctions<space>décoratives<space>aussi<space>euh<space>penser<space>au<space>rôle<space>de<space>la<space>de<space>la<space>décoration<space>par<space>exemple<space>euh<space>quand<space>vous<space>regardez<space>dans<space>les<space>euh<space>certaines<space>stations<space>de<space>métro<space>qui<space>sont<space>euh<space>décorées<space>avec<space>des<space>oeuvres<space>d'art<space>ou<space>avec<space>des<space>des<space>des<space>travaux<space>de<space>design<space>il<space>peut<space>avoir<space>des<space>lart<space>peut<space>avoir<space>des<space>fonctions<space>de<space>connaissance<space>aussi<space>dans<space>des

2026-01-28 21:14:29,543 | INFO | speech length: 377600
2026-01-28 21:14:29,582 | INFO | decoder input length: 589
2026-01-28 21:14:29,582 | INFO | max output length: 589
2026-01-28 21:14:29,582 | INFO | min output length: 58
2026-01-28 21:15:13,047 | INFO | end detected at 461
2026-01-28 21:15:13,048 | INFO | -206.79 * 0.5 = -103.39 for decoder
2026-01-28 21:15:13,048 | INFO | -44.30 * 0.5 = -22.15 for ctc
2026-01-28 21:15:13,049 | INFO | total log probability: -125.55
2026-01-28 21:15:13,049 | INFO | normalized log probability: -0.28
2026-01-28 21:15:13,049 | INFO | total number of ended hypotheses: 179
2026-01-28 21:15:13,054 | INFO | best hypo: des<space>fonctions<space>de<space>connaissance<space>des<space>fonctions<space>de<space>critique<space>y<space>a<space>beaucoup<space>de<space>fonctions<space>différentes<space>dans<space>l'art<space>et<space>encore<space>une<space>fois<space>elles<space>sont<space>pas<space>toutes<space>également<space>présentes<space>euh<space>selon<space>les<space>sociétés<space>selon<space>les<space>efforts<space>qu'<space>y<space>a<space>des<space>sociétés<space>où<space>la<space>dimension<space>de<space>connaissance<space>que<space>j'appellerais<space>à<space>des<space>intentions<space>cogmitives<space>et<space>plus<space>importantes<space>y<space>y<space>a<space>des<space>sociétés<space>où<space>la<space>dimension<space>euh<space>des<space>don<space>et<space>donistiques<space>le<space>plaisir<space>que<space>donne<space>l'expérince<space>artistique<space>sont<space>plus<space>importantes

2026-01-28 21:15:13,065 | INFO | Chunk: 0 | WER=2.631579 | S=0 D=0 I=1
2026-01-28 21:15:13,071 | INFO | Chunk: 1 | WER=23.711340 | S=10 D=6 I=7
2026-01-28 21:15:13,076 | INFO | Chunk: 2 | WER=17.977528 | S=5 D=7 I=4
2026-01-28 21:15:13,081 | INFO | Chunk: 3 | WER=20.689655 | S=8 D=1 I=9
2026-01-28 21:15:13,086 | INFO | Chunk: 4 | WER=28.409091 | S=15 D=5 I=5
2026-01-28 21:15:13,088 | INFO | Chunk: 5 | WER=16.949153 | S=4 D=1 I=5
2026-01-28 21:15:13,095 | INFO | Chunk: 6 | WER=19.811321 | S=8 D=3 I=10
2026-01-28 21:15:13,099 | INFO | Chunk: 7 | WER=19.736842 | S=5 D=2 I=8
2026-01-28 21:15:13,104 | INFO | Chunk: 8 | WER=10.344828 | S=4 D=0 I=5
2026-01-28 21:15:13,111 | INFO | Chunk: 9 | WER=14.953271 | S=8 D=4 I=4
2026-01-28 21:15:13,116 | INFO | Chunk: 10 | WER=20.652174 | S=10 D=0 I=9
2026-01-28 21:15:13,120 | INFO | Chunk: 11 | WER=18.181818 | S=3 D=4 I=7
2026-01-28 21:15:13,125 | INFO | Chunk: 12 | WER=20.731707 | S=8 D=1 I=8
2026-01-28 21:15:13,129 | INFO | Chunk: 13 | WER=15.068493 | S=5 D=1 I=5
2026-01-28 21:15:13,132 | INFO | Chunk: 14 | WER=27.272727 | S=13 D=4 I=4
2026-01-28 21:15:14,012 | INFO | File: Rhap-M2002.wav | WER=19.028340 | S=107 D=38 I=90
2026-01-28 21:15:14,014 | INFO | ------------------------------
2026-01-28 21:15:14,014 | INFO | Conf ester Done!
2026-01-28 21:18:59,894 | INFO | Chunk: 0 | WER=21.052632 | S=3 D=4 I=1
2026-01-28 21:18:59,902 | INFO | Chunk: 1 | WER=16.494845 | S=12 D=3 I=1
2026-01-28 21:18:59,909 | INFO | Chunk: 2 | WER=15.730337 | S=7 D=7 I=0
2026-01-28 21:18:59,915 | INFO | Chunk: 3 | WER=14.942529 | S=6 D=7 I=0
2026-01-28 21:18:59,921 | INFO | Chunk: 4 | WER=27.272727 | S=14 D=9 I=1
2026-01-28 21:18:59,924 | INFO | Chunk: 5 | WER=6.779661 | S=2 D=2 I=0
2026-01-28 21:18:59,933 | INFO | Chunk: 6 | WER=12.264151 | S=7 D=4 I=2
2026-01-28 21:18:59,938 | INFO | Chunk: 7 | WER=28.947368 | S=14 D=6 I=2
2026-01-28 21:18:59,945 | INFO | Chunk: 8 | WER=18.390805 | S=10 D=2 I=4
2026-01-28 21:18:59,953 | INFO | Chunk: 9 | WER=18.691589 | S=10 D=9 I=1
2026-01-28 21:18:59,961 | INFO | Chunk: 10 | WER=8.695652 | S=4 D=1 I=3
2026-01-28 21:18:59,966 | INFO | Chunk: 11 | WER=11.688312 | S=4 D=3 I=2
2026-01-28 21:18:59,972 | INFO | Chunk: 12 | WER=9.756098 | S=6 D=2 I=0
2026-01-28 21:18:59,977 | INFO | Chunk: 13 | WER=16.438356 | S=9 D=3 I=0
2026-01-28 21:18:59,985 | INFO | Chunk: 14 | WER=23.376623 | S=12 D=4 I=2
2026-01-28 21:19:01,098 | INFO | File: Rhap-M2002.wav | WER=16.599190 | S=120 D=66 I=19
2026-01-28 21:19:01,098 | INFO | ------------------------------
2026-01-28 21:19:01,099 | INFO | hmm_tdnn Done!
2026-01-28 21:19:01,326 | INFO | ==================================Rhap-M2003.wav=========================================
2026-01-28 21:19:01,555 | INFO | Using rVAD model
2026-01-28 21:19:36,776 | INFO | Chunk: 0 | WER=3.225806 | S=0 D=0 I=1
2026-01-28 21:19:36,777 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:19:36,777 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:19:36,777 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:19:36,778 | INFO | Chunk: 4 | WER=3.846154 | S=1 D=0 I=0
2026-01-28 21:19:36,778 | INFO | Chunk: 5 | WER=16.666667 | S=2 D=0 I=0
2026-01-28 21:19:36,778 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:19:36,779 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:19:36,779 | INFO | Chunk: 8 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:19:36,779 | INFO | Chunk: 9 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:19:36,780 | INFO | Chunk: 10 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:19:36,780 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:19:36,781 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:19:36,782 | INFO | Chunk: 13 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:19:36,782 | INFO | Chunk: 14 | WER=5.263158 | S=0 D=1 I=0
2026-01-28 21:19:36,783 | INFO | Chunk: 15 | WER=6.250000 | S=1 D=0 I=0
2026-01-28 21:19:36,783 | INFO | Chunk: 16 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:19:36,784 | INFO | Chunk: 17 | WER=3.125000 | S=1 D=0 I=0
2026-01-28 21:19:36,785 | INFO | Chunk: 18 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:19:36,785 | INFO | Chunk: 19 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:19:36,786 | INFO | Chunk: 20 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:19:36,789 | INFO | Chunk: 21 | WER=4.918033 | S=3 D=0 I=0
2026-01-28 21:19:36,789 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:19:36,789 | INFO | Chunk: 23 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:19:36,790 | INFO | Chunk: 24 | WER=6.250000 | S=1 D=0 I=0
2026-01-28 21:19:36,790 | INFO | Chunk: 25 | WER=10.000000 | S=0 D=0 I=1
2026-01-28 21:19:36,790 | INFO | Chunk: 26 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:19:36,791 | INFO | Chunk: 27 | WER=7.142857 | S=1 D=0 I=0
2026-01-28 21:19:36,791 | INFO | Chunk: 28 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:19:36,792 | INFO | Chunk: 29 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:19:36,795 | INFO | Chunk: 30 | WER=3.448276 | S=2 D=0 I=0
2026-01-28 21:19:36,795 | INFO | Chunk: 31 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:19:36,796 | INFO | Chunk: 32 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:19:36,799 | INFO | Chunk: 33 | WER=4.477612 | S=2 D=1 I=0
2026-01-28 21:19:36,799 | INFO | Chunk: 34 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:19:36,800 | INFO | Chunk: 35 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:19:37,083 | INFO | File: Rhap-M2003.wav | WER=2.482759 | S=14 D=2 I=2
2026-01-28 21:19:37,083 | INFO | ------------------------------
2026-01-28 21:19:37,083 | INFO | w2vec vad chunk Done!
2026-01-28 21:20:18,581 | INFO | Chunk: 0 | WER=3.225806 | S=0 D=0 I=1
2026-01-28 21:20:18,582 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:20:18,582 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:20:18,582 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:20:18,583 | INFO | Chunk: 4 | WER=7.692308 | S=2 D=0 I=0
2026-01-28 21:20:18,584 | INFO | Chunk: 5 | WER=8.333333 | S=1 D=0 I=0
2026-01-28 21:20:18,584 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:20:18,584 | INFO | Chunk: 7 | WER=20.000000 | S=2 D=0 I=0
2026-01-28 21:20:18,584 | INFO | Chunk: 8 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:20:18,585 | INFO | Chunk: 9 | WER=14.285714 | S=1 D=1 I=0
2026-01-28 21:20:18,585 | INFO | Chunk: 10 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:20:18,585 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:20:18,586 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:20:18,587 | INFO | Chunk: 13 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:20:18,587 | INFO | Chunk: 14 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:20:18,588 | INFO | Chunk: 15 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:20:18,588 | INFO | Chunk: 16 | WER=33.333333 | S=1 D=0 I=0
2026-01-28 21:20:18,589 | INFO | Chunk: 17 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:20:18,590 | INFO | Chunk: 18 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:20:18,590 | INFO | Chunk: 19 | WER=20.000000 | S=1 D=0 I=0
2026-01-28 21:20:18,591 | INFO | Chunk: 20 | WER=40.000000 | S=0 D=14 I=0
2026-01-28 21:20:18,593 | INFO | Chunk: 21 | WER=40.983607 | S=7 D=18 I=0
2026-01-28 21:20:18,593 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:20:18,593 | INFO | Chunk: 23 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:20:18,594 | INFO | Chunk: 24 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:20:18,594 | INFO | Chunk: 25 | WER=10.000000 | S=0 D=0 I=1
2026-01-28 21:20:18,594 | INFO | Chunk: 26 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:20:18,595 | INFO | Chunk: 27 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:20:18,595 | INFO | Chunk: 28 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:20:18,596 | INFO | Chunk: 29 | WER=2.500000 | S=1 D=0 I=0
2026-01-28 21:20:18,598 | INFO | Chunk: 30 | WER=43.103448 | S=4 D=19 I=2
2026-01-28 21:20:18,598 | INFO | Chunk: 31 | WER=12.500000 | S=1 D=0 I=0
2026-01-28 21:20:18,599 | INFO | Chunk: 32 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:20:18,602 | INFO | Chunk: 33 | WER=26.865672 | S=1 D=17 I=0
2026-01-28 21:20:18,602 | INFO | Chunk: 34 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:20:18,603 | INFO | Chunk: 35 | WER=3.125000 | S=1 D=0 I=0
2026-01-28 21:20:18,860 | INFO | File: Rhap-M2003.wav | WER=13.241379 | S=23 D=69 I=4
2026-01-28 21:20:18,860 | INFO | ------------------------------
2026-01-28 21:20:18,860 | INFO | whisper med Done!
2026-01-28 21:21:22,317 | INFO | Chunk: 0 | WER=9.677419 | S=1 D=0 I=2
2026-01-28 21:21:22,317 | INFO | Chunk: 1 | WER=5.263158 | S=1 D=0 I=0
2026-01-28 21:21:22,318 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:21:22,318 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:21:22,319 | INFO | Chunk: 4 | WER=3.846154 | S=1 D=0 I=0
2026-01-28 21:21:22,320 | INFO | Chunk: 5 | WER=8.333333 | S=1 D=0 I=0
2026-01-28 21:21:22,320 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:21:22,320 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:21:22,320 | INFO | Chunk: 8 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:21:22,321 | INFO | Chunk: 9 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:21:22,321 | INFO | Chunk: 10 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:21:22,322 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:21:22,323 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:21:22,324 | INFO | Chunk: 13 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:21:22,325 | INFO | Chunk: 14 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:21:22,325 | INFO | Chunk: 15 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:21:22,326 | INFO | Chunk: 16 | WER=33.333333 | S=1 D=0 I=0
2026-01-28 21:21:22,327 | INFO | Chunk: 17 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:21:22,328 | INFO | Chunk: 18 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:21:22,329 | INFO | Chunk: 19 | WER=20.000000 | S=1 D=0 I=0
2026-01-28 21:21:22,330 | INFO | Chunk: 20 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:21:22,334 | INFO | Chunk: 21 | WER=22.950820 | S=8 D=1 I=5
2026-01-28 21:21:22,334 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:21:22,334 | INFO | Chunk: 23 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:21:22,335 | INFO | Chunk: 24 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:21:22,335 | INFO | Chunk: 25 | WER=10.000000 | S=0 D=0 I=1
2026-01-28 21:21:22,336 | INFO | Chunk: 26 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:21:22,336 | INFO | Chunk: 27 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:21:22,336 | INFO | Chunk: 28 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:21:22,338 | INFO | Chunk: 29 | WER=7.500000 | S=2 D=0 I=1
2026-01-28 21:21:22,341 | INFO | Chunk: 30 | WER=20.689655 | S=6 D=6 I=0
2026-01-28 21:21:22,342 | INFO | Chunk: 31 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:21:22,343 | INFO | Chunk: 32 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:21:22,347 | INFO | Chunk: 33 | WER=17.910448 | S=7 D=0 I=5
2026-01-28 21:21:22,347 | INFO | Chunk: 34 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:21:22,349 | INFO | Chunk: 35 | WER=12.500000 | S=4 D=0 I=0
2026-01-28 21:21:22,748 | INFO | File: Rhap-M2003.wav | WER=7.448276 | S=33 D=7 I=14
2026-01-28 21:21:22,748 | INFO | ------------------------------
2026-01-28 21:21:22,748 | INFO | whisper large Done!
2026-01-28 21:21:22,951 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 21:21:23,001 | INFO | Vocabulary size: 350
2026-01-28 21:21:23,979 | INFO | Gradient checkpoint layers: []
2026-01-28 21:21:24,706 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 21:21:24,712 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 21:21:24,713 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 21:21:24,713 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 21:21:24,714 | INFO | speech length: 216960
2026-01-28 21:21:24,769 | INFO | decoder input length: 338
2026-01-28 21:21:24,769 | INFO | max output length: 338
2026-01-28 21:21:24,769 | INFO | min output length: 33
2026-01-28 21:21:32,073 | INFO | end detected at 79
2026-01-28 21:21:32,074 | INFO | -32.74 * 0.5 = -16.37 for decoder
2026-01-28 21:21:32,074 | INFO |  -0.36 * 0.5 =  -0.18 for ctc
2026-01-28 21:21:32,074 | INFO | total log probability: -16.55
2026-01-28 21:21:32,074 | INFO | normalized log probability: -0.22
2026-01-28 21:21:32,074 | INFO | total number of ended hypotheses: 137
2026-01-28 21:21:32,076 | INFO | best hypo: ▁c'est▁la▁première▁journée▁de▁jésus▁que▁nous▁venons▁d'entendre▁la▁première▁journée▁où▁il▁manifeste▁son▁autorité▁et▁la▁nouveauté▁la▁bonne▁nouvelle▁l'évangile▁qui▁arrive▁avec▁lui

2026-01-28 21:21:32,080 | INFO | speech length: 107360
2026-01-28 21:21:32,125 | INFO | decoder input length: 167
2026-01-28 21:21:32,126 | INFO | max output length: 167
2026-01-28 21:21:32,126 | INFO | min output length: 16
2026-01-28 21:21:35,654 | INFO | end detected at 55
2026-01-28 21:21:35,656 | INFO |  -3.60 * 0.5 =  -1.80 for decoder
2026-01-28 21:21:35,656 | INFO |  -0.33 * 0.5 =  -0.16 for ctc
2026-01-28 21:21:35,656 | INFO | total log probability: -1.96
2026-01-28 21:21:35,656 | INFO | normalized log probability: -0.04
2026-01-28 21:21:35,656 | INFO | total number of ended hypotheses: 150
2026-01-28 21:21:35,657 | INFO | best hypo: ▁et▁vous▁l'avez▁remarqué▁en▁écoutant▁l'évangile▁jésus▁ne▁dit▁rien▁avant▁la▁fin▁de▁l'épisode

2026-01-28 21:21:35,660 | INFO | speech length: 15840
2026-01-28 21:21:35,706 | INFO | decoder input length: 24
2026-01-28 21:21:35,706 | INFO | max output length: 24
2026-01-28 21:21:35,706 | INFO | min output length: 2
2026-01-28 21:21:36,170 | INFO | end detected at 10
2026-01-28 21:21:36,171 | INFO |  -0.41 * 0.5 =  -0.20 for decoder
2026-01-28 21:21:36,171 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 21:21:36,172 | INFO | total log probability: -0.21
2026-01-28 21:21:36,172 | INFO | normalized log probability: -0.03
2026-01-28 21:21:36,172 | INFO | total number of ended hypotheses: 137
2026-01-28 21:21:36,172 | INFO | best hypo: ▁il▁agit

2026-01-28 21:21:36,174 | INFO | speech length: 85120
2026-01-28 21:21:36,217 | INFO | decoder input length: 132
2026-01-28 21:21:36,217 | INFO | max output length: 132
2026-01-28 21:21:36,217 | INFO | min output length: 13
2026-01-28 21:21:38,249 | INFO | end detected at 34
2026-01-28 21:21:38,250 | INFO |  -2.23 * 0.5 =  -1.12 for decoder
2026-01-28 21:21:38,250 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-28 21:21:38,251 | INFO | total log probability: -1.13
2026-01-28 21:21:38,251 | INFO | normalized log probability: -0.04
2026-01-28 21:21:38,251 | INFO | total number of ended hypotheses: 141
2026-01-28 21:21:38,251 | INFO | best hypo: ▁il▁fait▁d'abord▁lever▁la▁belle▁mère▁de▁pierre▁et▁sa▁fièvre▁la▁quitte

2026-01-28 21:21:38,253 | INFO | speech length: 181760
2026-01-28 21:21:38,297 | INFO | decoder input length: 283
2026-01-28 21:21:38,297 | INFO | max output length: 283
2026-01-28 21:21:38,297 | INFO | min output length: 28
2026-01-28 21:21:43,728 | INFO | end detected at 66
2026-01-28 21:21:43,731 | INFO | -11.37 * 0.5 =  -5.68 for decoder
2026-01-28 21:21:43,731 | INFO |  -0.15 * 0.5 =  -0.08 for ctc
2026-01-28 21:21:43,731 | INFO | total log probability: -5.76
2026-01-28 21:21:43,731 | INFO | normalized log probability: -0.09
2026-01-28 21:21:43,731 | INFO | total number of ended hypotheses: 153
2026-01-28 21:21:43,732 | INFO | best hypo: ▁il▁guérit▁ensuite▁toutes▁sortes▁de▁malades▁comme▁plus▁tard▁dans▁l'évangile▁il▁acceptera▁de▁se▁laisser▁toucher▁ou▁de▁bénir▁en▁imposant▁les▁mains

2026-01-28 21:21:43,735 | INFO | speech length: 57120
2026-01-28 21:21:43,786 | INFO | decoder input length: 88
2026-01-28 21:21:43,786 | INFO | max output length: 88
2026-01-28 21:21:43,787 | INFO | min output length: 8
2026-01-28 21:21:45,384 | INFO | end detected at 30
2026-01-28 21:21:45,385 | INFO |  -1.81 * 0.5 =  -0.91 for decoder
2026-01-28 21:21:45,385 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-28 21:21:45,385 | INFO | total log probability: -0.92
2026-01-28 21:21:45,386 | INFO | normalized log probability: -0.04
2026-01-28 21:21:45,386 | INFO | total number of ended hypotheses: 139
2026-01-28 21:21:45,386 | INFO | best hypo: ▁il▁chasse▁beaucoup▁d'esprit▁ce▁qui▁fait▁du▁mal▁aux▁gens

2026-01-28 21:21:45,388 | INFO | speech length: 19040
2026-01-28 21:21:45,428 | INFO | decoder input length: 29
2026-01-28 21:21:45,428 | INFO | max output length: 29
2026-01-28 21:21:45,429 | INFO | min output length: 2
2026-01-28 21:21:46,073 | INFO | end detected at 14
2026-01-28 21:21:46,075 | INFO |  -0.76 * 0.5 =  -0.38 for decoder
2026-01-28 21:21:46,075 | INFO |  -0.45 * 0.5 =  -0.22 for ctc
2026-01-28 21:21:46,075 | INFO | total log probability: -0.60
2026-01-28 21:21:46,075 | INFO | normalized log probability: -0.07
2026-01-28 21:21:46,075 | INFO | total number of ended hypotheses: 165
2026-01-28 21:21:46,076 | INFO | best hypo: ▁avant▁de▁parler

2026-01-28 21:21:46,078 | INFO | speech length: 58080
2026-01-28 21:21:46,133 | INFO | decoder input length: 90
2026-01-28 21:21:46,133 | INFO | max output length: 90
2026-01-28 21:21:46,133 | INFO | min output length: 9
2026-01-28 21:21:48,357 | INFO | end detected at 27
2026-01-28 21:21:48,359 | INFO |  -1.73 * 0.5 =  -0.87 for decoder
2026-01-28 21:21:48,359 | INFO |  -0.07 * 0.5 =  -0.04 for ctc
2026-01-28 21:21:48,359 | INFO | total log probability: -0.90
2026-01-28 21:21:48,359 | INFO | normalized log probability: -0.04
2026-01-28 21:21:48,359 | INFO | total number of ended hypotheses: 147
2026-01-28 21:21:48,360 | INFO | best hypo: ▁avant▁toute▁parole▁la▁vie▁de▁jésus▁est▁une▁action

2026-01-28 21:21:48,364 | INFO | speech length: 28000
2026-01-28 21:21:48,422 | INFO | decoder input length: 43
2026-01-28 21:21:48,423 | INFO | max output length: 43
2026-01-28 21:21:48,423 | INFO | min output length: 4
2026-01-28 21:21:49,580 | INFO | end detected at 16
2026-01-28 21:21:49,581 | INFO |  -0.90 * 0.5 =  -0.45 for decoder
2026-01-28 21:21:49,582 | INFO |  -1.04 * 0.5 =  -0.52 for ctc
2026-01-28 21:21:49,582 | INFO | total log probability: -0.97
2026-01-28 21:21:49,582 | INFO | normalized log probability: -0.08
2026-01-28 21:21:49,582 | INFO | total number of ended hypotheses: 152
2026-01-28 21:21:49,582 | INFO | best hypo: ▁c'est▁une▁action▁salutaire

2026-01-28 21:21:49,585 | INFO | speech length: 83680
2026-01-28 21:21:49,634 | INFO | decoder input length: 130
2026-01-28 21:21:49,634 | INFO | max output length: 130
2026-01-28 21:21:49,634 | INFO | min output length: 13
2026-01-28 21:21:51,779 | INFO | end detected at 36
2026-01-28 21:21:51,782 | INFO |  -2.10 * 0.5 =  -1.05 for decoder
2026-01-28 21:21:51,782 | INFO |  -2.86 * 0.5 =  -1.43 for ctc
2026-01-28 21:21:51,783 | INFO | total log probability: -2.48
2026-01-28 21:21:51,783 | INFO | normalized log probability: -0.09
2026-01-28 21:21:51,783 | INFO | total number of ended hypotheses: 193
2026-01-28 21:21:51,783 | INFO | best hypo: ▁tout▁chez▁lui▁est▁fait▁de▁compassion▁et▁non▁de▁violence▁ou▁de▁châtiment

2026-01-28 21:21:51,786 | INFO | speech length: 93600
2026-01-28 21:21:51,835 | INFO | decoder input length: 145
2026-01-28 21:21:51,835 | INFO | max output length: 145
2026-01-28 21:21:51,835 | INFO | min output length: 14
2026-01-28 21:21:54,358 | INFO | end detected at 41
2026-01-28 21:21:54,360 | INFO |  -2.82 * 0.5 =  -1.41 for decoder
2026-01-28 21:21:54,360 | INFO |  -0.62 * 0.5 =  -0.31 for ctc
2026-01-28 21:21:54,360 | INFO | total log probability: -1.72
2026-01-28 21:21:54,361 | INFO | normalized log probability: -0.05
2026-01-28 21:21:54,361 | INFO | total number of ended hypotheses: 178
2026-01-28 21:21:54,361 | INFO | best hypo: ▁en▁fait▁tout▁l'évangile▁nous▁montre▁un▁jésus▁qui▁remet▁l'humanité▁debout

2026-01-28 21:21:54,364 | INFO | speech length: 104480
2026-01-28 21:21:54,407 | INFO | decoder input length: 162
2026-01-28 21:21:54,407 | INFO | max output length: 162
2026-01-28 21:21:54,407 | INFO | min output length: 16
2026-01-28 21:21:57,394 | INFO | end detected at 47
2026-01-28 21:21:57,397 | INFO |  -3.28 * 0.5 =  -1.64 for decoder
2026-01-28 21:21:57,397 | INFO |  -0.03 * 0.5 =  -0.01 for ctc
2026-01-28 21:21:57,397 | INFO | total log probability: -1.66
2026-01-28 21:21:57,397 | INFO | normalized log probability: -0.04
2026-01-28 21:21:57,397 | INFO | total number of ended hypotheses: 149
2026-01-28 21:21:57,398 | INFO | best hypo: ▁et▁aujourd'hui▁dans▁sa▁première▁journée▁il▁fait▁de▁la▁belle▁mère▁de▁pierre▁une▁des▁toutes▁premières▁disciples

2026-01-28 21:21:57,400 | INFO | speech length: 132320
2026-01-28 21:21:57,445 | INFO | decoder input length: 206
2026-01-28 21:21:57,445 | INFO | max output length: 206
2026-01-28 21:21:57,445 | INFO | min output length: 20
2026-01-28 21:22:01,679 | INFO | end detected at 61
2026-01-28 21:22:01,680 | INFO |  -4.37 * 0.5 =  -2.18 for decoder
2026-01-28 21:22:01,680 | INFO |  -1.61 * 0.5 =  -0.80 for ctc
2026-01-28 21:22:01,680 | INFO | total log probability: -2.99
2026-01-28 21:22:01,680 | INFO | normalized log probability: -0.05
2026-01-28 21:22:01,681 | INFO | total number of ended hypotheses: 149
2026-01-28 21:22:01,682 | INFO | best hypo: ▁l'évangile▁la▁bonne▁nouvelle▁comme▁le▁répète▁l'apôtre▁paul▁la▁bonne▁nouvelle▁en▁la▁personne▁de▁jésus▁c'est▁d'abord▁une▁action

2026-01-28 21:22:01,684 | INFO | speech length: 153760
2026-01-28 21:22:01,751 | INFO | decoder input length: 239
2026-01-28 21:22:01,752 | INFO | max output length: 239
2026-01-28 21:22:01,752 | INFO | min output length: 23
2026-01-28 21:22:08,859 | INFO | end detected at 58
2026-01-28 21:22:08,861 | INFO |  -4.78 * 0.5 =  -2.39 for decoder
2026-01-28 21:22:08,862 | INFO |  -0.45 * 0.5 =  -0.23 for ctc
2026-01-28 21:22:08,862 | INFO | total log probability: -2.61
2026-01-28 21:22:08,862 | INFO | normalized log probability: -0.05
2026-01-28 21:22:08,862 | INFO | total number of ended hypotheses: 146
2026-01-28 21:22:08,863 | INFO | best hypo: ▁une▁action▁pour▁restaurer▁la▁dignité▁de▁la▁personne▁humaine▁une▁action▁pour▁rétablir▁la▁personne▁en▁sa▁qualité▁de▁sujet▁de▁parole

2026-01-28 21:22:08,867 | INFO | speech length: 99360
2026-01-28 21:22:08,924 | INFO | decoder input length: 154
2026-01-28 21:22:08,924 | INFO | max output length: 154
2026-01-28 21:22:08,925 | INFO | min output length: 15
2026-01-28 21:22:13,027 | INFO | end detected at 40
2026-01-28 21:22:13,031 | INFO |  -2.79 * 0.5 =  -1.39 for decoder
2026-01-28 21:22:13,031 | INFO |  -0.07 * 0.5 =  -0.03 for ctc
2026-01-28 21:22:13,032 | INFO | total log probability: -1.43
2026-01-28 21:22:13,032 | INFO | normalized log probability: -0.04
2026-01-28 21:22:13,032 | INFO | total number of ended hypotheses: 167
2026-01-28 21:22:13,033 | INFO | best hypo: ▁une▁action▁pour▁rétablir▁les▁uns▁et▁les▁autres▁dans▁les▁relations▁avec▁les▁uns▁et▁avec▁les▁autres

2026-01-28 21:22:13,037 | INFO | speech length: 88160
2026-01-28 21:22:13,096 | INFO | decoder input length: 137
2026-01-28 21:22:13,096 | INFO | max output length: 137
2026-01-28 21:22:13,096 | INFO | min output length: 13
2026-01-28 21:22:16,861 | INFO | end detected at 40
2026-01-28 21:22:16,864 | INFO |  -3.96 * 0.5 =  -1.98 for decoder
2026-01-28 21:22:16,864 | INFO |  -1.71 * 0.5 =  -0.85 for ctc
2026-01-28 21:22:16,865 | INFO | total log probability: -2.84
2026-01-28 21:22:16,865 | INFO | normalized log probability: -0.08
2026-01-28 21:22:16,865 | INFO | total number of ended hypotheses: 165
2026-01-28 21:22:16,866 | INFO | best hypo: ▁jésus▁ne▁pose▁même▁pas▁ces▁gestes▁de▁bonté▁comme▁des▁appels▁à▁la▁fois▁non

2026-01-28 21:22:16,869 | INFO | speech length: 20800
2026-01-28 21:22:16,924 | INFO | decoder input length: 32
2026-01-28 21:22:16,924 | INFO | max output length: 32
2026-01-28 21:22:16,924 | INFO | min output length: 3
2026-01-28 21:22:17,780 | INFO | end detected at 12
2026-01-28 21:22:17,782 | INFO |  -1.82 * 0.5 =  -0.91 for decoder
2026-01-28 21:22:17,783 | INFO |  -1.15 * 0.5 =  -0.57 for ctc
2026-01-28 21:22:17,783 | INFO | total log probability: -1.48
2026-01-28 21:22:17,783 | INFO | normalized log probability: -0.19
2026-01-28 21:22:17,783 | INFO | total number of ended hypotheses: 149
2026-01-28 21:22:17,783 | INFO | best hypo: ▁en▁ces▁gestes

2026-01-28 21:22:17,786 | INFO | speech length: 214560
2026-01-28 21:22:17,839 | INFO | decoder input length: 334
2026-01-28 21:22:17,839 | INFO | max output length: 334
2026-01-28 21:22:17,839 | INFO | min output length: 33
2026-01-28 21:22:24,808 | INFO | end detected at 75
2026-01-28 21:22:24,809 | INFO | -39.87 * 0.5 = -19.94 for decoder
2026-01-28 21:22:24,809 | INFO |  -0.25 * 0.5 =  -0.12 for ctc
2026-01-28 21:22:24,809 | INFO | total log probability: -20.06
2026-01-28 21:22:24,809 | INFO | normalized log probability: -0.28
2026-01-28 21:22:24,809 | INFO | total number of ended hypotheses: 136
2026-01-28 21:22:24,810 | INFO | best hypo: ▁dans▁ce▁premier▁jour▁il▁inaugure▁en▁sa▁personne▁la▁venue▁de▁dieu▁parmi▁les▁hommes▁la▁venue▁définitive▁de▁dieu▁pour▁rétablir▁l'humanité▁dans▁sa▁vocation▁et▁dans▁sa▁dignité

2026-01-28 21:22:24,813 | INFO | speech length: 186880
2026-01-28 21:22:24,868 | INFO | decoder input length: 291
2026-01-28 21:22:24,868 | INFO | max output length: 291
2026-01-28 21:22:24,868 | INFO | min output length: 29
2026-01-28 21:22:31,974 | INFO | end detected at 86
2026-01-28 21:22:31,976 | INFO | -32.06 * 0.5 = -16.03 for decoder
2026-01-28 21:22:31,976 | INFO |  -0.63 * 0.5 =  -0.31 for ctc
2026-01-28 21:22:31,976 | INFO | total log probability: -16.34
2026-01-28 21:22:31,976 | INFO | normalized log probability: -0.20
2026-01-28 21:22:31,976 | INFO | total number of ended hypotheses: 162
2026-01-28 21:22:31,978 | INFO | best hypo: ▁son▁message▁initial▁au▁tout▁début▁de▁l'évangile▁convertissez▁vous▁le▁royaume▁de▁dieu▁est▁proche▁de▁vous▁son▁message▁s'inscrit▁aujourd'hui▁dans▁sa▁personne▁il▁trouve▁sa▁traduction▁en▁action

2026-01-28 21:22:31,980 | INFO | speech length: 28000
2026-01-28 21:22:32,034 | INFO | decoder input length: 43
2026-01-28 21:22:32,034 | INFO | max output length: 43
2026-01-28 21:22:32,034 | INFO | min output length: 4
2026-01-28 21:22:33,396 | INFO | end detected at 19
2026-01-28 21:22:33,398 | INFO |  -1.25 * 0.5 =  -0.63 for decoder
2026-01-28 21:22:33,398 | INFO |  -0.32 * 0.5 =  -0.16 for ctc
2026-01-28 21:22:33,399 | INFO | total log probability: -0.79
2026-01-28 21:22:33,399 | INFO | normalized log probability: -0.05
2026-01-28 21:22:33,399 | INFO | total number of ended hypotheses: 146
2026-01-28 21:22:33,399 | INFO | best hypo: ▁dans▁ces▁gestes▁de▁libération

2026-01-28 21:22:33,402 | INFO | speech length: 202400
2026-01-28 21:22:33,478 | INFO | decoder input length: 315
2026-01-28 21:22:33,478 | INFO | max output length: 315
2026-01-28 21:22:33,478 | INFO | min output length: 31
2026-01-28 21:22:46,296 | INFO | end detected at 91
2026-01-28 21:22:46,298 | INFO | -18.90 * 0.5 =  -9.45 for decoder
2026-01-28 21:22:46,298 | INFO |  -2.16 * 0.5 =  -1.08 for ctc
2026-01-28 21:22:46,298 | INFO | total log probability: -10.53
2026-01-28 21:22:46,298 | INFO | normalized log probability: -0.12
2026-01-28 21:22:46,298 | INFO | total number of ended hypotheses: 150
2026-01-28 21:22:46,300 | INFO | best hypo: ▁plus▁encore▁dans▁l'évangile▁d'aujourd'hui▁jésus▁s'accorde▁le▁pouvoir▁de▁guérir▁et▁de▁chasser▁les▁esprits▁mauvais▁il▁n'en▁appelle▁plus▁comme▁on▁le▁faisait▁en▁son▁temps▁à▁salomon

2026-01-28 21:22:46,304 | INFO | speech length: 346560
2026-01-28 21:22:46,374 | INFO | decoder input length: 541
2026-01-28 21:22:46,375 | INFO | max output length: 541
2026-01-28 21:22:46,375 | INFO | min output length: 54
2026-01-28 21:23:14,267 | INFO | end detected at 139
2026-01-28 21:23:14,270 | INFO | -189.22 * 0.5 = -94.61 for decoder
2026-01-28 21:23:14,270 | INFO | -15.54 * 0.5 =  -7.77 for ctc
2026-01-28 21:23:14,270 | INFO | total log probability: -102.38
2026-01-28 21:23:14,270 | INFO | normalized log probability: -0.76
2026-01-28 21:23:14,270 | INFO | total number of ended hypotheses: 163
2026-01-28 21:23:14,273 | INFO | best hypo: ▁comme▁le▁faisaient▁des▁guérisseurs▁de▁son▁temps▁il▁guérit▁de▁son▁propre▁chef▁les▁malades▁c'est▁sa▁propre▁vie▁qu'il▁communiquent▁à▁ce▁qu'il▁rencontre▁c'est▁sa▁propre▁autorité▁qui▁l'engage▁l'autorité▁de▁sa▁vie▁je▁le▁veux▁sois▁purifié▁entendant▁dans▁l'évangile▁ou▁encore▁fillette▁je▁te▁le▁dis▁lève▁toi

2026-01-28 21:23:14,278 | INFO | speech length: 42880
2026-01-28 21:23:14,325 | INFO | decoder input length: 66
2026-01-28 21:23:14,326 | INFO | max output length: 66
2026-01-28 21:23:14,326 | INFO | min output length: 6
2026-01-28 21:23:15,396 | INFO | end detected at 21
2026-01-28 21:23:15,398 | INFO |  -1.38 * 0.5 =  -0.69 for decoder
2026-01-28 21:23:15,398 | INFO |  -0.16 * 0.5 =  -0.08 for ctc
2026-01-28 21:23:15,398 | INFO | total log probability: -0.77
2026-01-28 21:23:15,398 | INFO | normalized log probability: -0.05
2026-01-28 21:23:15,398 | INFO | total number of ended hypotheses: 143
2026-01-28 21:23:15,398 | INFO | best hypo: ▁ou▁encore▁moi▁moi▁je▁te▁l'ordonne

2026-01-28 21:23:15,401 | INFO | speech length: 40640
2026-01-28 21:23:15,447 | INFO | decoder input length: 63
2026-01-28 21:23:15,447 | INFO | max output length: 63
2026-01-28 21:23:15,447 | INFO | min output length: 6
2026-01-28 21:23:16,455 | INFO | end detected at 20
2026-01-28 21:23:16,457 | INFO |  -1.15 * 0.5 =  -0.57 for decoder
2026-01-28 21:23:16,457 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 21:23:16,457 | INFO | total log probability: -0.58
2026-01-28 21:23:16,457 | INFO | normalized log probability: -0.04
2026-01-28 21:23:16,457 | INFO | total number of ended hypotheses: 146
2026-01-28 21:23:16,457 | INFO | best hypo: ▁c'est▁désormais▁en▁son▁propre▁nom

2026-01-28 21:23:16,459 | INFO | speech length: 101120
2026-01-28 21:23:16,500 | INFO | decoder input length: 157
2026-01-28 21:23:16,500 | INFO | max output length: 157
2026-01-28 21:23:16,500 | INFO | min output length: 15
2026-01-28 21:23:19,294 | INFO | end detected at 48
2026-01-28 21:23:19,296 | INFO |  -3.49 * 0.5 =  -1.75 for decoder
2026-01-28 21:23:19,296 | INFO |  -1.01 * 0.5 =  -0.50 for ctc
2026-01-28 21:23:19,296 | INFO | total log probability: -2.25
2026-01-28 21:23:19,296 | INFO | normalized log probability: -0.05
2026-01-28 21:23:19,296 | INFO | total number of ended hypotheses: 147
2026-01-28 21:23:19,297 | INFO | best hypo: ▁que▁jésus▁chasse▁les▁démons▁il▁enseigne▁avec▁l'autorité▁pas▁comme▁les▁pharisiens▁et▁les▁scribes

2026-01-28 21:23:19,299 | INFO | speech length: 64960
2026-01-28 21:23:19,343 | INFO | decoder input length: 101
2026-01-28 21:23:19,344 | INFO | max output length: 101
2026-01-28 21:23:19,344 | INFO | min output length: 10
2026-01-28 21:23:20,773 | INFO | end detected at 30
2026-01-28 21:23:20,774 | INFO |  -1.86 * 0.5 =  -0.93 for decoder
2026-01-28 21:23:20,774 | INFO |  -0.86 * 0.5 =  -0.43 for ctc
2026-01-28 21:23:20,774 | INFO | total log probability: -1.36
2026-01-28 21:23:20,774 | INFO | normalized log probability: -0.05
2026-01-28 21:23:20,774 | INFO | total number of ended hypotheses: 155
2026-01-28 21:23:20,775 | INFO | best hypo: ▁il▁guérit▁en▁donnant▁la▁vie▁en▁donnant▁sa▁propre▁vue

2026-01-28 21:23:20,777 | INFO | speech length: 12960
2026-01-28 21:23:20,820 | INFO | decoder input length: 19
2026-01-28 21:23:20,820 | INFO | max output length: 19
2026-01-28 21:23:20,820 | INFO | min output length: 1
2026-01-28 21:23:21,255 | INFO | end detected at 11
2026-01-28 21:23:21,257 | INFO |  -1.16 * 0.5 =  -0.58 for decoder
2026-01-28 21:23:21,257 | INFO |  -0.07 * 0.5 =  -0.04 for ctc
2026-01-28 21:23:21,257 | INFO | total log probability: -0.62
2026-01-28 21:23:21,257 | INFO | normalized log probability: -0.10
2026-01-28 21:23:21,257 | INFO | total number of ended hypotheses: 155
2026-01-28 21:23:21,257 | INFO | best hypo: ▁me▁voilà

2026-01-28 21:23:21,259 | INFO | speech length: 71520
2026-01-28 21:23:21,300 | INFO | decoder input length: 111
2026-01-28 21:23:21,300 | INFO | max output length: 111
2026-01-28 21:23:21,301 | INFO | min output length: 11
2026-01-28 21:23:23,380 | INFO | end detected at 40
2026-01-28 21:23:23,381 | INFO |  -2.69 * 0.5 =  -1.35 for decoder
2026-01-28 21:23:23,381 | INFO |  -0.12 * 0.5 =  -0.06 for ctc
2026-01-28 21:23:23,381 | INFO | total log probability: -1.41
2026-01-28 21:23:23,381 | INFO | normalized log probability: -0.04
2026-01-28 21:23:23,381 | INFO | total number of ended hypotheses: 149
2026-01-28 21:23:23,382 | INFO | best hypo: ▁le▁mal▁est▁toujours▁présent▁au▁milieu▁de▁nous▁depuis▁le▁temps▁de▁jésus

2026-01-28 21:23:23,385 | INFO | speech length: 29120
2026-01-28 21:23:23,425 | INFO | decoder input length: 45
2026-01-28 21:23:23,425 | INFO | max output length: 45
2026-01-28 21:23:23,425 | INFO | min output length: 4
2026-01-28 21:23:24,378 | INFO | end detected at 20
2026-01-28 21:23:24,380 | INFO |  -1.13 * 0.5 =  -0.57 for decoder
2026-01-28 21:23:24,380 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-28 21:23:24,380 | INFO | total log probability: -0.57
2026-01-28 21:23:24,380 | INFO | normalized log probability: -0.04
2026-01-28 21:23:24,380 | INFO | total number of ended hypotheses: 168
2026-01-28 21:23:24,380 | INFO | best hypo: ▁il▁y▁a▁le▁mal▁dans▁l'humanité

2026-01-28 21:23:24,383 | INFO | speech length: 225440
2026-01-28 21:23:24,433 | INFO | decoder input length: 351
2026-01-28 21:23:24,433 | INFO | max output length: 351
2026-01-28 21:23:24,434 | INFO | min output length: 35
2026-01-28 21:23:33,139 | INFO | end detected at 96
2026-01-28 21:23:33,141 | INFO | -127.83 * 0.5 = -63.92 for decoder
2026-01-28 21:23:33,142 | INFO | -25.94 * 0.5 = -12.97 for ctc
2026-01-28 21:23:33,142 | INFO | total log probability: -76.89
2026-01-28 21:23:33,142 | INFO | normalized log probability: -0.84
2026-01-28 21:23:33,142 | INFO | total number of ended hypotheses: 157
2026-01-28 21:23:33,143 | INFO | best hypo: ▁le▁mal▁moral▁celui▁que▁l'on▁fait▁en▁faisant▁du▁mal▁aux▁autres▁le▁mal▁physique▁qu'en▁surviennent▁aussi▁des▁catastrophes▁naturelles▁le▁cri▁de▁jobes▁que▁nous▁avons▁entendus▁dans▁la▁première▁lectures▁retentit▁à▁nos▁oreilles

2026-01-28 21:23:33,146 | INFO | speech length: 319360
2026-01-28 21:23:33,195 | INFO | decoder input length: 498
2026-01-28 21:23:33,195 | INFO | max output length: 498
2026-01-28 21:23:33,195 | INFO | min output length: 49
2026-01-28 21:23:45,762 | INFO | end detected at 132
2026-01-28 21:23:45,764 | INFO | -178.74 * 0.5 = -89.37 for decoder
2026-01-28 21:23:45,764 | INFO | -40.61 * 0.5 = -20.30 for ctc
2026-01-28 21:23:45,764 | INFO | total log probability: -109.67
2026-01-28 21:23:45,765 | INFO | normalized log probability: -0.87
2026-01-28 21:23:45,765 | INFO | total number of ended hypotheses: 173
2026-01-28 21:23:45,766 | INFO | best hypo: ▁vraiment▁dit▁job▁la▁vie▁de▁l'homme▁sur▁terre▁est▁une▁corvée▁il▁fait▁des▁journées▁de▁manoeuvre▁comme▁l'esclave▁qui▁désire▁un▁peu▁d'ombre▁comme▁le▁manoeuvre▁qui▁attend▁sa▁paye▁depuis▁des▁mois▁dit▁jobe▁je▁n'ay▁rien▁réagnée▁sinon▁que▁du▁néant▁je▁ne▁compte▁que▁des▁nuits▁de▁souffrance

2026-01-28 21:23:45,768 | INFO | speech length: 44160
2026-01-28 21:23:45,804 | INFO | decoder input length: 68
2026-01-28 21:23:45,804 | INFO | max output length: 68
2026-01-28 21:23:45,804 | INFO | min output length: 6
2026-01-28 21:23:46,933 | INFO | end detected at 26
2026-01-28 21:23:46,934 | INFO |  -2.06 * 0.5 =  -1.03 for decoder
2026-01-28 21:23:46,934 | INFO |  -1.59 * 0.5 =  -0.79 for ctc
2026-01-28 21:23:46,934 | INFO | total log probability: -1.82
2026-01-28 21:23:46,934 | INFO | normalized log probability: -0.09
2026-01-28 21:23:46,934 | INFO | total number of ended hypotheses: 173
2026-01-28 21:23:46,935 | INFO | best hypo: ▁que▁de▁nuits▁de▁souffrance▁dans▁notre▁humanité

2026-01-28 21:23:46,937 | INFO | speech length: 116320
2026-01-28 21:23:46,982 | INFO | decoder input length: 181
2026-01-28 21:23:46,983 | INFO | max output length: 181
2026-01-28 21:23:46,983 | INFO | min output length: 18
2026-01-28 21:23:50,598 | INFO | end detected at 65
2026-01-28 21:23:50,599 | INFO |  -7.62 * 0.5 =  -3.81 for decoder
2026-01-28 21:23:50,599 | INFO |  -0.11 * 0.5 =  -0.05 for ctc
2026-01-28 21:23:50,599 | INFO | total log probability: -3.86
2026-01-28 21:23:50,599 | INFO | normalized log probability: -0.06
2026-01-28 21:23:50,599 | INFO | total number of ended hypotheses: 155
2026-01-28 21:23:50,600 | INFO | best hypo: ▁et▁beaucoup▁il▁faut▁l'avouer▁beaucoup▁s'éloignent▁de▁la▁foi▁à▁cause▁de▁ce▁mal▁à▁cause▁de▁cette▁souffrance▁sans▁répit

2026-01-28 21:23:50,602 | INFO | speech length: 343520
2026-01-28 21:23:50,645 | INFO | decoder input length: 536
2026-01-28 21:23:50,646 | INFO | max output length: 536
2026-01-28 21:23:50,646 | INFO | min output length: 53
2026-01-28 21:24:07,142 | INFO | end detected at 167
2026-01-28 21:24:07,143 | INFO | -304.64 * 0.5 = -152.32 for decoder
2026-01-28 21:24:07,143 | INFO | -175.87 * 0.5 = -87.94 for ctc
2026-01-28 21:24:07,143 | INFO | total log probability: -240.26
2026-01-28 21:24:07,143 | INFO | normalized log probability: -1.49
2026-01-28 21:24:07,143 | INFO | total number of ended hypotheses: 153
2026-01-28 21:24:07,145 | INFO | best hypo: ▁aussi▁à▁l'approche▁de▁la▁journée▁des▁malades▁voulus▁par▁jean▁paul▁deux▁le▁onze▁février▁on▁la▁fête▁de▁notre▁dame▁de▁lourdes▁en▁ce▁jour▁de▁la▁pastorale▁de▁la▁santé▁la▁puissance▁de▁guérison▁et▁de▁jénus▁et▁le▁don▁de▁la▁vie▁qu'il▁communique▁interroge▁vraiment▁vraiment▁la▁restonsnabilité▁de▁l'évlisie▁c'est▁à▁dire▁la▁restonsbilité▁de▁chacun'entre▁nous

2026-01-28 21:24:07,148 | INFO | speech length: 49600
2026-01-28 21:24:07,200 | INFO | decoder input length: 77
2026-01-28 21:24:07,200 | INFO | max output length: 77
2026-01-28 21:24:07,200 | INFO | min output length: 7
2026-01-28 21:24:08,249 | INFO | end detected at 24
2026-01-28 21:24:08,250 | INFO |  -1.45 * 0.5 =  -0.72 for decoder
2026-01-28 21:24:08,250 | INFO |  -0.03 * 0.5 =  -0.02 for ctc
2026-01-28 21:24:08,250 | INFO | total log probability: -0.74
2026-01-28 21:24:08,250 | INFO | normalized log probability: -0.04
2026-01-28 21:24:08,250 | INFO | total number of ended hypotheses: 140
2026-01-28 21:24:08,250 | INFO | best hypo: ▁pour▁que▁la▁parole▁des▁chrétiens▁soit▁entendue

2026-01-28 21:24:08,252 | INFO | speech length: 186880
2026-01-28 21:24:08,288 | INFO | decoder input length: 291
2026-01-28 21:24:08,288 | INFO | max output length: 291
2026-01-28 21:24:08,288 | INFO | min output length: 29
2026-01-28 21:24:13,383 | INFO | end detected at 72
2026-01-28 21:24:13,384 | INFO | -13.80 * 0.5 =  -6.90 for decoder
2026-01-28 21:24:13,384 | INFO |  -0.70 * 0.5 =  -0.35 for ctc
2026-01-28 21:24:13,384 | INFO | total log probability: -7.25
2026-01-28 21:24:13,384 | INFO | normalized log probability: -0.11
2026-01-28 21:24:13,385 | INFO | total number of ended hypotheses: 149
2026-01-28 21:24:13,386 | INFO | best hypo: ▁comme▁la▁parole▁du▁christ▁qui▁donne▁la▁vie▁pour▁que▁notre▁parole▁soit▁entendue▁pour▁qu'elle▁soit▁comprise▁ne▁faut▁il▁pas▁comme▁pour▁jésus▁que▁d'abord▁nous▁agissions

2026-01-28 21:24:13,396 | INFO | Chunk: 0 | WER=3.225806 | S=0 D=0 I=1
2026-01-28 21:24:13,397 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:24:13,397 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:24:13,397 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:24:13,398 | INFO | Chunk: 4 | WER=3.846154 | S=1 D=0 I=0
2026-01-28 21:24:13,398 | INFO | Chunk: 5 | WER=8.333333 | S=1 D=0 I=0
2026-01-28 21:24:13,399 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:24:13,399 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:24:13,399 | INFO | Chunk: 8 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 21:24:13,400 | INFO | Chunk: 9 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:24:13,400 | INFO | Chunk: 10 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:24:13,401 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:24:13,401 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:24:13,402 | INFO | Chunk: 13 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:24:13,403 | INFO | Chunk: 14 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:24:13,403 | INFO | Chunk: 15 | WER=12.500000 | S=2 D=0 I=0
2026-01-28 21:24:13,403 | INFO | Chunk: 16 | WER=33.333333 | S=1 D=0 I=0
2026-01-28 21:24:13,404 | INFO | Chunk: 17 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:24:13,405 | INFO | Chunk: 18 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:24:13,405 | INFO | Chunk: 19 | WER=20.000000 | S=1 D=0 I=0
2026-01-28 21:24:13,407 | INFO | Chunk: 20 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:24:13,409 | INFO | Chunk: 21 | WER=9.836066 | S=5 D=1 I=0
2026-01-28 21:24:13,409 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:24:13,410 | INFO | Chunk: 23 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:24:13,410 | INFO | Chunk: 24 | WER=6.250000 | S=0 D=0 I=1
2026-01-28 21:24:13,410 | INFO | Chunk: 25 | WER=10.000000 | S=0 D=0 I=1
2026-01-28 21:24:13,410 | INFO | Chunk: 26 | WER=50.000000 | S=1 D=0 I=0
2026-01-28 21:24:13,411 | INFO | Chunk: 27 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:24:13,411 | INFO | Chunk: 28 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:24:13,412 | INFO | Chunk: 29 | WER=15.000000 | S=4 D=1 I=1
2026-01-28 21:24:13,415 | INFO | Chunk: 30 | WER=6.896552 | S=3 D=1 I=0
2026-01-28 21:24:13,415 | INFO | Chunk: 31 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:24:13,416 | INFO | Chunk: 32 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:24:13,419 | INFO | Chunk: 33 | WER=13.432836 | S=6 D=1 I=2
2026-01-28 21:24:13,419 | INFO | Chunk: 34 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:24:13,420 | INFO | Chunk: 35 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:24:13,704 | INFO | File: Rhap-M2003.wav | WER=4.965517 | S=25 D=4 I=7
2026-01-28 21:24:13,704 | INFO | ------------------------------
2026-01-28 21:24:13,704 | INFO | Conf cv Done!
2026-01-28 21:24:13,883 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 21:24:13,906 | INFO | Vocabulary size: 47
2026-01-28 21:24:14,875 | INFO | Gradient checkpoint layers: []
2026-01-28 21:24:15,589 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 21:24:15,593 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 21:24:15,593 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 21:24:15,594 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 21:24:15,596 | INFO | speech length: 216960
2026-01-28 21:24:15,642 | INFO | decoder input length: 338
2026-01-28 21:24:15,643 | INFO | max output length: 338
2026-01-28 21:24:15,643 | INFO | min output length: 33
2026-01-28 21:24:27,984 | INFO | end detected at 181
2026-01-28 21:24:27,985 | INFO | -14.69 * 0.5 =  -7.34 for decoder
2026-01-28 21:24:27,985 | INFO |  -0.18 * 0.5 =  -0.09 for ctc
2026-01-28 21:24:27,985 | INFO | total log probability: -7.43
2026-01-28 21:24:27,985 | INFO | normalized log probability: -0.04
2026-01-28 21:24:27,985 | INFO | total number of ended hypotheses: 137
2026-01-28 21:24:27,988 | INFO | best hypo: c'est<space>la<space>première<space>journée<space>de<space>jésus<space>que<space>nous<space>venons<space>d'entendre<space>la<space>première<space>journée<space>où<space>il<space>manifeste<space>son<space>autorité<space>et<space>la<space>nouveauté<space>la<space>bonne<space>nouvelle<space>l'évangile<space>qui<space>arrive<space>avec<space>lui

2026-01-28 21:24:27,990 | INFO | speech length: 107360
2026-01-28 21:24:28,037 | INFO | decoder input length: 167
2026-01-28 21:24:28,037 | INFO | max output length: 167
2026-01-28 21:24:28,037 | INFO | min output length: 16
2026-01-28 21:24:32,758 | INFO | end detected at 97
2026-01-28 21:24:32,760 | INFO |  -7.37 * 0.5 =  -3.69 for decoder
2026-01-28 21:24:32,760 | INFO |  -0.39 * 0.5 =  -0.20 for ctc
2026-01-28 21:24:32,760 | INFO | total log probability: -3.88
2026-01-28 21:24:32,760 | INFO | normalized log probability: -0.04
2026-01-28 21:24:32,760 | INFO | total number of ended hypotheses: 162
2026-01-28 21:24:32,762 | INFO | best hypo: et<space>vous<space>l'avez<space>remarqué<space>en<space>écoutant<space>l'évangile<space>jésus<space>ne<space>dit<space>rien<space>avant<space>la<space>fin<space>de<space>l'épisode

2026-01-28 21:24:32,764 | INFO | speech length: 15840
2026-01-28 21:24:32,809 | INFO | decoder input length: 24
2026-01-28 21:24:32,810 | INFO | max output length: 24
2026-01-28 21:24:32,810 | INFO | min output length: 2
2026-01-28 21:24:33,322 | INFO | end detected at 14
2026-01-28 21:24:33,323 | INFO |  -0.67 * 0.5 =  -0.33 for decoder
2026-01-28 21:24:33,323 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-28 21:24:33,323 | INFO | total log probability: -0.34
2026-01-28 21:24:33,323 | INFO | normalized log probability: -0.04
2026-01-28 21:24:33,324 | INFO | total number of ended hypotheses: 164
2026-01-28 21:24:33,324 | INFO | best hypo: il<space>agit

2026-01-28 21:24:33,326 | INFO | speech length: 85120
2026-01-28 21:24:33,362 | INFO | decoder input length: 132
2026-01-28 21:24:33,362 | INFO | max output length: 132
2026-01-28 21:24:33,362 | INFO | min output length: 13
2026-01-28 21:24:36,794 | INFO | end detected at 75
2026-01-28 21:24:36,796 | INFO |  -7.17 * 0.5 =  -3.59 for decoder
2026-01-28 21:24:36,796 | INFO |  -2.96 * 0.5 =  -1.48 for ctc
2026-01-28 21:24:36,796 | INFO | total log probability: -5.07
2026-01-28 21:24:36,796 | INFO | normalized log probability: -0.07
2026-01-28 21:24:36,796 | INFO | total number of ended hypotheses: 188
2026-01-28 21:24:36,797 | INFO | best hypo: il<space>fait<space>d'abord<space>lever<space>la<space>belle<space>mère<space>de<space>pierre<space>et<space>sa<space>fièvre<space>l'acquit

2026-01-28 21:24:36,800 | INFO | speech length: 181760
2026-01-28 21:24:36,848 | INFO | decoder input length: 283
2026-01-28 21:24:36,848 | INFO | max output length: 283
2026-01-28 21:24:36,848 | INFO | min output length: 28
2026-01-28 21:24:46,308 | INFO | end detected at 152
2026-01-28 21:24:46,310 | INFO | -12.10 * 0.5 =  -6.05 for decoder
2026-01-28 21:24:46,310 | INFO |  -3.51 * 0.5 =  -1.76 for ctc
2026-01-28 21:24:46,310 | INFO | total log probability: -7.81
2026-01-28 21:24:46,311 | INFO | normalized log probability: -0.05
2026-01-28 21:24:46,311 | INFO | total number of ended hypotheses: 176
2026-01-28 21:24:46,313 | INFO | best hypo: il<space>guérit<space>ensuite<space>toutes<space>sortes<space>de<space>malades<space>comme<space>plus<space>tard<space>dans<space>l'evangile<space>il<space>acceptera<space>de<space>se<space>laisser<space>toucher<space>ou<space>de<space>bainir<space>en<space>imposant<space>les<space>mains

2026-01-28 21:24:46,315 | INFO | speech length: 57120
2026-01-28 21:24:46,361 | INFO | decoder input length: 88
2026-01-28 21:24:46,361 | INFO | max output length: 88
2026-01-28 21:24:46,361 | INFO | min output length: 8
2026-01-28 21:24:48,826 | INFO | end detected at 62
2026-01-28 21:24:48,827 | INFO |  -4.90 * 0.5 =  -2.45 for decoder
2026-01-28 21:24:48,827 | INFO |  -0.28 * 0.5 =  -0.14 for ctc
2026-01-28 21:24:48,827 | INFO | total log probability: -2.59
2026-01-28 21:24:48,827 | INFO | normalized log probability: -0.05
2026-01-28 21:24:48,827 | INFO | total number of ended hypotheses: 159
2026-01-28 21:24:48,828 | INFO | best hypo: il<space>chasse<space>beaucoup<space>d'esprit<space>ce<space>qui<space>fait<space>du<space>mal<space>aux<space>gens

2026-01-28 21:24:48,830 | INFO | speech length: 19040
2026-01-28 21:24:48,870 | INFO | decoder input length: 29
2026-01-28 21:24:48,870 | INFO | max output length: 29
2026-01-28 21:24:48,870 | INFO | min output length: 2
2026-01-28 21:24:49,611 | INFO | end detected at 21
2026-01-28 21:24:49,612 | INFO |  -1.29 * 0.5 =  -0.65 for decoder
2026-01-28 21:24:49,612 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 21:24:49,612 | INFO | total log probability: -0.65
2026-01-28 21:24:49,612 | INFO | normalized log probability: -0.04
2026-01-28 21:24:49,613 | INFO | total number of ended hypotheses: 137
2026-01-28 21:24:49,613 | INFO | best hypo: avant<space>de<space>parler

2026-01-28 21:24:49,614 | INFO | speech length: 58080
2026-01-28 21:24:49,655 | INFO | decoder input length: 90
2026-01-28 21:24:49,655 | INFO | max output length: 90
2026-01-28 21:24:49,655 | INFO | min output length: 9
2026-01-28 21:24:52,081 | INFO | end detected at 56
2026-01-28 21:24:52,082 | INFO |  -4.02 * 0.5 =  -2.01 for decoder
2026-01-28 21:24:52,082 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-28 21:24:52,082 | INFO | total log probability: -2.02
2026-01-28 21:24:52,083 | INFO | normalized log probability: -0.04
2026-01-28 21:24:52,083 | INFO | total number of ended hypotheses: 167
2026-01-28 21:24:52,083 | INFO | best hypo: avant<space>toute<space>parole<space>la<space>vie<space>de<space>jésus<space>est<space>une<space>action

2026-01-28 21:24:52,085 | INFO | speech length: 28000
2026-01-28 21:24:52,120 | INFO | decoder input length: 43
2026-01-28 21:24:52,121 | INFO | max output length: 43
2026-01-28 21:24:52,121 | INFO | min output length: 4
2026-01-28 21:24:53,243 | INFO | end detected at 31
2026-01-28 21:24:53,244 | INFO |  -2.02 * 0.5 =  -1.01 for decoder
2026-01-28 21:24:53,244 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 21:24:53,244 | INFO | total log probability: -1.01
2026-01-28 21:24:53,244 | INFO | normalized log probability: -0.04
2026-01-28 21:24:53,244 | INFO | total number of ended hypotheses: 168
2026-01-28 21:24:53,245 | INFO | best hypo: est<space>une<space>action<space>salutaire

2026-01-28 21:24:53,247 | INFO | speech length: 83680
2026-01-28 21:24:53,302 | INFO | decoder input length: 130
2026-01-28 21:24:53,302 | INFO | max output length: 130
2026-01-28 21:24:53,302 | INFO | min output length: 13
2026-01-28 21:24:57,122 | INFO | end detected at 79
2026-01-28 21:24:57,124 | INFO |  -6.45 * 0.5 =  -3.23 for decoder
2026-01-28 21:24:57,124 | INFO |  -1.04 * 0.5 =  -0.52 for ctc
2026-01-28 21:24:57,125 | INFO | total log probability: -3.74
2026-01-28 21:24:57,125 | INFO | normalized log probability: -0.05
2026-01-28 21:24:57,125 | INFO | total number of ended hypotheses: 184
2026-01-28 21:24:57,126 | INFO | best hypo: tout<space>chez<space>lui<space>est<space>fait<space>de<space>compassion<space>et<space>non<space>de<space>violence<space>ou<space>de<space>châtiment

2026-01-28 21:24:57,128 | INFO | speech length: 93600
2026-01-28 21:24:57,175 | INFO | decoder input length: 145
2026-01-28 21:24:57,175 | INFO | max output length: 145
2026-01-28 21:24:57,175 | INFO | min output length: 14
2026-01-28 21:25:01,020 | INFO | end detected at 82
2026-01-28 21:25:01,022 | INFO |  -7.78 * 0.5 =  -3.89 for decoder
2026-01-28 21:25:01,022 | INFO |  -4.02 * 0.5 =  -2.01 for ctc
2026-01-28 21:25:01,022 | INFO | total log probability: -5.90
2026-01-28 21:25:01,022 | INFO | normalized log probability: -0.08
2026-01-28 21:25:01,022 | INFO | total number of ended hypotheses: 194
2026-01-28 21:25:01,023 | INFO | best hypo: en<space>fait<space>tous<space>l'évangile<space>nous<space>montrent<space>un<space>jésus<space>qui<space>remet<space>l'humanité<space>debout

2026-01-28 21:25:01,026 | INFO | speech length: 104480
2026-01-28 21:25:01,064 | INFO | decoder input length: 162
2026-01-28 21:25:01,064 | INFO | max output length: 162
2026-01-28 21:25:01,064 | INFO | min output length: 16
2026-01-28 21:25:06,382 | INFO | end detected at 115
2026-01-28 21:25:06,384 | INFO |  -8.73 * 0.5 =  -4.37 for decoder
2026-01-28 21:25:06,384 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-28 21:25:06,384 | INFO | total log probability: -4.37
2026-01-28 21:25:06,384 | INFO | normalized log probability: -0.04
2026-01-28 21:25:06,384 | INFO | total number of ended hypotheses: 150
2026-01-28 21:25:06,385 | INFO | best hypo: et<space>aujourd'hui<space>dans<space>sa<space>première<space>journée<space>il<space>fait<space>de<space>la<space>belle<space>mère<space>de<space>pierre<space>une<space>des<space>toutes<space>premières<space>disciples

2026-01-28 21:25:06,387 | INFO | speech length: 132320
2026-01-28 21:25:06,424 | INFO | decoder input length: 206
2026-01-28 21:25:06,424 | INFO | max output length: 206
2026-01-28 21:25:06,424 | INFO | min output length: 20
2026-01-28 21:25:13,048 | INFO | end detected at 130
2026-01-28 21:25:13,049 | INFO | -10.25 * 0.5 =  -5.13 for decoder
2026-01-28 21:25:13,049 | INFO |  -0.33 * 0.5 =  -0.17 for ctc
2026-01-28 21:25:13,049 | INFO | total log probability: -5.29
2026-01-28 21:25:13,049 | INFO | normalized log probability: -0.04
2026-01-28 21:25:13,049 | INFO | total number of ended hypotheses: 141
2026-01-28 21:25:13,051 | INFO | best hypo: l'évangile<space>la<space>bonne<space>nouvelle<space>comme<space>le<space>répète<space>la<space>pétropole<space>la<space>bonne<space>nouvelle<space>en<space>la<space>personne<space>de<space>jésus<space>c'est<space>d'abord<space>une<space>action

2026-01-28 21:25:13,052 | INFO | speech length: 153760
2026-01-28 21:25:13,093 | INFO | decoder input length: 239
2026-01-28 21:25:13,093 | INFO | max output length: 239
2026-01-28 21:25:13,093 | INFO | min output length: 23
2026-01-28 21:25:20,610 | INFO | end detected at 136
2026-01-28 21:25:20,611 | INFO | -10.38 * 0.5 =  -5.19 for decoder
2026-01-28 21:25:20,612 | INFO |  -0.54 * 0.5 =  -0.27 for ctc
2026-01-28 21:25:20,612 | INFO | total log probability: -5.46
2026-01-28 21:25:20,612 | INFO | normalized log probability: -0.04
2026-01-28 21:25:20,612 | INFO | total number of ended hypotheses: 164
2026-01-28 21:25:20,614 | INFO | best hypo: une<space>action<space>pour<space>restaurer<space>la<space>dignité<space>de<space>la<space>personne<space>humaine<space>une<space>action<space>pour<space>rétablir<space>la<space>personne<space>en<space>sa<space>qualité<space>de<space>sujet<space>de<space>parole

2026-01-28 21:25:20,616 | INFO | speech length: 99360
2026-01-28 21:25:20,653 | INFO | decoder input length: 154
2026-01-28 21:25:20,653 | INFO | max output length: 154
2026-01-28 21:25:20,654 | INFO | min output length: 15
2026-01-28 21:25:25,524 | INFO | end detected at 104
2026-01-28 21:25:25,526 | INFO |  -7.84 * 0.5 =  -3.92 for decoder
2026-01-28 21:25:25,526 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 21:25:25,526 | INFO | total log probability: -3.92
2026-01-28 21:25:25,526 | INFO | normalized log probability: -0.04
2026-01-28 21:25:25,526 | INFO | total number of ended hypotheses: 175
2026-01-28 21:25:25,527 | INFO | best hypo: une<space>action<space>pour<space>rétablir<space>les<space>uns<space>et<space>les<space>autres<space>dans<space>les<space>relations<space>avec<space>les<space>uns<space>et<space>avec<space>les<space>autres

2026-01-28 21:25:25,529 | INFO | speech length: 88160
2026-01-28 21:25:25,571 | INFO | decoder input length: 137
2026-01-28 21:25:25,571 | INFO | max output length: 137
2026-01-28 21:25:25,571 | INFO | min output length: 13
2026-01-28 21:25:29,218 | INFO | end detected at 80
2026-01-28 21:25:29,219 | INFO |  -6.20 * 0.5 =  -3.10 for decoder
2026-01-28 21:25:29,220 | INFO |  -0.34 * 0.5 =  -0.17 for ctc
2026-01-28 21:25:29,220 | INFO | total log probability: -3.27
2026-01-28 21:25:29,220 | INFO | normalized log probability: -0.04
2026-01-28 21:25:29,220 | INFO | total number of ended hypotheses: 165
2026-01-28 21:25:29,221 | INFO | best hypo: jésus<space>ne<space>pose<space>même<space>pas<space>ces<space>gestes<space>de<space>bonté<space>comme<space>des<space>appels<space>à<space>la<space>fois<space>non

2026-01-28 21:25:29,223 | INFO | speech length: 20800
2026-01-28 21:25:29,266 | INFO | decoder input length: 32
2026-01-28 21:25:29,267 | INFO | max output length: 32
2026-01-28 21:25:29,267 | INFO | min output length: 3
2026-01-28 21:25:29,958 | INFO | end detected at 19
2026-01-28 21:25:29,960 | INFO |  -1.14 * 0.5 =  -0.57 for decoder
2026-01-28 21:25:29,960 | INFO |  -0.03 * 0.5 =  -0.02 for ctc
2026-01-28 21:25:29,960 | INFO | total log probability: -0.59
2026-01-28 21:25:29,960 | INFO | normalized log probability: -0.04
2026-01-28 21:25:29,960 | INFO | total number of ended hypotheses: 135
2026-01-28 21:25:29,960 | INFO | best hypo: en<space>ces<space>gestes

2026-01-28 21:25:29,962 | INFO | speech length: 214560
2026-01-28 21:25:29,999 | INFO | decoder input length: 334
2026-01-28 21:25:29,999 | INFO | max output length: 334
2026-01-28 21:25:29,999 | INFO | min output length: 33
2026-01-28 21:25:41,594 | INFO | end detected at 176
2026-01-28 21:25:41,595 | INFO | -14.48 * 0.5 =  -7.24 for decoder
2026-01-28 21:25:41,595 | INFO |  -1.93 * 0.5 =  -0.97 for ctc
2026-01-28 21:25:41,595 | INFO | total log probability: -8.21
2026-01-28 21:25:41,595 | INFO | normalized log probability: -0.05
2026-01-28 21:25:41,595 | INFO | total number of ended hypotheses: 159
2026-01-28 21:25:41,597 | INFO | best hypo: dans<space>ce<space>premier<space>jour<space>il<space>inaugure<space>en<space>sa<space>personne<space>la<space>venue<space>de<space>dieu<space>parmi<space>les<space>hommes<space>la<space>venue<space>définitive<space>de<space>dieu<space>pour<space>rétablir<space>l'humanité<space>dans<space>sa<space>vocation<space>et<space>dans<space>sa<space>dignité

2026-01-28 21:25:41,600 | INFO | speech length: 186880
2026-01-28 21:25:41,637 | INFO | decoder input length: 291
2026-01-28 21:25:41,637 | INFO | max output length: 291
2026-01-28 21:25:41,637 | INFO | min output length: 29
2026-01-28 21:25:53,464 | INFO | end detected at 195
2026-01-28 21:25:53,465 | INFO | -15.15 * 0.5 =  -7.58 for decoder
2026-01-28 21:25:53,465 | INFO |  -0.06 * 0.5 =  -0.03 for ctc
2026-01-28 21:25:53,465 | INFO | total log probability: -7.61
2026-01-28 21:25:53,465 | INFO | normalized log probability: -0.04
2026-01-28 21:25:53,465 | INFO | total number of ended hypotheses: 178
2026-01-28 21:25:53,468 | INFO | best hypo: son<space>message<space>initial<space>au<space>tout<space>début<space>de<space>l'évangile<space>convertissez<space>vous<space>le<space>royaume<space>de<space>dieu<space>est<space>proche<space>de<space>vous<space>son<space>message<space>s'inscrit<space>aujourd'hui<space>dans<space>sa<space>personne<space>il<space>trouve<space>sa<space>traduction<space>en<space>action

2026-01-28 21:25:53,470 | INFO | speech length: 28000
2026-01-28 21:25:53,523 | INFO | decoder input length: 43
2026-01-28 21:25:53,524 | INFO | max output length: 43
2026-01-28 21:25:53,524 | INFO | min output length: 4
2026-01-28 21:25:54,807 | INFO | end detected at 35
2026-01-28 21:25:54,808 | INFO |  -2.67 * 0.5 =  -1.33 for decoder
2026-01-28 21:25:54,808 | INFO |  -0.15 * 0.5 =  -0.07 for ctc
2026-01-28 21:25:54,808 | INFO | total log probability: -1.41
2026-01-28 21:25:54,808 | INFO | normalized log probability: -0.05
2026-01-28 21:25:54,808 | INFO | total number of ended hypotheses: 144
2026-01-28 21:25:54,808 | INFO | best hypo: dans<space>ces<space>gestes<space>de<space>libération

2026-01-28 21:25:54,810 | INFO | speech length: 202400
2026-01-28 21:25:54,855 | INFO | decoder input length: 315
2026-01-28 21:25:54,855 | INFO | max output length: 315
2026-01-28 21:25:54,855 | INFO | min output length: 31
2026-01-28 21:26:06,918 | INFO | end detected at 186
2026-01-28 21:26:06,921 | INFO | -16.52 * 0.5 =  -8.26 for decoder
2026-01-28 21:26:06,921 | INFO |  -2.52 * 0.5 =  -1.26 for ctc
2026-01-28 21:26:06,921 | INFO | total log probability: -9.52
2026-01-28 21:26:06,921 | INFO | normalized log probability: -0.05
2026-01-28 21:26:06,921 | INFO | total number of ended hypotheses: 188
2026-01-28 21:26:06,923 | INFO | best hypo: plus<space>encore<space>dans<space>l'évangile<space>d'aujourd'hui<space>jésus<space>s'accorde<space>le<space>pouvoir<space>de<space>guérir<space>et<space>de<space>chasser<space>les<space>esprits<space>mauvais<space>il<space>n'en<space>appelle<space>plus<space>comme<space>on<space>le<space>faisait<space>en<space>son<space>temps<space>à<space>sa<space>loumont

2026-01-28 21:26:06,926 | INFO | speech length: 346560
2026-01-28 21:26:06,970 | INFO | decoder input length: 541
2026-01-28 21:26:06,970 | INFO | max output length: 541
2026-01-28 21:26:06,970 | INFO | min output length: 54
2026-01-28 21:26:34,005 | INFO | end detected at 307
2026-01-28 21:26:34,023 | INFO | -122.97 * 0.5 = -61.48 for decoder
2026-01-28 21:26:34,023 | INFO |  -9.31 * 0.5 =  -4.65 for ctc
2026-01-28 21:26:34,023 | INFO | total log probability: -66.14
2026-01-28 21:26:34,023 | INFO | normalized log probability: -0.22
2026-01-28 21:26:34,023 | INFO | total number of ended hypotheses: 196
2026-01-28 21:26:34,028 | INFO | best hypo: comme<space>le<space>faisait<space>des<space>guérisseurs<space>de<space>son<space>temps<space>il<space>guérit<space>de<space>son<space>propre<space>chef<space>les<space>malades<space>c'est<space>sa<space>propre<space>vie<space>qu'il<space>communique<space>à<space>ceux<space>qu'il<space>rencontre<space>c'est<space>sa<space>propre<space>autorité<space>qu'il<space>engage<space>l'autorité<space>de<space>sa<space>vie<space>je<space>le<space>veux<space>soit<space>purifiée<space>en<space>tentant<space>dans<space>l'evangile<space>ou<space>encore<space>fillette<space>je<space>te<space>le<space>dis<space>laf<space>tois

2026-01-28 21:26:34,032 | INFO | speech length: 42880
2026-01-28 21:26:34,092 | INFO | decoder input length: 66
2026-01-28 21:26:34,092 | INFO | max output length: 66
2026-01-28 21:26:34,092 | INFO | min output length: 6
2026-01-28 21:26:35,665 | INFO | end detected at 41
2026-01-28 21:26:35,666 | INFO |  -2.84 * 0.5 =  -1.42 for decoder
2026-01-28 21:26:35,666 | INFO |  -0.03 * 0.5 =  -0.01 for ctc
2026-01-28 21:26:35,666 | INFO | total log probability: -1.43
2026-01-28 21:26:35,666 | INFO | normalized log probability: -0.04
2026-01-28 21:26:35,666 | INFO | total number of ended hypotheses: 163
2026-01-28 21:26:35,667 | INFO | best hypo: ou<space>encore<space>moi<space>moi<space>je<space>te<space>leur<space>donne

2026-01-28 21:26:35,669 | INFO | speech length: 40640
2026-01-28 21:26:35,722 | INFO | decoder input length: 63
2026-01-28 21:26:35,722 | INFO | max output length: 63
2026-01-28 21:26:35,722 | INFO | min output length: 6
2026-01-28 21:26:37,445 | INFO | end detected at 39
2026-01-28 21:26:37,446 | INFO |  -2.74 * 0.5 =  -1.37 for decoder
2026-01-28 21:26:37,446 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 21:26:37,446 | INFO | total log probability: -1.37
2026-01-28 21:26:37,446 | INFO | normalized log probability: -0.04
2026-01-28 21:26:37,446 | INFO | total number of ended hypotheses: 142
2026-01-28 21:26:37,447 | INFO | best hypo: c'est<space>désormais<space>en<space>son<space>propre<space>nom

2026-01-28 21:26:37,449 | INFO | speech length: 101120
2026-01-28 21:26:37,502 | INFO | decoder input length: 157
2026-01-28 21:26:37,502 | INFO | max output length: 157
2026-01-28 21:26:37,502 | INFO | min output length: 15
2026-01-28 21:26:43,364 | INFO | end detected at 104
2026-01-28 21:26:43,366 | INFO | -10.43 * 0.5 =  -5.21 for decoder
2026-01-28 21:26:43,367 | INFO | -10.65 * 0.5 =  -5.32 for ctc
2026-01-28 21:26:43,367 | INFO | total log probability: -10.54
2026-01-28 21:26:43,367 | INFO | normalized log probability: -0.11
2026-01-28 21:26:43,367 | INFO | total number of ended hypotheses: 216
2026-01-28 21:26:43,368 | INFO | best hypo: que<space>jésus<space>chasse<space>les<space>démons<space>il<space>enseigne<space>avec<space>l'autorité<space>pas<space>comme<space>les<space>parisiens<space>et<space>l'escrit<space>ben

2026-01-28 21:26:43,372 | INFO | speech length: 64960
2026-01-28 21:26:43,431 | INFO | decoder input length: 101
2026-01-28 21:26:43,431 | INFO | max output length: 101
2026-01-28 21:26:43,431 | INFO | min output length: 10
2026-01-28 21:26:46,313 | INFO | end detected at 59
2026-01-28 21:26:46,315 | INFO |  -5.25 * 0.5 =  -2.63 for decoder
2026-01-28 21:26:46,315 | INFO |  -0.52 * 0.5 =  -0.26 for ctc
2026-01-28 21:26:46,315 | INFO | total log probability: -2.89
2026-01-28 21:26:46,315 | INFO | normalized log probability: -0.05
2026-01-28 21:26:46,315 | INFO | total number of ended hypotheses: 173
2026-01-28 21:26:46,316 | INFO | best hypo: il<space>guérit<space>en<space>donnant<space>la<space>vie<space>en<space>donnant<space>sa<space>propre<space>vue

2026-01-28 21:26:46,319 | INFO | speech length: 12960
2026-01-28 21:26:46,372 | INFO | decoder input length: 19
2026-01-28 21:26:46,373 | INFO | max output length: 19
2026-01-28 21:26:46,373 | INFO | min output length: 1
2026-01-28 21:26:47,229 | INFO | end detected at 16
2026-01-28 21:26:47,230 | INFO |  -0.88 * 0.5 =  -0.44 for decoder
2026-01-28 21:26:47,230 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 21:26:47,230 | INFO | total log probability: -0.44
2026-01-28 21:26:47,230 | INFO | normalized log probability: -0.04
2026-01-28 21:26:47,230 | INFO | total number of ended hypotheses: 138
2026-01-28 21:26:47,230 | INFO | best hypo: mais<space>voilà

2026-01-28 21:26:47,233 | INFO | speech length: 71520
2026-01-28 21:26:47,300 | INFO | decoder input length: 111
2026-01-28 21:26:47,301 | INFO | max output length: 111
2026-01-28 21:26:47,301 | INFO | min output length: 11
2026-01-28 21:26:53,053 | INFO | end detected at 77
2026-01-28 21:26:53,055 | INFO |  -5.69 * 0.5 =  -2.85 for decoder
2026-01-28 21:26:53,056 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-28 21:26:53,056 | INFO | total log probability: -2.85
2026-01-28 21:26:53,056 | INFO | normalized log probability: -0.04
2026-01-28 21:26:53,056 | INFO | total number of ended hypotheses: 173
2026-01-28 21:26:53,057 | INFO | best hypo: le<space>mal<space>est<space>toujours<space>présent<space>au<space>milieu<space>de<space>nous<space>depuis<space>le<space>temps<space>de<space>jésus

2026-01-28 21:26:53,062 | INFO | speech length: 29120
2026-01-28 21:26:53,122 | INFO | decoder input length: 45
2026-01-28 21:26:53,123 | INFO | max output length: 45
2026-01-28 21:26:53,123 | INFO | min output length: 4
2026-01-28 21:26:54,723 | INFO | end detected at 36
2026-01-28 21:26:54,724 | INFO |  -2.41 * 0.5 =  -1.20 for decoder
2026-01-28 21:26:54,724 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 21:26:54,724 | INFO | total log probability: -1.20
2026-01-28 21:26:54,724 | INFO | normalized log probability: -0.04
2026-01-28 21:26:54,724 | INFO | total number of ended hypotheses: 174
2026-01-28 21:26:54,725 | INFO | best hypo: il<space>y<space>a<space>le<space>mal<space>dans<space>l'humanité

2026-01-28 21:26:54,726 | INFO | speech length: 225440
2026-01-28 21:26:54,771 | INFO | decoder input length: 351
2026-01-28 21:26:54,771 | INFO | max output length: 351
2026-01-28 21:26:54,771 | INFO | min output length: 35
2026-01-28 21:27:10,382 | INFO | end detected at 224
2026-01-28 21:27:10,384 | INFO | -18.21 * 0.5 =  -9.10 for decoder
2026-01-28 21:27:10,384 | INFO |  -6.49 * 0.5 =  -3.24 for ctc
2026-01-28 21:27:10,385 | INFO | total log probability: -12.35
2026-01-28 21:27:10,385 | INFO | normalized log probability: -0.06
2026-01-28 21:27:10,385 | INFO | total number of ended hypotheses: 179
2026-01-28 21:27:10,388 | INFO | best hypo: le<space>mal<space>moral<space>celui<space>que<space>l'on<space>fait<space>en<space>faisant<space>du<space>mal<space>aux<space>autres<space>le<space>mal<space>physique<space>en<space>survienne<space>aussi<space>des<space>catastrophes<space>naturelles<space>et<space>le<space>cri<space>de<space>job<space>que<space>nous<space>avons<space>entendu<space>dans<space>la<space>première<space>lecture<space>retenti<space>à<space>nos<space>oreilles

2026-01-28 21:27:10,391 | INFO | speech length: 319360
2026-01-28 21:27:10,437 | INFO | decoder input length: 498
2026-01-28 21:27:10,438 | INFO | max output length: 498
2026-01-28 21:27:10,438 | INFO | min output length: 49
2026-01-28 21:27:34,156 | INFO | end detected at 287
2026-01-28 21:27:34,158 | INFO | -34.04 * 0.5 = -17.02 for decoder
2026-01-28 21:27:34,158 | INFO |  -5.32 * 0.5 =  -2.66 for ctc
2026-01-28 21:27:34,158 | INFO | total log probability: -19.68
2026-01-28 21:27:34,158 | INFO | normalized log probability: -0.07
2026-01-28 21:27:34,159 | INFO | total number of ended hypotheses: 215
2026-01-28 21:27:34,162 | INFO | best hypo: vraiment<space>dit<space>job<space>la<space>vie<space>de<space>l'homme<space>sur<space>terre<space>est<space>une<space>corvée<space>il<space>fait<space>des<space>journées<space>de<space>manoeuvre<space>comme<space>l'esclave<space>qui<space>désire<space>un<space>peu<space>d'ombre<space>comme<space>le<space>manoeuvre<space>qui<space>attend<space>sa<space>paye<space>depuis<space>des<space>mois<space>dit<space>job<space>je<space>n'ai<space>rien<space>gagné<space>sinon<space>que<space>du<space>néant<space>je<space>ne<space>compte<space>que<space>des<space>nuits<space>de<space>souffrance

2026-01-28 21:27:34,165 | INFO | speech length: 44160
2026-01-28 21:27:34,215 | INFO | decoder input length: 68
2026-01-28 21:27:34,215 | INFO | max output length: 68
2026-01-28 21:27:34,215 | INFO | min output length: 6
2026-01-28 21:27:36,125 | INFO | end detected at 51
2026-01-28 21:27:36,126 | INFO |  -3.83 * 0.5 =  -1.92 for decoder
2026-01-28 21:27:36,127 | INFO |  -0.79 * 0.5 =  -0.40 for ctc
2026-01-28 21:27:36,127 | INFO | total log probability: -2.31
2026-01-28 21:27:36,127 | INFO | normalized log probability: -0.05
2026-01-28 21:27:36,127 | INFO | total number of ended hypotheses: 151
2026-01-28 21:27:36,127 | INFO | best hypo: que<space>de<space>nuit<space>de<space>souffrance<space>dans<space>notre<space>humanité

2026-01-28 21:27:36,129 | INFO | speech length: 116320
2026-01-28 21:27:36,190 | INFO | decoder input length: 181
2026-01-28 21:27:36,190 | INFO | max output length: 181
2026-01-28 21:27:36,190 | INFO | min output length: 18
2026-01-28 21:27:42,257 | INFO | end detected at 123
2026-01-28 21:27:42,258 | INFO |  -9.41 * 0.5 =  -4.70 for decoder
2026-01-28 21:27:42,258 | INFO |  -0.80 * 0.5 =  -0.40 for ctc
2026-01-28 21:27:42,258 | INFO | total log probability: -5.10
2026-01-28 21:27:42,259 | INFO | normalized log probability: -0.04
2026-01-28 21:27:42,259 | INFO | total number of ended hypotheses: 175
2026-01-28 21:27:42,260 | INFO | best hypo: et<space>beaucoup<space>il<space>faut<space>l'avouer<space>beaucoup<space>s'éloignent<space>de<space>la<space>foi<space>à<space>cause<space>de<space>ce<space>mal<space>à<space>cause<space>de<space>cette<space>souffrance<space>sans<space>répit

2026-01-28 21:27:42,262 | INFO | speech length: 343520
2026-01-28 21:27:42,305 | INFO | decoder input length: 536
2026-01-28 21:27:42,305 | INFO | max output length: 536
2026-01-28 21:27:42,305 | INFO | min output length: 53
2026-01-28 21:28:12,710 | INFO | end detected at 347
2026-01-28 21:28:12,712 | INFO | -42.22 * 0.5 = -21.11 for decoder
2026-01-28 21:28:12,712 | INFO |  -6.50 * 0.5 =  -3.25 for ctc
2026-01-28 21:28:12,712 | INFO | total log probability: -24.36
2026-01-28 21:28:12,712 | INFO | normalized log probability: -0.07
2026-01-28 21:28:12,712 | INFO | total number of ended hypotheses: 186
2026-01-28 21:28:12,716 | INFO | best hypo: aussi<space>à<space>l'approche<space>de<space>la<space>journée<space>des<space>malades<space>voulus<space>par<space>jean<space>paul<space>ii<space>le<space>onze<space>février<space>en<space>la<space>fête<space>de<space>notre<space>dame<space>de<space>lourdes<space>en<space>ce<space>jour<space>de<space>la<space>pastorale<space>de<space>la<space>santé<space>la<space>puissance<space>de<space>guérison<space>de<space>jésus<space>le<space>don<space>de<space>la<space>vie<space>qu'il<space>communique<space>interroge<space>vraiment<space>vraiment<space>la<space>responsabilité<space>de<space>l'église<space>c'est<space>à<space>dire<space>la<space>responsabilité<space>de<space>chacun<space>d'entre<space>nous

2026-01-28 21:28:12,719 | INFO | speech length: 49600
2026-01-28 21:28:12,762 | INFO | decoder input length: 77
2026-01-28 21:28:12,763 | INFO | max output length: 77
2026-01-28 21:28:12,763 | INFO | min output length: 7
2026-01-28 21:28:14,796 | INFO | end detected at 53
2026-01-28 21:28:14,797 | INFO |  -3.79 * 0.5 =  -1.90 for decoder
2026-01-28 21:28:14,798 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-28 21:28:14,798 | INFO | total log probability: -1.90
2026-01-28 21:28:14,798 | INFO | normalized log probability: -0.04
2026-01-28 21:28:14,798 | INFO | total number of ended hypotheses: 167
2026-01-28 21:28:14,798 | INFO | best hypo: pour<space>que<space>la<space>parole<space>des<space>chrétiens<space>soit<space>entendue

2026-01-28 21:28:14,800 | INFO | speech length: 186880
2026-01-28 21:28:14,842 | INFO | decoder input length: 291
2026-01-28 21:28:14,843 | INFO | max output length: 291
2026-01-28 21:28:14,843 | INFO | min output length: 29
2026-01-28 21:28:26,219 | INFO | end detected at 171
2026-01-28 21:28:26,221 | INFO | -13.30 * 0.5 =  -6.65 for decoder
2026-01-28 21:28:26,221 | INFO |  -0.21 * 0.5 =  -0.11 for ctc
2026-01-28 21:28:26,221 | INFO | total log probability: -6.75
2026-01-28 21:28:26,221 | INFO | normalized log probability: -0.04
2026-01-28 21:28:26,221 | INFO | total number of ended hypotheses: 172
2026-01-28 21:28:26,224 | INFO | best hypo: comme<space>la<space>parole<space>du<space>christ<space>qui<space>donne<space>la<space>vie<space>pour<space>que<space>notre<space>parole<space>soit<space>entendue<space>pour<space>qu'elle<space>soit<space>comprise<space>ne<space>faut<space>il<space>pas<space>comme<space>pour<space>jésus<space>que<space>d'abord<space>nous<space>agissions

2026-01-28 21:28:26,237 | INFO | Chunk: 0 | WER=3.225806 | S=0 D=0 I=1
2026-01-28 21:28:26,238 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:28:26,238 | INFO | Chunk: 2 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:28:26,239 | INFO | Chunk: 3 | WER=13.333333 | S=2 D=0 I=0
2026-01-28 21:28:26,240 | INFO | Chunk: 4 | WER=7.692308 | S=2 D=0 I=0
2026-01-28 21:28:26,240 | INFO | Chunk: 5 | WER=8.333333 | S=1 D=0 I=0
2026-01-28 21:28:26,240 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:28:26,241 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:28:26,241 | INFO | Chunk: 8 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:28:26,241 | INFO | Chunk: 9 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:28:26,242 | INFO | Chunk: 10 | WER=14.285714 | S=2 D=0 I=0
2026-01-28 21:28:26,243 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:28:26,244 | INFO | Chunk: 12 | WER=12.000000 | S=2 D=1 I=0
2026-01-28 21:28:26,245 | INFO | Chunk: 13 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:28:26,245 | INFO | Chunk: 14 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:28:26,246 | INFO | Chunk: 15 | WER=12.500000 | S=2 D=0 I=0
2026-01-28 21:28:26,246 | INFO | Chunk: 16 | WER=33.333333 | S=1 D=0 I=0
2026-01-28 21:28:26,247 | INFO | Chunk: 17 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:28:26,249 | INFO | Chunk: 18 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:28:26,249 | INFO | Chunk: 19 | WER=20.000000 | S=1 D=0 I=0
2026-01-28 21:28:26,251 | INFO | Chunk: 20 | WER=5.714286 | S=1 D=0 I=1
2026-01-28 21:28:26,254 | INFO | Chunk: 21 | WER=11.475410 | S=7 D=0 I=0
2026-01-28 21:28:26,255 | INFO | Chunk: 22 | WER=25.000000 | S=2 D=0 I=0
2026-01-28 21:28:26,255 | INFO | Chunk: 23 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:28:26,255 | INFO | Chunk: 24 | WER=31.250000 | S=3 D=0 I=2
2026-01-28 21:28:26,256 | INFO | Chunk: 25 | WER=10.000000 | S=0 D=0 I=1
2026-01-28 21:28:26,256 | INFO | Chunk: 26 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:28:26,257 | INFO | Chunk: 27 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:28:26,257 | INFO | Chunk: 28 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:28:26,259 | INFO | Chunk: 29 | WER=7.500000 | S=3 D=0 I=0
2026-01-28 21:28:26,262 | INFO | Chunk: 30 | WER=1.724138 | S=0 D=1 I=0
2026-01-28 21:28:26,262 | INFO | Chunk: 31 | WER=12.500000 | S=1 D=0 I=0
2026-01-28 21:28:26,263 | INFO | Chunk: 32 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:28:26,267 | INFO | Chunk: 33 | WER=2.985075 | S=2 D=0 I=0
2026-01-28 21:28:26,268 | INFO | Chunk: 34 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:28:26,269 | INFO | Chunk: 35 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:28:26,656 | INFO | File: Rhap-M2003.wav | WER=5.379310 | S=32 D=2 I=5
2026-01-28 21:28:26,656 | INFO | ------------------------------
2026-01-28 21:28:26,657 | INFO | Conf ester Done!
2026-01-28 21:32:38,827 | INFO | Chunk: 0 | WER=3.225806 | S=0 D=0 I=1
2026-01-28 21:32:38,828 | INFO | Chunk: 1 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:32:38,829 | INFO | Chunk: 2 | WER=50.000000 | S=0 D=0 I=1
2026-01-28 21:32:38,830 | INFO | Chunk: 3 | WER=20.000000 | S=3 D=0 I=0
2026-01-28 21:32:38,832 | INFO | Chunk: 4 | WER=3.846154 | S=1 D=0 I=0
2026-01-28 21:32:38,832 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:32:38,833 | INFO | Chunk: 6 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:32:38,834 | INFO | Chunk: 7 | WER=10.000000 | S=1 D=0 I=0
2026-01-28 21:32:38,834 | INFO | Chunk: 8 | WER=25.000000 | S=1 D=0 I=0
2026-01-28 21:32:38,835 | INFO | Chunk: 9 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:32:38,836 | INFO | Chunk: 10 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:32:38,837 | INFO | Chunk: 11 | WER=5.000000 | S=1 D=0 I=0
2026-01-28 21:32:38,838 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:32:38,839 | INFO | Chunk: 13 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:32:38,841 | INFO | Chunk: 14 | WER=10.526316 | S=1 D=1 I=0
2026-01-28 21:32:38,842 | INFO | Chunk: 15 | WER=12.500000 | S=2 D=0 I=0
2026-01-28 21:32:38,842 | INFO | Chunk: 16 | WER=33.333333 | S=1 D=0 I=0
2026-01-28 21:32:38,844 | INFO | Chunk: 17 | WER=3.125000 | S=1 D=0 I=0
2026-01-28 21:32:38,847 | INFO | Chunk: 18 | WER=8.823529 | S=1 D=0 I=2
2026-01-28 21:32:38,847 | INFO | Chunk: 19 | WER=20.000000 | S=1 D=0 I=0
2026-01-28 21:32:38,850 | INFO | Chunk: 20 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:32:38,855 | INFO | Chunk: 21 | WER=9.836066 | S=6 D=0 I=0
2026-01-28 21:32:38,856 | INFO | Chunk: 22 | WER=25.000000 | S=2 D=0 I=0
2026-01-28 21:32:38,856 | INFO | Chunk: 23 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:32:38,857 | INFO | Chunk: 24 | WER=12.500000 | S=2 D=0 I=0
2026-01-28 21:32:38,858 | INFO | Chunk: 25 | WER=10.000000 | S=0 D=0 I=1
2026-01-28 21:32:38,858 | INFO | Chunk: 26 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:32:38,859 | INFO | Chunk: 27 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:32:38,860 | INFO | Chunk: 28 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:32:38,863 | INFO | Chunk: 29 | WER=5.000000 | S=2 D=0 I=0
2026-01-28 21:32:38,868 | INFO | Chunk: 30 | WER=3.448276 | S=1 D=1 I=0
2026-01-28 21:32:38,869 | INFO | Chunk: 31 | WER=37.500000 | S=2 D=0 I=1
2026-01-28 21:32:38,871 | INFO | Chunk: 32 | WER=4.166667 | S=1 D=0 I=0
2026-01-28 21:32:38,877 | INFO | Chunk: 33 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:32:38,878 | INFO | Chunk: 34 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:32:38,880 | INFO | Chunk: 35 | WER=3.125000 | S=1 D=0 I=0
2026-01-28 21:32:39,508 | INFO | File: Rhap-M2003.wav | WER=5.379310 | S=31 D=2 I=6
2026-01-28 21:32:39,508 | INFO | ------------------------------
2026-01-28 21:32:39,508 | INFO | hmm_tdnn Done!
2026-01-28 21:32:39,721 | INFO | ==================================Rhap-M2004.wav=========================================
2026-01-28 21:32:40,141 | INFO | Using rVAD model
2026-01-28 21:33:45,320 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,320 | INFO | Chunk: 1 | WER=16.666667 | S=0 D=0 I=1
2026-01-28 21:33:45,321 | INFO | Chunk: 2 | WER=9.523810 | S=0 D=0 I=2
2026-01-28 21:33:45,321 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,322 | INFO | Chunk: 4 | WER=30.000000 | S=0 D=1 I=2
2026-01-28 21:33:45,322 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,322 | INFO | Chunk: 6 | WER=12.500000 | S=0 D=0 I=2
2026-01-28 21:33:45,323 | INFO | Chunk: 7 | WER=6.666667 | S=1 D=0 I=0
2026-01-28 21:33:45,323 | INFO | Chunk: 8 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 21:33:45,324 | INFO | Chunk: 9 | WER=13.793103 | S=3 D=0 I=1
2026-01-28 21:33:45,324 | INFO | Chunk: 10 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,325 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,325 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,326 | INFO | Chunk: 13 | WER=27.272727 | S=1 D=0 I=2
2026-01-28 21:33:45,326 | INFO | Chunk: 14 | WER=12.500000 | S=1 D=0 I=0
2026-01-28 21:33:45,326 | INFO | Chunk: 15 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,327 | INFO | Chunk: 16 | WER=12.500000 | S=0 D=0 I=2
2026-01-28 21:33:45,327 | INFO | Chunk: 17 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,327 | INFO | Chunk: 18 | WER=16.666667 | S=0 D=0 I=1
2026-01-28 21:33:45,327 | INFO | Chunk: 19 | WER=60.000000 | S=0 D=0 I=3
2026-01-28 21:33:45,328 | INFO | Chunk: 20 | WER=23.076923 | S=1 D=2 I=0
2026-01-28 21:33:45,328 | INFO | Chunk: 21 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,328 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,329 | INFO | Chunk: 23 | WER=37.500000 | S=1 D=1 I=1
2026-01-28 21:33:45,329 | INFO | Chunk: 24 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,329 | INFO | Chunk: 25 | WER=7.692308 | S=0 D=0 I=1
2026-01-28 21:33:45,329 | INFO | Chunk: 26 | WER=60.000000 | S=0 D=0 I=3
2026-01-28 21:33:45,330 | INFO | Chunk: 27 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,330 | INFO | Chunk: 28 | WER=21.428571 | S=2 D=0 I=1
2026-01-28 21:33:45,330 | INFO | Chunk: 29 | WER=66.666667 | S=2 D=0 I=0
2026-01-28 21:33:45,330 | INFO | Chunk: 30 | WER=0.000000 | S=1 D=0 I=0
2026-01-28 21:33:45,331 | INFO | Chunk: 31 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,332 | INFO | Chunk: 32 | WER=12.500000 | S=2 D=0 I=1
2026-01-28 21:33:45,332 | INFO | Chunk: 33 | WER=40.000000 | S=0 D=0 I=2
2026-01-28 21:33:45,332 | INFO | Chunk: 34 | WER=300.000000 | S=0 D=0 I=3
2026-01-28 21:33:45,333 | INFO | Chunk: 35 | WER=6.250000 | S=1 D=0 I=0
2026-01-28 21:33:45,333 | INFO | Chunk: 36 | WER=33.333333 | S=0 D=0 I=2
2026-01-28 21:33:45,334 | INFO | Chunk: 37 | WER=6.060606 | S=0 D=0 I=2
2026-01-28 21:33:45,334 | INFO | Chunk: 38 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,335 | INFO | Chunk: 39 | WER=22.222222 | S=2 D=0 I=0
2026-01-28 21:33:45,335 | INFO | Chunk: 40 | WER=9.090909 | S=1 D=0 I=0
2026-01-28 21:33:45,335 | INFO | Chunk: 41 | WER=100.000000 | S=3 D=0 I=0
2026-01-28 21:33:45,335 | INFO | Chunk: 42 | WER=20.000000 | S=0 D=0 I=2
2026-01-28 21:33:45,336 | INFO | Chunk: 43 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,336 | INFO | Chunk: 44 | WER=12.500000 | S=1 D=0 I=0
2026-01-28 21:33:45,336 | INFO | Chunk: 45 | WER=5.882353 | S=1 D=0 I=0
2026-01-28 21:33:45,337 | INFO | Chunk: 46 | WER=33.333333 | S=0 D=0 I=1
2026-01-28 21:33:45,337 | INFO | Chunk: 47 | WER=33.333333 | S=1 D=0 I=1
2026-01-28 21:33:45,337 | INFO | Chunk: 48 | WER=50.000000 | S=1 D=0 I=1
2026-01-28 21:33:45,337 | INFO | Chunk: 49 | WER=6.666667 | S=0 D=0 I=1
2026-01-28 21:33:45,338 | INFO | Chunk: 50 | WER=100.000000 | S=1 D=0 I=1
2026-01-28 21:33:45,338 | INFO | Chunk: 51 | WER=50.000000 | S=1 D=0 I=1
2026-01-28 21:33:45,338 | INFO | Chunk: 52 | WER=83.333333 | S=2 D=1 I=2
2026-01-28 21:33:45,338 | INFO | Chunk: 53 | WER=300.000000 | S=0 D=0 I=3
2026-01-28 21:33:45,338 | INFO | Chunk: 54 | WER=41.666667 | S=2 D=3 I=0
2026-01-28 21:33:45,339 | INFO | Chunk: 55 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,339 | INFO | Chunk: 56 | WER=5.555556 | S=1 D=0 I=0
2026-01-28 21:33:45,339 | INFO | Chunk: 57 | WER=14.285714 | S=0 D=0 I=1
2026-01-28 21:33:45,340 | INFO | Chunk: 58 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,340 | INFO | Chunk: 59 | WER=15.384615 | S=0 D=0 I=2
2026-01-28 21:33:45,340 | INFO | Chunk: 60 | WER=60.000000 | S=2 D=0 I=1
2026-01-28 21:33:45,340 | INFO | Chunk: 61 | WER=66.666667 | S=0 D=1 I=1
2026-01-28 21:33:45,341 | INFO | Chunk: 62 | WER=25.000000 | S=0 D=0 I=2
2026-01-28 21:33:45,341 | INFO | Chunk: 63 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,341 | INFO | Chunk: 64 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 21:33:45,341 | INFO | Chunk: 65 | WER=66.666667 | S=0 D=1 I=1
2026-01-28 21:33:45,342 | INFO | Chunk: 66 | WER=14.285714 | S=2 D=0 I=1
2026-01-28 21:33:45,342 | INFO | Chunk: 67 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,342 | INFO | Chunk: 68 | WER=20.000000 | S=0 D=0 I=1
2026-01-28 21:33:45,343 | INFO | Chunk: 69 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,343 | INFO | Chunk: 70 | WER=7.692308 | S=0 D=0 I=1
2026-01-28 21:33:45,343 | INFO | Chunk: 71 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,344 | INFO | Chunk: 72 | WER=40.000000 | S=2 D=0 I=0
2026-01-28 21:33:45,344 | INFO | Chunk: 73 | WER=30.000000 | S=1 D=0 I=2
2026-01-28 21:33:45,344 | INFO | Chunk: 74 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,344 | INFO | Chunk: 75 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,345 | INFO | Chunk: 76 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 21:33:45,345 | INFO | Chunk: 77 | WER=12.500000 | S=0 D=0 I=2
2026-01-28 21:33:45,345 | INFO | Chunk: 78 | WER=20.000000 | S=0 D=0 I=2
2026-01-28 21:33:45,346 | INFO | Chunk: 79 | WER=20.000000 | S=0 D=0 I=3
2026-01-28 21:33:45,346 | INFO | Chunk: 80 | WER=6.666667 | S=0 D=0 I=1
2026-01-28 21:33:45,347 | INFO | Chunk: 81 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,347 | INFO | Chunk: 82 | WER=30.000000 | S=0 D=2 I=1
2026-01-28 21:33:45,347 | INFO | Chunk: 83 | WER=50.000000 | S=1 D=0 I=2
2026-01-28 21:33:45,347 | INFO | Chunk: 84 | WER=37.500000 | S=1 D=0 I=2
2026-01-28 21:33:45,348 | INFO | Chunk: 85 | WER=20.000000 | S=0 D=1 I=0
2026-01-28 21:33:45,348 | INFO | Chunk: 86 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,348 | INFO | Chunk: 87 | WER=20.000000 | S=0 D=0 I=1
2026-01-28 21:33:45,349 | INFO | Chunk: 88 | WER=11.111111 | S=1 D=0 I=1
2026-01-28 21:33:45,349 | INFO | Chunk: 89 | WER=5.882353 | S=0 D=0 I=1
2026-01-28 21:33:45,349 | INFO | Chunk: 90 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,350 | INFO | Chunk: 91 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,350 | INFO | Chunk: 92 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 21:33:45,350 | INFO | Chunk: 93 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,350 | INFO | Chunk: 94 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,350 | INFO | Chunk: 95 | WER=50.000000 | S=0 D=0 I=3
2026-01-28 21:33:45,351 | INFO | Chunk: 96 | WER=150.000000 | S=0 D=0 I=3
2026-01-28 21:33:45,351 | INFO | Chunk: 97 | WER=33.333333 | S=0 D=1 I=0
2026-01-28 21:33:45,351 | INFO | Chunk: 98 | WER=6.666667 | S=0 D=0 I=1
2026-01-28 21:33:45,351 | INFO | Chunk: 99 | WER=25.000000 | S=1 D=0 I=1
2026-01-28 21:33:45,352 | INFO | Chunk: 100 | WER=200.000000 | S=1 D=0 I=1
2026-01-28 21:33:45,352 | INFO | Chunk: 101 | WER=50.000000 | S=0 D=0 I=1
2026-01-28 21:33:45,352 | INFO | Chunk: 102 | WER=50.000000 | S=0 D=0 I=1
2026-01-28 21:33:45,352 | INFO | Chunk: 103 | WER=7.692308 | S=0 D=0 I=1
2026-01-28 21:33:45,352 | INFO | Chunk: 104 | WER=50.000000 | S=0 D=0 I=1
2026-01-28 21:33:45,353 | INFO | Chunk: 105 | WER=33.333333 | S=0 D=0 I=2
2026-01-28 21:33:45,353 | INFO | Chunk: 106 | WER=12.500000 | S=2 D=0 I=0
2026-01-28 21:33:45,353 | INFO | Chunk: 107 | WER=7.692308 | S=0 D=0 I=1
2026-01-28 21:33:45,354 | INFO | Chunk: 108 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,354 | INFO | Chunk: 109 | WER=6.250000 | S=0 D=0 I=1
2026-01-28 21:33:45,354 | INFO | Chunk: 110 | WER=22.222222 | S=0 D=1 I=1
2026-01-28 21:33:45,355 | INFO | Chunk: 111 | WER=14.285714 | S=2 D=0 I=0
2026-01-28 21:33:45,355 | INFO | Chunk: 112 | WER=18.181818 | S=0 D=0 I=2
2026-01-28 21:33:45,355 | INFO | Chunk: 113 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,356 | INFO | Chunk: 114 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,356 | INFO | Chunk: 115 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,356 | INFO | Chunk: 116 | WER=10.000000 | S=1 D=0 I=0
2026-01-28 21:33:45,356 | INFO | Chunk: 117 | WER=20.000000 | S=0 D=0 I=1
2026-01-28 21:33:45,356 | INFO | Chunk: 118 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:33:45,357 | INFO | Chunk: 119 | WER=16.666667 | S=0 D=0 I=2
2026-01-28 21:33:45,357 | INFO | Chunk: 120 | WER=5.555556 | S=0 D=0 I=1
2026-01-28 21:33:45,358 | INFO | Chunk: 121 | WER=60.000000 | S=0 D=0 I=3
2026-01-28 21:33:45,358 | INFO | Chunk: 122 | WER=9.090909 | S=1 D=0 I=0
2026-01-28 21:33:45,358 | INFO | Chunk: 123 | WER=33.333333 | S=0 D=0 I=1
2026-01-28 21:33:45,358 | INFO | Chunk: 124 | WER=33.333333 | S=0 D=0 I=2
2026-01-28 21:33:46,147 | INFO | File: Rhap-M2004.wav | WER=13.925152 | S=54 D=6 I=100
2026-01-28 21:33:46,148 | INFO | ------------------------------
2026-01-28 21:33:46,148 | INFO | w2vec vad chunk Done!
2026-01-28 21:35:17,844 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,844 | INFO | Chunk: 1 | WER=16.666667 | S=0 D=0 I=1
2026-01-28 21:35:17,845 | INFO | Chunk: 2 | WER=28.571429 | S=3 D=3 I=0
2026-01-28 21:35:17,845 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,846 | INFO | Chunk: 4 | WER=30.000000 | S=0 D=1 I=2
2026-01-28 21:35:17,846 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,847 | INFO | Chunk: 6 | WER=12.500000 | S=0 D=0 I=2
2026-01-28 21:35:17,847 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,847 | INFO | Chunk: 8 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 21:35:17,848 | INFO | Chunk: 9 | WER=13.793103 | S=3 D=0 I=1
2026-01-28 21:35:17,849 | INFO | Chunk: 10 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,849 | INFO | Chunk: 11 | WER=12.500000 | S=0 D=0 I=1
2026-01-28 21:35:17,850 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,850 | INFO | Chunk: 13 | WER=27.272727 | S=1 D=0 I=2
2026-01-28 21:35:17,851 | INFO | Chunk: 14 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,851 | INFO | Chunk: 15 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,851 | INFO | Chunk: 16 | WER=12.500000 | S=0 D=0 I=2
2026-01-28 21:35:17,852 | INFO | Chunk: 17 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,852 | INFO | Chunk: 18 | WER=33.333333 | S=1 D=0 I=1
2026-01-28 21:35:17,852 | INFO | Chunk: 19 | WER=60.000000 | S=0 D=0 I=3
2026-01-28 21:35:17,853 | INFO | Chunk: 20 | WER=23.076923 | S=1 D=2 I=0
2026-01-28 21:35:17,853 | INFO | Chunk: 21 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,853 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,854 | INFO | Chunk: 23 | WER=25.000000 | S=0 D=1 I=1
2026-01-28 21:35:17,854 | INFO | Chunk: 24 | WER=11.111111 | S=1 D=0 I=0
2026-01-28 21:35:17,854 | INFO | Chunk: 25 | WER=15.384615 | S=0 D=0 I=2
2026-01-28 21:35:17,855 | INFO | Chunk: 26 | WER=60.000000 | S=0 D=0 I=3
2026-01-28 21:35:17,855 | INFO | Chunk: 27 | WER=20.000000 | S=1 D=0 I=1
2026-01-28 21:35:17,856 | INFO | Chunk: 28 | WER=21.428571 | S=1 D=0 I=2
2026-01-28 21:35:17,856 | INFO | Chunk: 29 | WER=33.333333 | S=0 D=0 I=1
2026-01-28 21:35:17,856 | INFO | Chunk: 30 | WER=0.000000 | S=1 D=0 I=0
2026-01-28 21:35:17,857 | INFO | Chunk: 31 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,857 | INFO | Chunk: 32 | WER=29.166667 | S=6 D=0 I=1
2026-01-28 21:35:17,858 | INFO | Chunk: 33 | WER=40.000000 | S=0 D=0 I=2
2026-01-28 21:35:17,858 | INFO | Chunk: 34 | WER=300.000000 | S=0 D=0 I=3
2026-01-28 21:35:17,858 | INFO | Chunk: 35 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,859 | INFO | Chunk: 36 | WER=33.333333 | S=0 D=0 I=2
2026-01-28 21:35:17,860 | INFO | Chunk: 37 | WER=12.121212 | S=0 D=0 I=4
2026-01-28 21:35:17,860 | INFO | Chunk: 38 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,861 | INFO | Chunk: 39 | WER=11.111111 | S=1 D=0 I=0
2026-01-28 21:35:17,861 | INFO | Chunk: 40 | WER=9.090909 | S=1 D=0 I=0
2026-01-28 21:35:17,861 | INFO | Chunk: 41 | WER=66.666667 | S=0 D=1 I=1
2026-01-28 21:35:17,862 | INFO | Chunk: 42 | WER=20.000000 | S=0 D=0 I=2
2026-01-28 21:35:17,862 | INFO | Chunk: 43 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,862 | INFO | Chunk: 44 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,863 | INFO | Chunk: 45 | WER=5.882353 | S=1 D=0 I=0
2026-01-28 21:35:17,863 | INFO | Chunk: 46 | WER=33.333333 | S=0 D=0 I=1
2026-01-28 21:35:17,863 | INFO | Chunk: 47 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,863 | INFO | Chunk: 48 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,864 | INFO | Chunk: 49 | WER=6.666667 | S=0 D=0 I=1
2026-01-28 21:35:17,864 | INFO | Chunk: 50 | WER=50.000000 | S=1 D=0 I=0
2026-01-28 21:35:17,864 | INFO | Chunk: 51 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 21:35:17,864 | INFO | Chunk: 52 | WER=83.333333 | S=2 D=1 I=2
2026-01-28 21:35:17,865 | INFO | Chunk: 53 | WER=300.000000 | S=0 D=0 I=3
2026-01-28 21:35:17,865 | INFO | Chunk: 54 | WER=8.333333 | S=1 D=0 I=0
2026-01-28 21:35:17,865 | INFO | Chunk: 55 | WER=7.692308 | S=1 D=0 I=0
2026-01-28 21:35:17,866 | INFO | Chunk: 56 | WER=5.555556 | S=1 D=0 I=0
2026-01-28 21:35:17,866 | INFO | Chunk: 57 | WER=14.285714 | S=0 D=0 I=1
2026-01-28 21:35:17,867 | INFO | Chunk: 58 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,867 | INFO | Chunk: 59 | WER=15.384615 | S=0 D=0 I=2
2026-01-28 21:35:17,867 | INFO | Chunk: 60 | WER=20.000000 | S=0 D=0 I=1
2026-01-28 21:35:17,868 | INFO | Chunk: 61 | WER=66.666667 | S=0 D=1 I=1
2026-01-28 21:35:17,868 | INFO | Chunk: 62 | WER=25.000000 | S=0 D=0 I=2
2026-01-28 21:35:17,868 | INFO | Chunk: 63 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,868 | INFO | Chunk: 64 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 21:35:17,869 | INFO | Chunk: 65 | WER=66.666667 | S=0 D=1 I=1
2026-01-28 21:35:17,869 | INFO | Chunk: 66 | WER=23.809524 | S=4 D=0 I=1
2026-01-28 21:35:17,869 | INFO | Chunk: 67 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,870 | INFO | Chunk: 68 | WER=60.000000 | S=2 D=0 I=1
2026-01-28 21:35:17,870 | INFO | Chunk: 69 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,870 | INFO | Chunk: 70 | WER=7.692308 | S=0 D=0 I=1
2026-01-28 21:35:17,871 | INFO | Chunk: 71 | WER=7.692308 | S=1 D=0 I=0
2026-01-28 21:35:17,871 | INFO | Chunk: 72 | WER=40.000000 | S=2 D=0 I=0
2026-01-28 21:35:17,871 | INFO | Chunk: 73 | WER=30.000000 | S=1 D=0 I=2
2026-01-28 21:35:17,872 | INFO | Chunk: 74 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,872 | INFO | Chunk: 75 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,872 | INFO | Chunk: 76 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 21:35:17,873 | INFO | Chunk: 77 | WER=18.750000 | S=1 D=0 I=2
2026-01-28 21:35:17,873 | INFO | Chunk: 78 | WER=20.000000 | S=0 D=0 I=2
2026-01-28 21:35:17,874 | INFO | Chunk: 79 | WER=20.000000 | S=0 D=0 I=3
2026-01-28 21:35:17,874 | INFO | Chunk: 80 | WER=6.666667 | S=0 D=0 I=1
2026-01-28 21:35:17,875 | INFO | Chunk: 81 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,875 | INFO | Chunk: 82 | WER=30.000000 | S=0 D=2 I=1
2026-01-28 21:35:17,875 | INFO | Chunk: 83 | WER=16.666667 | S=0 D=0 I=1
2026-01-28 21:35:17,876 | INFO | Chunk: 84 | WER=37.500000 | S=1 D=0 I=2
2026-01-28 21:35:17,876 | INFO | Chunk: 85 | WER=40.000000 | S=0 D=1 I=1
2026-01-28 21:35:17,876 | INFO | Chunk: 86 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,876 | INFO | Chunk: 87 | WER=20.000000 | S=0 D=0 I=1
2026-01-28 21:35:17,877 | INFO | Chunk: 88 | WER=16.666667 | S=1 D=0 I=2
2026-01-28 21:35:17,877 | INFO | Chunk: 89 | WER=5.882353 | S=0 D=0 I=1
2026-01-28 21:35:17,878 | INFO | Chunk: 90 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,878 | INFO | Chunk: 91 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,878 | INFO | Chunk: 92 | WER=50.000000 | S=1 D=0 I=1
2026-01-28 21:35:17,879 | INFO | Chunk: 93 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,879 | INFO | Chunk: 94 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,879 | INFO | Chunk: 95 | WER=50.000000 | S=0 D=0 I=3
2026-01-28 21:35:17,879 | INFO | Chunk: 96 | WER=250.000000 | S=2 D=0 I=3
2026-01-28 21:35:17,880 | INFO | Chunk: 97 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,880 | INFO | Chunk: 98 | WER=6.666667 | S=0 D=0 I=1
2026-01-28 21:35:17,880 | INFO | Chunk: 99 | WER=25.000000 | S=1 D=0 I=1
2026-01-28 21:35:17,880 | INFO | Chunk: 100 | WER=200.000000 | S=1 D=0 I=1
2026-01-28 21:35:17,881 | INFO | Chunk: 101 | WER=50.000000 | S=0 D=0 I=1
2026-01-28 21:35:17,881 | INFO | Chunk: 102 | WER=100.000000 | S=1 D=0 I=1
2026-01-28 21:35:17,881 | INFO | Chunk: 103 | WER=7.692308 | S=0 D=0 I=1
2026-01-28 21:35:17,881 | INFO | Chunk: 104 | WER=50.000000 | S=0 D=0 I=1
2026-01-28 21:35:17,882 | INFO | Chunk: 105 | WER=33.333333 | S=0 D=0 I=2
2026-01-28 21:35:17,882 | INFO | Chunk: 106 | WER=12.500000 | S=2 D=0 I=0
2026-01-28 21:35:17,883 | INFO | Chunk: 107 | WER=7.692308 | S=0 D=0 I=1
2026-01-28 21:35:17,883 | INFO | Chunk: 108 | WER=33.333333 | S=1 D=0 I=0
2026-01-28 21:35:17,883 | INFO | Chunk: 109 | WER=6.250000 | S=0 D=0 I=1
2026-01-28 21:35:17,884 | INFO | Chunk: 110 | WER=22.222222 | S=0 D=1 I=1
2026-01-28 21:35:17,884 | INFO | Chunk: 111 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,885 | INFO | Chunk: 112 | WER=18.181818 | S=0 D=0 I=2
2026-01-28 21:35:17,885 | INFO | Chunk: 113 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,885 | INFO | Chunk: 114 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,885 | INFO | Chunk: 115 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,886 | INFO | Chunk: 116 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,886 | INFO | Chunk: 117 | WER=20.000000 | S=0 D=0 I=1
2026-01-28 21:35:17,886 | INFO | Chunk: 118 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:35:17,887 | INFO | Chunk: 119 | WER=16.666667 | S=0 D=0 I=2
2026-01-28 21:35:17,887 | INFO | Chunk: 120 | WER=5.555556 | S=0 D=0 I=1
2026-01-28 21:35:17,887 | INFO | Chunk: 121 | WER=60.000000 | S=0 D=0 I=3
2026-01-28 21:35:17,888 | INFO | Chunk: 122 | WER=9.090909 | S=1 D=0 I=0
2026-01-28 21:35:17,888 | INFO | Chunk: 123 | WER=33.333333 | S=0 D=0 I=1
2026-01-28 21:35:17,888 | INFO | Chunk: 124 | WER=16.666667 | S=0 D=0 I=1
2026-01-28 21:35:18,682 | INFO | File: Rhap-M2004.wav | WER=14.534378 | S=50 D=10 I=107
2026-01-28 21:35:18,682 | INFO | ------------------------------
2026-01-28 21:35:18,682 | INFO | whisper med Done!
2026-01-28 21:37:27,126 | INFO | Chunk: 0 | WER=33.333333 | S=1 D=0 I=0
2026-01-28 21:37:27,126 | INFO | Chunk: 1 | WER=16.666667 | S=0 D=0 I=1
2026-01-28 21:37:27,127 | INFO | Chunk: 2 | WER=9.523810 | S=0 D=0 I=2
2026-01-28 21:37:27,127 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,128 | INFO | Chunk: 4 | WER=30.000000 | S=0 D=1 I=2
2026-01-28 21:37:27,128 | INFO | Chunk: 5 | WER=27.272727 | S=3 D=0 I=0
2026-01-28 21:37:27,128 | INFO | Chunk: 6 | WER=25.000000 | S=2 D=0 I=2
2026-01-28 21:37:27,129 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,129 | INFO | Chunk: 8 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 21:37:27,130 | INFO | Chunk: 9 | WER=6.896552 | S=1 D=0 I=1
2026-01-28 21:37:27,130 | INFO | Chunk: 10 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,131 | INFO | Chunk: 11 | WER=12.500000 | S=0 D=0 I=1
2026-01-28 21:37:27,131 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,132 | INFO | Chunk: 13 | WER=27.272727 | S=1 D=0 I=2
2026-01-28 21:37:27,132 | INFO | Chunk: 14 | WER=12.500000 | S=1 D=0 I=0
2026-01-28 21:37:27,132 | INFO | Chunk: 15 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,133 | INFO | Chunk: 16 | WER=12.500000 | S=0 D=0 I=2
2026-01-28 21:37:27,133 | INFO | Chunk: 17 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,133 | INFO | Chunk: 18 | WER=16.666667 | S=0 D=0 I=1
2026-01-28 21:37:27,133 | INFO | Chunk: 19 | WER=60.000000 | S=0 D=0 I=3
2026-01-28 21:37:27,134 | INFO | Chunk: 20 | WER=7.692308 | S=0 D=1 I=0
2026-01-28 21:37:27,134 | INFO | Chunk: 21 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,134 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,135 | INFO | Chunk: 23 | WER=25.000000 | S=0 D=1 I=1
2026-01-28 21:37:27,135 | INFO | Chunk: 24 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,135 | INFO | Chunk: 25 | WER=7.692308 | S=0 D=0 I=1
2026-01-28 21:37:27,135 | INFO | Chunk: 26 | WER=60.000000 | S=0 D=0 I=3
2026-01-28 21:37:27,136 | INFO | Chunk: 27 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,136 | INFO | Chunk: 28 | WER=28.571429 | S=2 D=0 I=2
2026-01-28 21:37:27,136 | INFO | Chunk: 29 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,136 | INFO | Chunk: 30 | WER=0.000000 | S=1 D=0 I=0
2026-01-28 21:37:27,137 | INFO | Chunk: 31 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,138 | INFO | Chunk: 32 | WER=8.333333 | S=1 D=0 I=1
2026-01-28 21:37:27,138 | INFO | Chunk: 33 | WER=40.000000 | S=0 D=0 I=2
2026-01-28 21:37:27,138 | INFO | Chunk: 34 | WER=300.000000 | S=0 D=0 I=3
2026-01-28 21:37:27,139 | INFO | Chunk: 35 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,139 | INFO | Chunk: 36 | WER=33.333333 | S=0 D=0 I=2
2026-01-28 21:37:27,140 | INFO | Chunk: 37 | WER=12.121212 | S=2 D=0 I=2
2026-01-28 21:37:27,140 | INFO | Chunk: 38 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,141 | INFO | Chunk: 39 | WER=11.111111 | S=1 D=0 I=0
2026-01-28 21:37:27,141 | INFO | Chunk: 40 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,141 | INFO | Chunk: 41 | WER=100.000000 | S=3 D=0 I=0
2026-01-28 21:37:27,141 | INFO | Chunk: 42 | WER=20.000000 | S=0 D=0 I=2
2026-01-28 21:37:27,142 | INFO | Chunk: 43 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,142 | INFO | Chunk: 44 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,142 | INFO | Chunk: 45 | WER=5.882353 | S=1 D=0 I=0
2026-01-28 21:37:27,142 | INFO | Chunk: 46 | WER=33.333333 | S=0 D=0 I=1
2026-01-28 21:37:27,143 | INFO | Chunk: 47 | WER=33.333333 | S=1 D=0 I=1
2026-01-28 21:37:27,143 | INFO | Chunk: 48 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,143 | INFO | Chunk: 49 | WER=13.333333 | S=1 D=0 I=1
2026-01-28 21:37:27,143 | INFO | Chunk: 50 | WER=50.000000 | S=1 D=0 I=0
2026-01-28 21:37:27,144 | INFO | Chunk: 51 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 21:37:27,144 | INFO | Chunk: 52 | WER=83.333333 | S=2 D=1 I=2
2026-01-28 21:37:27,144 | INFO | Chunk: 53 | WER=300.000000 | S=0 D=0 I=3
2026-01-28 21:37:27,144 | INFO | Chunk: 54 | WER=8.333333 | S=1 D=0 I=0
2026-01-28 21:37:27,145 | INFO | Chunk: 55 | WER=23.076923 | S=3 D=0 I=0
2026-01-28 21:37:27,145 | INFO | Chunk: 56 | WER=5.555556 | S=1 D=0 I=0
2026-01-28 21:37:27,145 | INFO | Chunk: 57 | WER=14.285714 | S=0 D=0 I=1
2026-01-28 21:37:27,146 | INFO | Chunk: 58 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,146 | INFO | Chunk: 59 | WER=15.384615 | S=0 D=0 I=2
2026-01-28 21:37:27,146 | INFO | Chunk: 60 | WER=40.000000 | S=1 D=0 I=1
2026-01-28 21:37:27,146 | INFO | Chunk: 61 | WER=66.666667 | S=0 D=1 I=1
2026-01-28 21:37:27,147 | INFO | Chunk: 62 | WER=25.000000 | S=0 D=0 I=2
2026-01-28 21:37:27,147 | INFO | Chunk: 63 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,147 | INFO | Chunk: 64 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 21:37:27,147 | INFO | Chunk: 65 | WER=66.666667 | S=0 D=1 I=1
2026-01-28 21:37:27,148 | INFO | Chunk: 66 | WER=23.809524 | S=4 D=0 I=1
2026-01-28 21:37:27,148 | INFO | Chunk: 67 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,148 | INFO | Chunk: 68 | WER=60.000000 | S=2 D=0 I=1
2026-01-28 21:37:27,148 | INFO | Chunk: 69 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,149 | INFO | Chunk: 70 | WER=7.692308 | S=0 D=0 I=1
2026-01-28 21:37:27,149 | INFO | Chunk: 71 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,149 | INFO | Chunk: 72 | WER=40.000000 | S=2 D=0 I=0
2026-01-28 21:37:27,150 | INFO | Chunk: 73 | WER=30.000000 | S=1 D=0 I=2
2026-01-28 21:37:27,150 | INFO | Chunk: 74 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,150 | INFO | Chunk: 75 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,150 | INFO | Chunk: 76 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 21:37:27,151 | INFO | Chunk: 77 | WER=12.500000 | S=0 D=0 I=2
2026-01-28 21:37:27,151 | INFO | Chunk: 78 | WER=20.000000 | S=0 D=0 I=2
2026-01-28 21:37:27,152 | INFO | Chunk: 79 | WER=20.000000 | S=0 D=0 I=3
2026-01-28 21:37:27,152 | INFO | Chunk: 80 | WER=6.666667 | S=0 D=0 I=1
2026-01-28 21:37:27,152 | INFO | Chunk: 81 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,153 | INFO | Chunk: 82 | WER=40.000000 | S=1 D=2 I=1
2026-01-28 21:37:27,153 | INFO | Chunk: 83 | WER=16.666667 | S=0 D=0 I=1
2026-01-28 21:37:27,153 | INFO | Chunk: 84 | WER=37.500000 | S=1 D=0 I=2
2026-01-28 21:37:27,153 | INFO | Chunk: 85 | WER=40.000000 | S=1 D=1 I=0
2026-01-28 21:37:27,154 | INFO | Chunk: 86 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,154 | INFO | Chunk: 87 | WER=20.000000 | S=0 D=0 I=1
2026-01-28 21:37:27,154 | INFO | Chunk: 88 | WER=5.555556 | S=0 D=0 I=1
2026-01-28 21:37:27,155 | INFO | Chunk: 89 | WER=5.882353 | S=0 D=0 I=1
2026-01-28 21:37:27,155 | INFO | Chunk: 90 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,156 | INFO | Chunk: 91 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,156 | INFO | Chunk: 92 | WER=50.000000 | S=1 D=0 I=1
2026-01-28 21:37:27,156 | INFO | Chunk: 93 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,156 | INFO | Chunk: 94 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,156 | INFO | Chunk: 95 | WER=50.000000 | S=0 D=0 I=3
2026-01-28 21:37:27,156 | INFO | Chunk: 96 | WER=250.000000 | S=2 D=0 I=3
2026-01-28 21:37:27,157 | INFO | Chunk: 97 | WER=33.333333 | S=0 D=1 I=0
2026-01-28 21:37:27,157 | INFO | Chunk: 98 | WER=6.666667 | S=0 D=0 I=1
2026-01-28 21:37:27,157 | INFO | Chunk: 99 | WER=12.500000 | S=0 D=0 I=1
2026-01-28 21:37:27,157 | INFO | Chunk: 100 | WER=200.000000 | S=1 D=0 I=1
2026-01-28 21:37:27,158 | INFO | Chunk: 101 | WER=50.000000 | S=0 D=0 I=1
2026-01-28 21:37:27,158 | INFO | Chunk: 102 | WER=100.000000 | S=1 D=0 I=1
2026-01-28 21:37:27,158 | INFO | Chunk: 103 | WER=7.692308 | S=0 D=0 I=1
2026-01-28 21:37:27,158 | INFO | Chunk: 104 | WER=50.000000 | S=0 D=0 I=1
2026-01-28 21:37:27,158 | INFO | Chunk: 105 | WER=33.333333 | S=0 D=0 I=2
2026-01-28 21:37:27,159 | INFO | Chunk: 106 | WER=12.500000 | S=2 D=0 I=0
2026-01-28 21:37:27,159 | INFO | Chunk: 107 | WER=15.384615 | S=1 D=0 I=1
2026-01-28 21:37:27,159 | INFO | Chunk: 108 | WER=33.333333 | S=1 D=0 I=0
2026-01-28 21:37:27,160 | INFO | Chunk: 109 | WER=12.500000 | S=1 D=0 I=1
2026-01-28 21:37:27,160 | INFO | Chunk: 110 | WER=22.222222 | S=0 D=1 I=1
2026-01-28 21:37:27,161 | INFO | Chunk: 111 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,161 | INFO | Chunk: 112 | WER=18.181818 | S=0 D=0 I=2
2026-01-28 21:37:27,161 | INFO | Chunk: 113 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,161 | INFO | Chunk: 114 | WER=25.000000 | S=1 D=0 I=0
2026-01-28 21:37:27,162 | INFO | Chunk: 115 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,162 | INFO | Chunk: 116 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,162 | INFO | Chunk: 117 | WER=20.000000 | S=0 D=0 I=1
2026-01-28 21:37:27,162 | INFO | Chunk: 118 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:37:27,163 | INFO | Chunk: 119 | WER=16.666667 | S=0 D=0 I=2
2026-01-28 21:37:27,163 | INFO | Chunk: 120 | WER=5.555556 | S=0 D=0 I=1
2026-01-28 21:37:27,163 | INFO | Chunk: 121 | WER=40.000000 | S=0 D=0 I=2
2026-01-28 21:37:27,164 | INFO | Chunk: 122 | WER=9.090909 | S=1 D=0 I=0
2026-01-28 21:37:27,164 | INFO | Chunk: 123 | WER=33.333333 | S=0 D=0 I=1
2026-01-28 21:37:27,164 | INFO | Chunk: 124 | WER=16.666667 | S=0 D=0 I=1
2026-01-28 21:37:27,957 | INFO | File: Rhap-M2004.wav | WER=13.925152 | S=55 D=5 I=100
2026-01-28 21:37:27,957 | INFO | ------------------------------
2026-01-28 21:37:27,957 | INFO | whisper large Done!
2026-01-28 21:37:28,136 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 21:37:28,174 | INFO | Vocabulary size: 350
2026-01-28 21:37:29,520 | INFO | Gradient checkpoint layers: []
2026-01-28 21:37:30,373 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 21:37:30,378 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 21:37:30,379 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 21:37:30,379 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 21:37:30,380 | INFO | speech length: 15520
2026-01-28 21:37:30,430 | INFO | decoder input length: 23
2026-01-28 21:37:30,430 | INFO | max output length: 23
2026-01-28 21:37:30,430 | INFO | min output length: 2
2026-01-28 21:37:31,150 | INFO | end detected at 16
2026-01-28 21:37:31,152 | INFO |  -2.10 * 0.5 =  -1.05 for decoder
2026-01-28 21:37:31,152 | INFO |  -1.03 * 0.5 =  -0.51 for ctc
2026-01-28 21:37:31,152 | INFO | total log probability: -1.56
2026-01-28 21:37:31,152 | INFO | normalized log probability: -0.12
2026-01-28 21:37:31,152 | INFO | total number of ended hypotheses: 141
2026-01-28 21:37:31,153 | INFO | best hypo: ▁mes▁chers▁compatriotes

2026-01-28 21:37:31,157 | INFO | speech length: 44960
2026-01-28 21:37:31,202 | INFO | decoder input length: 69
2026-01-28 21:37:31,202 | INFO | max output length: 69
2026-01-28 21:37:31,203 | INFO | min output length: 6
2026-01-28 21:37:32,667 | INFO | end detected at 29
2026-01-28 21:37:32,669 | INFO |  -2.48 * 0.5 =  -1.24 for decoder
2026-01-28 21:37:32,669 | INFO |  -6.68 * 0.5 =  -3.34 for ctc
2026-01-28 21:37:32,669 | INFO | total log probability: -4.58
2026-01-28 21:37:32,669 | INFO | normalized log probability: -0.21
2026-01-28 21:37:32,669 | INFO | total number of ended hypotheses: 184
2026-01-28 21:37:32,670 | INFO | best hypo: ▁je▁voudrais▁d'abord▁exprimer▁ma▁sablette

2026-01-28 21:37:32,673 | INFO | speech length: 107360
2026-01-28 21:37:32,712 | INFO | decoder input length: 167
2026-01-28 21:37:32,712 | INFO | max output length: 167
2026-01-28 21:37:32,712 | INFO | min output length: 16
2026-01-28 21:37:36,143 | INFO | end detected at 49
2026-01-28 21:37:36,144 | INFO |  -4.19 * 0.5 =  -2.10 for decoder
2026-01-28 21:37:36,144 | INFO |  -1.73 * 0.5 =  -0.87 for ctc
2026-01-28 21:37:36,144 | INFO | total log probability: -2.96
2026-01-28 21:37:36,144 | INFO | normalized log probability: -0.07
2026-01-28 21:37:36,144 | INFO | total number of ended hypotheses: 144
2026-01-28 21:37:36,145 | INFO | best hypo: ▁a▁toute▁celle▁et▁à▁tous▁ceux▁qui▁vivent▁ces▁derniers▁jours▁de▁mille▁neuf▁cent▁quatre▁vingt▁dix▁neuf▁dans▁l'épreuve

2026-01-28 21:37:36,148 | INFO | speech length: 48800
2026-01-28 21:37:36,192 | INFO | decoder input length: 75
2026-01-28 21:37:36,192 | INFO | max output length: 75
2026-01-28 21:37:36,192 | INFO | min output length: 7
2026-01-28 21:37:37,732 | INFO | end detected at 28
2026-01-28 21:37:37,733 | INFO |  -1.59 * 0.5 =  -0.79 for decoder
2026-01-28 21:37:37,733 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-28 21:37:37,733 | INFO | total log probability: -0.80
2026-01-28 21:37:37,733 | INFO | normalized log probability: -0.03
2026-01-28 21:37:37,733 | INFO | total number of ended hypotheses: 141
2026-01-28 21:37:37,733 | INFO | best hypo: ▁je▁pense▁aux▁nombreuses▁victimes▁de▁la▁tempête

2026-01-28 21:37:37,735 | INFO | speech length: 59520
2026-01-28 21:37:37,776 | INFO | decoder input length: 92
2026-01-28 21:37:37,776 | INFO | max output length: 92
2026-01-28 21:37:37,776 | INFO | min output length: 9
2026-01-28 21:37:39,369 | INFO | end detected at 34
2026-01-28 21:37:39,371 | INFO |  -1.95 * 0.5 =  -0.98 for decoder
2026-01-28 21:37:39,371 | INFO |  -6.53 * 0.5 =  -3.26 for ctc
2026-01-28 21:37:39,371 | INFO | total log probability: -4.24
2026-01-28 21:37:39,371 | INFO | normalized log probability: -0.14
2026-01-28 21:37:39,371 | INFO | total number of ended hypotheses: 154
2026-01-28 21:37:39,371 | INFO | best hypo: ▁et▁à▁toutes▁les▁familles▁endeuillées▁dont▁nous▁partageons▁la▁paix

2026-01-28 21:37:39,373 | INFO | speech length: 75520
2026-01-28 21:37:39,432 | INFO | decoder input length: 117
2026-01-28 21:37:39,433 | INFO | max output length: 117
2026-01-28 21:37:39,433 | INFO | min output length: 11
2026-01-28 21:37:43,009 | INFO | end detected at 40
2026-01-28 21:37:43,013 | INFO |  -4.92 * 0.5 =  -2.46 for decoder
2026-01-28 21:37:43,013 | INFO |  -6.26 * 0.5 =  -3.13 for ctc
2026-01-28 21:37:43,013 | INFO | total log probability: -5.59
2026-01-28 21:37:43,014 | INFO | normalized log probability: -0.16
2026-01-28 21:37:43,014 | INFO | total number of ended hypotheses: 199
2026-01-28 21:37:43,014 | INFO | best hypo: ▁je▁pense▁à▁nos▁concitoyens▁cruellement▁touchés▁dans▁leur▁vie▁quotidienne

2026-01-28 21:37:43,019 | INFO | speech length: 92160
2026-01-28 21:37:43,079 | INFO | decoder input length: 143
2026-01-28 21:37:43,079 | INFO | max output length: 143
2026-01-28 21:37:43,079 | INFO | min output length: 14
2026-01-28 21:37:46,068 | INFO | end detected at 49
2026-01-28 21:37:46,070 | INFO |  -4.36 * 0.5 =  -2.18 for decoder
2026-01-28 21:37:46,070 | INFO |  -1.71 * 0.5 =  -0.86 for ctc
2026-01-28 21:37:46,070 | INFO | total log probability: -3.04
2026-01-28 21:37:46,070 | INFO | normalized log probability: -0.07
2026-01-28 21:37:46,070 | INFO | total number of ended hypotheses: 180
2026-01-28 21:37:46,071 | INFO | best hypo: ▁a▁ceux▁dont▁les▁biens▁ont▁été▁détruits▁à▁ceux▁qui▁craignent▁pour▁leur▁activité▁et▁leurs▁emplois

2026-01-28 21:37:46,074 | INFO | speech length: 122720
2026-01-28 21:37:46,116 | INFO | decoder input length: 191
2026-01-28 21:37:46,117 | INFO | max output length: 191
2026-01-28 21:37:46,117 | INFO | min output length: 19
2026-01-28 21:37:49,486 | INFO | end detected at 49
2026-01-28 21:37:49,487 | INFO |  -3.57 * 0.5 =  -1.78 for decoder
2026-01-28 21:37:49,487 | INFO |  -0.49 * 0.5 =  -0.24 for ctc
2026-01-28 21:37:49,487 | INFO | total log probability: -2.03
2026-01-28 21:37:49,487 | INFO | normalized log probability: -0.05
2026-01-28 21:37:49,487 | INFO | total number of ended hypotheses: 160
2026-01-28 21:37:49,488 | INFO | best hypo: ▁a▁ceux▁qui▁souffrent▁de▁voir▁notre▁patrimoine▁notre▁littoral▁nos▁forêts▁nos▁monuments▁défigurés

2026-01-28 21:37:49,491 | INFO | speech length: 20800
2026-01-28 21:37:49,538 | INFO | decoder input length: 32
2026-01-28 21:37:49,538 | INFO | max output length: 32
2026-01-28 21:37:49,538 | INFO | min output length: 3
2026-01-28 21:37:50,149 | INFO | end detected at 15
2026-01-28 21:37:50,150 | INFO |  -0.70 * 0.5 =  -0.35 for decoder
2026-01-28 21:37:50,150 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-28 21:37:50,150 | INFO | total log probability: -0.36
2026-01-28 21:37:50,150 | INFO | normalized log probability: -0.03
2026-01-28 21:37:50,150 | INFO | total number of ended hypotheses: 147
2026-01-28 21:37:50,150 | INFO | best hypo: ▁je▁vous▁redis▁mon▁émotion

2026-01-28 21:37:50,152 | INFO | speech length: 217280
2026-01-28 21:37:50,193 | INFO | decoder input length: 339
2026-01-28 21:37:50,193 | INFO | max output length: 339
2026-01-28 21:37:50,193 | INFO | min output length: 33
2026-01-28 21:37:57,753 | INFO | end detected at 96
2026-01-28 21:37:57,755 | INFO | -71.67 * 0.5 = -35.84 for decoder
2026-01-28 21:37:57,755 | INFO | -38.69 * 0.5 = -19.35 for ctc
2026-01-28 21:37:57,755 | INFO | total log probability: -55.18
2026-01-28 21:37:57,755 | INFO | normalized log probability: -0.61
2026-01-28 21:37:57,755 | INFO | total number of ended hypotheses: 157
2026-01-28 21:37:57,757 | INFO | best hypo: ▁mais▁aussi▁ma▁fierté▁devant▁l'exceptionnel▁élan▁de▁solidarité▁qui▁anime▁tant▁de▁bénévoles▁et▁d'associations▁mobilisés▁aux▁côtés▁et▁des▁services▁à▁public▁civils▁et▁militaires▁et▁des▁élus

2026-01-28 21:37:57,759 | INFO | speech length: 91360
2026-01-28 21:37:57,808 | INFO | decoder input length: 142
2026-01-28 21:37:57,808 | INFO | max output length: 142
2026-01-28 21:37:57,808 | INFO | min output length: 14
2026-01-28 21:37:59,897 | INFO | end detected at 40
2026-01-28 21:37:59,898 | INFO |  -3.46 * 0.5 =  -1.73 for decoder
2026-01-28 21:37:59,899 | INFO |  -0.69 * 0.5 =  -0.35 for ctc
2026-01-28 21:37:59,899 | INFO | total log probability: -2.08
2026-01-28 21:37:59,899 | INFO | normalized log probability: -0.06
2026-01-28 21:37:59,899 | INFO | total number of ended hypotheses: 154
2026-01-28 21:37:59,899 | INFO | best hypo: ▁en▁ses▁heures▁difficiles▁nous▁ressentons▁profondément▁la▁fragilité▁des▁choses

2026-01-28 21:37:59,901 | INFO | speech length: 56960
2026-01-28 21:37:59,956 | INFO | decoder input length: 88
2026-01-28 21:37:59,956 | INFO | max output length: 88
2026-01-28 21:37:59,956 | INFO | min output length: 8
2026-01-28 21:38:01,890 | INFO | end detected at 23
2026-01-28 21:38:01,893 | INFO |  -1.46 * 0.5 =  -0.73 for decoder
2026-01-28 21:38:01,893 | INFO |  -0.10 * 0.5 =  -0.05 for ctc
2026-01-28 21:38:01,893 | INFO | total log probability: -0.78
2026-01-28 21:38:01,893 | INFO | normalized log probability: -0.04
2026-01-28 21:38:01,893 | INFO | total number of ended hypotheses: 166
2026-01-28 21:38:01,894 | INFO | best hypo: ▁la▁précarité▁de▁ce▁qui▁nous▁semblait▁acquis

2026-01-28 21:38:01,897 | INFO | speech length: 140800
2026-01-28 21:38:01,956 | INFO | decoder input length: 219
2026-01-28 21:38:01,956 | INFO | max output length: 219
2026-01-28 21:38:01,956 | INFO | min output length: 21
2026-01-28 21:38:06,528 | INFO | end detected at 64
2026-01-28 21:38:06,530 | INFO |  -4.91 * 0.5 =  -2.46 for decoder
2026-01-28 21:38:06,530 | INFO |  -0.08 * 0.5 =  -0.04 for ctc
2026-01-28 21:38:06,530 | INFO | total log probability: -2.50
2026-01-28 21:38:06,530 | INFO | normalized log probability: -0.04
2026-01-28 21:38:06,530 | INFO | total number of ended hypotheses: 158
2026-01-28 21:38:06,531 | INFO | best hypo: ▁nous▁voyons▁combien▁tout▁peut▁être▁parfois▁remis▁en▁cause▁du▁fait▁de▁l'inconscience▁des▁hommes▁ou▁du▁déchaînement▁des▁éléments▁naturels

2026-01-28 21:38:06,534 | INFO | speech length: 84320
2026-01-28 21:38:06,589 | INFO | decoder input length: 131
2026-01-28 21:38:06,589 | INFO | max output length: 131
2026-01-28 21:38:06,589 | INFO | min output length: 13
2026-01-28 21:38:08,804 | INFO | end detected at 37
2026-01-28 21:38:08,821 | INFO |  -2.34 * 0.5 =  -1.17 for decoder
2026-01-28 21:38:08,821 | INFO |  -0.87 * 0.5 =  -0.44 for ctc
2026-01-28 21:38:08,821 | INFO | total log probability: -1.61
2026-01-28 21:38:08,821 | INFO | normalized log probability: -0.05
2026-01-28 21:38:08,822 | INFO | total number of ended hypotheses: 147
2026-01-28 21:38:08,822 | INFO | best hypo: ▁nous▁mesurons▁aussi▁l'importance▁du▁rôle▁de▁l'état▁dans▁notre▁société

2026-01-28 21:38:08,824 | INFO | speech length: 65600
2026-01-28 21:38:08,912 | INFO | decoder input length: 102
2026-01-28 21:38:08,912 | INFO | max output length: 102
2026-01-28 21:38:08,912 | INFO | min output length: 10
2026-01-28 21:38:10,867 | INFO | end detected at 35
2026-01-28 21:38:10,869 | INFO |  -2.58 * 0.5 =  -1.29 for decoder
2026-01-28 21:38:10,869 | INFO |  -1.99 * 0.5 =  -1.00 for ctc
2026-01-28 21:38:10,869 | INFO | total log probability: -2.28
2026-01-28 21:38:10,869 | INFO | normalized log probability: -0.07
2026-01-28 21:38:10,869 | INFO | total number of ended hypotheses: 144
2026-01-28 21:38:10,870 | INFO | best hypo: ▁un▁état▁sur▁lequel▁pèsent▁des▁responsabilités▁essentielles

2026-01-28 21:38:10,872 | INFO | speech length: 57600
2026-01-28 21:38:10,912 | INFO | decoder input length: 89
2026-01-28 21:38:10,912 | INFO | max output length: 89
2026-01-28 21:38:10,912 | INFO | min output length: 8
2026-01-28 21:38:12,420 | INFO | end detected at 28
2026-01-28 21:38:12,421 | INFO |  -1.57 * 0.5 =  -0.79 for decoder
2026-01-28 21:38:12,421 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-28 21:38:12,421 | INFO | total log probability: -0.80
2026-01-28 21:38:12,421 | INFO | normalized log probability: -0.03
2026-01-28 21:38:12,421 | INFO | total number of ended hypotheses: 134
2026-01-28 21:38:12,422 | INFO | best hypo: ▁le▁service▁public▁la▁sécurité▁la▁solidarité

2026-01-28 21:38:12,424 | INFO | speech length: 122080
2026-01-28 21:38:12,464 | INFO | decoder input length: 190
2026-01-28 21:38:12,464 | INFO | max output length: 190
2026-01-28 21:38:12,464 | INFO | min output length: 19
2026-01-28 21:38:15,814 | INFO | end detected at 49
2026-01-28 21:38:15,815 | INFO |  -4.03 * 0.5 =  -2.02 for decoder
2026-01-28 21:38:15,815 | INFO |  -0.69 * 0.5 =  -0.34 for ctc
2026-01-28 21:38:15,815 | INFO | total log probability: -2.36
2026-01-28 21:38:15,815 | INFO | normalized log probability: -0.05
2026-01-28 21:38:15,815 | INFO | total number of ended hypotheses: 155
2026-01-28 21:38:15,816 | INFO | best hypo: ▁un▁état▁auquel▁il▁appartient▁de▁prévoir▁de▁faire▁face▁d'assurer▁la▁coordination▁des▁moyens▁du▁pays

2026-01-28 21:38:15,818 | INFO | speech length: 70400
2026-01-28 21:38:15,867 | INFO | decoder input length: 109
2026-01-28 21:38:15,868 | INFO | max output length: 109
2026-01-28 21:38:15,868 | INFO | min output length: 10
2026-01-28 21:38:17,621 | INFO | end detected at 31
2026-01-28 21:38:17,623 | INFO |  -2.67 * 0.5 =  -1.33 for decoder
2026-01-28 21:38:17,623 | INFO |  -0.56 * 0.5 =  -0.28 for ctc
2026-01-28 21:38:17,623 | INFO | total log probability: -1.61
2026-01-28 21:38:17,623 | INFO | normalized log probability: -0.06
2026-01-28 21:38:17,623 | INFO | total number of ended hypotheses: 155
2026-01-28 21:38:17,624 | INFO | best hypo: ▁nous▁mesurons▁surtout▁le▁prix▁de▁l'aide▁fraternel

2026-01-28 21:38:17,626 | INFO | speech length: 45120
2026-01-28 21:38:17,669 | INFO | decoder input length: 70
2026-01-28 21:38:17,669 | INFO | max output length: 70
2026-01-28 21:38:17,669 | INFO | min output length: 7
2026-01-28 21:38:18,672 | INFO | end detected at 22
2026-01-28 21:38:18,673 | INFO |  -1.31 * 0.5 =  -0.66 for decoder
2026-01-28 21:38:18,673 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-28 21:38:18,673 | INFO | total log probability: -0.66
2026-01-28 21:38:18,673 | INFO | normalized log probability: -0.04
2026-01-28 21:38:18,673 | INFO | total number of ended hypotheses: 146
2026-01-28 21:38:18,674 | INFO | best hypo: ▁du▁soutien▁spontané▁de▁la▁main▁tendue

2026-01-28 21:38:18,676 | INFO | speech length: 35200
2026-01-28 21:38:18,712 | INFO | decoder input length: 54
2026-01-28 21:38:18,712 | INFO | max output length: 54
2026-01-28 21:38:18,712 | INFO | min output length: 5
2026-01-28 21:38:19,666 | INFO | end detected at 18
2026-01-28 21:38:19,666 | INFO |  -4.19 * 0.5 =  -2.10 for decoder
2026-01-28 21:38:19,667 | INFO |  -0.55 * 0.5 =  -0.27 for ctc
2026-01-28 21:38:19,667 | INFO | total log probability: -2.37
2026-01-28 21:38:19,667 | INFO | normalized log probability: -0.18
2026-01-28 21:38:19,667 | INFO | total number of ended hypotheses: 158
2026-01-28 21:38:19,667 | INFO | best hypo: ▁qui▁sont▁le▁ciment▁même▁de▁la▁nation

2026-01-28 21:38:19,669 | INFO | speech length: 54720
2026-01-28 21:38:19,727 | INFO | decoder input length: 85
2026-01-28 21:38:19,727 | INFO | max output length: 85
2026-01-28 21:38:19,727 | INFO | min output length: 8
2026-01-28 21:38:22,398 | INFO | end detected at 33
2026-01-28 21:38:22,402 | INFO |  -8.38 * 0.5 =  -4.19 for decoder
2026-01-28 21:38:22,403 | INFO |  -6.60 * 0.5 =  -3.30 for ctc
2026-01-28 21:38:22,403 | INFO | total log probability: -7.49
2026-01-28 21:38:22,403 | INFO | normalized log probability: -0.31
2026-01-28 21:38:22,403 | INFO | total number of ended hypotheses: 203
2026-01-28 21:38:22,404 | INFO | best hypo: ▁au▁moment▁où▁nous▁touchons▁aux▁portes▁de▁leur▁demiille

2026-01-28 21:38:22,408 | INFO | speech length: 83200
2026-01-28 21:38:22,462 | INFO | decoder input length: 129
2026-01-28 21:38:22,462 | INFO | max output length: 129
2026-01-28 21:38:22,462 | INFO | min output length: 12
2026-01-28 21:38:25,667 | INFO | end detected at 33
2026-01-28 21:38:25,669 | INFO |  -2.09 * 0.5 =  -1.04 for decoder
2026-01-28 21:38:25,669 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-28 21:38:25,669 | INFO | total log probability: -1.05
2026-01-28 21:38:25,669 | INFO | normalized log probability: -0.04
2026-01-28 21:38:25,669 | INFO | total number of ended hypotheses: 138
2026-01-28 21:38:25,670 | INFO | best hypo: ▁rien▁n'est▁décidément▁plus▁moderne▁plus▁nécessaire▁plus▁solide

2026-01-28 21:38:25,673 | INFO | speech length: 56000
2026-01-28 21:38:25,727 | INFO | decoder input length: 87
2026-01-28 21:38:25,727 | INFO | max output length: 87
2026-01-28 21:38:25,727 | INFO | min output length: 8
2026-01-28 21:38:28,061 | INFO | end detected at 29
2026-01-28 21:38:28,062 | INFO |  -1.96 * 0.5 =  -0.98 for decoder
2026-01-28 21:38:28,063 | INFO |  -0.14 * 0.5 =  -0.07 for ctc
2026-01-28 21:38:28,063 | INFO | total log probability: -1.05
2026-01-28 21:38:28,063 | INFO | normalized log probability: -0.04
2026-01-28 21:38:28,063 | INFO | total number of ended hypotheses: 146
2026-01-28 21:38:28,064 | INFO | best hypo: ▁que▁le▁sentiment▁d'appartenir▁à▁une▁même▁communauté

2026-01-28 21:38:28,067 | INFO | speech length: 50720
2026-01-28 21:38:28,113 | INFO | decoder input length: 78
2026-01-28 21:38:28,113 | INFO | max output length: 78
2026-01-28 21:38:28,113 | INFO | min output length: 7
2026-01-28 21:38:29,411 | INFO | end detected at 25
2026-01-28 21:38:29,414 | INFO |  -1.50 * 0.5 =  -0.75 for decoder
2026-01-28 21:38:29,414 | INFO |  -0.10 * 0.5 =  -0.05 for ctc
2026-01-28 21:38:29,414 | INFO | total log probability: -0.80
2026-01-28 21:38:29,414 | INFO | normalized log probability: -0.04
2026-01-28 21:38:29,414 | INFO | total number of ended hypotheses: 150
2026-01-28 21:38:29,415 | INFO | best hypo: ▁et▁d'être▁responsables▁les▁uns▁des▁autres

2026-01-28 21:38:29,417 | INFO | speech length: 98560
2026-01-28 21:38:29,472 | INFO | decoder input length: 153
2026-01-28 21:38:29,472 | INFO | max output length: 153
2026-01-28 21:38:29,472 | INFO | min output length: 15
2026-01-28 21:38:32,883 | INFO | end detected at 33
2026-01-28 21:38:32,886 | INFO |  -2.24 * 0.5 =  -1.12 for decoder
2026-01-28 21:38:32,886 | INFO |  -0.13 * 0.5 =  -0.06 for ctc
2026-01-28 21:38:32,886 | INFO | total log probability: -1.18
2026-01-28 21:38:32,886 | INFO | normalized log probability: -0.04
2026-01-28 21:38:32,886 | INFO | total number of ended hypotheses: 154
2026-01-28 21:38:32,887 | INFO | best hypo: ▁la▁france▁blessée▁veut▁se▁retrouver▁rassemblée▁et▁fraternelle

2026-01-28 21:38:32,890 | INFO | speech length: 95040
2026-01-28 21:38:32,946 | INFO | decoder input length: 148
2026-01-28 21:38:32,946 | INFO | max output length: 148
2026-01-28 21:38:32,946 | INFO | min output length: 14
2026-01-28 21:38:35,777 | INFO | end detected at 46
2026-01-28 21:38:35,780 | INFO |  -6.73 * 0.5 =  -3.37 for decoder
2026-01-28 21:38:35,780 | INFO |  -4.69 * 0.5 =  -2.34 for ctc
2026-01-28 21:38:35,780 | INFO | total log probability: -5.71
2026-01-28 21:38:35,780 | INFO | normalized log probability: -0.15
2026-01-28 21:38:35,780 | INFO | total number of ended hypotheses: 190
2026-01-28 21:38:35,781 | INFO | best hypo: ▁parce▁que▁nos▁compatriotes▁ont▁toujours▁su▁dans▁les▁preuves▁faire▁parler▁leur▁coeur

2026-01-28 21:38:35,783 | INFO | speech length: 56960
2026-01-28 21:38:35,836 | INFO | decoder input length: 88
2026-01-28 21:38:35,836 | INFO | max output length: 88
2026-01-28 21:38:35,836 | INFO | min output length: 8
2026-01-28 21:38:37,855 | INFO | end detected at 24
2026-01-28 21:38:37,857 | INFO |  -1.36 * 0.5 =  -0.68 for decoder
2026-01-28 21:38:37,857 | INFO |  -1.09 * 0.5 =  -0.54 for ctc
2026-01-28 21:38:37,857 | INFO | total log probability: -1.22
2026-01-28 21:38:37,857 | INFO | normalized log probability: -0.06
2026-01-28 21:38:37,857 | INFO | total number of ended hypotheses: 146
2026-01-28 21:38:37,858 | INFO | best hypo: ▁je▁voudrais▁dire▁merci▁à▁tous▁les▁francs

2026-01-28 21:38:37,861 | INFO | speech length: 83680
2026-01-28 21:38:37,923 | INFO | decoder input length: 130
2026-01-28 21:38:37,923 | INFO | max output length: 130
2026-01-28 21:38:37,924 | INFO | min output length: 13
2026-01-28 21:38:41,027 | INFO | end detected at 32
2026-01-28 21:38:41,031 | INFO |  -2.16 * 0.5 =  -1.08 for decoder
2026-01-28 21:38:41,031 | INFO |  -0.08 * 0.5 =  -0.04 for ctc
2026-01-28 21:38:41,031 | INFO | total log probability: -1.12
2026-01-28 21:38:41,031 | INFO | normalized log probability: -0.04
2026-01-28 21:38:41,031 | INFO | total number of ended hypotheses: 167
2026-01-28 21:38:41,032 | INFO | best hypo: ▁ce▁soir▁nous▁vivons▁ensemble▁un▁moment▁fort▁et▁singulier

2026-01-28 21:38:41,036 | INFO | speech length: 92160
2026-01-28 21:38:41,089 | INFO | decoder input length: 143
2026-01-28 21:38:41,089 | INFO | max output length: 143
2026-01-28 21:38:41,089 | INFO | min output length: 14
2026-01-28 21:38:43,852 | INFO | end detected at 43
2026-01-28 21:38:43,854 | INFO |  -3.28 * 0.5 =  -1.64 for decoder
2026-01-28 21:38:43,855 | INFO |  -0.32 * 0.5 =  -0.16 for ctc
2026-01-28 21:38:43,855 | INFO | total log probability: -1.80
2026-01-28 21:38:43,855 | INFO | normalized log probability: -0.05
2026-01-28 21:38:43,855 | INFO | total number of ended hypotheses: 162
2026-01-28 21:38:43,855 | INFO | best hypo: ▁ce▁qui▁paraissait▁très▁lointain▁qui▁a▁longtemps▁symbolisé▁le▁futur▁l'en▁deux▁mille

2026-01-28 21:38:43,858 | INFO | speech length: 29760
2026-01-28 21:38:43,919 | INFO | decoder input length: 46
2026-01-28 21:38:43,920 | INFO | max output length: 46
2026-01-28 21:38:43,920 | INFO | min output length: 4
2026-01-28 21:38:45,189 | INFO | end detected at 17
2026-01-28 21:38:45,192 | INFO |  -1.87 * 0.5 =  -0.94 for decoder
2026-01-28 21:38:45,192 | INFO |  -0.92 * 0.5 =  -0.46 for ctc
2026-01-28 21:38:45,192 | INFO | total log probability: -1.40
2026-01-28 21:38:45,193 | INFO | normalized log probability: -0.12
2026-01-28 21:38:45,193 | INFO | total number of ended hypotheses: 160
2026-01-28 21:38:45,193 | INFO | best hypo: ▁est▁devenu▁contemporain

2026-01-28 21:38:45,197 | INFO | speech length: 9760
2026-01-28 21:38:45,244 | INFO | decoder input length: 14
2026-01-28 21:38:45,244 | INFO | max output length: 14
2026-01-28 21:38:45,244 | INFO | min output length: 1
2026-01-28 21:38:45,991 | INFO | end detected at 11
2026-01-28 21:38:45,993 | INFO |  -0.38 * 0.5 =  -0.19 for decoder
2026-01-28 21:38:45,993 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-28 21:38:45,993 | INFO | total log probability: -0.19
2026-01-28 21:38:45,993 | INFO | normalized log probability: -0.03
2026-01-28 21:38:45,993 | INFO | total number of ended hypotheses: 135
2026-01-28 21:38:45,994 | INFO | best hypo: ▁immédiat

2026-01-28 21:38:45,997 | INFO | speech length: 106240
2026-01-28 21:38:46,049 | INFO | decoder input length: 165
2026-01-28 21:38:46,050 | INFO | max output length: 165
2026-01-28 21:38:46,050 | INFO | min output length: 16
2026-01-28 21:38:49,091 | INFO | end detected at 53
2026-01-28 21:38:49,093 | INFO |  -6.29 * 0.5 =  -3.14 for decoder
2026-01-28 21:38:49,093 | INFO |  -4.65 * 0.5 =  -2.32 for ctc
2026-01-28 21:38:49,093 | INFO | total log probability: -5.47
2026-01-28 21:38:49,093 | INFO | normalized log probability: -0.11
2026-01-28 21:38:49,093 | INFO | total number of ended hypotheses: 163
2026-01-28 21:38:49,094 | INFO | best hypo: ▁je▁suis▁sûr▁que▁beaucoup▁d'entre▁vous▁vont▁vivre▁ces▁instants▁avec▁un▁peu▁d'émotion▁un▁peu▁d'étonnement

2026-01-28 21:38:49,097 | INFO | speech length: 164320
2026-01-28 21:38:49,151 | INFO | decoder input length: 256
2026-01-28 21:38:49,152 | INFO | max output length: 256
2026-01-28 21:38:49,152 | INFO | min output length: 25
2026-01-28 21:38:55,051 | INFO | end detected at 79
2026-01-28 21:38:55,053 | INFO | -21.56 * 0.5 = -10.78 for decoder
2026-01-28 21:38:55,053 | INFO |  -0.74 * 0.5 =  -0.37 for ctc
2026-01-28 21:38:55,053 | INFO | total log probability: -11.15
2026-01-28 21:38:55,053 | INFO | normalized log probability: -0.15
2026-01-28 21:38:55,053 | INFO | total number of ended hypotheses: 147
2026-01-28 21:38:55,054 | INFO | best hypo: ▁une▁certaine▁appréhension▁parfois▁n'est▁du▁sentiment▁que▁s'achève▁une▁époque▁dont▁on▁possédait▁les▁clés▁dont▁on▁maîtrisait▁les▁règles▁et▁les▁habitudes

2026-01-28 21:38:55,056 | INFO | speech length: 30560
2026-01-28 21:38:55,112 | INFO | decoder input length: 47
2026-01-28 21:38:55,113 | INFO | max output length: 47
2026-01-28 21:38:55,113 | INFO | min output length: 4
2026-01-28 21:38:56,009 | INFO | end detected at 21
2026-01-28 21:38:56,010 | INFO |  -1.87 * 0.5 =  -0.93 for decoder
2026-01-28 21:38:56,010 | INFO |  -1.83 * 0.5 =  -0.92 for ctc
2026-01-28 21:38:56,010 | INFO | total log probability: -1.85
2026-01-28 21:38:56,010 | INFO | normalized log probability: -0.11
2026-01-28 21:38:56,010 | INFO | total number of ended hypotheses: 157
2026-01-28 21:38:56,011 | INFO | best hypo: ▁je▁comprends▁ces▁mouvements▁de▁lames

2026-01-28 21:38:56,013 | INFO | speech length: 15040
2026-01-28 21:38:56,067 | INFO | decoder input length: 23
2026-01-28 21:38:56,068 | INFO | max output length: 23
2026-01-28 21:38:56,068 | INFO | min output length: 2
2026-01-28 21:38:57,296 | INFO | end detected at 18
2026-01-28 21:38:57,298 | INFO |  -3.43 * 0.5 =  -1.71 for decoder
2026-01-28 21:38:57,298 | INFO |  -1.12 * 0.5 =  -0.56 for ctc
2026-01-28 21:38:57,298 | INFO | total log probability: -2.28
2026-01-28 21:38:57,298 | INFO | normalized log probability: -0.16
2026-01-28 21:38:57,298 | INFO | total number of ended hypotheses: 159
2026-01-28 21:38:57,299 | INFO | best hypo: ▁pourtant▁j'ai▁confiant

2026-01-28 21:38:57,303 | INFO | speech length: 99200
2026-01-28 21:38:57,348 | INFO | decoder input length: 154
2026-01-28 21:38:57,348 | INFO | max output length: 154
2026-01-28 21:38:57,348 | INFO | min output length: 15
2026-01-28 21:38:59,963 | INFO | end detected at 45
2026-01-28 21:38:59,964 | INFO |  -3.46 * 0.5 =  -1.73 for decoder
2026-01-28 21:38:59,964 | INFO |  -0.99 * 0.5 =  -0.50 for ctc
2026-01-28 21:38:59,964 | INFO | total log probability: -2.23
2026-01-28 21:38:59,965 | INFO | normalized log probability: -0.05
2026-01-28 21:38:59,965 | INFO | total number of ended hypotheses: 151
2026-01-28 21:38:59,965 | INFO | best hypo: ▁la▁france▁franchira▁des▁obstacles▁comme▁elle▁l'a▁toujours▁fait▁au▁long▁de▁son▁histoire

2026-01-28 21:38:59,967 | INFO | speech length: 31840
2026-01-28 21:39:00,013 | INFO | decoder input length: 49
2026-01-28 21:39:00,013 | INFO | max output length: 49
2026-01-28 21:39:00,013 | INFO | min output length: 4
2026-01-28 21:39:00,890 | INFO | end detected at 21
2026-01-28 21:39:00,891 | INFO | -10.05 * 0.5 =  -5.03 for decoder
2026-01-28 21:39:00,891 | INFO |  -0.37 * 0.5 =  -0.18 for ctc
2026-01-28 21:39:00,891 | INFO | total log probability: -5.21
2026-01-28 21:39:00,891 | INFO | normalized log probability: -0.35
2026-01-28 21:39:00,891 | INFO | total number of ended hypotheses: 166
2026-01-28 21:39:00,891 | INFO | best hypo: ▁pour▁peu▁qu'elle▁soit▁fidèlele

2026-01-28 21:39:00,893 | INFO | speech length: 220640
2026-01-28 21:39:00,937 | INFO | decoder input length: 344
2026-01-28 21:39:00,937 | INFO | max output length: 344
2026-01-28 21:39:00,937 | INFO | min output length: 34
2026-01-28 21:39:10,786 | INFO | end detected at 88
2026-01-28 21:39:10,788 | INFO | -44.30 * 0.5 = -22.15 for decoder
2026-01-28 21:39:10,788 | INFO |  -9.57 * 0.5 =  -4.78 for ctc
2026-01-28 21:39:10,788 | INFO | total log probability: -26.93
2026-01-28 21:39:10,788 | INFO | normalized log probability: -0.32
2026-01-28 21:39:10,788 | INFO | total number of ended hypotheses: 157
2026-01-28 21:39:10,790 | INFO | best hypo: ▁même▁si▁le▁passé▁est▁bien▁présent▁dans▁notre▁mémoire▁je▁ne▁m'attarderai▁pas▁sur▁le▁siècle▁qui▁s'achève▁siècle▁de▁progrès▁sans▁précédent▁pour▁la▁santé▁d'éducation▁les▁conditions▁de▁vie

2026-01-28 21:39:10,793 | INFO | speech length: 81600
2026-01-28 21:39:10,841 | INFO | decoder input length: 127
2026-01-28 21:39:10,841 | INFO | max output length: 127
2026-01-28 21:39:10,841 | INFO | min output length: 12
2026-01-28 21:39:13,112 | INFO | end detected at 40
2026-01-28 21:39:13,115 | INFO |  -2.63 * 0.5 =  -1.32 for decoder
2026-01-28 21:39:13,115 | INFO |  -1.03 * 0.5 =  -0.51 for ctc
2026-01-28 21:39:13,115 | INFO | total log probability: -1.83
2026-01-28 21:39:13,115 | INFO | normalized log probability: -0.05
2026-01-28 21:39:13,115 | INFO | total number of ended hypotheses: 163
2026-01-28 21:39:13,116 | INFO | best hypo: ▁pour▁les▁libertés▁la▁vie▁démocratique▁la▁situation▁des▁femmes▁les▁solidarités

2026-01-28 21:39:13,118 | INFO | speech length: 71360
2026-01-28 21:39:13,169 | INFO | decoder input length: 111
2026-01-28 21:39:13,169 | INFO | max output length: 111
2026-01-28 21:39:13,169 | INFO | min output length: 11
2026-01-28 21:39:15,070 | INFO | end detected at 33
2026-01-28 21:39:15,073 | INFO |  -2.89 * 0.5 =  -1.45 for decoder
2026-01-28 21:39:15,073 | INFO |  -1.33 * 0.5 =  -0.66 for ctc
2026-01-28 21:39:15,073 | INFO | total log probability: -2.11
2026-01-28 21:39:15,073 | INFO | normalized log probability: -0.08
2026-01-28 21:39:15,073 | INFO | total number of ended hypotheses: 168
2026-01-28 21:39:15,074 | INFO | best hypo: ▁mais▁aussi▁siècle▁d'horreur▁de▁tragédie▁de▁convulsions

2026-01-28 21:39:15,077 | INFO | speech length: 83520
2026-01-28 21:39:15,124 | INFO | decoder input length: 130
2026-01-28 21:39:15,124 | INFO | max output length: 130
2026-01-28 21:39:15,124 | INFO | min output length: 13
2026-01-28 21:39:17,325 | INFO | end detected at 37
2026-01-28 21:39:17,328 | INFO |  -2.92 * 0.5 =  -1.46 for decoder
2026-01-28 21:39:17,328 | INFO |  -6.42 * 0.5 =  -3.21 for ctc
2026-01-28 21:39:17,328 | INFO | total log probability: -4.67
2026-01-28 21:39:17,328 | INFO | normalized log probability: -0.14
2026-01-28 21:39:17,328 | INFO | total number of ended hypotheses: 139
2026-01-28 21:39:17,329 | INFO | best hypo: ▁qui▁a▁vu▁deux▁guerres▁mondiales▁le▁boulag▁les▁dictatures▁totalitaires

2026-01-28 21:39:17,331 | INFO | speech length: 13920
2026-01-28 21:39:17,388 | INFO | decoder input length: 21
2026-01-28 21:39:17,388 | INFO | max output length: 21
2026-01-28 21:39:17,388 | INFO | min output length: 2
2026-01-28 21:39:18,198 | INFO | end detected at 12
2026-01-28 21:39:18,200 | INFO |  -3.48 * 0.5 =  -1.74 for decoder
2026-01-28 21:39:18,201 | INFO |  -1.36 * 0.5 =  -0.68 for ctc
2026-01-28 21:39:18,201 | INFO | total log probability: -2.42
2026-01-28 21:39:18,201 | INFO | normalized log probability: -0.35
2026-01-28 21:39:18,201 | INFO | total number of ended hypotheses: 163
2026-01-28 21:39:18,201 | INFO | best hypo: ▁et▁la▁choa

2026-01-28 21:39:18,204 | INFO | speech length: 65440
2026-01-28 21:39:18,250 | INFO | decoder input length: 101
2026-01-28 21:39:18,250 | INFO | max output length: 101
2026-01-28 21:39:18,250 | INFO | min output length: 10
2026-01-28 21:39:19,980 | INFO | end detected at 31
2026-01-28 21:39:19,982 | INFO |  -6.50 * 0.5 =  -3.25 for decoder
2026-01-28 21:39:19,982 | INFO |  -3.07 * 0.5 =  -1.54 for ctc
2026-01-28 21:39:19,982 | INFO | total log probability: -4.79
2026-01-28 21:39:19,982 | INFO | normalized log probability: -0.21
2026-01-28 21:39:19,982 | INFO | total number of ended hypotheses: 191
2026-01-28 21:39:19,983 | INFO | best hypo: ▁mais▁ce▁soir▁ce▁qui▁importe▁c'est▁l'avenir▁notre

2026-01-28 21:39:19,985 | INFO | speech length: 18240
2026-01-28 21:39:20,032 | INFO | decoder input length: 28
2026-01-28 21:39:20,032 | INFO | max output length: 28
2026-01-28 21:39:20,032 | INFO | min output length: 2
2026-01-28 21:39:20,756 | INFO | end detected at 16
2026-01-28 21:39:20,757 | INFO |  -0.83 * 0.5 =  -0.41 for decoder
2026-01-28 21:39:20,757 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 21:39:20,758 | INFO | total log probability: -0.42
2026-01-28 21:39:20,758 | INFO | normalized log probability: -0.03
2026-01-28 21:39:20,758 | INFO | total number of ended hypotheses: 129
2026-01-28 21:39:20,758 | INFO | best hypo: ▁celui▁de▁nos▁enfants

2026-01-28 21:39:20,760 | INFO | speech length: 50560
2026-01-28 21:39:20,806 | INFO | decoder input length: 78
2026-01-28 21:39:20,807 | INFO | max output length: 78
2026-01-28 21:39:20,807 | INFO | min output length: 7
2026-01-28 21:39:22,257 | INFO | end detected at 28
2026-01-28 21:39:22,260 | INFO |  -1.90 * 0.5 =  -0.95 for decoder
2026-01-28 21:39:22,260 | INFO |  -0.24 * 0.5 =  -0.12 for ctc
2026-01-28 21:39:22,260 | INFO | total log probability: -1.07
2026-01-28 21:39:22,260 | INFO | normalized log probability: -0.04
2026-01-28 21:39:22,260 | INFO | total number of ended hypotheses: 149
2026-01-28 21:39:22,260 | INFO | best hypo: ▁le▁progrès▁va▁se▁poursuivre▁avec▁ses▁hésitations

2026-01-28 21:39:22,263 | INFO | speech length: 106240
2026-01-28 21:39:22,309 | INFO | decoder input length: 165
2026-01-28 21:39:22,310 | INFO | max output length: 165
2026-01-28 21:39:22,310 | INFO | min output length: 16
2026-01-28 21:39:28,144 | INFO | end detected at 48
2026-01-28 21:39:28,145 | INFO |  -4.30 * 0.5 =  -2.15 for decoder
2026-01-28 21:39:28,145 | INFO |  -4.15 * 0.5 =  -2.08 for ctc
2026-01-28 21:39:28,145 | INFO | total log probability: -4.22
2026-01-28 21:39:28,145 | INFO | normalized log probability: -0.10
2026-01-28 21:39:28,145 | INFO | total number of ended hypotheses: 152
2026-01-28 21:39:28,146 | INFO | best hypo: ▁avec▁ses▁limites▁que▁nous▁mesurons▁bien▁face▁aux▁événements▁récents▁qui▁nous▁invitent▁à▁l'humilité

2026-01-28 21:39:28,148 | INFO | speech length: 24480
2026-01-28 21:39:28,198 | INFO | decoder input length: 37
2026-01-28 21:39:28,198 | INFO | max output length: 37
2026-01-28 21:39:28,198 | INFO | min output length: 3
2026-01-28 21:39:30,030 | INFO | end detected at 16
2026-01-28 21:39:30,032 | INFO |  -0.79 * 0.5 =  -0.39 for decoder
2026-01-28 21:39:30,032 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-28 21:39:30,032 | INFO | total log probability: -0.40
2026-01-28 21:39:30,033 | INFO | normalized log probability: -0.03
2026-01-28 21:39:30,033 | INFO | total number of ended hypotheses: 143
2026-01-28 21:39:30,033 | INFO | best hypo: ▁progrès▁de▁la▁science

2026-01-28 21:39:30,036 | INFO | speech length: 37280
2026-01-28 21:39:30,091 | INFO | decoder input length: 57
2026-01-28 21:39:30,091 | INFO | max output length: 57
2026-01-28 21:39:30,091 | INFO | min output length: 5
2026-01-28 21:39:32,801 | INFO | end detected at 25
2026-01-28 21:39:32,803 | INFO |  -8.27 * 0.5 =  -4.13 for decoder
2026-01-28 21:39:32,803 | INFO |  -0.31 * 0.5 =  -0.15 for ctc
2026-01-28 21:39:32,803 | INFO | total log probability: -4.29
2026-01-28 21:39:32,803 | INFO | normalized log probability: -0.20
2026-01-28 21:39:32,803 | INFO | total number of ended hypotheses: 143
2026-01-28 21:39:32,804 | INFO | best hypo: ▁pro▁créer▁des▁communications▁entre▁les▁hommes

2026-01-28 21:39:32,806 | INFO | speech length: 16960
2026-01-28 21:39:32,868 | INFO | decoder input length: 26
2026-01-28 21:39:32,868 | INFO | max output length: 26
2026-01-28 21:39:32,868 | INFO | min output length: 2
2026-01-28 21:39:34,919 | INFO | end detected at 17
2026-01-28 21:39:34,920 | INFO |  -2.60 * 0.5 =  -1.30 for decoder
2026-01-28 21:39:34,921 | INFO |  -6.84 * 0.5 =  -3.42 for ctc
2026-01-28 21:39:34,921 | INFO | total log probability: -4.72
2026-01-28 21:39:34,921 | INFO | normalized log probability: -0.34
2026-01-28 21:39:34,921 | INFO | total number of ended hypotheses: 145
2026-01-28 21:39:34,921 | INFO | best hypo: ▁on▁grève▁de▁la▁médecine

2026-01-28 21:39:34,923 | INFO | speech length: 97280
2026-01-28 21:39:34,974 | INFO | decoder input length: 151
2026-01-28 21:39:34,974 | INFO | max output length: 151
2026-01-28 21:39:34,974 | INFO | min output length: 15
2026-01-28 21:39:38,232 | INFO | end detected at 40
2026-01-28 21:39:38,233 | INFO |  -3.23 * 0.5 =  -1.62 for decoder
2026-01-28 21:39:38,234 | INFO |  -1.53 * 0.5 =  -0.77 for ctc
2026-01-28 21:39:38,234 | INFO | total log probability: -2.38
2026-01-28 21:39:38,234 | INFO | normalized log probability: -0.07
2026-01-28 21:39:38,234 | INFO | total number of ended hypotheses: 164
2026-01-28 21:39:38,234 | INFO | best hypo: ▁un▁grand▁nombre▁des▁enfants▁qui▁vont▁naître▁cette▁année▁verront▁l'an▁deux▁mille▁cent

2026-01-28 21:39:38,237 | INFO | speech length: 17920
2026-01-28 21:39:38,288 | INFO | decoder input length: 27
2026-01-28 21:39:38,288 | INFO | max output length: 27
2026-01-28 21:39:38,288 | INFO | min output length: 2
2026-01-28 21:39:38,911 | INFO | end detected at 14
2026-01-28 21:39:38,913 | INFO |  -0.84 * 0.5 =  -0.42 for decoder
2026-01-28 21:39:38,913 | INFO |  -0.23 * 0.5 =  -0.11 for ctc
2026-01-28 21:39:38,913 | INFO | total log probability: -0.53
2026-01-28 21:39:38,913 | INFO | normalized log probability: -0.05
2026-01-28 21:39:38,913 | INFO | total number of ended hypotheses: 146
2026-01-28 21:39:38,913 | INFO | best hypo: ▁c'est▁progrès

2026-01-28 21:39:38,915 | INFO | speech length: 26080
2026-01-28 21:39:38,955 | INFO | decoder input length: 40
2026-01-28 21:39:38,955 | INFO | max output length: 40
2026-01-28 21:39:38,955 | INFO | min output length: 4
2026-01-28 21:39:39,867 | INFO | end detected at 19
2026-01-28 21:39:39,870 | INFO |  -2.41 * 0.5 =  -1.21 for decoder
2026-01-28 21:39:39,870 | INFO |  -3.15 * 0.5 =  -1.58 for ctc
2026-01-28 21:39:39,870 | INFO | total log probability: -2.78
2026-01-28 21:39:39,870 | INFO | normalized log probability: -0.21
2026-01-28 21:39:39,870 | INFO | total number of ended hypotheses: 167
2026-01-28 21:39:39,870 | INFO | best hypo: ▁ne▁prendront▁tout▁leur▁sang

2026-01-28 21:39:39,873 | INFO | speech length: 27520
2026-01-28 21:39:39,915 | INFO | decoder input length: 42
2026-01-28 21:39:39,915 | INFO | max output length: 42
2026-01-28 21:39:39,915 | INFO | min output length: 4
2026-01-28 21:39:40,973 | INFO | end detected at 23
2026-01-28 21:39:40,976 | INFO |  -1.68 * 0.5 =  -0.84 for decoder
2026-01-28 21:39:40,976 | INFO |  -2.16 * 0.5 =  -1.08 for ctc
2026-01-28 21:39:40,976 | INFO | total log probability: -1.92
2026-01-28 21:39:40,976 | INFO | normalized log probability: -0.10
2026-01-28 21:39:40,976 | INFO | total number of ended hypotheses: 163
2026-01-28 21:39:40,976 | INFO | best hypo: ▁que▁s'ils▁bénéficient▁à▁l'homme

2026-01-28 21:39:40,979 | INFO | speech length: 14720
2026-01-28 21:39:41,018 | INFO | decoder input length: 22
2026-01-28 21:39:41,018 | INFO | max output length: 22
2026-01-28 21:39:41,018 | INFO | min output length: 2
2026-01-28 21:39:41,689 | INFO | end detected at 15
2026-01-28 21:39:41,691 | INFO |  -4.66 * 0.5 =  -2.33 for decoder
2026-01-28 21:39:41,691 | INFO |  -5.15 * 0.5 =  -2.57 for ctc
2026-01-28 21:39:41,691 | INFO | total log probability: -4.90
2026-01-28 21:39:41,691 | INFO | normalized log probability: -0.54
2026-01-28 21:39:41,691 | INFO | total number of ended hypotheses: 180
2026-01-28 21:39:41,692 | INFO | best hypo: ▁a▁tous▁les▁ans

2026-01-28 21:39:41,694 | INFO | speech length: 67520
2026-01-28 21:39:41,741 | INFO | decoder input length: 105
2026-01-28 21:39:41,741 | INFO | max output length: 105
2026-01-28 21:39:41,741 | INFO | min output length: 10
2026-01-28 21:39:43,480 | INFO | end detected at 31
2026-01-28 21:39:43,481 | INFO |  -1.91 * 0.5 =  -0.95 for decoder
2026-01-28 21:39:43,481 | INFO |  -1.28 * 0.5 =  -0.64 for ctc
2026-01-28 21:39:43,481 | INFO | total log probability: -1.59
2026-01-28 21:39:43,481 | INFO | normalized log probability: -0.06
2026-01-28 21:39:43,481 | INFO | total number of ended hypotheses: 143
2026-01-28 21:39:43,482 | INFO | best hypo: ▁le▁vingt▁et▁unième▁siècle▁doit▁être▁le▁siècle▁de▁l'éthique

2026-01-28 21:39:43,484 | INFO | speech length: 57440
2026-01-28 21:39:43,530 | INFO | decoder input length: 89
2026-01-28 21:39:43,530 | INFO | max output length: 89
2026-01-28 21:39:43,530 | INFO | min output length: 8
2026-01-28 21:39:46,141 | INFO | end detected at 41
2026-01-28 21:39:46,142 | INFO |  -4.26 * 0.5 =  -2.13 for decoder
2026-01-28 21:39:46,142 | INFO |  -1.09 * 0.5 =  -0.54 for ctc
2026-01-28 21:39:46,142 | INFO | total log probability: -2.67
2026-01-28 21:39:46,142 | INFO | normalized log probability: -0.07
2026-01-28 21:39:46,143 | INFO | total number of ended hypotheses: 155
2026-01-28 21:39:46,143 | INFO | best hypo: ▁je▁sais▁que▁bien▁les▁tragédies▁aujourd'hui▁font▁douter▁de▁cet▁espérance

2026-01-28 21:39:46,145 | INFO | speech length: 108960
2026-01-28 21:39:46,188 | INFO | decoder input length: 169
2026-01-28 21:39:46,188 | INFO | max output length: 169
2026-01-28 21:39:46,188 | INFO | min output length: 16
2026-01-28 21:39:51,682 | INFO | end detected at 45
2026-01-28 21:39:51,683 | INFO |  -3.03 * 0.5 =  -1.52 for decoder
2026-01-28 21:39:51,683 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 21:39:51,683 | INFO | total log probability: -1.52
2026-01-28 21:39:51,683 | INFO | normalized log probability: -0.04
2026-01-28 21:39:51,683 | INFO | total number of ended hypotheses: 149
2026-01-28 21:39:51,684 | INFO | best hypo: ▁pourtant▁de▁plus▁en▁plus▁les▁nations▁s'accordent▁pour▁mieux▁faire▁respecter▁les▁droits▁de▁l'homme

2026-01-28 21:39:51,686 | INFO | speech length: 59520
2026-01-28 21:39:51,731 | INFO | decoder input length: 92
2026-01-28 21:39:51,731 | INFO | max output length: 92
2026-01-28 21:39:51,731 | INFO | min output length: 9
2026-01-28 21:39:53,467 | INFO | end detected at 27
2026-01-28 21:39:53,468 | INFO |  -1.55 * 0.5 =  -0.77 for decoder
2026-01-28 21:39:53,468 | INFO |  -0.13 * 0.5 =  -0.06 for ctc
2026-01-28 21:39:53,468 | INFO | total log probability: -0.84
2026-01-28 21:39:53,468 | INFO | normalized log probability: -0.04
2026-01-28 21:39:53,468 | INFO | total number of ended hypotheses: 144
2026-01-28 21:39:53,469 | INFO | best hypo: ▁pour▁défendre▁la▁liberté▁et▁la▁dignité▁humaine

2026-01-28 21:39:53,471 | INFO | speech length: 50720
2026-01-28 21:39:53,513 | INFO | decoder input length: 78
2026-01-28 21:39:53,514 | INFO | max output length: 78
2026-01-28 21:39:53,514 | INFO | min output length: 7
2026-01-28 21:39:54,675 | INFO | end detected at 27
2026-01-28 21:39:54,676 | INFO |  -1.73 * 0.5 =  -0.87 for decoder
2026-01-28 21:39:54,676 | INFO |  -0.17 * 0.5 =  -0.09 for ctc
2026-01-28 21:39:54,676 | INFO | total log probability: -0.95
2026-01-28 21:39:54,676 | INFO | normalized log probability: -0.04
2026-01-28 21:39:54,676 | INFO | total number of ended hypotheses: 147
2026-01-28 21:39:54,677 | INFO | best hypo: ▁un▁nouvel▁ordre▁international▁s'affirme▁peu▁à▁peu

2026-01-28 21:39:54,679 | INFO | speech length: 100000
2026-01-28 21:39:54,732 | INFO | decoder input length: 155
2026-01-28 21:39:54,732 | INFO | max output length: 155
2026-01-28 21:39:54,732 | INFO | min output length: 15
2026-01-28 21:39:56,750 | INFO | end detected at 36
2026-01-28 21:39:56,751 | INFO |  -3.73 * 0.5 =  -1.87 for decoder
2026-01-28 21:39:56,751 | INFO |  -0.45 * 0.5 =  -0.23 for ctc
2026-01-28 21:39:56,751 | INFO | total log probability: -2.09
2026-01-28 21:39:56,751 | INFO | normalized log probability: -0.07
2026-01-28 21:39:56,751 | INFO | total number of ended hypotheses: 149
2026-01-28 21:39:56,751 | INFO | best hypo: ▁demain▁il▁ne▁devra▁plus▁y▁avoir▁de▁repos▁pour▁les▁criminels▁contre▁l'humanite

2026-01-28 21:39:56,753 | INFO | speech length: 16800
2026-01-28 21:39:56,797 | INFO | decoder input length: 25
2026-01-28 21:39:56,797 | INFO | max output length: 25
2026-01-28 21:39:56,797 | INFO | min output length: 2
2026-01-28 21:39:57,401 | INFO | end detected at 15
2026-01-28 21:39:57,402 | INFO |  -3.45 * 0.5 =  -1.72 for decoder
2026-01-28 21:39:57,402 | INFO |  -1.76 * 0.5 =  -0.88 for ctc
2026-01-28 21:39:57,402 | INFO | total log probability: -2.61
2026-01-28 21:39:57,402 | INFO | normalized log probability: -0.29
2026-01-28 21:39:57,402 | INFO | total number of ended hypotheses: 182
2026-01-28 21:39:57,403 | INFO | best hypo: ▁sur▁au▁nom▁de▁la▁france

2026-01-28 21:39:57,405 | INFO | speech length: 14720
2026-01-28 21:39:57,457 | INFO | decoder input length: 22
2026-01-28 21:39:57,457 | INFO | max output length: 22
2026-01-28 21:39:57,457 | INFO | min output length: 2
2026-01-28 21:39:58,011 | INFO | end detected at 10
2026-01-28 21:39:58,013 | INFO |  -1.93 * 0.5 =  -0.96 for decoder
2026-01-28 21:39:58,013 | INFO |  -1.01 * 0.5 =  -0.50 for ctc
2026-01-28 21:39:58,013 | INFO | total log probability: -1.47
2026-01-28 21:39:58,013 | INFO | normalized log probability: -0.24
2026-01-28 21:39:58,013 | INFO | total number of ended hypotheses: 151
2026-01-28 21:39:58,013 | INFO | best hypo: ▁en▁votre▁nom

2026-01-28 21:39:58,016 | INFO | speech length: 44160
2026-01-28 21:39:58,069 | INFO | decoder input length: 68
2026-01-28 21:39:58,070 | INFO | max output length: 68
2026-01-28 21:39:58,070 | INFO | min output length: 6
2026-01-28 21:39:59,659 | INFO | end detected at 26
2026-01-28 21:39:59,660 | INFO |  -1.50 * 0.5 =  -0.75 for decoder
2026-01-28 21:39:59,661 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-28 21:39:59,661 | INFO | total log probability: -0.75
2026-01-28 21:39:59,661 | INFO | normalized log probability: -0.03
2026-01-28 21:39:59,661 | INFO | total number of ended hypotheses: 148
2026-01-28 21:39:59,661 | INFO | best hypo: ▁c'est▁le▁combat▁difficile▁que▁je▁mène▁chaque▁jour

2026-01-28 21:39:59,664 | INFO | speech length: 32160
2026-01-28 21:39:59,720 | INFO | decoder input length: 49
2026-01-28 21:39:59,720 | INFO | max output length: 49
2026-01-28 21:39:59,720 | INFO | min output length: 4
2026-01-28 21:40:00,904 | INFO | end detected at 18
2026-01-28 21:40:00,905 | INFO |  -1.04 * 0.5 =  -0.52 for decoder
2026-01-28 21:40:00,905 | INFO |  -0.10 * 0.5 =  -0.05 for ctc
2026-01-28 21:40:00,905 | INFO | total log probability: -0.57
2026-01-28 21:40:00,905 | INFO | normalized log probability: -0.04
2026-01-28 21:40:00,905 | INFO | total number of ended hypotheses: 141
2026-01-28 21:40:00,906 | INFO | best hypo: ▁a▁l'intérieur▁de▁chaque▁nation

2026-01-28 21:40:00,908 | INFO | speech length: 32480
2026-01-28 21:40:00,959 | INFO | decoder input length: 50
2026-01-28 21:40:00,959 | INFO | max output length: 50
2026-01-28 21:40:00,959 | INFO | min output length: 5
2026-01-28 21:40:02,924 | INFO | end detected at 17
2026-01-28 21:40:02,925 | INFO |  -5.76 * 0.5 =  -2.88 for decoder
2026-01-28 21:40:02,926 | INFO |  -3.50 * 0.5 =  -1.75 for ctc
2026-01-28 21:40:02,926 | INFO | total log probability: -4.63
2026-01-28 21:40:02,926 | INFO | normalized log probability: -0.42
2026-01-28 21:40:02,926 | INFO | total number of ended hypotheses: 164
2026-01-28 21:40:02,926 | INFO | best hypo: ▁une▁exigence▁se▁fait▁entre

2026-01-28 21:40:02,929 | INFO | speech length: 15360
2026-01-28 21:40:02,981 | INFO | decoder input length: 23
2026-01-28 21:40:02,981 | INFO | max output length: 23
2026-01-28 21:40:02,981 | INFO | min output length: 2
2026-01-28 21:40:04,588 | INFO | end detected at 15
2026-01-28 21:40:04,590 | INFO |  -0.84 * 0.5 =  -0.42 for decoder
2026-01-28 21:40:04,590 | INFO |  -1.10 * 0.5 =  -0.55 for ctc
2026-01-28 21:40:04,590 | INFO | total log probability: -0.97
2026-01-28 21:40:04,590 | INFO | normalized log probability: -0.09
2026-01-28 21:40:04,590 | INFO | total number of ended hypotheses: 154
2026-01-28 21:40:04,590 | INFO | best hypo: ▁toujours▁plus▁fort

2026-01-28 21:40:04,593 | INFO | speech length: 117280
2026-01-28 21:40:04,694 | INFO | decoder input length: 182
2026-01-28 21:40:04,694 | INFO | max output length: 182
2026-01-28 21:40:04,695 | INFO | min output length: 18
2026-01-28 21:40:09,232 | INFO | end detected at 55
2026-01-28 21:40:09,234 | INFO |  -3.88 * 0.5 =  -1.94 for decoder
2026-01-28 21:40:09,234 | INFO |  -0.80 * 0.5 =  -0.40 for ctc
2026-01-28 21:40:09,234 | INFO | total log probability: -2.34
2026-01-28 21:40:09,234 | INFO | normalized log probability: -0.05
2026-01-28 21:40:09,234 | INFO | total number of ended hypotheses: 150
2026-01-28 21:40:09,235 | INFO | best hypo: ▁pour▁que▁les▁avancées▁de▁la▁science▁soient▁orientées▁vers▁le▁bien▁de▁l'homme▁et▁ne▁se▁retournent▁jamais▁contre▁lui

2026-01-28 21:40:09,237 | INFO | speech length: 29920
2026-01-28 21:40:09,293 | INFO | decoder input length: 46
2026-01-28 21:40:09,293 | INFO | max output length: 46
2026-01-28 21:40:09,293 | INFO | min output length: 4
2026-01-28 21:40:10,112 | INFO | end detected at 14
2026-01-28 21:40:10,113 | INFO |  -0.89 * 0.5 =  -0.44 for decoder
2026-01-28 21:40:10,113 | INFO |  -1.87 * 0.5 =  -0.94 for ctc
2026-01-28 21:40:10,114 | INFO | total log probability: -1.38
2026-01-28 21:40:10,114 | INFO | normalized log probability: -0.14
2026-01-28 21:40:10,114 | INFO | total number of ended hypotheses: 144
2026-01-28 21:40:10,114 | INFO | best hypo: ▁je▁pense▁par▁exemple

2026-01-28 21:40:10,117 | INFO | speech length: 46080
2026-01-28 21:40:10,171 | INFO | decoder input length: 71
2026-01-28 21:40:10,171 | INFO | max output length: 71
2026-01-28 21:40:10,171 | INFO | min output length: 7
2026-01-28 21:40:12,894 | INFO | end detected at 25
2026-01-28 21:40:12,896 | INFO |  -3.26 * 0.5 =  -1.63 for decoder
2026-01-28 21:40:12,897 | INFO |  -5.70 * 0.5 =  -2.85 for ctc
2026-01-28 21:40:12,897 | INFO | total log probability: -4.48
2026-01-28 21:40:12,897 | INFO | normalized log probability: -0.22
2026-01-28 21:40:12,897 | INFO | total number of ended hypotheses: 167
2026-01-28 21:40:12,897 | INFO | best hypo: ▁aux▁manipulations▁génétiques▁au▁clonage

2026-01-28 21:40:12,900 | INFO | speech length: 27840
2026-01-28 21:40:12,954 | INFO | decoder input length: 43
2026-01-28 21:40:12,954 | INFO | max output length: 43
2026-01-28 21:40:12,954 | INFO | min output length: 4
2026-01-28 21:40:14,901 | INFO | end detected at 21
2026-01-28 21:40:14,902 | INFO |  -1.16 * 0.5 =  -0.58 for decoder
2026-01-28 21:40:14,902 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-28 21:40:14,902 | INFO | total log probability: -0.58
2026-01-28 21:40:14,902 | INFO | normalized log probability: -0.03
2026-01-28 21:40:14,902 | INFO | total number of ended hypotheses: 138
2026-01-28 21:40:14,902 | INFO | best hypo: ▁de▁même▁dans▁le▁domaine▁de▁l'environnement

2026-01-28 21:40:14,904 | INFO | speech length: 96320
2026-01-28 21:40:14,961 | INFO | decoder input length: 150
2026-01-28 21:40:14,961 | INFO | max output length: 150
2026-01-28 21:40:14,961 | INFO | min output length: 15
2026-01-28 21:40:17,460 | INFO | end detected at 41
2026-01-28 21:40:17,461 | INFO |  -2.91 * 0.5 =  -1.45 for decoder
2026-01-28 21:40:17,462 | INFO |  -0.16 * 0.5 =  -0.08 for ctc
2026-01-28 21:40:17,462 | INFO | total log probability: -1.53
2026-01-28 21:40:17,462 | INFO | normalized log probability: -0.04
2026-01-28 21:40:17,462 | INFO | total number of ended hypotheses: 152
2026-01-28 21:40:17,462 | INFO | best hypo: ▁les▁peuples▁ne▁veulent▁plus▁que▁la▁course▁à▁la▁productivité▁épuise▁la▁planète

2026-01-28 21:40:17,465 | INFO | speech length: 92800
2026-01-28 21:40:17,512 | INFO | decoder input length: 144
2026-01-28 21:40:17,512 | INFO | max output length: 144
2026-01-28 21:40:17,512 | INFO | min output length: 14
2026-01-28 21:40:19,972 | INFO | end detected at 40
2026-01-28 21:40:19,973 | INFO |  -2.70 * 0.5 =  -1.35 for decoder
2026-01-28 21:40:19,973 | INFO |  -0.17 * 0.5 =  -0.09 for ctc
2026-01-28 21:40:19,974 | INFO | total log probability: -1.44
2026-01-28 21:40:19,974 | INFO | normalized log probability: -0.04
2026-01-28 21:40:19,974 | INFO | total number of ended hypotheses: 149
2026-01-28 21:40:19,974 | INFO | best hypo: ▁la▁responsabilité▁de▁tous▁ceux▁qui▁dans▁le▁monde▁dégradent▁le▁patrimoine▁naturel

2026-01-28 21:40:19,977 | INFO | speech length: 41280
2026-01-28 21:40:20,026 | INFO | decoder input length: 64
2026-01-28 21:40:20,026 | INFO | max output length: 64
2026-01-28 21:40:20,026 | INFO | min output length: 6
2026-01-28 21:40:21,099 | INFO | end detected at 21
2026-01-28 21:40:21,101 | INFO |  -1.15 * 0.5 =  -0.57 for decoder
2026-01-28 21:40:21,101 | INFO |  -0.29 * 0.5 =  -0.14 for ctc
2026-01-28 21:40:21,101 | INFO | total log probability: -0.72
2026-01-28 21:40:21,101 | INFO | normalized log probability: -0.04
2026-01-28 21:40:21,101 | INFO | total number of ended hypotheses: 162
2026-01-28 21:40:21,101 | INFO | best hypo: ▁doit▁être▁recherché▁et▁sanctionné

2026-01-28 21:40:21,104 | INFO | speech length: 68800
2026-01-28 21:40:21,150 | INFO | decoder input length: 107
2026-01-28 21:40:21,151 | INFO | max output length: 107
2026-01-28 21:40:21,151 | INFO | min output length: 10
2026-01-28 21:40:23,009 | INFO | end detected at 33
2026-01-28 21:40:23,011 | INFO |  -9.24 * 0.5 =  -4.62 for decoder
2026-01-28 21:40:23,011 | INFO |  -0.81 * 0.5 =  -0.41 for ctc
2026-01-28 21:40:23,011 | INFO | total log probability: -5.03
2026-01-28 21:40:23,011 | INFO | normalized log probability: -0.19
2026-01-28 21:40:23,011 | INFO | total number of ended hypotheses: 179
2026-01-28 21:40:23,012 | INFO | best hypo: ▁car▁il▁s'agit▁du▁patrimoine▁que▁nous▁lèguerons▁à▁nos▁ens

2026-01-28 21:40:23,014 | INFO | speech length: 54720
2026-01-28 21:40:23,061 | INFO | decoder input length: 85
2026-01-28 21:40:23,061 | INFO | max output length: 85
2026-01-28 21:40:23,061 | INFO | min output length: 8
2026-01-28 21:40:24,448 | INFO | end detected at 26
2026-01-28 21:40:24,449 | INFO |  -1.57 * 0.5 =  -0.78 for decoder
2026-01-28 21:40:24,450 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 21:40:24,450 | INFO | total log probability: -0.79
2026-01-28 21:40:24,450 | INFO | normalized log probability: -0.04
2026-01-28 21:40:24,450 | INFO | total number of ended hypotheses: 138
2026-01-28 21:40:24,450 | INFO | best hypo: ▁même▁si▁le▁monde▁change▁comme▁il▁n'a▁jamais▁changé

2026-01-28 21:40:24,452 | INFO | speech length: 42560
2026-01-28 21:40:24,504 | INFO | decoder input length: 66
2026-01-28 21:40:24,505 | INFO | max output length: 66
2026-01-28 21:40:24,505 | INFO | min output length: 6
2026-01-28 21:40:25,789 | INFO | end detected at 21
2026-01-28 21:40:25,790 | INFO |  -1.19 * 0.5 =  -0.59 for decoder
2026-01-28 21:40:25,790 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 21:40:25,791 | INFO | total log probability: -0.59
2026-01-28 21:40:25,791 | INFO | normalized log probability: -0.03
2026-01-28 21:40:25,791 | INFO | total number of ended hypotheses: 152
2026-01-28 21:40:25,791 | INFO | best hypo: ▁la▁modernité▁ne▁doit▁pas▁nous▁diviser

2026-01-28 21:40:25,793 | INFO | speech length: 36800
2026-01-28 21:40:25,839 | INFO | decoder input length: 57
2026-01-28 21:40:25,839 | INFO | max output length: 57
2026-01-28 21:40:25,840 | INFO | min output length: 5
2026-01-28 21:40:26,688 | INFO | end detected at 17
2026-01-28 21:40:26,689 | INFO |  -0.92 * 0.5 =  -0.46 for decoder
2026-01-28 21:40:26,689 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-28 21:40:26,689 | INFO | total log probability: -0.46
2026-01-28 21:40:26,689 | INFO | normalized log probability: -0.04
2026-01-28 21:40:26,689 | INFO | total number of ended hypotheses: 141
2026-01-28 21:40:26,689 | INFO | best hypo: ▁elle▁doit▁profiter▁à▁chacun

2026-01-28 21:40:26,691 | INFO | speech length: 113440
2026-01-28 21:40:26,748 | INFO | decoder input length: 176
2026-01-28 21:40:26,748 | INFO | max output length: 176
2026-01-28 21:40:26,748 | INFO | min output length: 17
2026-01-28 21:40:30,301 | INFO | end detected at 54
2026-01-28 21:40:30,302 | INFO |  -7.23 * 0.5 =  -3.61 for decoder
2026-01-28 21:40:30,303 | INFO |  -6.98 * 0.5 =  -3.49 for ctc
2026-01-28 21:40:30,303 | INFO | total log probability: -7.11
2026-01-28 21:40:30,303 | INFO | normalized log probability: -0.15
2026-01-28 21:40:30,303 | INFO | total number of ended hypotheses: 173
2026-01-28 21:40:30,304 | INFO | best hypo: ▁nous▁réussirons▁nous▁réussirons▁parce▁que▁nous▁avons▁pris▁des▁décisions▁qui▁engagent▁et▁qui▁garantissent▁nos▁travail

2026-01-28 21:40:30,306 | INFO | speech length: 56960
2026-01-28 21:40:30,353 | INFO | decoder input length: 88
2026-01-28 21:40:30,354 | INFO | max output length: 88
2026-01-28 21:40:30,354 | INFO | min output length: 8
2026-01-28 21:40:32,160 | INFO | end detected at 34
2026-01-28 21:40:32,162 | INFO |  -2.08 * 0.5 =  -1.04 for decoder
2026-01-28 21:40:32,162 | INFO |  -0.04 * 0.5 =  -0.02 for ctc
2026-01-28 21:40:32,162 | INFO | total log probability: -1.06
2026-01-28 21:40:32,162 | INFO | normalized log probability: -0.04
2026-01-28 21:40:32,162 | INFO | total number of ended hypotheses: 140
2026-01-28 21:40:32,163 | INFO | best hypo: ▁nous▁avons▁choisi▁ensemble▁de▁faire▁grandir▁la▁france▁dans▁l'europe

2026-01-28 21:40:32,165 | INFO | speech length: 101280
2026-01-28 21:40:32,213 | INFO | decoder input length: 157
2026-01-28 21:40:32,213 | INFO | max output length: 157
2026-01-28 21:40:32,213 | INFO | min output length: 15
2026-01-28 21:40:35,315 | INFO | end detected at 47
2026-01-28 21:40:35,316 | INFO |  -3.85 * 0.5 =  -1.93 for decoder
2026-01-28 21:40:35,316 | INFO |  -0.87 * 0.5 =  -0.44 for ctc
2026-01-28 21:40:35,316 | INFO | total log probability: -2.36
2026-01-28 21:40:35,316 | INFO | normalized log probability: -0.05
2026-01-28 21:40:35,316 | INFO | total number of ended hypotheses: 152
2026-01-28 21:40:35,317 | INFO | best hypo: ▁une▁europe▁qui▁nous▁garantit▁la▁paix▁une▁europe▁qui▁nous▁permet▁de▁peser▁davantage▁dans▁le▁monde

2026-01-28 21:40:35,319 | INFO | speech length: 105600
2026-01-28 21:40:35,366 | INFO | decoder input length: 164
2026-01-28 21:40:35,366 | INFO | max output length: 164
2026-01-28 21:40:35,366 | INFO | min output length: 16
2026-01-28 21:40:37,714 | INFO | end detected at 42
2026-01-28 21:40:37,715 | INFO |  -2.57 * 0.5 =  -1.29 for decoder
2026-01-28 21:40:37,715 | INFO |  -0.06 * 0.5 =  -0.03 for ctc
2026-01-28 21:40:37,715 | INFO | total log probability: -1.32
2026-01-28 21:40:37,715 | INFO | normalized log probability: -0.03
2026-01-28 21:40:37,715 | INFO | total number of ended hypotheses: 145
2026-01-28 21:40:37,715 | INFO | best hypo: ▁nous▁avons▁choisi▁aussi▁de▁prendre▁part▁à▁la▁mondialisation▁d'en▁prendre▁toute▁notre▁part

2026-01-28 21:40:37,718 | INFO | speech length: 92640
2026-01-28 21:40:37,772 | INFO | decoder input length: 144
2026-01-28 21:40:37,772 | INFO | max output length: 144
2026-01-28 21:40:37,772 | INFO | min output length: 14
2026-01-28 21:40:39,937 | INFO | end detected at 41
2026-01-28 21:40:39,937 | INFO |  -2.65 * 0.5 =  -1.32 for decoder
2026-01-28 21:40:39,938 | INFO |  -0.09 * 0.5 =  -0.05 for ctc
2026-01-28 21:40:39,938 | INFO | total log probability: -1.37
2026-01-28 21:40:39,938 | INFO | normalized log probability: -0.04
2026-01-28 21:40:39,938 | INFO | total number of ended hypotheses: 143
2026-01-28 21:40:39,938 | INFO | best hypo: ▁mais▁une▁mondialisation▁maîtrisée▁organisée▁respectueuse▁de▁l'environnement

2026-01-28 21:40:39,940 | INFO | speech length: 67840
2026-01-28 21:40:39,987 | INFO | decoder input length: 105
2026-01-28 21:40:39,987 | INFO | max output length: 105
2026-01-28 21:40:39,988 | INFO | min output length: 10
2026-01-28 21:40:41,493 | INFO | end detected at 31
2026-01-28 21:40:41,494 | INFO |  -1.88 * 0.5 =  -0.94 for decoder
2026-01-28 21:40:41,495 | INFO |  -0.27 * 0.5 =  -0.14 for ctc
2026-01-28 21:40:41,495 | INFO | total log probability: -1.07
2026-01-28 21:40:41,495 | INFO | normalized log probability: -0.04
2026-01-28 21:40:41,495 | INFO | total number of ended hypotheses: 160
2026-01-28 21:40:41,495 | INFO | best hypo: ▁capable▁de▁prendre▁en▁compte▁les▁aspirations▁des▁hommes

2026-01-28 21:40:41,497 | INFO | speech length: 34560
2026-01-28 21:40:41,543 | INFO | decoder input length: 53
2026-01-28 21:40:41,543 | INFO | max output length: 53
2026-01-28 21:40:41,543 | INFO | min output length: 5
2026-01-28 21:40:42,471 | INFO | end detected at 22
2026-01-28 21:40:42,472 | INFO |  -5.97 * 0.5 =  -2.98 for decoder
2026-01-28 21:40:42,472 | INFO |  -0.55 * 0.5 =  -0.27 for ctc
2026-01-28 21:40:42,472 | INFO | total log probability: -3.26
2026-01-28 21:40:42,472 | INFO | normalized log probability: -0.19
2026-01-28 21:40:42,473 | INFO | total number of ended hypotheses: 145
2026-01-28 21:40:42,473 | INFO | best hypo: ▁il▁est▁capable▁de▁faire▁reculer▁la▁pauvre

2026-01-28 21:40:42,475 | INFO | speech length: 52000
2026-01-28 21:40:42,522 | INFO | decoder input length: 80
2026-01-28 21:40:42,522 | INFO | max output length: 80
2026-01-28 21:40:42,522 | INFO | min output length: 8
2026-01-28 21:40:43,505 | INFO | end detected at 21
2026-01-28 21:40:43,506 | INFO |  -1.16 * 0.5 =  -0.58 for decoder
2026-01-28 21:40:43,506 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-28 21:40:43,506 | INFO | total log probability: -0.58
2026-01-28 21:40:43,506 | INFO | normalized log probability: -0.03
2026-01-28 21:40:43,506 | INFO | total number of ended hypotheses: 137
2026-01-28 21:40:43,507 | INFO | best hypo: ▁ce▁sera▁tout▁le▁sens▁du▁combat▁de▁la▁france

2026-01-28 21:40:43,508 | INFO | speech length: 21600
2026-01-28 21:40:43,562 | INFO | decoder input length: 33
2026-01-28 21:40:43,562 | INFO | max output length: 33
2026-01-28 21:40:43,562 | INFO | min output length: 3
2026-01-28 21:40:44,390 | INFO | end detected at 21
2026-01-28 21:40:44,391 | INFO |  -3.52 * 0.5 =  -1.76 for decoder
2026-01-28 21:40:44,391 | INFO |  -5.38 * 0.5 =  -2.69 for ctc
2026-01-28 21:40:44,391 | INFO | total log probability: -4.45
2026-01-28 21:40:44,391 | INFO | normalized log probability: -0.34
2026-01-28 21:40:44,391 | INFO | total number of ended hypotheses: 179
2026-01-28 21:40:44,391 | INFO | best hypo: ▁dans▁les▁grandes▁négociations

2026-01-28 21:40:44,393 | INFO | speech length: 20160
2026-01-28 21:40:44,456 | INFO | decoder input length: 31
2026-01-28 21:40:44,456 | INFO | max output length: 31
2026-01-28 21:40:44,456 | INFO | min output length: 3
2026-01-28 21:40:45,238 | INFO | end detected at 17
2026-01-28 21:40:45,240 | INFO |  -0.90 * 0.5 =  -0.45 for decoder
2026-01-28 21:40:45,240 | INFO |  -0.23 * 0.5 =  -0.12 for ctc
2026-01-28 21:40:45,240 | INFO | total log probability: -0.57
2026-01-28 21:40:45,240 | INFO | normalized log probability: -0.04
2026-01-28 21:40:45,240 | INFO | total number of ended hypotheses: 135
2026-01-28 21:40:45,241 | INFO | best hypo: ▁mes▁chers▁compatriotes

2026-01-28 21:40:45,242 | INFO | speech length: 37440
2026-01-28 21:40:45,291 | INFO | decoder input length: 58
2026-01-28 21:40:45,291 | INFO | max output length: 58
2026-01-28 21:40:45,291 | INFO | min output length: 5
2026-01-28 21:40:46,377 | INFO | end detected at 21
2026-01-28 21:40:46,379 | INFO |  -1.17 * 0.5 =  -0.58 for decoder
2026-01-28 21:40:46,380 | INFO |  -0.33 * 0.5 =  -0.17 for ctc
2026-01-28 21:40:46,380 | INFO | total log probability: -0.75
2026-01-28 21:40:46,380 | INFO | normalized log probability: -0.04
2026-01-28 21:40:46,380 | INFO | total number of ended hypotheses: 151
2026-01-28 21:40:46,380 | INFO | best hypo: ▁nous▁avons▁en▁commun▁certaines▁valeurs

2026-01-28 21:40:46,382 | INFO | speech length: 116960
2026-01-28 21:40:46,430 | INFO | decoder input length: 182
2026-01-28 21:40:46,430 | INFO | max output length: 182
2026-01-28 21:40:46,430 | INFO | min output length: 18
2026-01-28 21:40:49,384 | INFO | end detected at 52
2026-01-28 21:40:49,386 | INFO |  -5.18 * 0.5 =  -2.59 for decoder
2026-01-28 21:40:49,386 | INFO |  -4.54 * 0.5 =  -2.27 for ctc
2026-01-28 21:40:49,386 | INFO | total log probability: -4.86
2026-01-28 21:40:49,386 | INFO | normalized log probability: -0.10
2026-01-28 21:40:49,386 | INFO | total number of ended hypotheses: 161
2026-01-28 21:40:49,386 | INFO | best hypo: ▁la▁volonté▁de▁donner▁à▁chacun▁sa▁chance▁pour▁que▁notre▁société▁soit▁plus▁alente▁plus▁mobile▁plus▁optimiste

2026-01-28 21:40:49,388 | INFO | speech length: 118400
2026-01-28 21:40:49,436 | INFO | decoder input length: 184
2026-01-28 21:40:49,436 | INFO | max output length: 184
2026-01-28 21:40:49,436 | INFO | min output length: 18
2026-01-28 21:40:52,720 | INFO | end detected at 58
2026-01-28 21:40:52,721 | INFO |  -3.96 * 0.5 =  -1.98 for decoder
2026-01-28 21:40:52,721 | INFO |  -0.40 * 0.5 =  -0.20 for ctc
2026-01-28 21:40:52,721 | INFO | total log probability: -2.18
2026-01-28 21:40:52,721 | INFO | normalized log probability: -0.04
2026-01-28 21:40:52,721 | INFO | total number of ended hypotheses: 143
2026-01-28 21:40:52,722 | INFO | best hypo: ▁l'exigence▁de▁solidarité▁une▁solidarité▁plus▁responsable▁où▁chacun▁s'efforcerait▁de▁prendre▁sa▁part▁du▁contrat

2026-01-28 21:40:52,724 | INFO | speech length: 83520
2026-01-28 21:40:52,770 | INFO | decoder input length: 130
2026-01-28 21:40:52,770 | INFO | max output length: 130
2026-01-28 21:40:52,770 | INFO | min output length: 13
2026-01-28 21:40:54,790 | INFO | end detected at 38
2026-01-28 21:40:54,791 | INFO |  -2.99 * 0.5 =  -1.50 for decoder
2026-01-28 21:40:54,791 | INFO |  -5.03 * 0.5 =  -2.51 for ctc
2026-01-28 21:40:54,791 | INFO | total log probability: -4.01
2026-01-28 21:40:54,791 | INFO | normalized log probability: -0.13
2026-01-28 21:40:54,791 | INFO | total number of ended hypotheses: 171
2026-01-28 21:40:54,792 | INFO | best hypo: ▁l'attachement▁à▁la▁famille▁parce▁qu'elle▁est▁chaleur▁entre▁aide▁sécurité

2026-01-28 21:40:54,794 | INFO | speech length: 61120
2026-01-28 21:40:54,832 | INFO | decoder input length: 95
2026-01-28 21:40:54,832 | INFO | max output length: 95
2026-01-28 21:40:54,832 | INFO | min output length: 9
2026-01-28 21:40:56,322 | INFO | end detected at 32
2026-01-28 21:40:56,324 | INFO |  -2.07 * 0.5 =  -1.03 for decoder
2026-01-28 21:40:56,324 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-28 21:40:56,324 | INFO | total log probability: -1.04
2026-01-28 21:40:56,324 | INFO | normalized log probability: -0.04
2026-01-28 21:40:56,324 | INFO | total number of ended hypotheses: 147
2026-01-28 21:40:56,324 | INFO | best hypo: ▁le▁désir▁d'être▁utile▁de▁trouver▁sa▁place▁dans▁la▁société

2026-01-28 21:40:56,326 | INFO | speech length: 29280
2026-01-28 21:40:56,381 | INFO | decoder input length: 45
2026-01-28 21:40:56,381 | INFO | max output length: 45
2026-01-28 21:40:56,381 | INFO | min output length: 4
2026-01-28 21:40:57,145 | INFO | end detected at 17
2026-01-28 21:40:57,146 | INFO |  -1.03 * 0.5 =  -0.51 for decoder
2026-01-28 21:40:57,146 | INFO |  -0.38 * 0.5 =  -0.19 for ctc
2026-01-28 21:40:57,146 | INFO | total log probability: -0.70
2026-01-28 21:40:57,146 | INFO | normalized log probability: -0.05
2026-01-28 21:40:57,146 | INFO | total number of ended hypotheses: 152
2026-01-28 21:40:57,146 | INFO | best hypo: ▁de▁donner▁autour▁de▁soi

2026-01-28 21:40:57,148 | INFO | speech length: 13600
2026-01-28 21:40:57,206 | INFO | decoder input length: 20
2026-01-28 21:40:57,206 | INFO | max output length: 20
2026-01-28 21:40:57,206 | INFO | min output length: 2
2026-01-28 21:40:58,020 | INFO | end detected at 12
2026-01-28 21:40:58,022 | INFO |  -1.15 * 0.5 =  -0.57 for decoder
2026-01-28 21:40:58,022 | INFO |  -1.93 * 0.5 =  -0.97 for ctc
2026-01-28 21:40:58,022 | INFO | total log probability: -1.54
2026-01-28 21:40:58,022 | INFO | normalized log probability: -0.19
2026-01-28 21:40:58,022 | INFO | total number of ended hypotheses: 139
2026-01-28 21:40:58,023 | INFO | best hypo: ▁elle▁se▁réalise

2026-01-28 21:40:58,026 | INFO | speech length: 13600
2026-01-28 21:40:58,075 | INFO | decoder input length: 20
2026-01-28 21:40:58,075 | INFO | max output length: 20
2026-01-28 21:40:58,075 | INFO | min output length: 2
2026-01-28 21:40:58,614 | INFO | end detected at 12
2026-01-28 21:40:58,615 | INFO |  -0.52 * 0.5 =  -0.26 for decoder
2026-01-28 21:40:58,615 | INFO |  -0.06 * 0.5 =  -0.03 for ctc
2026-01-28 21:40:58,615 | INFO | total log probability: -0.29
2026-01-28 21:40:58,615 | INFO | normalized log probability: -0.04
2026-01-28 21:40:58,615 | INFO | total number of ended hypotheses: 139
2026-01-28 21:40:58,615 | INFO | best hypo: ▁la▁tolérance

2026-01-28 21:40:58,617 | INFO | speech length: 50720
2026-01-28 21:40:58,671 | INFO | decoder input length: 78
2026-01-28 21:40:58,672 | INFO | max output length: 78
2026-01-28 21:40:58,672 | INFO | min output length: 7
2026-01-28 21:41:00,546 | INFO | end detected at 23
2026-01-28 21:41:00,548 | INFO |  -9.95 * 0.5 =  -4.98 for decoder
2026-01-28 21:41:00,548 | INFO |  -1.64 * 0.5 =  -0.82 for ctc
2026-01-28 21:41:00,548 | INFO | total log probability: -5.80
2026-01-28 21:41:00,548 | INFO | normalized log probability: -0.32
2026-01-28 21:41:00,548 | INFO | total number of ended hypotheses: 163
2026-01-28 21:41:00,549 | INFO | best hypo: ▁qui▁ne▁doit▁pas▁être▁renoncement▁à▁ses▁condé

2026-01-28 21:41:00,552 | INFO | speech length: 16480
2026-01-28 21:41:00,608 | INFO | decoder input length: 25
2026-01-28 21:41:00,608 | INFO | max output length: 25
2026-01-28 21:41:00,608 | INFO | min output length: 2
2026-01-28 21:41:01,499 | INFO | end detected at 20
2026-01-28 21:41:01,500 | INFO |  -1.42 * 0.5 =  -0.71 for decoder
2026-01-28 21:41:01,500 | INFO |  -2.15 * 0.5 =  -1.07 for ctc
2026-01-28 21:41:01,501 | INFO | total log probability: -1.78
2026-01-28 21:41:01,501 | INFO | normalized log probability: -0.11
2026-01-28 21:41:01,501 | INFO | total number of ended hypotheses: 173
2026-01-28 21:41:01,501 | INFO | best hypo: ▁mes▁respects▁de▁l'eau

2026-01-28 21:41:01,504 | INFO | speech length: 18720
2026-01-28 21:41:01,553 | INFO | decoder input length: 28
2026-01-28 21:41:01,553 | INFO | max output length: 28
2026-01-28 21:41:01,553 | INFO | min output length: 2
2026-01-28 21:41:02,360 | INFO | end detected at 18
2026-01-28 21:41:02,363 | INFO |  -6.99 * 0.5 =  -3.50 for decoder
2026-01-28 21:41:02,363 | INFO |  -3.23 * 0.5 =  -1.62 for ctc
2026-01-28 21:41:02,363 | INFO | total log probability: -5.11
2026-01-28 21:41:02,363 | INFO | normalized log probability: -0.39
2026-01-28 21:41:02,363 | INFO | total number of ended hypotheses: 168
2026-01-28 21:41:02,363 | INFO | best hypo: ▁sprit▁républicain

2026-01-28 21:41:02,366 | INFO | speech length: 80800
2026-01-28 21:41:02,422 | INFO | decoder input length: 125
2026-01-28 21:41:02,422 | INFO | max output length: 125
2026-01-28 21:41:02,422 | INFO | min output length: 12
2026-01-28 21:41:04,768 | INFO | end detected at 41
2026-01-28 21:41:04,769 | INFO |  -2.80 * 0.5 =  -1.40 for decoder
2026-01-28 21:41:04,769 | INFO |  -0.82 * 0.5 =  -0.41 for ctc
2026-01-28 21:41:04,769 | INFO | total log probability: -1.81
2026-01-28 21:41:04,769 | INFO | normalized log probability: -0.05
2026-01-28 21:41:04,770 | INFO | total number of ended hypotheses: 166
2026-01-28 21:41:04,770 | INFO | best hypo: ▁et▁le▁sens▁de▁l'intérêt▁général▁qui▁impose▁que▁l'état▁conserve▁toute▁sa▁place

2026-01-28 21:41:04,772 | INFO | speech length: 64160
2026-01-28 21:41:04,817 | INFO | decoder input length: 99
2026-01-28 21:41:04,817 | INFO | max output length: 99
2026-01-28 21:41:04,817 | INFO | min output length: 9
2026-01-28 21:41:06,117 | INFO | end detected at 27
2026-01-28 21:41:06,118 | INFO |  -1.73 * 0.5 =  -0.86 for decoder
2026-01-28 21:41:06,118 | INFO |  -0.06 * 0.5 =  -0.03 for ctc
2026-01-28 21:41:06,118 | INFO | total log probability: -0.90
2026-01-28 21:41:06,118 | INFO | normalized log probability: -0.04
2026-01-28 21:41:06,118 | INFO | total number of ended hypotheses: 145
2026-01-28 21:41:06,119 | INFO | best hypo: ▁pour▁dire▁le▁droit▁le▁faire▁respecter▁avec▁autorité

2026-01-28 21:41:06,120 | INFO | speech length: 8640
2026-01-28 21:41:06,148 | INFO | decoder input length: 13
2026-01-28 21:41:06,148 | INFO | max output length: 13
2026-01-28 21:41:06,148 | INFO | min output length: 1
2026-01-28 21:41:06,576 | INFO | end detected at 11
2026-01-28 21:41:06,577 | INFO |  -7.02 * 0.5 =  -3.51 for decoder
2026-01-28 21:41:06,577 | INFO |  -1.94 * 0.5 =  -0.97 for ctc
2026-01-28 21:41:06,577 | INFO | total log probability: -4.48
2026-01-28 21:41:06,578 | INFO | normalized log probability: -0.90
2026-01-28 21:41:06,578 | INFO | total number of ended hypotheses: 170
2026-01-28 21:41:06,578 | INFO | best hypo: ▁avec▁juis

2026-01-28 21:41:06,580 | INFO | speech length: 19521
2026-01-28 21:41:06,638 | INFO | decoder input length: 30
2026-01-28 21:41:06,638 | INFO | max output length: 30
2026-01-28 21:41:06,638 | INFO | min output length: 3
2026-01-28 21:41:07,332 | INFO | end detected at 16
2026-01-28 21:41:07,334 | INFO |  -1.87 * 0.5 =  -0.94 for decoder
2026-01-28 21:41:07,334 | INFO |  -2.39 * 0.5 =  -1.19 for ctc
2026-01-28 21:41:07,334 | INFO | total log probability: -2.13
2026-01-28 21:41:07,334 | INFO | normalized log probability: -0.19
2026-01-28 21:41:07,334 | INFO | total number of ended hypotheses: 159
2026-01-28 21:41:07,334 | INFO | best hypo: ▁gardons▁ses▁exiges

2026-01-28 21:41:07,336 | INFO | speech length: 19520
2026-01-28 21:41:07,389 | INFO | decoder input length: 30
2026-01-28 21:41:07,390 | INFO | max output length: 30
2026-01-28 21:41:07,390 | INFO | min output length: 3
2026-01-28 21:41:08,486 | INFO | end detected at 16
2026-01-28 21:41:08,488 | INFO |  -1.04 * 0.5 =  -0.52 for decoder
2026-01-28 21:41:08,489 | INFO |  -0.24 * 0.5 =  -0.12 for ctc
2026-01-28 21:41:08,489 | INFO | total log probability: -0.64
2026-01-28 21:41:08,489 | INFO | normalized log probability: -0.05
2026-01-28 21:41:08,489 | INFO | total number of ended hypotheses: 132
2026-01-28 21:41:08,489 | INFO | best hypo: ▁gardons▁ces▁valeurs

2026-01-28 21:41:08,492 | INFO | speech length: 81600
2026-01-28 21:41:08,551 | INFO | decoder input length: 127
2026-01-28 21:41:08,552 | INFO | max output length: 127
2026-01-28 21:41:08,552 | INFO | min output length: 12
2026-01-28 21:41:12,160 | INFO | end detected at 36
2026-01-28 21:41:12,162 | INFO |  -2.32 * 0.5 =  -1.16 for decoder
2026-01-28 21:41:12,162 | INFO |  -0.06 * 0.5 =  -0.03 for ctc
2026-01-28 21:41:12,162 | INFO | total log probability: -1.19
2026-01-28 21:41:12,162 | INFO | normalized log probability: -0.04
2026-01-28 21:41:12,163 | INFO | total number of ended hypotheses: 157
2026-01-28 21:41:12,163 | INFO | best hypo: ▁en▁les▁faisant▁vivre▁nous▁serons▁plus▁forts▁pour▁aborder▁les▁temps▁qui▁viennent

2026-01-28 21:41:12,167 | INFO | speech length: 17440
2026-01-28 21:41:12,218 | INFO | decoder input length: 26
2026-01-28 21:41:12,218 | INFO | max output length: 26
2026-01-28 21:41:12,218 | INFO | min output length: 2
2026-01-28 21:41:12,771 | INFO | end detected at 12
2026-01-28 21:41:12,773 | INFO |  -0.72 * 0.5 =  -0.36 for decoder
2026-01-28 21:41:12,773 | INFO |  -0.12 * 0.5 =  -0.06 for ctc
2026-01-28 21:41:12,773 | INFO | total log probability: -0.42
2026-01-28 21:41:12,773 | INFO | normalized log probability: -0.05
2026-01-28 21:41:12,773 | INFO | total number of ended hypotheses: 142
2026-01-28 21:41:12,773 | INFO | best hypo: ▁la▁france▁chante

2026-01-28 21:41:12,775 | INFO | speech length: 26400
2026-01-28 21:41:12,831 | INFO | decoder input length: 40
2026-01-28 21:41:12,831 | INFO | max output length: 40
2026-01-28 21:41:12,831 | INFO | min output length: 4
2026-01-28 21:41:14,263 | INFO | end detected at 21
2026-01-28 21:41:14,265 | INFO |  -1.21 * 0.5 =  -0.61 for decoder
2026-01-28 21:41:14,265 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-28 21:41:14,265 | INFO | total log probability: -0.61
2026-01-28 21:41:14,265 | INFO | normalized log probability: -0.04
2026-01-28 21:41:14,265 | INFO | total number of ended hypotheses: 146
2026-01-28 21:41:14,266 | INFO | best hypo: ▁elle▁doit▁le▁faire▁au▁rythme▁du▁monde

2026-01-28 21:41:14,269 | INFO | speech length: 104960
2026-01-28 21:41:14,323 | INFO | decoder input length: 163
2026-01-28 21:41:14,323 | INFO | max output length: 163
2026-01-28 21:41:14,323 | INFO | min output length: 16
2026-01-28 21:41:17,324 | INFO | end detected at 47
2026-01-28 21:41:17,325 | INFO |  -3.23 * 0.5 =  -1.62 for decoder
2026-01-28 21:41:17,325 | INFO |  -0.91 * 0.5 =  -0.46 for ctc
2026-01-28 21:41:17,326 | INFO | total log probability: -2.07
2026-01-28 21:41:17,326 | INFO | normalized log probability: -0.05
2026-01-28 21:41:17,326 | INFO | total number of ended hypotheses: 148
2026-01-28 21:41:17,326 | INFO | best hypo: ▁en▁étant▁fidèle▁à▁son▁génie▁propre▁elle▁saura▁conjuguer▁le▁changement▁et▁la▁cohésion▁sociale

2026-01-28 21:41:17,329 | INFO | speech length: 92000
2026-01-28 21:41:17,376 | INFO | decoder input length: 143
2026-01-28 21:41:17,376 | INFO | max output length: 143
2026-01-28 21:41:17,376 | INFO | min output length: 14
2026-01-28 21:41:20,007 | INFO | end detected at 43
2026-01-28 21:41:20,008 | INFO |  -3.12 * 0.5 =  -1.56 for decoder
2026-01-28 21:41:20,008 | INFO |  -0.36 * 0.5 =  -0.18 for ctc
2026-01-28 21:41:20,008 | INFO | total log probability: -1.74
2026-01-28 21:41:20,008 | INFO | normalized log probability: -0.04
2026-01-28 21:41:20,008 | INFO | total number of ended hypotheses: 151
2026-01-28 21:41:20,009 | INFO | best hypo: ▁l'esprit▁d'initiative▁est▁la▁sécurité▁la▁modernité▁et▁le▁bien▁vivre▁ensemble

2026-01-28 21:41:20,012 | INFO | speech length: 20160
2026-01-28 21:41:20,051 | INFO | decoder input length: 31
2026-01-28 21:41:20,051 | INFO | max output length: 31
2026-01-28 21:41:20,052 | INFO | min output length: 3
2026-01-28 21:41:20,831 | INFO | end detected at 17
2026-01-28 21:41:20,833 | INFO |  -1.01 * 0.5 =  -0.51 for decoder
2026-01-28 21:41:20,834 | INFO |  -3.41 * 0.5 =  -1.71 for ctc
2026-01-28 21:41:20,834 | INFO | total log probability: -2.21
2026-01-28 21:41:20,834 | INFO | normalized log probability: -0.17
2026-01-28 21:41:20,834 | INFO | total number of ended hypotheses: 164
2026-01-28 21:41:20,834 | INFO | best hypo: ▁mes▁chers▁compatriotes

2026-01-28 21:41:20,836 | INFO | speech length: 106240
2026-01-28 21:41:20,877 | INFO | decoder input length: 165
2026-01-28 21:41:20,877 | INFO | max output length: 165
2026-01-28 21:41:20,877 | INFO | min output length: 16
2026-01-28 21:41:23,958 | INFO | end detected at 48
2026-01-28 21:41:23,960 | INFO |  -3.73 * 0.5 =  -1.87 for decoder
2026-01-28 21:41:23,960 | INFO |  -2.22 * 0.5 =  -1.11 for ctc
2026-01-28 21:41:23,960 | INFO | total log probability: -2.98
2026-01-28 21:41:23,960 | INFO | normalized log probability: -0.07
2026-01-28 21:41:23,960 | INFO | total number of ended hypotheses: 156
2026-01-28 21:41:23,961 | INFO | best hypo: ▁je▁mesure▁l'honneur▁et▁la▁responsabilité▁qui▁m'échouent▁de▁m'adresser▁à▁vous▁ce▁soir

2026-01-28 21:41:23,963 | INFO | speech length: 55040
2026-01-28 21:41:24,003 | INFO | decoder input length: 85
2026-01-28 21:41:24,004 | INFO | max output length: 85
2026-01-28 21:41:24,004 | INFO | min output length: 8
2026-01-28 21:41:25,297 | INFO | end detected at 24
2026-01-28 21:41:25,298 | INFO |  -1.36 * 0.5 =  -0.68 for decoder
2026-01-28 21:41:25,299 | INFO |  -0.07 * 0.5 =  -0.03 for ctc
2026-01-28 21:41:25,299 | INFO | total log probability: -0.71
2026-01-28 21:41:25,299 | INFO | normalized log probability: -0.04
2026-01-28 21:41:25,299 | INFO | total number of ended hypotheses: 151
2026-01-28 21:41:25,299 | INFO | best hypo: ▁alors▁que▁notre▁nation▁franchit▁le▁cap▁du▁siècle

2026-01-28 21:41:25,301 | INFO | speech length: 80480
2026-01-28 21:41:25,345 | INFO | decoder input length: 125
2026-01-28 21:41:25,345 | INFO | max output length: 125
2026-01-28 21:41:25,345 | INFO | min output length: 12
2026-01-28 21:41:27,804 | INFO | end detected at 42
2026-01-28 21:41:27,806 | INFO |  -5.91 * 0.5 =  -2.96 for decoder
2026-01-28 21:41:27,806 | INFO |  -2.80 * 0.5 =  -1.40 for ctc
2026-01-28 21:41:27,806 | INFO | total log probability: -4.36
2026-01-28 21:41:27,806 | INFO | normalized log probability: -0.12
2026-01-28 21:41:27,806 | INFO | total number of ended hypotheses: 189
2026-01-28 21:41:27,807 | INFO | best hypo: ▁la▁france▁a▁plus▁de▁mille▁ans▁riches▁de▁fièvres▁de▁passion▁d'enthousiasme

2026-01-28 21:41:27,810 | INFO | speech length: 88000
2026-01-28 21:41:27,857 | INFO | decoder input length: 137
2026-01-28 21:41:27,857 | INFO | max output length: 137
2026-01-28 21:41:27,857 | INFO | min output length: 13
2026-01-28 21:41:29,988 | INFO | end detected at 35
2026-01-28 21:41:29,989 | INFO |  -8.91 * 0.5 =  -4.45 for decoder
2026-01-28 21:41:29,989 | INFO |  -2.59 * 0.5 =  -1.30 for ctc
2026-01-28 21:41:29,989 | INFO | total log probability: -5.75
2026-01-28 21:41:29,989 | INFO | normalized log probability: -0.19
2026-01-28 21:41:29,989 | INFO | total number of ended hypotheses: 156
2026-01-28 21:41:29,990 | INFO | best hypo: ▁elle▁continue▁comme▁ière▁à▁ouvrir▁et▁à▁défricher▁les▁chemins▁du▁monde

2026-01-28 21:41:29,992 | INFO | speech length: 42560
2026-01-28 21:41:30,032 | INFO | decoder input length: 66
2026-01-28 21:41:30,032 | INFO | max output length: 66
2026-01-28 21:41:30,032 | INFO | min output length: 6
2026-01-28 21:41:31,090 | INFO | end detected at 21
2026-01-28 21:41:31,091 | INFO |  -1.27 * 0.5 =  -0.63 for decoder
2026-01-28 21:41:31,092 | INFO |  -0.12 * 0.5 =  -0.06 for ctc
2026-01-28 21:41:31,092 | INFO | total log probability: -0.70
2026-01-28 21:41:31,092 | INFO | normalized log probability: -0.04
2026-01-28 21:41:31,092 | INFO | total number of ended hypotheses: 143
2026-01-28 21:41:31,092 | INFO | best hypo: ▁le▁nouveau▁siècle▁est▁à▁inventer

2026-01-28 21:41:31,094 | INFO | speech length: 37120
2026-01-28 21:41:31,139 | INFO | decoder input length: 57
2026-01-28 21:41:31,139 | INFO | max output length: 57
2026-01-28 21:41:31,139 | INFO | min output length: 5
2026-01-28 21:41:32,031 | INFO | end detected at 18
2026-01-28 21:41:32,034 | INFO |  -1.05 * 0.5 =  -0.52 for decoder
2026-01-28 21:41:32,034 | INFO |  -1.05 * 0.5 =  -0.52 for ctc
2026-01-28 21:41:32,034 | INFO | total log probability: -1.05
2026-01-28 21:41:32,034 | INFO | normalized log probability: -0.08
2026-01-28 21:41:32,034 | INFO | total number of ended hypotheses: 154
2026-01-28 21:41:32,034 | INFO | best hypo: ▁plus▁fraternel▁plus▁volontaire

2026-01-28 21:41:32,037 | INFO | speech length: 41440
2026-01-28 21:41:32,091 | INFO | decoder input length: 64
2026-01-28 21:41:32,092 | INFO | max output length: 64
2026-01-28 21:41:32,092 | INFO | min output length: 6
2026-01-28 21:41:33,929 | INFO | end detected at 22
2026-01-28 21:41:33,932 | INFO |  -1.31 * 0.5 =  -0.65 for decoder
2026-01-28 21:41:33,932 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-28 21:41:33,932 | INFO | total log probability: -0.66
2026-01-28 21:41:33,932 | INFO | normalized log probability: -0.04
2026-01-28 21:41:33,932 | INFO | total number of ended hypotheses: 142
2026-01-28 21:41:33,932 | INFO | best hypo: ▁il▁aura▁les▁couleurs▁que▁nous▁lui▁donnerons

2026-01-28 21:41:33,947 | INFO | speech length: 48480
2026-01-28 21:41:34,037 | INFO | decoder input length: 75
2026-01-28 21:41:34,037 | INFO | max output length: 75
2026-01-28 21:41:34,037 | INFO | min output length: 7
2026-01-28 21:41:36,136 | INFO | end detected at 27
2026-01-28 21:41:36,139 | INFO |  -1.61 * 0.5 =  -0.81 for decoder
2026-01-28 21:41:36,139 | INFO |  -0.23 * 0.5 =  -0.11 for ctc
2026-01-28 21:41:36,139 | INFO | total log probability: -0.92
2026-01-28 21:41:36,139 | INFO | normalized log probability: -0.04
2026-01-28 21:41:36,139 | INFO | total number of ended hypotheses: 166
2026-01-28 21:41:36,140 | INFO | best hypo: ▁la▁france▁sera▁ce▁que▁nous▁voudrons▁qu'elle▁soit

2026-01-28 21:41:36,143 | INFO | speech length: 59840
2026-01-28 21:41:36,194 | INFO | decoder input length: 93
2026-01-28 21:41:36,194 | INFO | max output length: 93
2026-01-28 21:41:36,194 | INFO | min output length: 9
2026-01-28 21:41:37,250 | INFO | end detected at 22
2026-01-28 21:41:37,251 | INFO |  -1.22 * 0.5 =  -0.61 for decoder
2026-01-28 21:41:37,251 | INFO |  -0.03 * 0.5 =  -0.01 for ctc
2026-01-28 21:41:37,251 | INFO | total log probability: -0.62
2026-01-28 21:41:37,251 | INFO | normalized log probability: -0.03
2026-01-28 21:41:37,251 | INFO | total number of ended hypotheses: 144
2026-01-28 21:41:37,251 | INFO | best hypo: ▁une▁nation▁unie▁vivante▁solidaire▁ouverte

2026-01-28 21:41:37,253 | INFO | speech length: 31040
2026-01-28 21:41:37,290 | INFO | decoder input length: 48
2026-01-28 21:41:37,290 | INFO | max output length: 48
2026-01-28 21:41:37,290 | INFO | min output length: 4
2026-01-28 21:41:38,115 | INFO | end detected at 20
2026-01-28 21:41:38,117 | INFO |  -1.61 * 0.5 =  -0.80 for decoder
2026-01-28 21:41:38,117 | INFO |  -0.22 * 0.5 =  -0.11 for ctc
2026-01-28 21:41:38,117 | INFO | total log probability: -0.91
2026-01-28 21:41:38,117 | INFO | normalized log probability: -0.06
2026-01-28 21:41:38,117 | INFO | total number of ended hypotheses: 142
2026-01-28 21:41:38,117 | INFO | best hypo: ▁il▁n'accepte▁aucune▁fatalité

2026-01-28 21:41:38,119 | INFO | speech length: 72960
2026-01-28 21:41:38,162 | INFO | decoder input length: 113
2026-01-28 21:41:38,162 | INFO | max output length: 113
2026-01-28 21:41:38,162 | INFO | min output length: 11
2026-01-28 21:41:39,892 | INFO | end detected at 35
2026-01-28 21:41:39,893 | INFO |  -2.49 * 0.5 =  -1.25 for decoder
2026-01-28 21:41:39,893 | INFO |  -2.69 * 0.5 =  -1.34 for ctc
2026-01-28 21:41:39,893 | INFO | total log probability: -2.59
2026-01-28 21:41:39,893 | INFO | normalized log probability: -0.08
2026-01-28 21:41:39,893 | INFO | total number of ended hypotheses: 151
2026-01-28 21:41:39,894 | INFO | best hypo: ▁car▁dans▁un▁monde▁où▁rien▁n'est▁figé▁l'avenir▁dépend▁de▁nous

2026-01-28 21:41:39,896 | INFO | speech length: 132960
2026-01-28 21:41:39,957 | INFO | decoder input length: 207
2026-01-28 21:41:39,957 | INFO | max output length: 207
2026-01-28 21:41:39,957 | INFO | min output length: 20
2026-01-28 21:41:43,703 | INFO | end detected at 56
2026-01-28 21:41:43,705 | INFO |  -5.37 * 0.5 =  -2.68 for decoder
2026-01-28 21:41:43,705 | INFO |  -3.93 * 0.5 =  -1.96 for ctc
2026-01-28 21:41:43,705 | INFO | total log probability: -4.65
2026-01-28 21:41:43,705 | INFO | normalized log probability: -0.09
2026-01-28 21:41:43,705 | INFO | total number of ended hypotheses: 187
2026-01-28 21:41:43,706 | INFO | best hypo: ▁l'avenir▁dépend▁de▁notre▁capacité▁à▁construire▁à▁créer▁à▁réver▁ensemble▁les▁voies▁de▁l'aventure▁humaine

2026-01-28 21:41:43,708 | INFO | speech length: 24480
2026-01-28 21:41:43,744 | INFO | decoder input length: 37
2026-01-28 21:41:43,744 | INFO | max output length: 37
2026-01-28 21:41:43,744 | INFO | min output length: 3
2026-01-28 21:41:44,550 | INFO | end detected at 20
2026-01-28 21:41:44,551 | INFO |  -3.60 * 0.5 =  -1.80 for decoder
2026-01-28 21:41:44,551 | INFO |  -1.37 * 0.5 =  -0.68 for ctc
2026-01-28 21:41:44,551 | INFO | total log probability: -2.48
2026-01-28 21:41:44,551 | INFO | normalized log probability: -0.16
2026-01-28 21:41:44,551 | INFO | total number of ended hypotheses: 151
2026-01-28 21:41:44,552 | INFO | best hypo: ▁chacune▁est▁à▁chacun▁d'entre▁eux

2026-01-28 21:41:44,554 | INFO | speech length: 77120
2026-01-28 21:41:44,601 | INFO | decoder input length: 120
2026-01-28 21:41:44,601 | INFO | max output length: 120
2026-01-28 21:41:44,601 | INFO | min output length: 12
2026-01-28 21:41:46,214 | INFO | end detected at 30
2026-01-28 21:41:46,215 | INFO |  -2.20 * 0.5 =  -1.10 for decoder
2026-01-28 21:41:46,215 | INFO |  -1.05 * 0.5 =  -0.52 for ctc
2026-01-28 21:41:46,215 | INFO | total log probability: -1.62
2026-01-28 21:41:46,215 | INFO | normalized log probability: -0.06
2026-01-28 21:41:46,215 | INFO | total number of ended hypotheses: 152
2026-01-28 21:41:46,216 | INFO | best hypo: ▁françaises▁et▁français▁de▁métropole▁d'outre▁mer▁de▁l'étranger

2026-01-28 21:41:46,218 | INFO | speech length: 34240
2026-01-28 21:41:46,263 | INFO | decoder input length: 53
2026-01-28 21:41:46,263 | INFO | max output length: 53
2026-01-28 21:41:46,263 | INFO | min output length: 5
2026-01-28 21:41:47,207 | INFO | end detected at 19
2026-01-28 21:41:47,208 | INFO |  -0.89 * 0.5 =  -0.45 for decoder
2026-01-28 21:41:47,208 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-28 21:41:47,208 | INFO | total log probability: -0.45
2026-01-28 21:41:47,209 | INFO | normalized log probability: -0.03
2026-01-28 21:41:47,209 | INFO | total number of ended hypotheses: 135
2026-01-28 21:41:47,209 | INFO | best hypo: ▁je▁souhaite▁très▁chaleureuse

2026-01-28 21:41:47,211 | INFO | speech length: 50880
2026-01-28 21:41:47,258 | INFO | decoder input length: 79
2026-01-28 21:41:47,258 | INFO | max output length: 79
2026-01-28 21:41:47,258 | INFO | min output length: 7
2026-01-28 21:41:48,429 | INFO | end detected at 22
2026-01-28 21:41:48,431 | INFO |  -1.61 * 0.5 =  -0.81 for decoder
2026-01-28 21:41:48,432 | INFO |  -0.41 * 0.5 =  -0.21 for ctc
2026-01-28 21:41:48,432 | INFO | total log probability: -1.01
2026-01-28 21:41:48,432 | INFO | normalized log probability: -0.06
2026-01-28 21:41:48,432 | INFO | total number of ended hypotheses: 151
2026-01-28 21:41:48,432 | INFO | best hypo: ▁une▁bonne▁et▁une▁heureuse▁année▁demie

2026-01-28 21:41:48,456 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,457 | INFO | Chunk: 1 | WER=16.666667 | S=0 D=0 I=1
2026-01-28 21:41:48,457 | INFO | Chunk: 2 | WER=19.047619 | S=2 D=0 I=2
2026-01-28 21:41:48,458 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,458 | INFO | Chunk: 4 | WER=30.000000 | S=0 D=1 I=2
2026-01-28 21:41:48,459 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,459 | INFO | Chunk: 6 | WER=12.500000 | S=0 D=0 I=2
2026-01-28 21:41:48,460 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,460 | INFO | Chunk: 8 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 21:41:48,462 | INFO | Chunk: 9 | WER=13.793103 | S=1 D=0 I=3
2026-01-28 21:41:48,462 | INFO | Chunk: 10 | WER=9.090909 | S=1 D=0 I=0
2026-01-28 21:41:48,462 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,463 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,464 | INFO | Chunk: 13 | WER=27.272727 | S=1 D=0 I=2
2026-01-28 21:41:48,464 | INFO | Chunk: 14 | WER=12.500000 | S=1 D=0 I=0
2026-01-28 21:41:48,464 | INFO | Chunk: 15 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,465 | INFO | Chunk: 16 | WER=12.500000 | S=0 D=0 I=2
2026-01-28 21:41:48,465 | INFO | Chunk: 17 | WER=11.111111 | S=1 D=0 I=0
2026-01-28 21:41:48,466 | INFO | Chunk: 18 | WER=16.666667 | S=0 D=0 I=1
2026-01-28 21:41:48,466 | INFO | Chunk: 19 | WER=60.000000 | S=0 D=0 I=3
2026-01-28 21:41:48,466 | INFO | Chunk: 20 | WER=38.461538 | S=2 D=3 I=0
2026-01-28 21:41:48,467 | INFO | Chunk: 21 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,467 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,467 | INFO | Chunk: 23 | WER=25.000000 | S=0 D=1 I=1
2026-01-28 21:41:48,468 | INFO | Chunk: 24 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,468 | INFO | Chunk: 25 | WER=23.076923 | S=2 D=0 I=1
2026-01-28 21:41:48,469 | INFO | Chunk: 26 | WER=60.000000 | S=0 D=0 I=3
2026-01-28 21:41:48,469 | INFO | Chunk: 27 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,470 | INFO | Chunk: 28 | WER=28.571429 | S=1 D=1 I=2
2026-01-28 21:41:48,470 | INFO | Chunk: 29 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,470 | INFO | Chunk: 30 | WER=0.000000 | S=1 D=0 I=0
2026-01-28 21:41:48,471 | INFO | Chunk: 31 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,472 | INFO | Chunk: 32 | WER=12.500000 | S=1 D=0 I=2
2026-01-28 21:41:48,472 | INFO | Chunk: 33 | WER=20.000000 | S=0 D=0 I=1
2026-01-28 21:41:48,472 | INFO | Chunk: 34 | WER=300.000000 | S=0 D=0 I=3
2026-01-28 21:41:48,473 | INFO | Chunk: 35 | WER=6.250000 | S=1 D=0 I=0
2026-01-28 21:41:48,473 | INFO | Chunk: 36 | WER=16.666667 | S=1 D=0 I=0
2026-01-28 21:41:48,475 | INFO | Chunk: 37 | WER=9.090909 | S=1 D=0 I=2
2026-01-28 21:41:48,475 | INFO | Chunk: 38 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,475 | INFO | Chunk: 39 | WER=11.111111 | S=1 D=0 I=0
2026-01-28 21:41:48,476 | INFO | Chunk: 40 | WER=9.090909 | S=1 D=0 I=0
2026-01-28 21:41:48,476 | INFO | Chunk: 41 | WER=66.666667 | S=0 D=1 I=1
2026-01-28 21:41:48,476 | INFO | Chunk: 42 | WER=10.000000 | S=0 D=0 I=1
2026-01-28 21:41:48,477 | INFO | Chunk: 43 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,477 | INFO | Chunk: 44 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,478 | INFO | Chunk: 45 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,478 | INFO | Chunk: 46 | WER=33.333333 | S=0 D=0 I=1
2026-01-28 21:41:48,478 | INFO | Chunk: 47 | WER=33.333333 | S=1 D=0 I=1
2026-01-28 21:41:48,478 | INFO | Chunk: 48 | WER=50.000000 | S=1 D=0 I=1
2026-01-28 21:41:48,479 | INFO | Chunk: 49 | WER=6.666667 | S=0 D=0 I=1
2026-01-28 21:41:48,479 | INFO | Chunk: 50 | WER=100.000000 | S=1 D=0 I=1
2026-01-28 21:41:48,479 | INFO | Chunk: 51 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 21:41:48,480 | INFO | Chunk: 52 | WER=50.000000 | S=0 D=1 I=2
2026-01-28 21:41:48,480 | INFO | Chunk: 53 | WER=300.000000 | S=0 D=0 I=3
2026-01-28 21:41:48,480 | INFO | Chunk: 54 | WER=8.333333 | S=1 D=0 I=0
2026-01-28 21:41:48,481 | INFO | Chunk: 55 | WER=15.384615 | S=2 D=0 I=0
2026-01-28 21:41:48,481 | INFO | Chunk: 56 | WER=5.555556 | S=1 D=0 I=0
2026-01-28 21:41:48,482 | INFO | Chunk: 57 | WER=14.285714 | S=0 D=0 I=1
2026-01-28 21:41:48,482 | INFO | Chunk: 58 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,483 | INFO | Chunk: 59 | WER=15.384615 | S=0 D=0 I=2
2026-01-28 21:41:48,483 | INFO | Chunk: 60 | WER=40.000000 | S=1 D=0 I=1
2026-01-28 21:41:48,483 | INFO | Chunk: 61 | WER=66.666667 | S=0 D=1 I=1
2026-01-28 21:41:48,483 | INFO | Chunk: 62 | WER=25.000000 | S=0 D=0 I=2
2026-01-28 21:41:48,484 | INFO | Chunk: 63 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,484 | INFO | Chunk: 64 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 21:41:48,484 | INFO | Chunk: 65 | WER=66.666667 | S=0 D=1 I=1
2026-01-28 21:41:48,485 | INFO | Chunk: 66 | WER=19.047619 | S=3 D=0 I=1
2026-01-28 21:41:48,485 | INFO | Chunk: 67 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,485 | INFO | Chunk: 68 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,486 | INFO | Chunk: 69 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,486 | INFO | Chunk: 70 | WER=7.692308 | S=0 D=0 I=1
2026-01-28 21:41:48,487 | INFO | Chunk: 71 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,487 | INFO | Chunk: 72 | WER=40.000000 | S=2 D=0 I=0
2026-01-28 21:41:48,487 | INFO | Chunk: 73 | WER=20.000000 | S=0 D=0 I=2
2026-01-28 21:41:48,488 | INFO | Chunk: 74 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,488 | INFO | Chunk: 75 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,488 | INFO | Chunk: 76 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 21:41:48,489 | INFO | Chunk: 77 | WER=12.500000 | S=0 D=0 I=2
2026-01-28 21:41:48,489 | INFO | Chunk: 78 | WER=20.000000 | S=0 D=0 I=2
2026-01-28 21:41:48,490 | INFO | Chunk: 79 | WER=20.000000 | S=0 D=0 I=3
2026-01-28 21:41:48,491 | INFO | Chunk: 80 | WER=6.666667 | S=0 D=0 I=1
2026-01-28 21:41:48,491 | INFO | Chunk: 81 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,491 | INFO | Chunk: 82 | WER=30.000000 | S=0 D=2 I=1
2026-01-28 21:41:48,492 | INFO | Chunk: 83 | WER=50.000000 | S=1 D=0 I=2
2026-01-28 21:41:48,492 | INFO | Chunk: 84 | WER=37.500000 | S=1 D=0 I=2
2026-01-28 21:41:48,492 | INFO | Chunk: 85 | WER=20.000000 | S=0 D=1 I=0
2026-01-28 21:41:48,493 | INFO | Chunk: 86 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,493 | INFO | Chunk: 87 | WER=20.000000 | S=0 D=0 I=1
2026-01-28 21:41:48,493 | INFO | Chunk: 88 | WER=11.111111 | S=1 D=0 I=1
2026-01-28 21:41:48,494 | INFO | Chunk: 89 | WER=5.882353 | S=0 D=0 I=1
2026-01-28 21:41:48,495 | INFO | Chunk: 90 | WER=16.666667 | S=1 D=0 I=1
2026-01-28 21:41:48,495 | INFO | Chunk: 91 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,495 | INFO | Chunk: 92 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 21:41:48,495 | INFO | Chunk: 93 | WER=66.666667 | S=2 D=0 I=0
2026-01-28 21:41:48,496 | INFO | Chunk: 94 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,496 | INFO | Chunk: 95 | WER=50.000000 | S=0 D=0 I=3
2026-01-28 21:41:48,496 | INFO | Chunk: 96 | WER=250.000000 | S=2 D=0 I=3
2026-01-28 21:41:48,496 | INFO | Chunk: 97 | WER=66.666667 | S=1 D=1 I=0
2026-01-28 21:41:48,497 | INFO | Chunk: 98 | WER=6.666667 | S=0 D=0 I=1
2026-01-28 21:41:48,497 | INFO | Chunk: 99 | WER=12.500000 | S=0 D=0 I=1
2026-01-28 21:41:48,497 | INFO | Chunk: 100 | WER=200.000000 | S=1 D=0 I=1
2026-01-28 21:41:48,498 | INFO | Chunk: 101 | WER=100.000000 | S=1 D=0 I=1
2026-01-28 21:41:48,498 | INFO | Chunk: 102 | WER=50.000000 | S=0 D=0 I=1
2026-01-28 21:41:48,498 | INFO | Chunk: 103 | WER=7.692308 | S=0 D=0 I=1
2026-01-28 21:41:48,499 | INFO | Chunk: 104 | WER=50.000000 | S=0 D=0 I=1
2026-01-28 21:41:48,499 | INFO | Chunk: 105 | WER=33.333333 | S=0 D=0 I=2
2026-01-28 21:41:48,499 | INFO | Chunk: 106 | WER=6.250000 | S=1 D=0 I=0
2026-01-28 21:41:48,500 | INFO | Chunk: 107 | WER=15.384615 | S=1 D=0 I=1
2026-01-28 21:41:48,500 | INFO | Chunk: 108 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,501 | INFO | Chunk: 109 | WER=12.500000 | S=1 D=0 I=1
2026-01-28 21:41:48,501 | INFO | Chunk: 110 | WER=22.222222 | S=0 D=1 I=1
2026-01-28 21:41:48,502 | INFO | Chunk: 111 | WER=14.285714 | S=2 D=0 I=0
2026-01-28 21:41:48,502 | INFO | Chunk: 112 | WER=27.272727 | S=1 D=0 I=2
2026-01-28 21:41:48,502 | INFO | Chunk: 113 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,503 | INFO | Chunk: 114 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,503 | INFO | Chunk: 115 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,503 | INFO | Chunk: 116 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:41:48,504 | INFO | Chunk: 117 | WER=20.000000 | S=0 D=0 I=1
2026-01-28 21:41:48,504 | INFO | Chunk: 118 | WER=20.000000 | S=1 D=0 I=0
2026-01-28 21:41:48,504 | INFO | Chunk: 119 | WER=16.666667 | S=0 D=0 I=2
2026-01-28 21:41:48,505 | INFO | Chunk: 120 | WER=5.555556 | S=0 D=0 I=1
2026-01-28 21:41:48,505 | INFO | Chunk: 121 | WER=100.000000 | S=1 D=1 I=3
2026-01-28 21:41:48,506 | INFO | Chunk: 122 | WER=9.090909 | S=1 D=0 I=0
2026-01-28 21:41:48,506 | INFO | Chunk: 123 | WER=33.333333 | S=0 D=0 I=1
2026-01-28 21:41:48,506 | INFO | Chunk: 124 | WER=16.666667 | S=0 D=0 I=1
2026-01-28 21:41:49,569 | INFO | File: Rhap-M2004.wav | WER=14.273281 | S=53 D=9 I=102
2026-01-28 21:41:49,569 | INFO | ------------------------------
2026-01-28 21:41:49,569 | INFO | Conf cv Done!
2026-01-28 21:41:49,777 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 21:41:49,806 | INFO | Vocabulary size: 47
2026-01-28 21:41:50,722 | INFO | Gradient checkpoint layers: []
2026-01-28 21:41:52,629 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 21:41:52,642 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 21:41:52,643 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 21:41:52,643 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 21:41:52,648 | INFO | speech length: 15520
2026-01-28 21:41:52,697 | INFO | decoder input length: 23
2026-01-28 21:41:52,697 | INFO | max output length: 23
2026-01-28 21:41:52,697 | INFO | min output length: 2
2026-01-28 21:41:53,564 | INFO | adding <eos> in the last position in the loop
2026-01-28 21:41:53,572 | INFO | no hypothesis. Finish decoding.
2026-01-28 21:41:53,573 | INFO |  -2.84 * 0.5 =  -1.42 for decoder
2026-01-28 21:41:53,573 | INFO |  -2.30 * 0.5 =  -1.15 for ctc
2026-01-28 21:41:53,573 | INFO | total log probability: -2.57
2026-01-28 21:41:53,573 | INFO | normalized log probability: -0.10
2026-01-28 21:41:53,573 | INFO | total number of ended hypotheses: 53
2026-01-28 21:41:53,574 | INFO | best hypo: mes<space>chers<space>compatriotes<sos/eos>

2026-01-28 21:41:53,574 | WARNING | best hypo length: 23 == max output length: 23
2026-01-28 21:41:53,574 | WARNING | decoding may be stopped by the max output length limitation, please consider to increase the maxlenratio.
2026-01-28 21:41:53,575 | INFO | speech length: 44960
2026-01-28 21:41:53,615 | INFO | decoder input length: 69
2026-01-28 21:41:53,615 | INFO | max output length: 69
2026-01-28 21:41:53,615 | INFO | min output length: 6
2026-01-28 21:41:55,837 | INFO | end detected at 50
2026-01-28 21:41:55,839 | INFO |  -4.48 * 0.5 =  -2.24 for decoder
2026-01-28 21:41:55,839 | INFO |  -4.31 * 0.5 =  -2.15 for ctc
2026-01-28 21:41:55,839 | INFO | total log probability: -4.39
2026-01-28 21:41:55,839 | INFO | normalized log probability: -0.10
2026-01-28 21:41:55,839 | INFO | total number of ended hypotheses: 184
2026-01-28 21:41:55,840 | INFO | best hypo: je<space>voudrais<space>d'abord<space>euh<space>exprimer<space>ma<space>sadette

2026-01-28 21:41:55,843 | INFO | speech length: 107360
2026-01-28 21:41:55,883 | INFO | decoder input length: 167
2026-01-28 21:41:55,883 | INFO | max output length: 167
2026-01-28 21:41:55,883 | INFO | min output length: 16
2026-01-28 21:42:01,674 | INFO | end detected at 122
2026-01-28 21:42:01,675 | INFO |  -9.49 * 0.5 =  -4.75 for decoder
2026-01-28 21:42:01,676 | INFO |  -0.90 * 0.5 =  -0.45 for ctc
2026-01-28 21:42:01,676 | INFO | total log probability: -5.20
2026-01-28 21:42:01,676 | INFO | normalized log probability: -0.04
2026-01-28 21:42:01,676 | INFO | total number of ended hypotheses: 177
2026-01-28 21:42:01,677 | INFO | best hypo: à<space>toute<space>seule<space>et<space>à<space>tous<space>ceux<space>qui<space>vivent<space>ces<space>derniers<space>jours<space>de<space>mille<space>neuf<space>cent<space>quatre<space>vingt<space>dix<space>neuf<space>dans<space>l'épreuve

2026-01-28 21:42:01,680 | INFO | speech length: 48800
2026-01-28 21:42:01,718 | INFO | decoder input length: 75
2026-01-28 21:42:01,718 | INFO | max output length: 75
2026-01-28 21:42:01,718 | INFO | min output length: 7
2026-01-28 21:42:04,132 | INFO | end detected at 55
2026-01-28 21:42:04,133 | INFO |  -3.78 * 0.5 =  -1.89 for decoder
2026-01-28 21:42:04,134 | INFO |  -0.63 * 0.5 =  -0.31 for ctc
2026-01-28 21:42:04,134 | INFO | total log probability: -2.20
2026-01-28 21:42:04,134 | INFO | normalized log probability: -0.05
2026-01-28 21:42:04,134 | INFO | total number of ended hypotheses: 180
2026-01-28 21:42:04,134 | INFO | best hypo: je<space>pense<space>aux<space>nombreuses<space>victimes<space>de<space>la<space>tempête

2026-01-28 21:42:04,136 | INFO | speech length: 59520
2026-01-28 21:42:04,175 | INFO | decoder input length: 92
2026-01-28 21:42:04,175 | INFO | max output length: 92
2026-01-28 21:42:04,175 | INFO | min output length: 9
2026-01-28 21:42:07,060 | INFO | end detected at 72
2026-01-28 21:42:07,062 | INFO |  -5.25 * 0.5 =  -2.62 for decoder
2026-01-28 21:42:07,062 | INFO |  -0.03 * 0.5 =  -0.02 for ctc
2026-01-28 21:42:07,062 | INFO | total log probability: -2.64
2026-01-28 21:42:07,062 | INFO | normalized log probability: -0.04
2026-01-28 21:42:07,062 | INFO | total number of ended hypotheses: 174
2026-01-28 21:42:07,063 | INFO | best hypo: et<space>à<space>toutes<space>les<space>familles<space>endeuillées<space>dont<space>nous<space>partageons<space>la<space>paix

2026-01-28 21:42:07,065 | INFO | speech length: 75520
2026-01-28 21:42:07,111 | INFO | decoder input length: 117
2026-01-28 21:42:07,111 | INFO | max output length: 117
2026-01-28 21:42:07,111 | INFO | min output length: 11
2026-01-28 21:42:10,621 | INFO | end detected at 79
2026-01-28 21:42:10,623 | INFO |  -5.83 * 0.5 =  -2.91 for decoder
2026-01-28 21:42:10,623 | INFO |  -0.03 * 0.5 =  -0.01 for ctc
2026-01-28 21:42:10,623 | INFO | total log probability: -2.93
2026-01-28 21:42:10,623 | INFO | normalized log probability: -0.04
2026-01-28 21:42:10,623 | INFO | total number of ended hypotheses: 171
2026-01-28 21:42:10,624 | INFO | best hypo: je<space>pense<space>à<space>nos<space>concitoyens<space>cruellement<space>touchés<space>dans<space>leur<space>vie<space>quotidienne

2026-01-28 21:42:10,626 | INFO | speech length: 92160
2026-01-28 21:42:10,664 | INFO | decoder input length: 143
2026-01-28 21:42:10,664 | INFO | max output length: 143
2026-01-28 21:42:10,664 | INFO | min output length: 14
2026-01-28 21:42:15,612 | INFO | end detected at 100
2026-01-28 21:42:15,614 | INFO |  -7.51 * 0.5 =  -3.75 for decoder
2026-01-28 21:42:15,615 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-28 21:42:15,615 | INFO | total log probability: -3.76
2026-01-28 21:42:15,615 | INFO | normalized log probability: -0.04
2026-01-28 21:42:15,615 | INFO | total number of ended hypotheses: 178
2026-01-28 21:42:15,616 | INFO | best hypo: à<space>ceux<space>dont<space>les<space>biens<space>ont<space>été<space>détruits<space>à<space>ceux<space>qui<space>craignent<space>pour<space>leur<space>activité<space>et<space>leur<space>emploi

2026-01-28 21:42:15,620 | INFO | speech length: 122720
2026-01-28 21:42:15,667 | INFO | decoder input length: 191
2026-01-28 21:42:15,667 | INFO | max output length: 191
2026-01-28 21:42:15,667 | INFO | min output length: 19
2026-01-28 21:42:20,910 | INFO | end detected at 102
2026-01-28 21:42:20,912 | INFO |  -8.50 * 0.5 =  -4.25 for decoder
2026-01-28 21:42:20,912 | INFO |  -2.59 * 0.5 =  -1.29 for ctc
2026-01-28 21:42:20,912 | INFO | total log probability: -5.55
2026-01-28 21:42:20,912 | INFO | normalized log probability: -0.06
2026-01-28 21:42:20,912 | INFO | total number of ended hypotheses: 169
2026-01-28 21:42:20,914 | INFO | best hypo: à<space>ceux<space>qui<space>souffrent<space>de<space>voir<space>notre<space>patrimoine<space>notre<space>littoral<space>nos<space>forêts<space>nos<space>monuments<space>défigurés

2026-01-28 21:42:20,916 | INFO | speech length: 20800
2026-01-28 21:42:20,959 | INFO | decoder input length: 32
2026-01-28 21:42:20,960 | INFO | max output length: 32
2026-01-28 21:42:20,960 | INFO | min output length: 3
2026-01-28 21:42:22,014 | INFO | adding <eos> in the last position in the loop
2026-01-28 21:42:22,023 | INFO | no hypothesis. Finish decoding.
2026-01-28 21:42:22,025 | INFO |  -3.97 * 0.5 =  -1.99 for decoder
2026-01-28 21:42:22,025 | INFO |  -2.28 * 0.5 =  -1.14 for ctc
2026-01-28 21:42:22,025 | INFO | total log probability: -3.12
2026-01-28 21:42:22,025 | INFO | normalized log probability: -0.12
2026-01-28 21:42:22,025 | INFO | total number of ended hypotheses: 159
2026-01-28 21:42:22,025 | INFO | best hypo: je<space>vous<space>redit<space>mon<space>émotion

2026-01-28 21:42:22,027 | INFO | speech length: 217280
2026-01-28 21:42:22,071 | INFO | decoder input length: 339
2026-01-28 21:42:22,071 | INFO | max output length: 339
2026-01-28 21:42:22,071 | INFO | min output length: 33
2026-01-28 21:42:34,729 | INFO | end detected at 188
2026-01-28 21:42:34,730 | INFO | -14.99 * 0.5 =  -7.50 for decoder
2026-01-28 21:42:34,730 | INFO |  -2.57 * 0.5 =  -1.29 for ctc
2026-01-28 21:42:34,730 | INFO | total log probability: -8.78
2026-01-28 21:42:34,730 | INFO | normalized log probability: -0.05
2026-01-28 21:42:34,730 | INFO | total number of ended hypotheses: 167
2026-01-28 21:42:34,733 | INFO | best hypo: mais<space>aussi<space>ma<space>fierté<space>devant<space>l'exceptionnel<space>élan<space>de<space>solidarité<space>qui<space>anime<space>tant<space>de<space>bénévoles<space>et<space>d'associations<space>mobilisées<space>aux<space>côtés<space>des<space>services<space>publics<space>civils<space>et<space>militaires<space>et<space>des<space>élus

2026-01-28 21:42:34,735 | INFO | speech length: 91360
2026-01-28 21:42:34,779 | INFO | decoder input length: 142
2026-01-28 21:42:34,779 | INFO | max output length: 142
2026-01-28 21:42:34,779 | INFO | min output length: 14
2026-01-28 21:42:38,723 | INFO | end detected at 86
2026-01-28 21:42:38,725 | INFO |  -6.77 * 0.5 =  -3.39 for decoder
2026-01-28 21:42:38,726 | INFO |  -1.70 * 0.5 =  -0.85 for ctc
2026-01-28 21:42:38,726 | INFO | total log probability: -4.24
2026-01-28 21:42:38,726 | INFO | normalized log probability: -0.05
2026-01-28 21:42:38,726 | INFO | total number of ended hypotheses: 211
2026-01-28 21:42:38,727 | INFO | best hypo: en<space>ces<space>heures<space>difficiles<space>nous<space>ressentons<space>profondément<space>la<space>fragilité<space>des<space>choses

2026-01-28 21:42:38,730 | INFO | speech length: 56960
2026-01-28 21:42:38,774 | INFO | decoder input length: 88
2026-01-28 21:42:38,774 | INFO | max output length: 88
2026-01-28 21:42:38,775 | INFO | min output length: 8
2026-01-28 21:42:40,830 | INFO | end detected at 50
2026-01-28 21:42:40,831 | INFO |  -3.58 * 0.5 =  -1.79 for decoder
2026-01-28 21:42:40,831 | INFO |  -0.18 * 0.5 =  -0.09 for ctc
2026-01-28 21:42:40,831 | INFO | total log probability: -1.88
2026-01-28 21:42:40,831 | INFO | normalized log probability: -0.04
2026-01-28 21:42:40,832 | INFO | total number of ended hypotheses: 165
2026-01-28 21:42:40,832 | INFO | best hypo: la<space>précarité<space>de<space>ce<space>qui<space>nous<space>semblait<space>acquis

2026-01-28 21:42:40,834 | INFO | speech length: 140800
2026-01-28 21:42:40,875 | INFO | decoder input length: 219
2026-01-28 21:42:40,875 | INFO | max output length: 219
2026-01-28 21:42:40,875 | INFO | min output length: 21
2026-01-28 21:42:48,408 | INFO | end detected at 142
2026-01-28 21:42:48,410 | INFO | -10.86 * 0.5 =  -5.43 for decoder
2026-01-28 21:42:48,410 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-28 21:42:48,410 | INFO | total log probability: -5.43
2026-01-28 21:42:48,411 | INFO | normalized log probability: -0.04
2026-01-28 21:42:48,411 | INFO | total number of ended hypotheses: 174
2026-01-28 21:42:48,413 | INFO | best hypo: nous<space>voyons<space>combien<space>tout<space>peut<space>être<space>parfois<space>remis<space>en<space>cause<space>du<space>fait<space>de<space>l'inconscience<space>des<space>hommes<space>ou<space>du<space>déchaînement<space>des<space>éléments<space>naturels

2026-01-28 21:42:48,416 | INFO | speech length: 84320
2026-01-28 21:42:48,468 | INFO | decoder input length: 131
2026-01-28 21:42:48,468 | INFO | max output length: 131
2026-01-28 21:42:48,468 | INFO | min output length: 13
2026-01-28 21:42:51,952 | INFO | end detected at 77
2026-01-28 21:42:51,954 | INFO |  -6.62 * 0.5 =  -3.31 for decoder
2026-01-28 21:42:51,954 | INFO |  -5.11 * 0.5 =  -2.55 for ctc
2026-01-28 21:42:51,955 | INFO | total log probability: -5.86
2026-01-28 21:42:51,955 | INFO | normalized log probability: -0.08
2026-01-28 21:42:51,955 | INFO | total number of ended hypotheses: 179
2026-01-28 21:42:51,956 | INFO | best hypo: nous<space>mesurons<space>aussi<space>l'importance<space>du<space>rôle<space>de<space>l'état<space>dans<space>notre<space>société

2026-01-28 21:42:51,959 | INFO | speech length: 65600
2026-01-28 21:42:52,097 | INFO | decoder input length: 102
2026-01-28 21:42:52,097 | INFO | max output length: 102
2026-01-28 21:42:52,097 | INFO | min output length: 10
2026-01-28 21:42:55,044 | INFO | end detected at 65
2026-01-28 21:42:55,046 | INFO |  -4.89 * 0.5 =  -2.44 for decoder
2026-01-28 21:42:55,046 | INFO |  -0.32 * 0.5 =  -0.16 for ctc
2026-01-28 21:42:55,046 | INFO | total log probability: -2.60
2026-01-28 21:42:55,046 | INFO | normalized log probability: -0.04
2026-01-28 21:42:55,046 | INFO | total number of ended hypotheses: 173
2026-01-28 21:42:55,047 | INFO | best hypo: un<space>état<space>sur<space>lequel<space>pèsent<space>des<space>responsabilités<space>essentielles

2026-01-28 21:42:55,049 | INFO | speech length: 57600
2026-01-28 21:42:55,110 | INFO | decoder input length: 89
2026-01-28 21:42:55,110 | INFO | max output length: 89
2026-01-28 21:42:55,110 | INFO | min output length: 8
2026-01-28 21:42:57,190 | INFO | end detected at 50
2026-01-28 21:42:57,191 | INFO |  -3.59 * 0.5 =  -1.80 for decoder
2026-01-28 21:42:57,191 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-28 21:42:57,191 | INFO | total log probability: -1.80
2026-01-28 21:42:57,191 | INFO | normalized log probability: -0.04
2026-01-28 21:42:57,191 | INFO | total number of ended hypotheses: 160
2026-01-28 21:42:57,192 | INFO | best hypo: le<space>service<space>public<space>la<space>sécurité<space>la<space>solidarité

2026-01-28 21:42:57,194 | INFO | speech length: 122080
2026-01-28 21:42:57,235 | INFO | decoder input length: 190
2026-01-28 21:42:57,235 | INFO | max output length: 190
2026-01-28 21:42:57,235 | INFO | min output length: 19
2026-01-28 21:43:02,989 | INFO | end detected at 105
2026-01-28 21:43:02,990 | INFO |  -7.96 * 0.5 =  -3.98 for decoder
2026-01-28 21:43:02,990 | INFO |  -0.03 * 0.5 =  -0.02 for ctc
2026-01-28 21:43:02,990 | INFO | total log probability: -4.00
2026-01-28 21:43:02,990 | INFO | normalized log probability: -0.04
2026-01-28 21:43:02,990 | INFO | total number of ended hypotheses: 177
2026-01-28 21:43:02,992 | INFO | best hypo: un<space>état<space>auquel<space>il<space>appartient<space>de<space>prévoir<space>de<space>faire<space>face<space>d'assurer<space>la<space>coordination<space>des<space>moyens<space>du<space>pays

2026-01-28 21:43:02,994 | INFO | speech length: 70400
2026-01-28 21:43:03,044 | INFO | decoder input length: 109
2026-01-28 21:43:03,045 | INFO | max output length: 109
2026-01-28 21:43:03,045 | INFO | min output length: 10
2026-01-28 21:43:05,654 | INFO | end detected at 58
2026-01-28 21:43:05,655 | INFO |  -4.15 * 0.5 =  -2.08 for decoder
2026-01-28 21:43:05,655 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 21:43:05,655 | INFO | total log probability: -2.08
2026-01-28 21:43:05,656 | INFO | normalized log probability: -0.04
2026-01-28 21:43:05,656 | INFO | total number of ended hypotheses: 172
2026-01-28 21:43:05,656 | INFO | best hypo: nous<space>mesurons<space>surtout<space>le<space>prix<space>de<space>l'aide<space>fraternelle

2026-01-28 21:43:05,658 | INFO | speech length: 45120
2026-01-28 21:43:05,697 | INFO | decoder input length: 70
2026-01-28 21:43:05,697 | INFO | max output length: 70
2026-01-28 21:43:05,697 | INFO | min output length: 7
2026-01-28 21:43:07,928 | INFO | end detected at 44
2026-01-28 21:43:07,930 | INFO |  -3.11 * 0.5 =  -1.55 for decoder
2026-01-28 21:43:07,930 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-28 21:43:07,930 | INFO | total log probability: -1.57
2026-01-28 21:43:07,930 | INFO | normalized log probability: -0.04
2026-01-28 21:43:07,930 | INFO | total number of ended hypotheses: 166
2026-01-28 21:43:07,931 | INFO | best hypo: du<space>soutien<space>spontané<space>de<space>la<space>main<space>tendue

2026-01-28 21:43:07,935 | INFO | speech length: 35200
2026-01-28 21:43:07,992 | INFO | decoder input length: 54
2026-01-28 21:43:07,992 | INFO | max output length: 54
2026-01-28 21:43:07,994 | INFO | min output length: 5
2026-01-28 21:43:09,583 | INFO | end detected at 42
2026-01-28 21:43:09,584 | INFO |  -2.98 * 0.5 =  -1.49 for decoder
2026-01-28 21:43:09,584 | INFO |  -0.10 * 0.5 =  -0.05 for ctc
2026-01-28 21:43:09,584 | INFO | total log probability: -1.54
2026-01-28 21:43:09,584 | INFO | normalized log probability: -0.04
2026-01-28 21:43:09,584 | INFO | total number of ended hypotheses: 156
2026-01-28 21:43:09,584 | INFO | best hypo: qui<space>sont<space>le<space>ciment<space>même<space>de<space>la<space>nation

2026-01-28 21:43:09,586 | INFO | speech length: 54720
2026-01-28 21:43:09,630 | INFO | decoder input length: 85
2026-01-28 21:43:09,630 | INFO | max output length: 85
2026-01-28 21:43:09,630 | INFO | min output length: 8
2026-01-28 21:43:12,235 | INFO | end detected at 66
2026-01-28 21:43:12,237 | INFO |  -4.81 * 0.5 =  -2.40 for decoder
2026-01-28 21:43:12,237 | INFO |  -0.22 * 0.5 =  -0.11 for ctc
2026-01-28 21:43:12,237 | INFO | total log probability: -2.51
2026-01-28 21:43:12,237 | INFO | normalized log probability: -0.04
2026-01-28 21:43:12,237 | INFO | total number of ended hypotheses: 179
2026-01-28 21:43:12,238 | INFO | best hypo: au<space>moment<space>où<space>où<space>nous<space>touchons<space>aux<space>portes<space>de<space>l'an<space>deux<space>mille

2026-01-28 21:43:12,240 | INFO | speech length: 83200
2026-01-28 21:43:12,279 | INFO | decoder input length: 129
2026-01-28 21:43:12,279 | INFO | max output length: 129
2026-01-28 21:43:12,279 | INFO | min output length: 12
2026-01-28 21:43:15,579 | INFO | end detected at 69
2026-01-28 21:43:15,580 | INFO |  -5.08 * 0.5 =  -2.54 for decoder
2026-01-28 21:43:15,581 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-28 21:43:15,581 | INFO | total log probability: -2.55
2026-01-28 21:43:15,581 | INFO | normalized log probability: -0.04
2026-01-28 21:43:15,581 | INFO | total number of ended hypotheses: 168
2026-01-28 21:43:15,582 | INFO | best hypo: rien<space>n'est<space>décidément<space>plus<space>moderne<space>plus<space>nécessaire<space>plus<space>solide

2026-01-28 21:43:15,584 | INFO | speech length: 56000
2026-01-28 21:43:15,624 | INFO | decoder input length: 87
2026-01-28 21:43:15,625 | INFO | max output length: 87
2026-01-28 21:43:15,625 | INFO | min output length: 8
2026-01-28 21:43:17,944 | INFO | end detected at 58
2026-01-28 21:43:17,946 | INFO |  -4.19 * 0.5 =  -2.09 for decoder
2026-01-28 21:43:17,946 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 21:43:17,946 | INFO | total log probability: -2.09
2026-01-28 21:43:17,946 | INFO | normalized log probability: -0.04
2026-01-28 21:43:17,946 | INFO | total number of ended hypotheses: 174
2026-01-28 21:43:17,947 | INFO | best hypo: que<space>le<space>sentiment<space>d'appartenir<space>à<space>une<space>même<space>communauté

2026-01-28 21:43:17,949 | INFO | speech length: 50720
2026-01-28 21:43:17,987 | INFO | decoder input length: 78
2026-01-28 21:43:17,987 | INFO | max output length: 78
2026-01-28 21:43:17,987 | INFO | min output length: 7
2026-01-28 21:43:20,142 | INFO | end detected at 48
2026-01-28 21:43:20,143 | INFO |  -3.85 * 0.5 =  -1.93 for decoder
2026-01-28 21:43:20,144 | INFO |  -1.97 * 0.5 =  -0.99 for ctc
2026-01-28 21:43:20,144 | INFO | total log probability: -2.91
2026-01-28 21:43:20,144 | INFO | normalized log probability: -0.07
2026-01-28 21:43:20,144 | INFO | total number of ended hypotheses: 164
2026-01-28 21:43:20,144 | INFO | best hypo: et<space>d'être<space>responsables<space>les<space>uns<space>des<space>autres

2026-01-28 21:43:20,146 | INFO | speech length: 98560
2026-01-28 21:43:20,183 | INFO | decoder input length: 153
2026-01-28 21:43:20,183 | INFO | max output length: 153
2026-01-28 21:43:20,183 | INFO | min output length: 15
2026-01-28 21:43:23,580 | INFO | end detected at 69
2026-01-28 21:43:23,582 | INFO |  -6.28 * 0.5 =  -3.14 for decoder
2026-01-28 21:43:23,582 | INFO |  -1.28 * 0.5 =  -0.64 for ctc
2026-01-28 21:43:23,582 | INFO | total log probability: -3.78
2026-01-28 21:43:23,583 | INFO | normalized log probability: -0.06
2026-01-28 21:43:23,583 | INFO | total number of ended hypotheses: 192
2026-01-28 21:43:23,584 | INFO | best hypo: la<space>france<space>blessée<space>veut<space>se<space>retrouver<space>rassembler<space>et<space>fraternelle

2026-01-28 21:43:23,586 | INFO | speech length: 95040
2026-01-28 21:43:23,623 | INFO | decoder input length: 148
2026-01-28 21:43:23,623 | INFO | max output length: 148
2026-01-28 21:43:23,623 | INFO | min output length: 14
2026-01-28 21:43:27,982 | INFO | end detected at 88
2026-01-28 21:43:27,983 | INFO |  -6.61 * 0.5 =  -3.30 for decoder
2026-01-28 21:43:27,983 | INFO |  -0.52 * 0.5 =  -0.26 for ctc
2026-01-28 21:43:27,983 | INFO | total log probability: -3.56
2026-01-28 21:43:27,983 | INFO | normalized log probability: -0.04
2026-01-28 21:43:27,984 | INFO | total number of ended hypotheses: 169
2026-01-28 21:43:27,985 | INFO | best hypo: parce<space>que<space>nos<space>compatriotes<space>ont<space>toujours<space>su<space>dans<space>l'épreuve<space>faire<space>parler<space>leur<space>coeur

2026-01-28 21:43:27,987 | INFO | speech length: 56960
2026-01-28 21:43:28,024 | INFO | decoder input length: 88
2026-01-28 21:43:28,024 | INFO | max output length: 88
2026-01-28 21:43:28,024 | INFO | min output length: 8
2026-01-28 21:43:30,031 | INFO | end detected at 49
2026-01-28 21:43:30,032 | INFO |  -3.45 * 0.5 =  -1.72 for decoder
2026-01-28 21:43:30,033 | INFO |  -3.25 * 0.5 =  -1.62 for ctc
2026-01-28 21:43:30,033 | INFO | total log probability: -3.35
2026-01-28 21:43:30,033 | INFO | normalized log probability: -0.08
2026-01-28 21:43:30,033 | INFO | total number of ended hypotheses: 176
2026-01-28 21:43:30,033 | INFO | best hypo: je<space>voudrais<space>dire<space>merci<space>à<space>tous<space>les<space>français

2026-01-28 21:43:30,035 | INFO | speech length: 83680
2026-01-28 21:43:30,071 | INFO | decoder input length: 130
2026-01-28 21:43:30,071 | INFO | max output length: 130
2026-01-28 21:43:30,071 | INFO | min output length: 13
2026-01-28 21:43:33,117 | INFO | end detected at 67
2026-01-28 21:43:33,119 | INFO |  -5.20 * 0.5 =  -2.60 for decoder
2026-01-28 21:43:33,119 | INFO |  -0.69 * 0.5 =  -0.35 for ctc
2026-01-28 21:43:33,119 | INFO | total log probability: -2.94
2026-01-28 21:43:33,119 | INFO | normalized log probability: -0.05
2026-01-28 21:43:33,119 | INFO | total number of ended hypotheses: 174
2026-01-28 21:43:33,120 | INFO | best hypo: ce<space>soir<space>euh<space>nous<space>vivons<space>ensemble<space>un<space>moment<space>fort<space>et<space>singulier

2026-01-28 21:43:33,122 | INFO | speech length: 92160
2026-01-28 21:43:33,168 | INFO | decoder input length: 143
2026-01-28 21:43:33,169 | INFO | max output length: 143
2026-01-28 21:43:33,169 | INFO | min output length: 14
2026-01-28 21:43:37,431 | INFO | end detected at 92
2026-01-28 21:43:37,432 | INFO |  -6.81 * 0.5 =  -3.41 for decoder
2026-01-28 21:43:37,433 | INFO |  -0.03 * 0.5 =  -0.02 for ctc
2026-01-28 21:43:37,433 | INFO | total log probability: -3.42
2026-01-28 21:43:37,433 | INFO | normalized log probability: -0.04
2026-01-28 21:43:37,433 | INFO | total number of ended hypotheses: 166
2026-01-28 21:43:37,434 | INFO | best hypo: ce<space>qui<space>paraissait<space>très<space>lointain<space>et<space>qui<space>a<space>longtemps<space>symbolisé<space>le<space>futur<space>l'an<space>deux<space>mille

2026-01-28 21:43:37,436 | INFO | speech length: 29760
2026-01-28 21:43:37,472 | INFO | decoder input length: 46
2026-01-28 21:43:37,472 | INFO | max output length: 46
2026-01-28 21:43:37,472 | INFO | min output length: 4
2026-01-28 21:43:38,685 | INFO | end detected at 33
2026-01-28 21:43:38,687 | INFO |  -3.05 * 0.5 =  -1.52 for decoder
2026-01-28 21:43:38,687 | INFO |  -0.62 * 0.5 =  -0.31 for ctc
2026-01-28 21:43:38,687 | INFO | total log probability: -1.84
2026-01-28 21:43:38,687 | INFO | normalized log probability: -0.07
2026-01-28 21:43:38,687 | INFO | total number of ended hypotheses: 198
2026-01-28 21:43:38,688 | INFO | best hypo: est<space>devenu<space>contemporain

2026-01-28 21:43:38,690 | INFO | speech length: 9760
2026-01-28 21:43:38,717 | INFO | decoder input length: 14
2026-01-28 21:43:38,717 | INFO | max output length: 14
2026-01-28 21:43:38,717 | INFO | min output length: 1
2026-01-28 21:43:39,170 | INFO | adding <eos> in the last position in the loop
2026-01-28 21:43:39,179 | INFO | no hypothesis. Finish decoding.
2026-01-28 21:43:39,180 | INFO |  -1.63 * 0.5 =  -0.82 for decoder
2026-01-28 21:43:39,180 | INFO |  -0.64 * 0.5 =  -0.32 for ctc
2026-01-28 21:43:39,180 | INFO | total log probability: -1.13
2026-01-28 21:43:39,180 | INFO | normalized log probability: -0.11
2026-01-28 21:43:39,180 | INFO | total number of ended hypotheses: 131
2026-01-28 21:43:39,180 | INFO | best hypo: immédiat

2026-01-28 21:43:39,182 | INFO | speech length: 106240
2026-01-28 21:43:39,223 | INFO | decoder input length: 165
2026-01-28 21:43:39,223 | INFO | max output length: 165
2026-01-28 21:43:39,223 | INFO | min output length: 16
2026-01-28 21:43:44,509 | INFO | end detected at 112
2026-01-28 21:43:44,510 | INFO | -11.58 * 0.5 =  -5.79 for decoder
2026-01-28 21:43:44,510 | INFO |  -1.11 * 0.5 =  -0.55 for ctc
2026-01-28 21:43:44,510 | INFO | total log probability: -6.34
2026-01-28 21:43:44,510 | INFO | normalized log probability: -0.06
2026-01-28 21:43:44,511 | INFO | total number of ended hypotheses: 191
2026-01-28 21:43:44,512 | INFO | best hypo: je<space>suis<space>sûr<space>que<space>beaucoup<space>d'entre<space>vous<space>vont<space>vivre<space>ces<space>instants<space>avec<space>un<space>peu<space>des<space>motions<space>un<space>peu<space>d'étonnement

2026-01-28 21:43:44,514 | INFO | speech length: 164320
2026-01-28 21:43:44,555 | INFO | decoder input length: 256
2026-01-28 21:43:44,555 | INFO | max output length: 256
2026-01-28 21:43:44,555 | INFO | min output length: 25
2026-01-28 21:43:53,340 | INFO | end detected at 154
2026-01-28 21:43:53,341 | INFO | -11.94 * 0.5 =  -5.97 for decoder
2026-01-28 21:43:53,341 | INFO |  -7.73 * 0.5 =  -3.87 for ctc
2026-01-28 21:43:53,341 | INFO | total log probability: -9.84
2026-01-28 21:43:53,342 | INFO | normalized log probability: -0.07
2026-01-28 21:43:53,342 | INFO | total number of ended hypotheses: 160
2026-01-28 21:43:53,344 | INFO | best hypo: une<space>certaine<space>appréhension<space>parfois<space>née<space>du<space>sentiment<space>que<space>s'achève<space>une<space>époque<space>dont<space>on<space>possédait<space>les<space>clés<space>dont<space>on<space>maîtrisait<space>les<space>règles<space>et<space>les<space>habitudes

2026-01-28 21:43:53,346 | INFO | speech length: 30560
2026-01-28 21:43:53,384 | INFO | decoder input length: 47
2026-01-28 21:43:53,384 | INFO | max output length: 47
2026-01-28 21:43:53,384 | INFO | min output length: 4
2026-01-28 21:43:54,907 | INFO | end detected at 43
2026-01-28 21:43:54,908 | INFO |  -3.10 * 0.5 =  -1.55 for decoder
2026-01-28 21:43:54,908 | INFO |  -1.85 * 0.5 =  -0.92 for ctc
2026-01-28 21:43:54,908 | INFO | total log probability: -2.47
2026-01-28 21:43:54,908 | INFO | normalized log probability: -0.07
2026-01-28 21:43:54,908 | INFO | total number of ended hypotheses: 181
2026-01-28 21:43:54,909 | INFO | best hypo: je<space>comprends<space>ces<space>mouvements<space>de<space>l'âme

2026-01-28 21:43:54,911 | INFO | speech length: 15040
2026-01-28 21:43:54,957 | INFO | decoder input length: 23
2026-01-28 21:43:54,957 | INFO | max output length: 23
2026-01-28 21:43:54,957 | INFO | min output length: 2
2026-01-28 21:43:56,027 | INFO | adding <eos> in the last position in the loop
2026-01-28 21:43:56,035 | INFO | no hypothesis. Finish decoding.
2026-01-28 21:43:56,036 | INFO |  -6.73 * 0.5 =  -3.37 for decoder
2026-01-28 21:43:56,036 | INFO |  -2.46 * 0.5 =  -1.23 for ctc
2026-01-28 21:43:56,036 | INFO | total log probability: -4.60
2026-01-28 21:43:56,036 | INFO | normalized log probability: -0.18
2026-01-28 21:43:56,036 | INFO | total number of ended hypotheses: 57
2026-01-28 21:43:56,037 | INFO | best hypo: pourtant<space>j'ai<space>confirme<sos/eos>

2026-01-28 21:43:56,037 | WARNING | best hypo length: 23 == max output length: 23
2026-01-28 21:43:56,037 | WARNING | decoding may be stopped by the max output length limitation, please consider to increase the maxlenratio.
2026-01-28 21:43:56,038 | INFO | speech length: 99200
2026-01-28 21:43:56,078 | INFO | decoder input length: 154
2026-01-28 21:43:56,078 | INFO | max output length: 154
2026-01-28 21:43:56,078 | INFO | min output length: 15
2026-01-28 21:44:01,168 | INFO | end detected at 93
2026-01-28 21:44:01,170 | INFO |  -7.85 * 0.5 =  -3.92 for decoder
2026-01-28 21:44:01,170 | INFO |  -1.85 * 0.5 =  -0.92 for ctc
2026-01-28 21:44:01,170 | INFO | total log probability: -4.85
2026-01-28 21:44:01,170 | INFO | normalized log probability: -0.06
2026-01-28 21:44:01,170 | INFO | total number of ended hypotheses: 164
2026-01-28 21:44:01,172 | INFO | best hypo: la<space>france<space>franchir<space>les<space>obstacles<space>comme<space>elle<space>l'a<space>toujours<space>fait<space>au<space>long<space>de<space>son<space>histoire

2026-01-28 21:44:01,174 | INFO | speech length: 31840
2026-01-28 21:44:01,214 | INFO | decoder input length: 49
2026-01-28 21:44:01,214 | INFO | max output length: 49
2026-01-28 21:44:01,214 | INFO | min output length: 4
2026-01-28 21:44:02,935 | INFO | end detected at 41
2026-01-28 21:44:02,938 | INFO |  -3.72 * 0.5 =  -1.86 for decoder
2026-01-28 21:44:02,938 | INFO |  -6.51 * 0.5 =  -3.25 for ctc
2026-01-28 21:44:02,938 | INFO | total log probability: -5.11
2026-01-28 21:44:02,938 | INFO | normalized log probability: -0.16
2026-01-28 21:44:02,938 | INFO | total number of ended hypotheses: 219
2026-01-28 21:44:02,939 | INFO | best hypo: pour<space>quoi<space>qu'elle<space>soit<space>fidèle

2026-01-28 21:44:02,942 | INFO | speech length: 220640
2026-01-28 21:44:02,982 | INFO | decoder input length: 344
2026-01-28 21:44:02,982 | INFO | max output length: 344
2026-01-28 21:44:02,982 | INFO | min output length: 34
2026-01-28 21:44:15,901 | INFO | end detected at 190
2026-01-28 21:44:15,903 | INFO | -18.86 * 0.5 =  -9.43 for decoder
2026-01-28 21:44:15,903 | INFO |  -0.21 * 0.5 =  -0.10 for ctc
2026-01-28 21:44:15,903 | INFO | total log probability: -9.53
2026-01-28 21:44:15,903 | INFO | normalized log probability: -0.05
2026-01-28 21:44:15,904 | INFO | total number of ended hypotheses: 164
2026-01-28 21:44:15,907 | INFO | best hypo: même<space>si<space>le<space>passé<space>est<space>bien<space>présent<space>dans<space>notre<space>mémoire<space>je<space>ne<space>m'attarderai<space>pas<space>sur<space>le<space>siècle<space>qui<space>s'achève<space>siècle<space>de<space>progrès<space>sans<space>précédent<space>pour<space>la<space>santé<space>l'éducation<space>les<space>conditions<space>de<space>vie

2026-01-28 21:44:15,910 | INFO | speech length: 81600
2026-01-28 21:44:15,957 | INFO | decoder input length: 127
2026-01-28 21:44:15,957 | INFO | max output length: 127
2026-01-28 21:44:15,957 | INFO | min output length: 12
2026-01-28 21:44:20,087 | INFO | end detected at 85
2026-01-28 21:44:20,089 | INFO |  -6.31 * 0.5 =  -3.16 for decoder
2026-01-28 21:44:20,089 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-28 21:44:20,089 | INFO | total log probability: -3.17
2026-01-28 21:44:20,089 | INFO | normalized log probability: -0.04
2026-01-28 21:44:20,089 | INFO | total number of ended hypotheses: 183
2026-01-28 21:44:20,090 | INFO | best hypo: pour<space>les<space>libertés<space>la<space>vie<space>démocratique<space>la<space>situation<space>des<space>femmes<space>les<space>solidarités

2026-01-28 21:44:20,092 | INFO | speech length: 71360
2026-01-28 21:44:20,129 | INFO | decoder input length: 111
2026-01-28 21:44:20,129 | INFO | max output length: 111
2026-01-28 21:44:20,129 | INFO | min output length: 11
2026-01-28 21:44:23,081 | INFO | end detected at 68
2026-01-28 21:44:23,083 | INFO |  -6.33 * 0.5 =  -3.17 for decoder
2026-01-28 21:44:23,083 | INFO |  -8.26 * 0.5 =  -4.13 for ctc
2026-01-28 21:44:23,083 | INFO | total log probability: -7.29
2026-01-28 21:44:23,083 | INFO | normalized log probability: -0.12
2026-01-28 21:44:23,083 | INFO | total number of ended hypotheses: 209
2026-01-28 21:44:23,084 | INFO | best hypo: mais<space>aussi<space>euh<space>siècle<space>d'horreur<space>de<space>tragédie<space>de<space>convulsion

2026-01-28 21:44:23,087 | INFO | speech length: 83520
2026-01-28 21:44:23,131 | INFO | decoder input length: 130
2026-01-28 21:44:23,131 | INFO | max output length: 130
2026-01-28 21:44:23,131 | INFO | min output length: 13
2026-01-28 21:44:26,642 | INFO | end detected at 78
2026-01-28 21:44:26,643 | INFO |  -8.84 * 0.5 =  -4.42 for decoder
2026-01-28 21:44:26,643 | INFO |  -2.63 * 0.5 =  -1.31 for ctc
2026-01-28 21:44:26,643 | INFO | total log probability: -5.73
2026-01-28 21:44:26,643 | INFO | normalized log probability: -0.08
2026-01-28 21:44:26,644 | INFO | total number of ended hypotheses: 157
2026-01-28 21:44:26,645 | INFO | best hypo: qui<space>a<space>vu<space>deux<space>guerres<space>mondiales<space>le<space>goulingue<space>les<space>dictatures<space>totalitaires

2026-01-28 21:44:26,647 | INFO | speech length: 13920
2026-01-28 21:44:26,683 | INFO | decoder input length: 21
2026-01-28 21:44:26,683 | INFO | max output length: 21
2026-01-28 21:44:26,683 | INFO | min output length: 2
2026-01-28 21:44:27,318 | INFO | end detected at 18
2026-01-28 21:44:27,319 | INFO |  -1.00 * 0.5 =  -0.50 for decoder
2026-01-28 21:44:27,319 | INFO |  -0.29 * 0.5 =  -0.15 for ctc
2026-01-28 21:44:27,319 | INFO | total log probability: -0.65
2026-01-28 21:44:27,319 | INFO | normalized log probability: -0.05
2026-01-28 21:44:27,319 | INFO | total number of ended hypotheses: 170
2026-01-28 21:44:27,320 | INFO | best hypo: et<space>la<space>choix

2026-01-28 21:44:27,321 | INFO | speech length: 65440
2026-01-28 21:44:27,376 | INFO | decoder input length: 101
2026-01-28 21:44:27,376 | INFO | max output length: 101
2026-01-28 21:44:27,376 | INFO | min output length: 10
2026-01-28 21:44:30,256 | INFO | end detected at 56
2026-01-28 21:44:30,258 | INFO |  -4.02 * 0.5 =  -2.01 for decoder
2026-01-28 21:44:30,259 | INFO |  -0.32 * 0.5 =  -0.16 for ctc
2026-01-28 21:44:30,259 | INFO | total log probability: -2.17
2026-01-28 21:44:30,259 | INFO | normalized log probability: -0.04
2026-01-28 21:44:30,259 | INFO | total number of ended hypotheses: 172
2026-01-28 21:44:30,259 | INFO | best hypo: mais<space>ce<space>soir<space>ce<space>qui<space>importe<space>c'est<space>l'avenir<space>notre

2026-01-28 21:44:30,261 | INFO | speech length: 18240
2026-01-28 21:44:30,322 | INFO | decoder input length: 28
2026-01-28 21:44:30,323 | INFO | max output length: 28
2026-01-28 21:44:30,323 | INFO | min output length: 2
2026-01-28 21:44:31,222 | INFO | end detected at 26
2026-01-28 21:44:31,223 | INFO |  -1.69 * 0.5 =  -0.84 for decoder
2026-01-28 21:44:31,223 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 21:44:31,223 | INFO | total log probability: -0.84
2026-01-28 21:44:31,223 | INFO | normalized log probability: -0.04
2026-01-28 21:44:31,223 | INFO | total number of ended hypotheses: 142
2026-01-28 21:44:31,223 | INFO | best hypo: celui<space>de<space>nos<space>enfants

2026-01-28 21:44:31,225 | INFO | speech length: 50560
2026-01-28 21:44:31,265 | INFO | decoder input length: 78
2026-01-28 21:44:31,265 | INFO | max output length: 78
2026-01-28 21:44:31,266 | INFO | min output length: 7
2026-01-28 21:44:33,691 | INFO | end detected at 55
2026-01-28 21:44:33,693 | INFO |  -4.07 * 0.5 =  -2.03 for decoder
2026-01-28 21:44:33,693 | INFO |  -0.11 * 0.5 =  -0.06 for ctc
2026-01-28 21:44:33,693 | INFO | total log probability: -2.09
2026-01-28 21:44:33,693 | INFO | normalized log probability: -0.04
2026-01-28 21:44:33,693 | INFO | total number of ended hypotheses: 166
2026-01-28 21:44:33,694 | INFO | best hypo: le<space>progrès<space>va<space>se<space>poursuivre<space>avec<space>ces<space>hésitations

2026-01-28 21:44:33,696 | INFO | speech length: 106240
2026-01-28 21:44:33,739 | INFO | decoder input length: 165
2026-01-28 21:44:33,739 | INFO | max output length: 165
2026-01-28 21:44:33,739 | INFO | min output length: 16
2026-01-28 21:44:38,774 | INFO | end detected at 104
2026-01-28 21:44:38,775 | INFO |  -8.07 * 0.5 =  -4.04 for decoder
2026-01-28 21:44:38,775 | INFO |  -0.50 * 0.5 =  -0.25 for ctc
2026-01-28 21:44:38,775 | INFO | total log probability: -4.29
2026-01-28 21:44:38,776 | INFO | normalized log probability: -0.04
2026-01-28 21:44:38,776 | INFO | total number of ended hypotheses: 151
2026-01-28 21:44:38,777 | INFO | best hypo: avec<space>ses<space>limites<space>que<space>nous<space>mesurons<space>bien<space>face<space>aux<space>événements<space>récents<space>qui<space>nous<space>invitent<space>à<space>l'humilité

2026-01-28 21:44:38,779 | INFO | speech length: 24480
2026-01-28 21:44:38,822 | INFO | decoder input length: 37
2026-01-28 21:44:38,822 | INFO | max output length: 37
2026-01-28 21:44:38,822 | INFO | min output length: 3
2026-01-28 21:44:39,861 | INFO | end detected at 28
2026-01-28 21:44:39,863 | INFO |  -1.78 * 0.5 =  -0.89 for decoder
2026-01-28 21:44:39,863 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 21:44:39,863 | INFO | total log probability: -0.89
2026-01-28 21:44:39,863 | INFO | normalized log probability: -0.04
2026-01-28 21:44:39,863 | INFO | total number of ended hypotheses: 170
2026-01-28 21:44:39,863 | INFO | best hypo: progrès<space>de<space>la<space>science

2026-01-28 21:44:39,865 | INFO | speech length: 37280
2026-01-28 21:44:39,907 | INFO | decoder input length: 57
2026-01-28 21:44:39,907 | INFO | max output length: 57
2026-01-28 21:44:39,907 | INFO | min output length: 5
2026-01-28 21:44:41,949 | INFO | end detected at 49
2026-01-28 21:44:41,950 | INFO |  -3.59 * 0.5 =  -1.79 for decoder
2026-01-28 21:44:41,950 | INFO |  -0.46 * 0.5 =  -0.23 for ctc
2026-01-28 21:44:41,950 | INFO | total log probability: -2.02
2026-01-28 21:44:41,950 | INFO | normalized log probability: -0.04
2026-01-28 21:44:41,950 | INFO | total number of ended hypotheses: 158
2026-01-28 21:44:41,951 | INFO | best hypo: progrès<space>des<space>communications<space>entre<space>les<space>hommes

2026-01-28 21:44:41,953 | INFO | speech length: 16960
2026-01-28 21:44:41,996 | INFO | decoder input length: 26
2026-01-28 21:44:41,996 | INFO | max output length: 26
2026-01-28 21:44:41,996 | INFO | min output length: 2
2026-01-28 21:44:42,987 | INFO | adding <eos> in the last position in the loop
2026-01-28 21:44:42,996 | INFO | no hypothesis. Finish decoding.
2026-01-28 21:44:42,998 | INFO |  -2.17 * 0.5 =  -1.09 for decoder
2026-01-28 21:44:42,998 | INFO |  -0.80 * 0.5 =  -0.40 for ctc
2026-01-28 21:44:42,998 | INFO | total log probability: -1.48
2026-01-28 21:44:42,998 | INFO | normalized log probability: -0.06
2026-01-28 21:44:42,998 | INFO | total number of ended hypotheses: 93
2026-01-28 21:44:42,998 | INFO | best hypo: progrès<space>de<space>la<space>médecine

2026-01-28 21:44:43,000 | INFO | speech length: 97280
2026-01-28 21:44:43,047 | INFO | decoder input length: 151
2026-01-28 21:44:43,047 | INFO | max output length: 151
2026-01-28 21:44:43,047 | INFO | min output length: 15
2026-01-28 21:44:48,162 | INFO | end detected at 91
2026-01-28 21:44:48,164 | INFO |  -9.05 * 0.5 =  -4.52 for decoder
2026-01-28 21:44:48,164 | INFO |  -0.74 * 0.5 =  -0.37 for ctc
2026-01-28 21:44:48,165 | INFO | total log probability: -4.89
2026-01-28 21:44:48,165 | INFO | normalized log probability: -0.06
2026-01-28 21:44:48,165 | INFO | total number of ended hypotheses: 173
2026-01-28 21:44:48,166 | INFO | best hypo: un<space>grand<space>nombre<space>des<space>enfants<space>qui<space>vont<space>naître<space>cette<space>année<space>versont<space>l'an<space>deux<space>mille<space>cent

2026-01-28 21:44:48,169 | INFO | speech length: 17920
2026-01-28 21:44:48,251 | INFO | decoder input length: 27
2026-01-28 21:44:48,251 | INFO | max output length: 27
2026-01-28 21:44:48,251 | INFO | min output length: 2
2026-01-28 21:44:49,714 | INFO | end detected at 18
2026-01-28 21:44:49,719 | INFO |  -0.94 * 0.5 =  -0.47 for decoder
2026-01-28 21:44:49,722 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-28 21:44:49,722 | INFO | total log probability: -0.48
2026-01-28 21:44:49,722 | INFO | normalized log probability: -0.04
2026-01-28 21:44:49,722 | INFO | total number of ended hypotheses: 164
2026-01-28 21:44:49,722 | INFO | best hypo: ces<space>progrès

2026-01-28 21:44:49,726 | INFO | speech length: 26080
2026-01-28 21:44:49,781 | INFO | decoder input length: 40
2026-01-28 21:44:49,782 | INFO | max output length: 40
2026-01-28 21:44:49,782 | INFO | min output length: 4
2026-01-28 21:44:51,261 | INFO | end detected at 35
2026-01-28 21:44:51,263 | INFO |  -3.17 * 0.5 =  -1.59 for decoder
2026-01-28 21:44:51,263 | INFO |  -1.47 * 0.5 =  -0.74 for ctc
2026-01-28 21:44:51,263 | INFO | total log probability: -2.32
2026-01-28 21:44:51,263 | INFO | normalized log probability: -0.08
2026-01-28 21:44:51,263 | INFO | total number of ended hypotheses: 173
2026-01-28 21:44:51,264 | INFO | best hypo: ne<space>prendront<space>tous<space>leur<space>sang

2026-01-28 21:44:51,266 | INFO | speech length: 27520
2026-01-28 21:44:51,312 | INFO | decoder input length: 42
2026-01-28 21:44:51,313 | INFO | max output length: 42
2026-01-28 21:44:51,313 | INFO | min output length: 4
2026-01-28 21:44:52,872 | INFO | end detected at 37
2026-01-28 21:44:52,874 | INFO |  -4.33 * 0.5 =  -2.16 for decoder
2026-01-28 21:44:52,874 | INFO |  -2.71 * 0.5 =  -1.35 for ctc
2026-01-28 21:44:52,875 | INFO | total log probability: -3.52
2026-01-28 21:44:52,875 | INFO | normalized log probability: -0.11
2026-01-28 21:44:52,875 | INFO | total number of ended hypotheses: 181
2026-01-28 21:44:52,875 | INFO | best hypo: que<space>s'ils<space>bénéficient<space>à<space>l'homme

2026-01-28 21:44:52,879 | INFO | speech length: 14720
2026-01-28 21:44:52,939 | INFO | decoder input length: 22
2026-01-28 21:44:52,939 | INFO | max output length: 22
2026-01-28 21:44:52,939 | INFO | min output length: 2
2026-01-28 21:44:53,750 | INFO | adding <eos> in the last position in the loop
2026-01-28 21:44:53,760 | INFO | no hypothesis. Finish decoding.
2026-01-28 21:44:53,761 | INFO |  -1.45 * 0.5 =  -0.73 for decoder
2026-01-28 21:44:53,761 | INFO |  -0.03 * 0.5 =  -0.01 for ctc
2026-01-28 21:44:53,761 | INFO | total log probability: -0.74
2026-01-28 21:44:53,761 | INFO | normalized log probability: -0.04
2026-01-28 21:44:53,761 | INFO | total number of ended hypotheses: 129
2026-01-28 21:44:53,761 | INFO | best hypo: à<space>tous<space>les<space>hommes

2026-01-28 21:44:53,763 | INFO | speech length: 67520
2026-01-28 21:44:53,824 | INFO | decoder input length: 105
2026-01-28 21:44:53,824 | INFO | max output length: 105
2026-01-28 21:44:53,824 | INFO | min output length: 10
2026-01-28 21:44:57,190 | INFO | end detected at 64
2026-01-28 21:44:57,192 | INFO |  -4.71 * 0.5 =  -2.36 for decoder
2026-01-28 21:44:57,192 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 21:44:57,192 | INFO | total log probability: -2.36
2026-01-28 21:44:57,192 | INFO | normalized log probability: -0.04
2026-01-28 21:44:57,192 | INFO | total number of ended hypotheses: 146
2026-01-28 21:44:57,193 | INFO | best hypo: le<space>vingt<space>et<space>unième<space>siècle<space>doit<space>être<space>le<space>siècle<space>de<space>l'éthique

2026-01-28 21:44:57,196 | INFO | speech length: 57440
2026-01-28 21:44:57,246 | INFO | decoder input length: 89
2026-01-28 21:44:57,247 | INFO | max output length: 89
2026-01-28 21:44:57,247 | INFO | min output length: 8
2026-01-28 21:45:00,576 | INFO | end detected at 79
2026-01-28 21:45:00,577 | INFO |  -6.01 * 0.5 =  -3.00 for decoder
2026-01-28 21:45:00,577 | INFO |  -0.19 * 0.5 =  -0.10 for ctc
2026-01-28 21:45:00,577 | INFO | total log probability: -3.10
2026-01-28 21:45:00,577 | INFO | normalized log probability: -0.04
2026-01-28 21:45:00,577 | INFO | total number of ended hypotheses: 160
2026-01-28 21:45:00,578 | INFO | best hypo: je<space>sais<space>que<space>bien<space>les<space>tragédies<space>aujourd'hui<space>font<space>douter<space>de<space>cette<space>espérance

2026-01-28 21:45:00,581 | INFO | speech length: 108960
2026-01-28 21:45:00,628 | INFO | decoder input length: 169
2026-01-28 21:45:00,628 | INFO | max output length: 169
2026-01-28 21:45:00,628 | INFO | min output length: 16
2026-01-28 21:45:06,576 | INFO | end detected at 104
2026-01-28 21:45:06,578 | INFO |  -7.81 * 0.5 =  -3.91 for decoder
2026-01-28 21:45:06,578 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 21:45:06,578 | INFO | total log probability: -3.91
2026-01-28 21:45:06,578 | INFO | normalized log probability: -0.04
2026-01-28 21:45:06,578 | INFO | total number of ended hypotheses: 173
2026-01-28 21:45:06,580 | INFO | best hypo: pourtant<space>de<space>plus<space>en<space>plus<space>les<space>nations<space>s'accordent<space>pour<space>mieux<space>faire<space>respecter<space>les<space>droits<space>de<space>l'homme

2026-01-28 21:45:06,583 | INFO | speech length: 59520
2026-01-28 21:45:06,628 | INFO | decoder input length: 92
2026-01-28 21:45:06,628 | INFO | max output length: 92
2026-01-28 21:45:06,628 | INFO | min output length: 9
2026-01-28 21:45:10,530 | INFO | end detected at 53
2026-01-28 21:45:10,532 | INFO |  -3.80 * 0.5 =  -1.90 for decoder
2026-01-28 21:45:10,532 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 21:45:10,532 | INFO | total log probability: -1.90
2026-01-28 21:45:10,532 | INFO | normalized log probability: -0.04
2026-01-28 21:45:10,532 | INFO | total number of ended hypotheses: 167
2026-01-28 21:45:10,533 | INFO | best hypo: pour<space>défendre<space>la<space>liberté<space>et<space>la<space>dignité<space>humaine

2026-01-28 21:45:10,536 | INFO | speech length: 50720
2026-01-28 21:45:10,601 | INFO | decoder input length: 78
2026-01-28 21:45:10,602 | INFO | max output length: 78
2026-01-28 21:45:10,602 | INFO | min output length: 7
2026-01-28 21:45:15,830 | INFO | end detected at 56
2026-01-28 21:45:15,833 | INFO |  -4.00 * 0.5 =  -2.00 for decoder
2026-01-28 21:45:15,833 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-28 21:45:15,833 | INFO | total log probability: -2.01
2026-01-28 21:45:15,833 | INFO | normalized log probability: -0.04
2026-01-28 21:45:15,833 | INFO | total number of ended hypotheses: 172
2026-01-28 21:45:15,834 | INFO | best hypo: un<space>nouvel<space>ordre<space>international<space>s'affirme<space>peu<space>à<space>peu

2026-01-28 21:45:15,837 | INFO | speech length: 100000
2026-01-28 21:45:15,879 | INFO | decoder input length: 155
2026-01-28 21:45:15,879 | INFO | max output length: 155
2026-01-28 21:45:15,879 | INFO | min output length: 15
2026-01-28 21:45:20,637 | INFO | end detected at 84
2026-01-28 21:45:20,639 | INFO |  -6.25 * 0.5 =  -3.13 for decoder
2026-01-28 21:45:20,639 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-28 21:45:20,639 | INFO | total log probability: -3.13
2026-01-28 21:45:20,639 | INFO | normalized log probability: -0.04
2026-01-28 21:45:20,639 | INFO | total number of ended hypotheses: 168
2026-01-28 21:45:20,641 | INFO | best hypo: demain<space>il<space>ne<space>devra<space>plus<space>y<space>avoir<space>de<space>repos<space>pour<space>les<space>criminels<space>contre<space>l'humanité

2026-01-28 21:45:20,644 | INFO | speech length: 16800
2026-01-28 21:45:20,691 | INFO | decoder input length: 25
2026-01-28 21:45:20,691 | INFO | max output length: 25
2026-01-28 21:45:20,691 | INFO | min output length: 2
2026-01-28 21:45:21,525 | INFO | adding <eos> in the last position in the loop
2026-01-28 21:45:21,534 | INFO | no hypothesis. Finish decoding.
2026-01-28 21:45:21,535 | INFO |  -1.93 * 0.5 =  -0.97 for decoder
2026-01-28 21:45:21,535 | INFO |  -0.96 * 0.5 =  -0.48 for ctc
2026-01-28 21:45:21,535 | INFO | total log probability: -1.45
2026-01-28 21:45:21,536 | INFO | normalized log probability: -0.07
2026-01-28 21:45:21,536 | INFO | total number of ended hypotheses: 133
2026-01-28 21:45:21,536 | INFO | best hypo: au<space>nom<space>de<space>la<space>france

2026-01-28 21:45:21,537 | INFO | speech length: 14720
2026-01-28 21:45:21,573 | INFO | decoder input length: 22
2026-01-28 21:45:21,573 | INFO | max output length: 22
2026-01-28 21:45:21,574 | INFO | min output length: 2
2026-01-28 21:45:22,302 | INFO | adding <eos> in the last position in the loop
2026-01-28 21:45:22,313 | INFO | end detected at 21
2026-01-28 21:45:22,315 | INFO |  -2.24 * 0.5 =  -1.12 for decoder
2026-01-28 21:45:22,315 | INFO |  -3.49 * 0.5 =  -1.75 for ctc
2026-01-28 21:45:22,315 | INFO | total log probability: -2.87
2026-01-28 21:45:22,315 | INFO | normalized log probability: -0.20
2026-01-28 21:45:22,315 | INFO | total number of ended hypotheses: 201
2026-01-28 21:45:22,316 | INFO | best hypo: on<space>votre<space>nom

2026-01-28 21:45:22,318 | INFO | speech length: 44160
2026-01-28 21:45:22,354 | INFO | decoder input length: 68
2026-01-28 21:45:22,354 | INFO | max output length: 68
2026-01-28 21:45:22,354 | INFO | min output length: 6
2026-01-28 21:45:24,419 | INFO | end detected at 55
2026-01-28 21:45:24,420 | INFO |  -4.00 * 0.5 =  -2.00 for decoder
2026-01-28 21:45:24,420 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 21:45:24,420 | INFO | total log probability: -2.00
2026-01-28 21:45:24,420 | INFO | normalized log probability: -0.04
2026-01-28 21:45:24,420 | INFO | total number of ended hypotheses: 144
2026-01-28 21:45:24,421 | INFO | best hypo: c'est<space>le<space>combat<space>difficile<space>que<space>je<space>mène<space>chaque<space>jour

2026-01-28 21:45:24,423 | INFO | speech length: 32160
2026-01-28 21:45:24,463 | INFO | decoder input length: 49
2026-01-28 21:45:24,463 | INFO | max output length: 49
2026-01-28 21:45:24,463 | INFO | min output length: 4
2026-01-28 21:45:26,247 | INFO | end detected at 36
2026-01-28 21:45:26,248 | INFO |  -2.50 * 0.5 =  -1.25 for decoder
2026-01-28 21:45:26,249 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 21:45:26,249 | INFO | total log probability: -1.25
2026-01-28 21:45:26,249 | INFO | normalized log probability: -0.04
2026-01-28 21:45:26,249 | INFO | total number of ended hypotheses: 139
2026-01-28 21:45:26,249 | INFO | best hypo: à<space>l'intérieur<space>de<space>chaque<space>nation

2026-01-28 21:45:26,251 | INFO | speech length: 32480
2026-01-28 21:45:26,293 | INFO | decoder input length: 50
2026-01-28 21:45:26,293 | INFO | max output length: 50
2026-01-28 21:45:26,293 | INFO | min output length: 5
2026-01-28 21:45:29,585 | INFO | end detected at 32
2026-01-28 21:45:29,588 | INFO |  -5.67 * 0.5 =  -2.83 for decoder
2026-01-28 21:45:29,588 | INFO |  -3.30 * 0.5 =  -1.65 for ctc
2026-01-28 21:45:29,588 | INFO | total log probability: -4.48
2026-01-28 21:45:29,588 | INFO | normalized log probability: -0.17
2026-01-28 21:45:29,588 | INFO | total number of ended hypotheses: 185
2026-01-28 21:45:29,588 | INFO | best hypo: une<space>exigence<space>se<space>faitante

2026-01-28 21:45:29,591 | INFO | speech length: 15360
2026-01-28 21:45:29,633 | INFO | decoder input length: 23
2026-01-28 21:45:29,633 | INFO | max output length: 23
2026-01-28 21:45:29,633 | INFO | min output length: 2
2026-01-28 21:45:32,070 | INFO | adding <eos> in the last position in the loop
2026-01-28 21:45:32,081 | INFO | no hypothesis. Finish decoding.
2026-01-28 21:45:32,082 | INFO |  -1.63 * 0.5 =  -0.81 for decoder
2026-01-28 21:45:32,083 | INFO |  -0.26 * 0.5 =  -0.13 for ctc
2026-01-28 21:45:32,083 | INFO | total log probability: -0.95
2026-01-28 21:45:32,083 | INFO | normalized log probability: -0.05
2026-01-28 21:45:32,083 | INFO | total number of ended hypotheses: 129
2026-01-28 21:45:32,083 | INFO | best hypo: toujours<space>plus<space>fort

2026-01-28 21:45:32,085 | INFO | speech length: 117280
2026-01-28 21:45:32,129 | INFO | decoder input length: 182
2026-01-28 21:45:32,129 | INFO | max output length: 182
2026-01-28 21:45:32,129 | INFO | min output length: 18
2026-01-28 21:45:41,459 | INFO | end detected at 120
2026-01-28 21:45:41,461 | INFO |  -9.18 * 0.5 =  -4.59 for decoder
2026-01-28 21:45:41,461 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-28 21:45:41,461 | INFO | total log probability: -4.60
2026-01-28 21:45:41,461 | INFO | normalized log probability: -0.04
2026-01-28 21:45:41,461 | INFO | total number of ended hypotheses: 151
2026-01-28 21:45:41,463 | INFO | best hypo: pour<space>que<space>les<space>avancées<space>de<space>la<space>science<space>soient<space>orientées<space>vers<space>le<space>bien<space>de<space>l'homme<space>et<space>ne<space>se<space>retournent<space>jamais<space>contre<space>lui

2026-01-28 21:45:41,466 | INFO | speech length: 29920
2026-01-28 21:45:41,508 | INFO | decoder input length: 46
2026-01-28 21:45:41,508 | INFO | max output length: 46
2026-01-28 21:45:41,508 | INFO | min output length: 4
2026-01-28 21:45:42,672 | INFO | end detected at 27
2026-01-28 21:45:42,673 | INFO |  -1.72 * 0.5 =  -0.86 for decoder
2026-01-28 21:45:42,673 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 21:45:42,673 | INFO | total log probability: -0.86
2026-01-28 21:45:42,673 | INFO | normalized log probability: -0.04
2026-01-28 21:45:42,673 | INFO | total number of ended hypotheses: 165
2026-01-28 21:45:42,674 | INFO | best hypo: je<space>pense<space>par<space>exemple

2026-01-28 21:45:42,676 | INFO | speech length: 46080
2026-01-28 21:45:42,720 | INFO | decoder input length: 71
2026-01-28 21:45:42,720 | INFO | max output length: 71
2026-01-28 21:45:42,720 | INFO | min output length: 7
2026-01-28 21:45:47,076 | INFO | end detected at 47
2026-01-28 21:45:47,079 | INFO |  -4.66 * 0.5 =  -2.33 for decoder
2026-01-28 21:45:47,079 | INFO |  -2.61 * 0.5 =  -1.30 for ctc
2026-01-28 21:45:47,079 | INFO | total log probability: -3.63
2026-01-28 21:45:47,079 | INFO | normalized log probability: -0.09
2026-01-28 21:45:47,079 | INFO | total number of ended hypotheses: 173
2026-01-28 21:45:47,080 | INFO | best hypo: aux<space>manipulations<space>génétiques<space>aux<space>clonaes

2026-01-28 21:45:47,082 | INFO | speech length: 27840
2026-01-28 21:45:47,125 | INFO | decoder input length: 43
2026-01-28 21:45:47,125 | INFO | max output length: 43
2026-01-28 21:45:47,125 | INFO | min output length: 4
2026-01-28 21:45:50,738 | INFO | adding <eos> in the last position in the loop
2026-01-28 21:45:50,746 | INFO | no hypothesis. Finish decoding.
2026-01-28 21:45:50,747 | INFO |  -3.52 * 0.5 =  -1.76 for decoder
2026-01-28 21:45:50,747 | INFO |  -0.04 * 0.5 =  -0.02 for ctc
2026-01-28 21:45:50,747 | INFO | total log probability: -1.78
2026-01-28 21:45:50,747 | INFO | normalized log probability: -0.04
2026-01-28 21:45:50,747 | INFO | total number of ended hypotheses: 72
2026-01-28 21:45:50,748 | INFO | best hypo: de<space>même<space>dans<space>le<space>domaine<space>de<space>l'environnement<sos/eos>

2026-01-28 21:45:50,748 | WARNING | best hypo length: 43 == max output length: 43
2026-01-28 21:45:50,748 | WARNING | decoding may be stopped by the max output length limitation, please consider to increase the maxlenratio.
2026-01-28 21:45:50,750 | INFO | speech length: 96320
2026-01-28 21:45:50,798 | INFO | decoder input length: 150
2026-01-28 21:45:50,798 | INFO | max output length: 150
2026-01-28 21:45:50,798 | INFO | min output length: 15
2026-01-28 21:45:55,369 | INFO | end detected at 84
2026-01-28 21:45:55,370 | INFO |  -6.31 * 0.5 =  -3.16 for decoder
2026-01-28 21:45:55,370 | INFO |  -0.25 * 0.5 =  -0.12 for ctc
2026-01-28 21:45:55,371 | INFO | total log probability: -3.28
2026-01-28 21:45:55,371 | INFO | normalized log probability: -0.04
2026-01-28 21:45:55,371 | INFO | total number of ended hypotheses: 163
2026-01-28 21:45:55,372 | INFO | best hypo: les<space>peuples<space>ne<space>veulent<space>plus<space>que<space>la<space>course<space>à<space>la<space>productivité<space>épuise<space>la<space>planète

2026-01-28 21:45:55,374 | INFO | speech length: 92800
2026-01-28 21:45:55,415 | INFO | decoder input length: 144
2026-01-28 21:45:55,415 | INFO | max output length: 144
2026-01-28 21:45:55,415 | INFO | min output length: 14
2026-01-28 21:46:00,215 | INFO | end detected at 87
2026-01-28 21:46:00,217 | INFO |  -6.49 * 0.5 =  -3.25 for decoder
2026-01-28 21:46:00,217 | INFO |  -1.17 * 0.5 =  -0.58 for ctc
2026-01-28 21:46:00,217 | INFO | total log probability: -3.83
2026-01-28 21:46:00,217 | INFO | normalized log probability: -0.05
2026-01-28 21:46:00,218 | INFO | total number of ended hypotheses: 164
2026-01-28 21:46:00,219 | INFO | best hypo: la<space>responsabilité<space>de<space>tous<space>ceux<space>qui<space>dans<space>le<space>monde<space>dégradent<space>le<space>patrimoine<space>naturel

2026-01-28 21:46:00,221 | INFO | speech length: 41280
2026-01-28 21:46:00,267 | INFO | decoder input length: 64
2026-01-28 21:46:00,267 | INFO | max output length: 64
2026-01-28 21:46:00,268 | INFO | min output length: 6
2026-01-28 21:46:04,507 | INFO | end detected at 42
2026-01-28 21:46:04,510 | INFO |  -4.18 * 0.5 =  -2.09 for decoder
2026-01-28 21:46:04,510 | INFO |  -1.27 * 0.5 =  -0.63 for ctc
2026-01-28 21:46:04,510 | INFO | total log probability: -2.73
2026-01-28 21:46:04,510 | INFO | normalized log probability: -0.08
2026-01-28 21:46:04,510 | INFO | total number of ended hypotheses: 180
2026-01-28 21:46:04,511 | INFO | best hypo: doit<space>être<space>recherché<space>et<space>sanctionné

2026-01-28 21:46:04,514 | INFO | speech length: 68800
2026-01-28 21:46:04,569 | INFO | decoder input length: 107
2026-01-28 21:46:04,569 | INFO | max output length: 107
2026-01-28 21:46:04,569 | INFO | min output length: 10
2026-01-28 21:46:11,560 | INFO | end detected at 67
2026-01-28 21:46:11,563 | INFO |  -4.93 * 0.5 =  -2.46 for decoder
2026-01-28 21:46:11,563 | INFO |  -1.62 * 0.5 =  -0.81 for ctc
2026-01-28 21:46:11,564 | INFO | total log probability: -3.27
2026-01-28 21:46:11,564 | INFO | normalized log probability: -0.05
2026-01-28 21:46:11,564 | INFO | total number of ended hypotheses: 188
2026-01-28 21:46:11,565 | INFO | best hypo: car<space>il<space>s'agit<space>du<space>patrimoine<space>que<space>nous<space>lèverons<space>à<space>nos<space>enfants

2026-01-28 21:46:11,569 | INFO | speech length: 54720
2026-01-28 21:46:11,617 | INFO | decoder input length: 85
2026-01-28 21:46:11,617 | INFO | max output length: 85
2026-01-28 21:46:11,617 | INFO | min output length: 8
2026-01-28 21:46:14,264 | INFO | end detected at 57
2026-01-28 21:46:14,266 | INFO |  -4.09 * 0.5 =  -2.04 for decoder
2026-01-28 21:46:14,266 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 21:46:14,266 | INFO | total log probability: -2.05
2026-01-28 21:46:14,266 | INFO | normalized log probability: -0.04
2026-01-28 21:46:14,266 | INFO | total number of ended hypotheses: 173
2026-01-28 21:46:14,267 | INFO | best hypo: même<space>si<space>le<space>monde<space>change<space>comme<space>il<space>n'a<space>jamais<space>changé

2026-01-28 21:46:14,270 | INFO | speech length: 42560
2026-01-28 21:46:14,316 | INFO | decoder input length: 66
2026-01-28 21:46:14,316 | INFO | max output length: 66
2026-01-28 21:46:14,316 | INFO | min output length: 6
2026-01-28 21:46:16,712 | INFO | end detected at 43
2026-01-28 21:46:16,713 | INFO |  -3.06 * 0.5 =  -1.53 for decoder
2026-01-28 21:46:16,713 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 21:46:16,713 | INFO | total log probability: -1.53
2026-01-28 21:46:16,713 | INFO | normalized log probability: -0.04
2026-01-28 21:46:16,713 | INFO | total number of ended hypotheses: 141
2026-01-28 21:46:16,714 | INFO | best hypo: la<space>modernité<space>ne<space>doit<space>pas<space>nous<space>diviser

2026-01-28 21:46:16,716 | INFO | speech length: 36800
2026-01-28 21:46:16,764 | INFO | decoder input length: 57
2026-01-28 21:46:16,764 | INFO | max output length: 57
2026-01-28 21:46:16,764 | INFO | min output length: 5
2026-01-28 21:46:19,787 | INFO | end detected at 34
2026-01-28 21:46:19,789 | INFO |  -2.25 * 0.5 =  -1.13 for decoder
2026-01-28 21:46:19,789 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 21:46:19,789 | INFO | total log probability: -1.13
2026-01-28 21:46:19,789 | INFO | normalized log probability: -0.04
2026-01-28 21:46:19,789 | INFO | total number of ended hypotheses: 169
2026-01-28 21:46:19,790 | INFO | best hypo: elle<space>doit<space>profiter<space>à<space>chacun

2026-01-28 21:46:19,792 | INFO | speech length: 113440
2026-01-28 21:46:19,848 | INFO | decoder input length: 176
2026-01-28 21:46:19,849 | INFO | max output length: 176
2026-01-28 21:46:19,849 | INFO | min output length: 17
2026-01-28 21:46:32,813 | INFO | end detected at 124
2026-01-28 21:46:32,816 | INFO | -10.15 * 0.5 =  -5.07 for decoder
2026-01-28 21:46:32,816 | INFO |  -4.99 * 0.5 =  -2.49 for ctc
2026-01-28 21:46:32,816 | INFO | total log probability: -7.57
2026-01-28 21:46:32,816 | INFO | normalized log probability: -0.06
2026-01-28 21:46:32,816 | INFO | total number of ended hypotheses: 192
2026-01-28 21:46:32,819 | INFO | best hypo: nous<space>réussirons<space>nous<space>réussirons<space>parce<space>que<space>nous<space>avons<space>pris<space>les<space>décisions<space>qui<space>engagent<space>et<space>qui<space>garantissent<space>notre<space>oeuvre

2026-01-28 21:46:32,823 | INFO | speech length: 56960
2026-01-28 21:46:32,882 | INFO | decoder input length: 88
2026-01-28 21:46:32,882 | INFO | max output length: 88
2026-01-28 21:46:32,883 | INFO | min output length: 8
2026-01-28 21:46:38,135 | INFO | end detected at 74
2026-01-28 21:46:38,138 | INFO |  -5.41 * 0.5 =  -2.71 for decoder
2026-01-28 21:46:38,138 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 21:46:38,138 | INFO | total log probability: -2.71
2026-01-28 21:46:38,138 | INFO | normalized log probability: -0.04
2026-01-28 21:46:38,138 | INFO | total number of ended hypotheses: 171
2026-01-28 21:46:38,140 | INFO | best hypo: nous<space>avons<space>choisi<space>ensemble<space>de<space>faire<space>grandir<space>la<space>france<space>dans<space>l'europe

2026-01-28 21:46:38,143 | INFO | speech length: 101280
2026-01-28 21:46:38,198 | INFO | decoder input length: 157
2026-01-28 21:46:38,198 | INFO | max output length: 157
2026-01-28 21:46:38,198 | INFO | min output length: 15
2026-01-28 21:46:49,354 | INFO | end detected at 103
2026-01-28 21:46:49,356 | INFO |  -7.87 * 0.5 =  -3.93 for decoder
2026-01-28 21:46:49,356 | INFO |  -0.90 * 0.5 =  -0.45 for ctc
2026-01-28 21:46:49,356 | INFO | total log probability: -4.38
2026-01-28 21:46:49,356 | INFO | normalized log probability: -0.04
2026-01-28 21:46:49,356 | INFO | total number of ended hypotheses: 170
2026-01-28 21:46:49,357 | INFO | best hypo: une<space>europe<space>qui<space>nous<space>garantit<space>la<space>paix<space>une<space>europe<space>qui<space>nous<space>permet<space>de<space>peser<space>davantage<space>dans<space>le<space>monde

2026-01-28 21:46:49,360 | INFO | speech length: 105600
2026-01-28 21:46:49,416 | INFO | decoder input length: 164
2026-01-28 21:46:49,416 | INFO | max output length: 164
2026-01-28 21:46:49,417 | INFO | min output length: 16
2026-01-28 21:46:58,582 | INFO | end detected at 96
2026-01-28 21:46:58,584 | INFO |  -7.23 * 0.5 =  -3.62 for decoder
2026-01-28 21:46:58,584 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-28 21:46:58,584 | INFO | total log probability: -3.62
2026-01-28 21:46:58,585 | INFO | normalized log probability: -0.04
2026-01-28 21:46:58,585 | INFO | total number of ended hypotheses: 177
2026-01-28 21:46:58,587 | INFO | best hypo: nous<space>avons<space>choisi<space>aussi<space>de<space>prendre<space>part<space>à<space>la<space>mondialisation<space>d'en<space>prendre<space>toute<space>notre<space>part

2026-01-28 21:46:58,590 | INFO | speech length: 92640
2026-01-28 21:46:58,644 | INFO | decoder input length: 144
2026-01-28 21:46:58,645 | INFO | max output length: 144
2026-01-28 21:46:58,645 | INFO | min output length: 14
2026-01-28 21:47:07,678 | INFO | end detected at 82
2026-01-28 21:47:07,680 | INFO |  -6.13 * 0.5 =  -3.06 for decoder
2026-01-28 21:47:07,680 | INFO |  -0.01 * 0.5 =  -0.00 for ctc
2026-01-28 21:47:07,680 | INFO | total log probability: -3.07
2026-01-28 21:47:07,680 | INFO | normalized log probability: -0.04
2026-01-28 21:47:07,680 | INFO | total number of ended hypotheses: 168
2026-01-28 21:47:07,681 | INFO | best hypo: mais<space>une<space>mondialisation<space>maîtrisée<space>organisée<space>respectueuse<space>de<space>l'environnement

2026-01-28 21:47:07,683 | INFO | speech length: 67840
2026-01-28 21:47:07,730 | INFO | decoder input length: 105
2026-01-28 21:47:07,730 | INFO | max output length: 105
2026-01-28 21:47:07,730 | INFO | min output length: 10
2026-01-28 21:47:11,946 | INFO | end detected at 62
2026-01-28 21:47:11,948 | INFO |  -5.27 * 0.5 =  -2.63 for decoder
2026-01-28 21:47:11,948 | INFO |  -0.32 * 0.5 =  -0.16 for ctc
2026-01-28 21:47:11,948 | INFO | total log probability: -2.79
2026-01-28 21:47:11,948 | INFO | normalized log probability: -0.05
2026-01-28 21:47:11,948 | INFO | total number of ended hypotheses: 154
2026-01-28 21:47:11,949 | INFO | best hypo: capable<space>de<space>prendre<space>en<space>compte<space>les<space>aspirations<space>des<space>hommes

2026-01-28 21:47:11,950 | INFO | speech length: 34560
2026-01-28 21:47:12,003 | INFO | decoder input length: 53
2026-01-28 21:47:12,003 | INFO | max output length: 53
2026-01-28 21:47:12,003 | INFO | min output length: 5
2026-01-28 21:47:15,855 | INFO | end detected at 46
2026-01-28 21:47:15,857 | INFO |  -3.37 * 0.5 =  -1.69 for decoder
2026-01-28 21:47:15,857 | INFO |  -1.78 * 0.5 =  -0.89 for ctc
2026-01-28 21:47:15,857 | INFO | total log probability: -2.57
2026-01-28 21:47:15,857 | INFO | normalized log probability: -0.06
2026-01-28 21:47:15,857 | INFO | total number of ended hypotheses: 163
2026-01-28 21:47:15,858 | INFO | best hypo: est<space>capable<space>de<space>faire<space>reculer<space>la<space>pauvreté

2026-01-28 21:47:15,860 | INFO | speech length: 52000
2026-01-28 21:47:15,906 | INFO | decoder input length: 80
2026-01-28 21:47:15,906 | INFO | max output length: 80
2026-01-28 21:47:15,906 | INFO | min output length: 8
2026-01-28 21:47:18,162 | INFO | end detected at 50
2026-01-28 21:47:18,164 | INFO |  -3.57 * 0.5 =  -1.79 for decoder
2026-01-28 21:47:18,164 | INFO |  -0.03 * 0.5 =  -0.01 for ctc
2026-01-28 21:47:18,164 | INFO | total log probability: -1.80
2026-01-28 21:47:18,164 | INFO | normalized log probability: -0.04
2026-01-28 21:47:18,164 | INFO | total number of ended hypotheses: 169
2026-01-28 21:47:18,165 | INFO | best hypo: ce<space>sera<space>tout<space>le<space>sens<space>du<space>combat<space>de<space>la<space>france

2026-01-28 21:47:18,168 | INFO | speech length: 21600
2026-01-28 21:47:18,212 | INFO | decoder input length: 33
2026-01-28 21:47:18,212 | INFO | max output length: 33
2026-01-28 21:47:18,213 | INFO | min output length: 3
2026-01-28 21:47:19,795 | INFO | adding <eos> in the last position in the loop
2026-01-28 21:47:19,805 | INFO | no hypothesis. Finish decoding.
2026-01-28 21:47:19,806 | INFO |  -3.38 * 0.5 =  -1.69 for decoder
2026-01-28 21:47:19,806 | INFO | -12.61 * 0.5 =  -6.30 for ctc
2026-01-28 21:47:19,806 | INFO | total log probability: -7.99
2026-01-28 21:47:19,807 | INFO | normalized log probability: -0.26
2026-01-28 21:47:19,807 | INFO | total number of ended hypotheses: 78
2026-01-28 21:47:19,807 | INFO | best hypo: dans<space>les<space>grandes<space>négociations

2026-01-28 21:47:19,809 | INFO | speech length: 20160
2026-01-28 21:47:19,865 | INFO | decoder input length: 31
2026-01-28 21:47:19,865 | INFO | max output length: 31
2026-01-28 21:47:19,865 | INFO | min output length: 3
2026-01-28 21:47:22,930 | INFO | end detected at 29
2026-01-28 21:47:22,931 | INFO |  -3.05 * 0.5 =  -1.52 for decoder
2026-01-28 21:47:22,932 | INFO |  -1.19 * 0.5 =  -0.60 for ctc
2026-01-28 21:47:22,932 | INFO | total log probability: -2.12
2026-01-28 21:47:22,932 | INFO | normalized log probability: -0.08
2026-01-28 21:47:22,932 | INFO | total number of ended hypotheses: 164
2026-01-28 21:47:22,932 | INFO | best hypo: mais<space>chers<space>compatriotes

2026-01-28 21:47:22,934 | INFO | speech length: 37440
2026-01-28 21:47:22,971 | INFO | decoder input length: 58
2026-01-28 21:47:22,971 | INFO | max output length: 58
2026-01-28 21:47:22,971 | INFO | min output length: 5
2026-01-28 21:47:25,163 | INFO | end detected at 45
2026-01-28 21:47:25,165 | INFO |  -3.20 * 0.5 =  -1.60 for decoder
2026-01-28 21:47:25,165 | INFO |  -0.06 * 0.5 =  -0.03 for ctc
2026-01-28 21:47:25,165 | INFO | total log probability: -1.63
2026-01-28 21:47:25,165 | INFO | normalized log probability: -0.04
2026-01-28 21:47:25,165 | INFO | total number of ended hypotheses: 171
2026-01-28 21:47:25,165 | INFO | best hypo: nous<space>avons<space>en<space>commun<space>certaines<space>valeurs

2026-01-28 21:47:25,167 | INFO | speech length: 116960
2026-01-28 21:47:25,214 | INFO | decoder input length: 182
2026-01-28 21:47:25,214 | INFO | max output length: 182
2026-01-28 21:47:25,214 | INFO | min output length: 18
2026-01-28 21:47:30,847 | INFO | end detected at 114
2026-01-28 21:47:30,849 | INFO |  -8.84 * 0.5 =  -4.42 for decoder
2026-01-28 21:47:30,849 | INFO |  -1.10 * 0.5 =  -0.55 for ctc
2026-01-28 21:47:30,849 | INFO | total log probability: -4.97
2026-01-28 21:47:30,849 | INFO | normalized log probability: -0.05
2026-01-28 21:47:30,849 | INFO | total number of ended hypotheses: 172
2026-01-28 21:47:30,851 | INFO | best hypo: la<space>volonté<space>de<space>donner<space>à<space>chacun<space>sa<space>chance<space>pour<space>que<space>notre<space>société<space>soit<space>plus<space>alente<space>plus<space>mobile<space>plus<space>optimiste

2026-01-28 21:47:30,853 | INFO | speech length: 118400
2026-01-28 21:47:30,893 | INFO | decoder input length: 184
2026-01-28 21:47:30,893 | INFO | max output length: 184
2026-01-28 21:47:30,893 | INFO | min output length: 18
2026-01-28 21:47:40,158 | INFO | end detected at 117
2026-01-28 21:47:40,160 | INFO |  -8.90 * 0.5 =  -4.45 for decoder
2026-01-28 21:47:40,160 | INFO |  -0.15 * 0.5 =  -0.08 for ctc
2026-01-28 21:47:40,160 | INFO | total log probability: -4.53
2026-01-28 21:47:40,160 | INFO | normalized log probability: -0.04
2026-01-28 21:47:40,160 | INFO | total number of ended hypotheses: 167
2026-01-28 21:47:40,161 | INFO | best hypo: l'exigence<space>de<space>solidarité<space>une<space>solidarité<space>plus<space>responsable<space>où<space>chacun<space>s'efforcerait<space>de<space>prendre<space>sa<space>part<space>du<space>contrat

2026-01-28 21:47:40,164 | INFO | speech length: 83520
2026-01-28 21:47:40,222 | INFO | decoder input length: 130
2026-01-28 21:47:40,222 | INFO | max output length: 130
2026-01-28 21:47:40,222 | INFO | min output length: 13
2026-01-28 21:47:46,890 | INFO | end detected at 80
2026-01-28 21:47:46,892 | INFO |  -6.11 * 0.5 =  -3.05 for decoder
2026-01-28 21:47:46,893 | INFO |  -0.41 * 0.5 =  -0.21 for ctc
2026-01-28 21:47:46,893 | INFO | total log probability: -3.26
2026-01-28 21:47:46,893 | INFO | normalized log probability: -0.04
2026-01-28 21:47:46,893 | INFO | total number of ended hypotheses: 174
2026-01-28 21:47:46,895 | INFO | best hypo: l'attachement<space>à<space>la<space>famille<space>parce<space>qu'elle<space>est<space>chaleur<space>entre<space>aides<space>sécurité

2026-01-28 21:47:46,898 | INFO | speech length: 61120
2026-01-28 21:47:46,954 | INFO | decoder input length: 95
2026-01-28 21:47:46,954 | INFO | max output length: 95
2026-01-28 21:47:46,954 | INFO | min output length: 9
2026-01-28 21:47:53,991 | INFO | end detected at 64
2026-01-28 21:47:53,993 | INFO |  -4.67 * 0.5 =  -2.34 for decoder
2026-01-28 21:47:53,993 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 21:47:53,993 | INFO | total log probability: -2.34
2026-01-28 21:47:53,993 | INFO | normalized log probability: -0.04
2026-01-28 21:47:53,993 | INFO | total number of ended hypotheses: 177
2026-01-28 21:47:53,994 | INFO | best hypo: le<space>désir<space>d'être<space>utile<space>de<space>trouver<space>sa<space>place<space>dans<space>la<space>société

2026-01-28 21:47:53,998 | INFO | speech length: 29280
2026-01-28 21:47:54,054 | INFO | decoder input length: 45
2026-01-28 21:47:54,054 | INFO | max output length: 45
2026-01-28 21:47:54,054 | INFO | min output length: 4
2026-01-28 21:47:57,320 | INFO | end detected at 29
2026-01-28 21:47:57,322 | INFO |  -1.97 * 0.5 =  -0.99 for decoder
2026-01-28 21:47:57,322 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 21:47:57,322 | INFO | total log probability: -0.99
2026-01-28 21:47:57,322 | INFO | normalized log probability: -0.04
2026-01-28 21:47:57,322 | INFO | total number of ended hypotheses: 134
2026-01-28 21:47:57,323 | INFO | best hypo: de<space>donner<space>autour<space>de<space>soi

2026-01-28 21:47:57,326 | INFO | speech length: 13600
2026-01-28 21:47:57,372 | INFO | decoder input length: 20
2026-01-28 21:47:57,372 | INFO | max output length: 20
2026-01-28 21:47:57,372 | INFO | min output length: 2
2026-01-28 21:47:58,081 | INFO | adding <eos> in the last position in the loop
2026-01-28 21:47:58,091 | INFO | no hypothesis. Finish decoding.
2026-01-28 21:47:58,092 | INFO |  -1.20 * 0.5 =  -0.60 for decoder
2026-01-28 21:47:58,092 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 21:47:58,092 | INFO | total log probability: -0.60
2026-01-28 21:47:58,092 | INFO | normalized log probability: -0.04
2026-01-28 21:47:58,092 | INFO | total number of ended hypotheses: 150
2026-01-28 21:47:58,092 | INFO | best hypo: de<space>se<space>réaliser

2026-01-28 21:47:58,094 | INFO | speech length: 13600
2026-01-28 21:47:58,134 | INFO | decoder input length: 20
2026-01-28 21:47:58,135 | INFO | max output length: 20
2026-01-28 21:47:58,135 | INFO | min output length: 2
2026-01-28 21:47:58,847 | INFO | end detected at 18
2026-01-28 21:47:58,848 | INFO |  -1.07 * 0.5 =  -0.53 for decoder
2026-01-28 21:47:58,848 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 21:47:58,848 | INFO | total log probability: -0.53
2026-01-28 21:47:58,848 | INFO | normalized log probability: -0.04
2026-01-28 21:47:58,848 | INFO | total number of ended hypotheses: 139
2026-01-28 21:47:58,848 | INFO | best hypo: la<space>tolérance

2026-01-28 21:47:58,850 | INFO | speech length: 50720
2026-01-28 21:47:58,890 | INFO | decoder input length: 78
2026-01-28 21:47:58,891 | INFO | max output length: 78
2026-01-28 21:47:58,891 | INFO | min output length: 7
2026-01-28 21:48:01,202 | INFO | end detected at 49
2026-01-28 21:48:01,205 | INFO |  -3.30 * 0.5 =  -1.65 for decoder
2026-01-28 21:48:01,205 | INFO |  -1.83 * 0.5 =  -0.92 for ctc
2026-01-28 21:48:01,205 | INFO | total log probability: -2.57
2026-01-28 21:48:01,205 | INFO | normalized log probability: -0.06
2026-01-28 21:48:01,205 | INFO | total number of ended hypotheses: 210
2026-01-28 21:48:01,206 | INFO | best hypo: qui<space>ne<space>doit<space>pas<space>être<space>renoncement<space>à<space>ces

2026-01-28 21:48:01,208 | INFO | speech length: 16480
2026-01-28 21:48:01,246 | INFO | decoder input length: 25
2026-01-28 21:48:01,246 | INFO | max output length: 25
2026-01-28 21:48:01,246 | INFO | min output length: 2
2026-01-28 21:48:03,651 | INFO | adding <eos> in the last position in the loop
2026-01-28 21:48:03,660 | INFO | no hypothesis. Finish decoding.
2026-01-28 21:48:03,661 | INFO |  -2.58 * 0.5 =  -1.29 for decoder
2026-01-28 21:48:03,661 | INFO |  -7.08 * 0.5 =  -3.54 for ctc
2026-01-28 21:48:03,661 | INFO | total log probability: -4.83
2026-01-28 21:48:03,661 | INFO | normalized log probability: -0.19
2026-01-28 21:48:03,662 | INFO | total number of ended hypotheses: 92
2026-01-28 21:48:03,662 | INFO | best hypo: mais<space>respect<space>de<space>l'autre

2026-01-28 21:48:03,663 | INFO | speech length: 18720
2026-01-28 21:48:03,701 | INFO | decoder input length: 28
2026-01-28 21:48:03,701 | INFO | max output length: 28
2026-01-28 21:48:03,701 | INFO | min output length: 2
2026-01-28 21:48:06,301 | INFO | end detected at 26
2026-01-28 21:48:06,303 | INFO |  -3.27 * 0.5 =  -1.63 for decoder
2026-01-28 21:48:06,303 | INFO |  -7.52 * 0.5 =  -3.76 for ctc
2026-01-28 21:48:06,303 | INFO | total log probability: -5.39
2026-01-28 21:48:06,303 | INFO | normalized log probability: -0.27
2026-01-28 21:48:06,303 | INFO | total number of ended hypotheses: 194
2026-01-28 21:48:06,303 | INFO | best hypo: esprit<space>républicain

2026-01-28 21:48:06,306 | INFO | speech length: 80800
2026-01-28 21:48:06,343 | INFO | decoder input length: 125
2026-01-28 21:48:06,343 | INFO | max output length: 125
2026-01-28 21:48:06,343 | INFO | min output length: 12
2026-01-28 21:48:11,650 | INFO | end detected at 83
2026-01-28 21:48:11,651 | INFO |  -6.30 * 0.5 =  -3.15 for decoder
2026-01-28 21:48:11,651 | INFO |  -0.06 * 0.5 =  -0.03 for ctc
2026-01-28 21:48:11,651 | INFO | total log probability: -3.18
2026-01-28 21:48:11,651 | INFO | normalized log probability: -0.04
2026-01-28 21:48:11,651 | INFO | total number of ended hypotheses: 137
2026-01-28 21:48:11,653 | INFO | best hypo: et<space>le<space>sens<space>de<space>l'intérêt<space>général<space>qui<space>impose<space>que<space>l'état<space>conserve<space>toute<space>sa<space>place

2026-01-28 21:48:11,655 | INFO | speech length: 64160
2026-01-28 21:48:11,698 | INFO | decoder input length: 99
2026-01-28 21:48:11,698 | INFO | max output length: 99
2026-01-28 21:48:11,698 | INFO | min output length: 9
2026-01-28 21:48:14,638 | INFO | end detected at 60
2026-01-28 21:48:14,641 | INFO |  -4.42 * 0.5 =  -2.21 for decoder
2026-01-28 21:48:14,641 | INFO |  -0.27 * 0.5 =  -0.14 for ctc
2026-01-28 21:48:14,641 | INFO | total log probability: -2.35
2026-01-28 21:48:14,641 | INFO | normalized log probability: -0.04
2026-01-28 21:48:14,641 | INFO | total number of ended hypotheses: 191
2026-01-28 21:48:14,642 | INFO | best hypo: pour<space>dire<space>le<space>droit<space>le<space>faire<space>respecter<space>avec<space>autorité

2026-01-28 21:48:14,645 | INFO | speech length: 8640
2026-01-28 21:48:14,677 | INFO | decoder input length: 13
2026-01-28 21:48:14,678 | INFO | max output length: 13
2026-01-28 21:48:14,678 | INFO | min output length: 1
2026-01-28 21:48:15,153 | INFO | adding <eos> in the last position in the loop
2026-01-28 21:48:15,162 | INFO | no hypothesis. Finish decoding.
2026-01-28 21:48:15,163 | INFO |  -1.02 * 0.5 =  -0.51 for decoder
2026-01-28 21:48:15,163 | INFO |  -0.39 * 0.5 =  -0.20 for ctc
2026-01-28 21:48:15,163 | INFO | total log probability: -0.71
2026-01-28 21:48:15,163 | INFO | normalized log probability: -0.06
2026-01-28 21:48:15,163 | INFO | total number of ended hypotheses: 83
2026-01-28 21:48:15,163 | INFO | best hypo: avec<space>juste

2026-01-28 21:48:15,165 | INFO | speech length: 19521
2026-01-28 21:48:15,206 | INFO | decoder input length: 30
2026-01-28 21:48:15,206 | INFO | max output length: 30
2026-01-28 21:48:15,206 | INFO | min output length: 3
2026-01-28 21:48:16,301 | INFO | end detected at 27
2026-01-28 21:48:16,303 | INFO |  -3.74 * 0.5 =  -1.87 for decoder
2026-01-28 21:48:16,303 | INFO |  -1.46 * 0.5 =  -0.73 for ctc
2026-01-28 21:48:16,303 | INFO | total log probability: -2.60
2026-01-28 21:48:16,303 | INFO | normalized log probability: -0.12
2026-01-28 21:48:16,303 | INFO | total number of ended hypotheses: 172
2026-01-28 21:48:16,304 | INFO | best hypo: gardons<space>ses<space>exigents

2026-01-28 21:48:16,306 | INFO | speech length: 19520
2026-01-28 21:48:16,352 | INFO | decoder input length: 30
2026-01-28 21:48:16,352 | INFO | max output length: 30
2026-01-28 21:48:16,352 | INFO | min output length: 3
2026-01-28 21:48:17,364 | INFO | end detected at 25
2026-01-28 21:48:17,365 | INFO |  -2.16 * 0.5 =  -1.08 for decoder
2026-01-28 21:48:17,365 | INFO |  -0.75 * 0.5 =  -0.38 for ctc
2026-01-28 21:48:17,365 | INFO | total log probability: -1.46
2026-01-28 21:48:17,365 | INFO | normalized log probability: -0.07
2026-01-28 21:48:17,365 | INFO | total number of ended hypotheses: 145
2026-01-28 21:48:17,366 | INFO | best hypo: gardant<space>ses<space>valeurs

2026-01-28 21:48:17,368 | INFO | speech length: 81600
2026-01-28 21:48:17,414 | INFO | decoder input length: 127
2026-01-28 21:48:17,414 | INFO | max output length: 127
2026-01-28 21:48:17,415 | INFO | min output length: 12
2026-01-28 21:48:24,548 | INFO | end detected at 86
2026-01-28 21:48:24,550 | INFO |  -6.37 * 0.5 =  -3.18 for decoder
2026-01-28 21:48:24,550 | INFO |  -0.11 * 0.5 =  -0.05 for ctc
2026-01-28 21:48:24,550 | INFO | total log probability: -3.24
2026-01-28 21:48:24,550 | INFO | normalized log probability: -0.04
2026-01-28 21:48:24,550 | INFO | total number of ended hypotheses: 174
2026-01-28 21:48:24,551 | INFO | best hypo: en<space>les<space>faisant<space>vivre<space>nous<space>serons<space>plus<space>forts<space>pour<space>aborder<space>les<space>temps<space>qui<space>viennent

2026-01-28 21:48:24,554 | INFO | speech length: 17440
2026-01-28 21:48:24,593 | INFO | decoder input length: 26
2026-01-28 21:48:24,594 | INFO | max output length: 26
2026-01-28 21:48:24,594 | INFO | min output length: 2
2026-01-28 21:48:25,496 | INFO | end detected at 22
2026-01-28 21:48:25,498 | INFO |  -1.44 * 0.5 =  -0.72 for decoder
2026-01-28 21:48:25,498 | INFO |  -5.28 * 0.5 =  -2.64 for ctc
2026-01-28 21:48:25,498 | INFO | total log probability: -3.36
2026-01-28 21:48:25,498 | INFO | normalized log probability: -0.19
2026-01-28 21:48:25,498 | INFO | total number of ended hypotheses: 174
2026-01-28 21:48:25,499 | INFO | best hypo: la<space>france<space>chante

2026-01-28 21:48:25,501 | INFO | speech length: 26400
2026-01-28 21:48:25,560 | INFO | decoder input length: 40
2026-01-28 21:48:25,560 | INFO | max output length: 40
2026-01-28 21:48:25,560 | INFO | min output length: 4
2026-01-28 21:48:27,723 | INFO | adding <eos> in the last position in the loop
2026-01-28 21:48:27,731 | INFO | no hypothesis. Finish decoding.
2026-01-28 21:48:27,732 | INFO |  -3.07 * 0.5 =  -1.53 for decoder
2026-01-28 21:48:27,732 | INFO |  -0.49 * 0.5 =  -0.24 for ctc
2026-01-28 21:48:27,732 | INFO | total log probability: -1.78
2026-01-28 21:48:27,732 | INFO | normalized log probability: -0.05
2026-01-28 21:48:27,732 | INFO | total number of ended hypotheses: 102
2026-01-28 21:48:27,733 | INFO | best hypo: elle<space>doit<space>le<space>faire<space>au<space>rythme<space>du<space>monde

2026-01-28 21:48:27,734 | INFO | speech length: 104960
2026-01-28 21:48:27,794 | INFO | decoder input length: 163
2026-01-28 21:48:27,794 | INFO | max output length: 163
2026-01-28 21:48:27,794 | INFO | min output length: 16
2026-01-28 21:48:33,532 | INFO | end detected at 98
2026-01-28 21:48:33,534 | INFO |  -7.56 * 0.5 =  -3.78 for decoder
2026-01-28 21:48:33,534 | INFO |  -1.10 * 0.5 =  -0.55 for ctc
2026-01-28 21:48:33,534 | INFO | total log probability: -4.33
2026-01-28 21:48:33,534 | INFO | normalized log probability: -0.05
2026-01-28 21:48:33,534 | INFO | total number of ended hypotheses: 155
2026-01-28 21:48:33,535 | INFO | best hypo: en<space>étant<space>fidèle<space>à<space>son<space>génie<space>propre<space>elle<space>sera<space>conjuguée<space>le<space>changement<space>et<space>la<space>cohésion<space>sociale

2026-01-28 21:48:33,537 | INFO | speech length: 92000
2026-01-28 21:48:33,588 | INFO | decoder input length: 143
2026-01-28 21:48:33,588 | INFO | max output length: 143
2026-01-28 21:48:33,588 | INFO | min output length: 14
2026-01-28 21:48:41,966 | INFO | end detected at 83
2026-01-28 21:48:41,968 | INFO |  -6.42 * 0.5 =  -3.21 for decoder
2026-01-28 21:48:41,968 | INFO |  -0.27 * 0.5 =  -0.14 for ctc
2026-01-28 21:48:41,968 | INFO | total log probability: -3.35
2026-01-28 21:48:41,968 | INFO | normalized log probability: -0.04
2026-01-28 21:48:41,968 | INFO | total number of ended hypotheses: 179
2026-01-28 21:48:41,969 | INFO | best hypo: l'esprit<space>d'initiative<space>et<space>la<space>sécurité<space>la<space>modernité<space>et<space>le<space>bien<space>vivre<space>ensemble

2026-01-28 21:48:41,971 | INFO | speech length: 20160
2026-01-28 21:48:42,031 | INFO | decoder input length: 31
2026-01-28 21:48:42,031 | INFO | max output length: 31
2026-01-28 21:48:42,031 | INFO | min output length: 3
2026-01-28 21:48:42,972 | INFO | end detected at 28
2026-01-28 21:48:42,973 | INFO |  -2.45 * 0.5 =  -1.22 for decoder
2026-01-28 21:48:42,974 | INFO |  -1.39 * 0.5 =  -0.69 for ctc
2026-01-28 21:48:42,974 | INFO | total log probability: -1.92
2026-01-28 21:48:42,974 | INFO | normalized log probability: -0.08
2026-01-28 21:48:42,974 | INFO | total number of ended hypotheses: 169
2026-01-28 21:48:42,974 | INFO | best hypo: et<space>chers<space>compatriotes

2026-01-28 21:48:42,976 | INFO | speech length: 106240
2026-01-28 21:48:43,023 | INFO | decoder input length: 165
2026-01-28 21:48:43,023 | INFO | max output length: 165
2026-01-28 21:48:43,023 | INFO | min output length: 16
2026-01-28 21:48:48,400 | INFO | end detected at 91
2026-01-28 21:48:48,402 | INFO |  -7.12 * 0.5 =  -3.56 for decoder
2026-01-28 21:48:48,402 | INFO |  -0.99 * 0.5 =  -0.49 for ctc
2026-01-28 21:48:48,402 | INFO | total log probability: -4.05
2026-01-28 21:48:48,402 | INFO | normalized log probability: -0.05
2026-01-28 21:48:48,402 | INFO | total number of ended hypotheses: 200
2026-01-28 21:48:48,403 | INFO | best hypo: je<space>mesure<space>l'honneur<space>et<space>la<space>responsabilité<space>qui<space>m'échoit<space>de<space>m'adresser<space>à<space>vous<space>ce<space>soir

2026-01-28 21:48:48,405 | INFO | speech length: 55040
2026-01-28 21:48:48,457 | INFO | decoder input length: 85
2026-01-28 21:48:48,458 | INFO | max output length: 85
2026-01-28 21:48:48,458 | INFO | min output length: 8
2026-01-28 21:48:54,427 | INFO | end detected at 55
2026-01-28 21:48:54,428 | INFO |  -3.92 * 0.5 =  -1.96 for decoder
2026-01-28 21:48:54,428 | INFO |  -0.03 * 0.5 =  -0.01 for ctc
2026-01-28 21:48:54,429 | INFO | total log probability: -1.97
2026-01-28 21:48:54,429 | INFO | normalized log probability: -0.04
2026-01-28 21:48:54,429 | INFO | total number of ended hypotheses: 169
2026-01-28 21:48:54,429 | INFO | best hypo: alors<space>que<space>notre<space>nation<space>franchit<space>le<space>cap<space>du<space>siècle

2026-01-28 21:48:54,431 | INFO | speech length: 80480
2026-01-28 21:48:54,477 | INFO | decoder input length: 125
2026-01-28 21:48:54,477 | INFO | max output length: 125
2026-01-28 21:48:54,477 | INFO | min output length: 12
2026-01-28 21:48:58,148 | INFO | end detected at 81
2026-01-28 21:48:58,149 | INFO |  -6.80 * 0.5 =  -3.40 for decoder
2026-01-28 21:48:58,150 | INFO |  -0.83 * 0.5 =  -0.41 for ctc
2026-01-28 21:48:58,150 | INFO | total log probability: -3.81
2026-01-28 21:48:58,150 | INFO | normalized log probability: -0.05
2026-01-28 21:48:58,150 | INFO | total number of ended hypotheses: 172
2026-01-28 21:48:58,151 | INFO | best hypo: la<space>france<space>a<space>plus<space>de<space>mille<space>ans<space>riches<space>de<space>fièvres<space>de<space>passions<space>d'enthousiasme

2026-01-28 21:48:58,153 | INFO | speech length: 88000
2026-01-28 21:48:58,203 | INFO | decoder input length: 137
2026-01-28 21:48:58,203 | INFO | max output length: 137
2026-01-28 21:48:58,203 | INFO | min output length: 13
2026-01-28 21:49:02,183 | INFO | end detected at 78
2026-01-28 21:49:02,185 | INFO |  -5.76 * 0.5 =  -2.88 for decoder
2026-01-28 21:49:02,185 | INFO |  -5.74 * 0.5 =  -2.87 for ctc
2026-01-28 21:49:02,185 | INFO | total log probability: -5.75
2026-01-28 21:49:02,185 | INFO | normalized log probability: -0.08
2026-01-28 21:49:02,185 | INFO | total number of ended hypotheses: 182
2026-01-28 21:49:02,186 | INFO | best hypo: elle<space>continue<space>comme<space>hier<space>à<space>ouvrir<space>et<space>à<space>défricher<space>les<space>chemins<space>du<space>monde

2026-01-28 21:49:02,189 | INFO | speech length: 42560
2026-01-28 21:49:02,225 | INFO | decoder input length: 66
2026-01-28 21:49:02,225 | INFO | max output length: 66
2026-01-28 21:49:02,225 | INFO | min output length: 6
2026-01-28 21:49:03,743 | INFO | end detected at 39
2026-01-28 21:49:03,744 | INFO |  -2.75 * 0.5 =  -1.38 for decoder
2026-01-28 21:49:03,744 | INFO |  -0.83 * 0.5 =  -0.42 for ctc
2026-01-28 21:49:03,744 | INFO | total log probability: -1.79
2026-01-28 21:49:03,744 | INFO | normalized log probability: -0.05
2026-01-28 21:49:03,744 | INFO | total number of ended hypotheses: 170
2026-01-28 21:49:03,745 | INFO | best hypo: le<space>nouveau<space>siècle<space>est<space>à<space>inventer

2026-01-28 21:49:03,747 | INFO | speech length: 37120
2026-01-28 21:49:03,788 | INFO | decoder input length: 57
2026-01-28 21:49:03,788 | INFO | max output length: 57
2026-01-28 21:49:03,788 | INFO | min output length: 5
2026-01-28 21:49:05,464 | INFO | end detected at 39
2026-01-28 21:49:05,466 | INFO |  -2.75 * 0.5 =  -1.37 for decoder
2026-01-28 21:49:05,466 | INFO |  -0.54 * 0.5 =  -0.27 for ctc
2026-01-28 21:49:05,466 | INFO | total log probability: -1.64
2026-01-28 21:49:05,466 | INFO | normalized log probability: -0.05
2026-01-28 21:49:05,466 | INFO | total number of ended hypotheses: 196
2026-01-28 21:49:05,467 | INFO | best hypo: plus<space>fraternel<space>plus<space>volontaire

2026-01-28 21:49:05,469 | INFO | speech length: 41440
2026-01-28 21:49:05,506 | INFO | decoder input length: 64
2026-01-28 21:49:05,506 | INFO | max output length: 64
2026-01-28 21:49:05,506 | INFO | min output length: 6
2026-01-28 21:49:10,220 | INFO | end detected at 50
2026-01-28 21:49:10,221 | INFO |  -3.52 * 0.5 =  -1.76 for decoder
2026-01-28 21:49:10,221 | INFO |  -0.01 * 0.5 =  -0.01 for ctc
2026-01-28 21:49:10,221 | INFO | total log probability: -1.77
2026-01-28 21:49:10,222 | INFO | normalized log probability: -0.04
2026-01-28 21:49:10,222 | INFO | total number of ended hypotheses: 177
2026-01-28 21:49:10,222 | INFO | best hypo: il<space>aura<space>les<space>couleurs<space>que<space>nous<space>lui<space>donnerons

2026-01-28 21:49:10,224 | INFO | speech length: 48480
2026-01-28 21:49:10,266 | INFO | decoder input length: 75
2026-01-28 21:49:10,266 | INFO | max output length: 75
2026-01-28 21:49:10,266 | INFO | min output length: 7
2026-01-28 21:49:15,839 | INFO | end detected at 55
2026-01-28 21:49:15,841 | INFO |  -3.94 * 0.5 =  -1.97 for decoder
2026-01-28 21:49:15,841 | INFO |  -0.02 * 0.5 =  -0.01 for ctc
2026-01-28 21:49:15,841 | INFO | total log probability: -1.98
2026-01-28 21:49:15,841 | INFO | normalized log probability: -0.04
2026-01-28 21:49:15,841 | INFO | total number of ended hypotheses: 171
2026-01-28 21:49:15,842 | INFO | best hypo: la<space>france<space>sera<space>ce<space>que<space>nous<space>voudrons<space>qu'elle<space>soit

2026-01-28 21:49:15,844 | INFO | speech length: 59840
2026-01-28 21:49:15,882 | INFO | decoder input length: 93
2026-01-28 21:49:15,882 | INFO | max output length: 93
2026-01-28 21:49:15,882 | INFO | min output length: 9
2026-01-28 21:49:19,479 | INFO | end detected at 48
2026-01-28 21:49:19,480 | INFO |  -3.53 * 0.5 =  -1.76 for decoder
2026-01-28 21:49:19,480 | INFO |  -0.08 * 0.5 =  -0.04 for ctc
2026-01-28 21:49:19,480 | INFO | total log probability: -1.81
2026-01-28 21:49:19,480 | INFO | normalized log probability: -0.04
2026-01-28 21:49:19,480 | INFO | total number of ended hypotheses: 164
2026-01-28 21:49:19,481 | INFO | best hypo: une<space>nation<space>unie<space>vivante<space>solidaire<space>ouverte

2026-01-28 21:49:19,482 | INFO | speech length: 31040
2026-01-28 21:49:19,537 | INFO | decoder input length: 48
2026-01-28 21:49:19,537 | INFO | max output length: 48
2026-01-28 21:49:19,537 | INFO | min output length: 4
2026-01-28 21:49:21,892 | INFO | end detected at 37
2026-01-28 21:49:21,894 | INFO |  -3.36 * 0.5 =  -1.68 for decoder
2026-01-28 21:49:21,894 | INFO |  -1.24 * 0.5 =  -0.62 for ctc
2026-01-28 21:49:21,894 | INFO | total log probability: -2.30
2026-01-28 21:49:21,894 | INFO | normalized log probability: -0.07
2026-01-28 21:49:21,894 | INFO | total number of ended hypotheses: 160
2026-01-28 21:49:21,895 | INFO | best hypo: qui<space>n'acceptent<space>aucune<space>fatalité

2026-01-28 21:49:21,898 | INFO | speech length: 72960
2026-01-28 21:49:21,944 | INFO | decoder input length: 113
2026-01-28 21:49:21,945 | INFO | max output length: 113
2026-01-28 21:49:21,945 | INFO | min output length: 11
2026-01-28 21:49:24,852 | INFO | end detected at 67
2026-01-28 21:49:24,853 | INFO |  -4.84 * 0.5 =  -2.42 for decoder
2026-01-28 21:49:24,853 | INFO |  -0.00 * 0.5 =  -0.00 for ctc
2026-01-28 21:49:24,853 | INFO | total log probability: -2.42
2026-01-28 21:49:24,853 | INFO | normalized log probability: -0.04
2026-01-28 21:49:24,853 | INFO | total number of ended hypotheses: 165
2026-01-28 21:49:24,854 | INFO | best hypo: car<space>dans<space>un<space>monde<space>où<space>rien<space>n'est<space>figé<space>l'avenir<space>dépend<space>de<space>nous

2026-01-28 21:49:24,856 | INFO | speech length: 132960
2026-01-28 21:49:24,900 | INFO | decoder input length: 207
2026-01-28 21:49:24,900 | INFO | max output length: 207
2026-01-28 21:49:24,900 | INFO | min output length: 20
2026-01-28 21:49:35,052 | INFO | end detected at 109
2026-01-28 21:49:35,053 | INFO |  -8.56 * 0.5 =  -4.28 for decoder
2026-01-28 21:49:35,053 | INFO |  -0.64 * 0.5 =  -0.32 for ctc
2026-01-28 21:49:35,053 | INFO | total log probability: -4.60
2026-01-28 21:49:35,053 | INFO | normalized log probability: -0.04
2026-01-28 21:49:35,053 | INFO | total number of ended hypotheses: 156
2026-01-28 21:49:35,055 | INFO | best hypo: l'avenir<space>dépend<space>de<space>notre<space>capacité<space>à<space>construire<space>à<space>créer<space>à<space>rêver<space>ensemble<space>les<space>voix<space>de<space>l'aventure<space>humaine

2026-01-28 21:49:35,056 | INFO | speech length: 24480
2026-01-28 21:49:35,101 | INFO | decoder input length: 37
2026-01-28 21:49:35,101 | INFO | max output length: 37
2026-01-28 21:49:35,101 | INFO | min output length: 3
2026-01-28 21:49:36,299 | INFO | adding <eos> in the last position in the loop
2026-01-28 21:49:36,308 | INFO | no hypothesis. Finish decoding.
2026-01-28 21:49:36,309 | INFO |  -2.73 * 0.5 =  -1.36 for decoder
2026-01-28 21:49:36,309 | INFO |  -0.88 * 0.5 =  -0.44 for ctc
2026-01-28 21:49:36,309 | INFO | total log probability: -1.80
2026-01-28 21:49:36,309 | INFO | normalized log probability: -0.05
2026-01-28 21:49:36,309 | INFO | total number of ended hypotheses: 118
2026-01-28 21:49:36,310 | INFO | best hypo: à<space>chacune<space>et<space>à<space>chacun<space>d'entre<space>eux

2026-01-28 21:49:36,311 | INFO | speech length: 77120
2026-01-28 21:49:36,355 | INFO | decoder input length: 120
2026-01-28 21:49:36,356 | INFO | max output length: 120
2026-01-28 21:49:36,356 | INFO | min output length: 12
2026-01-28 21:49:39,350 | INFO | end detected at 68
2026-01-28 21:49:39,351 | INFO |  -5.25 * 0.5 =  -2.62 for decoder
2026-01-28 21:49:39,351 | INFO |  -0.18 * 0.5 =  -0.09 for ctc
2026-01-28 21:49:39,351 | INFO | total log probability: -2.71
2026-01-28 21:49:39,351 | INFO | normalized log probability: -0.04
2026-01-28 21:49:39,351 | INFO | total number of ended hypotheses: 174
2026-01-28 21:49:39,352 | INFO | best hypo: françaises<space>et<space>français<space>de<space>métropole<space>d'outre<space>mer<space>de<space>l'étranger

2026-01-28 21:49:39,354 | INFO | speech length: 34240
2026-01-28 21:49:39,398 | INFO | decoder input length: 53
2026-01-28 21:49:39,398 | INFO | max output length: 53
2026-01-28 21:49:39,398 | INFO | min output length: 5
2026-01-28 21:49:42,347 | INFO | end detected at 38
2026-01-28 21:49:42,349 | INFO |  -2.94 * 0.5 =  -1.47 for decoder
2026-01-28 21:49:42,349 | INFO |  -1.55 * 0.5 =  -0.78 for ctc
2026-01-28 21:49:42,349 | INFO | total log probability: -2.25
2026-01-28 21:49:42,349 | INFO | normalized log probability: -0.07
2026-01-28 21:49:42,349 | INFO | total number of ended hypotheses: 218
2026-01-28 21:49:42,350 | INFO | best hypo: je<space>souhaite<space>très<space>chaleureuse

2026-01-28 21:49:42,352 | INFO | speech length: 50880
2026-01-28 21:49:42,394 | INFO | decoder input length: 79
2026-01-28 21:49:42,394 | INFO | max output length: 79
2026-01-28 21:49:42,394 | INFO | min output length: 7
2026-01-28 21:49:46,435 | INFO | end detected at 48
2026-01-28 21:49:46,437 | INFO |  -3.45 * 0.5 =  -1.73 for decoder
2026-01-28 21:49:46,437 | INFO |  -0.05 * 0.5 =  -0.02 for ctc
2026-01-28 21:49:46,437 | INFO | total log probability: -1.75
2026-01-28 21:49:46,437 | INFO | normalized log probability: -0.04
2026-01-28 21:49:46,437 | INFO | total number of ended hypotheses: 152
2026-01-28 21:49:46,438 | INFO | best hypo: une<space>bonne<space>et<space>une<space>heureuse<space>année<space>deux<space>mille

2026-01-28 21:49:46,463 | INFO | Chunk: 0 | WER=66.666667 | S=0 D=0 I=2
2026-01-28 21:49:46,464 | INFO | Chunk: 1 | WER=33.333333 | S=0 D=0 I=2
2026-01-28 21:49:46,464 | INFO | Chunk: 2 | WER=19.047619 | S=2 D=0 I=2
2026-01-28 21:49:46,465 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:49:46,465 | INFO | Chunk: 4 | WER=30.000000 | S=0 D=1 I=2
2026-01-28 21:49:46,466 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:49:46,466 | INFO | Chunk: 6 | WER=12.500000 | S=0 D=0 I=2
2026-01-28 21:49:46,467 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:49:46,467 | INFO | Chunk: 8 | WER=50.000000 | S=1 D=0 I=1
2026-01-28 21:49:46,468 | INFO | Chunk: 9 | WER=6.896552 | S=1 D=0 I=1
2026-01-28 21:49:46,469 | INFO | Chunk: 10 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:49:46,469 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:49:46,470 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:49:46,471 | INFO | Chunk: 13 | WER=27.272727 | S=1 D=0 I=2
2026-01-28 21:49:46,471 | INFO | Chunk: 14 | WER=12.500000 | S=1 D=0 I=0
2026-01-28 21:49:46,471 | INFO | Chunk: 15 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:49:46,472 | INFO | Chunk: 16 | WER=12.500000 | S=0 D=0 I=2
2026-01-28 21:49:46,472 | INFO | Chunk: 17 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:49:46,473 | INFO | Chunk: 18 | WER=16.666667 | S=0 D=0 I=1
2026-01-28 21:49:46,473 | INFO | Chunk: 19 | WER=60.000000 | S=0 D=0 I=3
2026-01-28 21:49:46,473 | INFO | Chunk: 20 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:49:46,474 | INFO | Chunk: 21 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:49:46,474 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:49:46,474 | INFO | Chunk: 23 | WER=25.000000 | S=0 D=1 I=1
2026-01-28 21:49:46,475 | INFO | Chunk: 24 | WER=11.111111 | S=1 D=0 I=0
2026-01-28 21:49:46,475 | INFO | Chunk: 25 | WER=7.692308 | S=0 D=0 I=1
2026-01-28 21:49:46,476 | INFO | Chunk: 26 | WER=60.000000 | S=0 D=0 I=3
2026-01-28 21:49:46,476 | INFO | Chunk: 27 | WER=10.000000 | S=0 D=0 I=1
2026-01-28 21:49:46,477 | INFO | Chunk: 28 | WER=14.285714 | S=0 D=0 I=2
2026-01-28 21:49:46,477 | INFO | Chunk: 29 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:49:46,477 | INFO | Chunk: 30 | WER=0.000000 | S=1 D=0 I=0
2026-01-28 21:49:46,478 | INFO | Chunk: 31 | WER=9.523810 | S=2 D=0 I=0
2026-01-28 21:49:46,479 | INFO | Chunk: 32 | WER=4.166667 | S=0 D=0 I=1
2026-01-28 21:49:46,479 | INFO | Chunk: 33 | WER=40.000000 | S=0 D=0 I=2
2026-01-28 21:49:46,479 | INFO | Chunk: 34 | WER=500.000000 | S=0 D=0 I=5
2026-01-28 21:49:46,480 | INFO | Chunk: 35 | WER=6.250000 | S=1 D=0 I=0
2026-01-28 21:49:46,480 | INFO | Chunk: 36 | WER=16.666667 | S=1 D=0 I=0
2026-01-28 21:49:46,482 | INFO | Chunk: 37 | WER=6.060606 | S=0 D=0 I=2
2026-01-28 21:49:46,482 | INFO | Chunk: 38 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:49:46,482 | INFO | Chunk: 39 | WER=33.333333 | S=2 D=0 I=1
2026-01-28 21:49:46,483 | INFO | Chunk: 40 | WER=9.090909 | S=1 D=0 I=0
2026-01-28 21:49:46,483 | INFO | Chunk: 41 | WER=66.666667 | S=0 D=1 I=1
2026-01-28 21:49:46,483 | INFO | Chunk: 42 | WER=10.000000 | S=0 D=0 I=1
2026-01-28 21:49:46,484 | INFO | Chunk: 43 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:49:46,484 | INFO | Chunk: 44 | WER=12.500000 | S=1 D=0 I=0
2026-01-28 21:49:46,485 | INFO | Chunk: 45 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:49:46,485 | INFO | Chunk: 46 | WER=33.333333 | S=0 D=0 I=1
2026-01-28 21:49:46,485 | INFO | Chunk: 47 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:49:46,485 | INFO | Chunk: 48 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:49:46,486 | INFO | Chunk: 49 | WER=13.333333 | S=1 D=0 I=1
2026-01-28 21:49:46,486 | INFO | Chunk: 50 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:49:46,486 | INFO | Chunk: 51 | WER=50.000000 | S=1 D=0 I=1
2026-01-28 21:49:46,487 | INFO | Chunk: 52 | WER=50.000000 | S=0 D=1 I=2
2026-01-28 21:49:46,487 | INFO | Chunk: 53 | WER=300.000000 | S=0 D=0 I=3
2026-01-28 21:49:46,487 | INFO | Chunk: 54 | WER=8.333333 | S=1 D=0 I=0
2026-01-28 21:49:46,488 | INFO | Chunk: 55 | WER=7.692308 | S=1 D=0 I=0
2026-01-28 21:49:46,488 | INFO | Chunk: 56 | WER=5.555556 | S=1 D=0 I=0
2026-01-28 21:49:46,489 | INFO | Chunk: 57 | WER=14.285714 | S=0 D=0 I=1
2026-01-28 21:49:46,489 | INFO | Chunk: 58 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:49:46,490 | INFO | Chunk: 59 | WER=15.384615 | S=0 D=0 I=2
2026-01-28 21:49:46,490 | INFO | Chunk: 60 | WER=40.000000 | S=0 D=1 I=1
2026-01-28 21:49:46,490 | INFO | Chunk: 61 | WER=100.000000 | S=1 D=1 I=1
2026-01-28 21:49:46,490 | INFO | Chunk: 62 | WER=25.000000 | S=0 D=0 I=2
2026-01-28 21:49:46,491 | INFO | Chunk: 63 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:49:46,491 | INFO | Chunk: 64 | WER=25.000000 | S=1 D=0 I=0
2026-01-28 21:49:46,491 | INFO | Chunk: 65 | WER=66.666667 | S=0 D=1 I=1
2026-01-28 21:49:46,492 | INFO | Chunk: 66 | WER=19.047619 | S=3 D=0 I=1
2026-01-28 21:49:46,492 | INFO | Chunk: 67 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:49:46,492 | INFO | Chunk: 68 | WER=40.000000 | S=2 D=0 I=0
2026-01-28 21:49:46,493 | INFO | Chunk: 69 | WER=25.000000 | S=0 D=0 I=2
2026-01-28 21:49:46,493 | INFO | Chunk: 70 | WER=7.692308 | S=0 D=0 I=1
2026-01-28 21:49:46,494 | INFO | Chunk: 71 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:49:46,494 | INFO | Chunk: 72 | WER=40.000000 | S=2 D=0 I=0
2026-01-28 21:49:46,494 | INFO | Chunk: 73 | WER=30.000000 | S=1 D=0 I=2
2026-01-28 21:49:46,495 | INFO | Chunk: 74 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:49:46,495 | INFO | Chunk: 75 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:49:46,495 | INFO | Chunk: 76 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 21:49:46,496 | INFO | Chunk: 77 | WER=18.750000 | S=1 D=0 I=2
2026-01-28 21:49:46,496 | INFO | Chunk: 78 | WER=20.000000 | S=0 D=0 I=2
2026-01-28 21:49:46,497 | INFO | Chunk: 79 | WER=20.000000 | S=0 D=0 I=3
2026-01-28 21:49:46,498 | INFO | Chunk: 80 | WER=6.666667 | S=0 D=0 I=1
2026-01-28 21:49:46,498 | INFO | Chunk: 81 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:49:46,498 | INFO | Chunk: 82 | WER=30.000000 | S=0 D=2 I=1
2026-01-28 21:49:46,499 | INFO | Chunk: 83 | WER=33.333333 | S=1 D=0 I=1
2026-01-28 21:49:46,499 | INFO | Chunk: 84 | WER=37.500000 | S=1 D=0 I=2
2026-01-28 21:49:46,499 | INFO | Chunk: 85 | WER=20.000000 | S=0 D=1 I=0
2026-01-28 21:49:46,500 | INFO | Chunk: 86 | WER=33.333333 | S=1 D=0 I=0
2026-01-28 21:49:46,500 | INFO | Chunk: 87 | WER=20.000000 | S=0 D=0 I=1
2026-01-28 21:49:46,500 | INFO | Chunk: 88 | WER=11.111111 | S=1 D=0 I=1
2026-01-28 21:49:46,501 | INFO | Chunk: 89 | WER=5.882353 | S=0 D=0 I=1
2026-01-28 21:49:46,502 | INFO | Chunk: 90 | WER=16.666667 | S=1 D=0 I=1
2026-01-28 21:49:46,502 | INFO | Chunk: 91 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:49:46,502 | INFO | Chunk: 92 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 21:49:46,503 | INFO | Chunk: 93 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:49:46,503 | INFO | Chunk: 94 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:49:46,503 | INFO | Chunk: 95 | WER=33.333333 | S=0 D=0 I=2
2026-01-28 21:49:46,503 | INFO | Chunk: 96 | WER=150.000000 | S=0 D=0 I=3
2026-01-28 21:49:46,503 | INFO | Chunk: 97 | WER=33.333333 | S=0 D=1 I=0
2026-01-28 21:49:46,504 | INFO | Chunk: 98 | WER=6.666667 | S=0 D=0 I=1
2026-01-28 21:49:46,504 | INFO | Chunk: 99 | WER=12.500000 | S=0 D=0 I=1
2026-01-28 21:49:46,504 | INFO | Chunk: 100 | WER=200.000000 | S=1 D=0 I=1
2026-01-28 21:49:46,505 | INFO | Chunk: 101 | WER=100.000000 | S=1 D=0 I=1
2026-01-28 21:49:46,505 | INFO | Chunk: 102 | WER=150.000000 | S=2 D=0 I=1
2026-01-28 21:49:46,505 | INFO | Chunk: 103 | WER=7.692308 | S=0 D=0 I=1
2026-01-28 21:49:46,506 | INFO | Chunk: 104 | WER=50.000000 | S=0 D=0 I=1
2026-01-28 21:49:46,506 | INFO | Chunk: 105 | WER=33.333333 | S=0 D=0 I=2
2026-01-28 21:49:46,506 | INFO | Chunk: 106 | WER=12.500000 | S=2 D=0 I=0
2026-01-28 21:49:46,507 | INFO | Chunk: 107 | WER=7.692308 | S=0 D=0 I=1
2026-01-28 21:49:46,507 | INFO | Chunk: 108 | WER=33.333333 | S=1 D=0 I=0
2026-01-28 21:49:46,508 | INFO | Chunk: 109 | WER=12.500000 | S=1 D=0 I=1
2026-01-28 21:49:46,508 | INFO | Chunk: 110 | WER=22.222222 | S=0 D=1 I=1
2026-01-28 21:49:46,509 | INFO | Chunk: 111 | WER=21.428571 | S=3 D=0 I=0
2026-01-28 21:49:46,509 | INFO | Chunk: 112 | WER=18.181818 | S=0 D=0 I=2
2026-01-28 21:49:46,509 | INFO | Chunk: 113 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:49:46,510 | INFO | Chunk: 114 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:49:46,510 | INFO | Chunk: 115 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:49:46,510 | INFO | Chunk: 116 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 21:49:46,511 | INFO | Chunk: 117 | WER=20.000000 | S=0 D=0 I=1
2026-01-28 21:49:46,511 | INFO | Chunk: 118 | WER=20.000000 | S=1 D=0 I=0
2026-01-28 21:49:46,511 | INFO | Chunk: 119 | WER=16.666667 | S=0 D=0 I=2
2026-01-28 21:49:46,512 | INFO | Chunk: 120 | WER=11.111111 | S=1 D=0 I=1
2026-01-28 21:49:46,512 | INFO | Chunk: 121 | WER=60.000000 | S=0 D=0 I=3
2026-01-28 21:49:46,513 | INFO | Chunk: 122 | WER=9.090909 | S=1 D=0 I=0
2026-01-28 21:49:46,513 | INFO | Chunk: 123 | WER=33.333333 | S=0 D=0 I=1
2026-01-28 21:49:46,513 | INFO | Chunk: 124 | WER=33.333333 | S=0 D=0 I=2
2026-01-28 21:49:47,585 | INFO | File: Rhap-M2004.wav | WER=14.099217 | S=51 D=6 I=105
2026-01-28 21:49:47,585 | INFO | ------------------------------
2026-01-28 21:49:47,586 | INFO | Conf ester Done!
2026-01-28 22:01:46,472 | INFO | Chunk: 0 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,473 | INFO | Chunk: 1 | WER=16.666667 | S=0 D=0 I=1
2026-01-28 22:01:46,474 | INFO | Chunk: 2 | WER=9.523810 | S=0 D=0 I=2
2026-01-28 22:01:46,474 | INFO | Chunk: 3 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,475 | INFO | Chunk: 4 | WER=30.000000 | S=0 D=1 I=2
2026-01-28 22:01:46,475 | INFO | Chunk: 5 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,476 | INFO | Chunk: 6 | WER=25.000000 | S=2 D=0 I=2
2026-01-28 22:01:46,476 | INFO | Chunk: 7 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,477 | INFO | Chunk: 8 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 22:01:46,478 | INFO | Chunk: 9 | WER=17.241379 | S=4 D=0 I=1
2026-01-28 22:01:46,478 | INFO | Chunk: 10 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,479 | INFO | Chunk: 11 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,480 | INFO | Chunk: 12 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,480 | INFO | Chunk: 13 | WER=27.272727 | S=1 D=0 I=2
2026-01-28 22:01:46,480 | INFO | Chunk: 14 | WER=12.500000 | S=1 D=0 I=0
2026-01-28 22:01:46,481 | INFO | Chunk: 15 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,481 | INFO | Chunk: 16 | WER=12.500000 | S=0 D=0 I=2
2026-01-28 22:01:46,482 | INFO | Chunk: 17 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,482 | INFO | Chunk: 18 | WER=16.666667 | S=0 D=0 I=1
2026-01-28 22:01:46,482 | INFO | Chunk: 19 | WER=60.000000 | S=0 D=0 I=3
2026-01-28 22:01:46,483 | INFO | Chunk: 20 | WER=7.692308 | S=0 D=1 I=0
2026-01-28 22:01:46,483 | INFO | Chunk: 21 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,484 | INFO | Chunk: 22 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,484 | INFO | Chunk: 23 | WER=25.000000 | S=0 D=1 I=1
2026-01-28 22:01:46,484 | INFO | Chunk: 24 | WER=22.222222 | S=2 D=0 I=0
2026-01-28 22:01:46,485 | INFO | Chunk: 25 | WER=7.692308 | S=0 D=0 I=1
2026-01-28 22:01:46,485 | INFO | Chunk: 26 | WER=60.000000 | S=0 D=0 I=3
2026-01-28 22:01:46,486 | INFO | Chunk: 27 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,486 | INFO | Chunk: 28 | WER=14.285714 | S=0 D=0 I=2
2026-01-28 22:01:46,486 | INFO | Chunk: 29 | WER=100.000000 | S=2 D=0 I=1
2026-01-28 22:01:46,486 | INFO | Chunk: 30 | WER=0.000000 | S=1 D=0 I=0
2026-01-28 22:01:46,487 | INFO | Chunk: 31 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,488 | INFO | Chunk: 32 | WER=8.333333 | S=1 D=0 I=1
2026-01-28 22:01:46,489 | INFO | Chunk: 33 | WER=20.000000 | S=0 D=0 I=1
2026-01-28 22:01:46,489 | INFO | Chunk: 34 | WER=300.000000 | S=0 D=0 I=3
2026-01-28 22:01:46,489 | INFO | Chunk: 35 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,490 | INFO | Chunk: 36 | WER=16.666667 | S=0 D=1 I=0
2026-01-28 22:01:46,491 | INFO | Chunk: 37 | WER=15.151515 | S=3 D=0 I=2
2026-01-28 22:01:46,492 | INFO | Chunk: 38 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,492 | INFO | Chunk: 39 | WER=33.333333 | S=2 D=0 I=1
2026-01-28 22:01:46,492 | INFO | Chunk: 40 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,493 | INFO | Chunk: 41 | WER=66.666667 | S=0 D=1 I=1
2026-01-28 22:01:46,493 | INFO | Chunk: 42 | WER=10.000000 | S=0 D=0 I=1
2026-01-28 22:01:46,493 | INFO | Chunk: 43 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,494 | INFO | Chunk: 44 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,494 | INFO | Chunk: 45 | WER=5.882353 | S=1 D=0 I=0
2026-01-28 22:01:46,494 | INFO | Chunk: 46 | WER=33.333333 | S=0 D=0 I=1
2026-01-28 22:01:46,495 | INFO | Chunk: 47 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,495 | INFO | Chunk: 48 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,496 | INFO | Chunk: 49 | WER=6.666667 | S=0 D=0 I=1
2026-01-28 22:01:46,496 | INFO | Chunk: 50 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,496 | INFO | Chunk: 51 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 22:01:46,496 | INFO | Chunk: 52 | WER=66.666667 | S=3 D=1 I=0
2026-01-28 22:01:46,496 | INFO | Chunk: 53 | WER=300.000000 | S=0 D=0 I=3
2026-01-28 22:01:46,497 | INFO | Chunk: 54 | WER=8.333333 | S=1 D=0 I=0
2026-01-28 22:01:46,497 | INFO | Chunk: 55 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,498 | INFO | Chunk: 56 | WER=5.555556 | S=1 D=0 I=0
2026-01-28 22:01:46,498 | INFO | Chunk: 57 | WER=14.285714 | S=0 D=0 I=1
2026-01-28 22:01:46,499 | INFO | Chunk: 58 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,499 | INFO | Chunk: 59 | WER=15.384615 | S=0 D=0 I=2
2026-01-28 22:01:46,499 | INFO | Chunk: 60 | WER=40.000000 | S=0 D=1 I=1
2026-01-28 22:01:46,500 | INFO | Chunk: 61 | WER=66.666667 | S=0 D=1 I=1
2026-01-28 22:01:46,500 | INFO | Chunk: 62 | WER=25.000000 | S=0 D=0 I=2
2026-01-28 22:01:46,500 | INFO | Chunk: 63 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,500 | INFO | Chunk: 64 | WER=50.000000 | S=2 D=0 I=0
2026-01-28 22:01:46,501 | INFO | Chunk: 65 | WER=66.666667 | S=0 D=1 I=1
2026-01-28 22:01:46,501 | INFO | Chunk: 66 | WER=23.809524 | S=4 D=0 I=1
2026-01-28 22:01:46,502 | INFO | Chunk: 67 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,502 | INFO | Chunk: 68 | WER=20.000000 | S=0 D=1 I=0
2026-01-28 22:01:46,502 | INFO | Chunk: 69 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,503 | INFO | Chunk: 70 | WER=7.692308 | S=0 D=0 I=1
2026-01-28 22:01:46,503 | INFO | Chunk: 71 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,504 | INFO | Chunk: 72 | WER=20.000000 | S=1 D=0 I=0
2026-01-28 22:01:46,504 | INFO | Chunk: 73 | WER=30.000000 | S=1 D=0 I=2
2026-01-28 22:01:46,504 | INFO | Chunk: 74 | WER=9.090909 | S=1 D=0 I=0
2026-01-28 22:01:46,505 | INFO | Chunk: 75 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,505 | INFO | Chunk: 76 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 22:01:46,506 | INFO | Chunk: 77 | WER=6.250000 | S=0 D=0 I=1
2026-01-28 22:01:46,506 | INFO | Chunk: 78 | WER=20.000000 | S=0 D=0 I=2
2026-01-28 22:01:46,507 | INFO | Chunk: 79 | WER=20.000000 | S=0 D=0 I=3
2026-01-28 22:01:46,507 | INFO | Chunk: 80 | WER=6.666667 | S=0 D=0 I=1
2026-01-28 22:01:46,508 | INFO | Chunk: 81 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,508 | INFO | Chunk: 82 | WER=30.000000 | S=0 D=2 I=1
2026-01-28 22:01:46,508 | INFO | Chunk: 83 | WER=16.666667 | S=0 D=0 I=1
2026-01-28 22:01:46,509 | INFO | Chunk: 84 | WER=37.500000 | S=1 D=0 I=2
2026-01-28 22:01:46,509 | INFO | Chunk: 85 | WER=20.000000 | S=0 D=1 I=0
2026-01-28 22:01:46,509 | INFO | Chunk: 86 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,509 | INFO | Chunk: 87 | WER=20.000000 | S=0 D=0 I=1
2026-01-28 22:01:46,510 | INFO | Chunk: 88 | WER=11.111111 | S=1 D=0 I=1
2026-01-28 22:01:46,511 | INFO | Chunk: 89 | WER=11.764706 | S=1 D=0 I=1
2026-01-28 22:01:46,511 | INFO | Chunk: 90 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,512 | INFO | Chunk: 91 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,512 | INFO | Chunk: 92 | WER=25.000000 | S=0 D=0 I=1
2026-01-28 22:01:46,512 | INFO | Chunk: 93 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,512 | INFO | Chunk: 94 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,513 | INFO | Chunk: 95 | WER=50.000000 | S=0 D=0 I=3
2026-01-28 22:01:46,513 | INFO | Chunk: 96 | WER=150.000000 | S=0 D=0 I=3
2026-01-28 22:01:46,513 | INFO | Chunk: 97 | WER=33.333333 | S=0 D=1 I=0
2026-01-28 22:01:46,514 | INFO | Chunk: 98 | WER=6.666667 | S=0 D=0 I=1
2026-01-28 22:01:46,514 | INFO | Chunk: 99 | WER=25.000000 | S=1 D=0 I=1
2026-01-28 22:01:46,514 | INFO | Chunk: 100 | WER=200.000000 | S=1 D=0 I=1
2026-01-28 22:01:46,514 | INFO | Chunk: 101 | WER=50.000000 | S=0 D=0 I=1
2026-01-28 22:01:46,515 | INFO | Chunk: 102 | WER=50.000000 | S=0 D=0 I=1
2026-01-28 22:01:46,515 | INFO | Chunk: 103 | WER=7.692308 | S=0 D=0 I=1
2026-01-28 22:01:46,515 | INFO | Chunk: 104 | WER=50.000000 | S=0 D=0 I=1
2026-01-28 22:01:46,515 | INFO | Chunk: 105 | WER=33.333333 | S=0 D=0 I=2
2026-01-28 22:01:46,516 | INFO | Chunk: 106 | WER=6.250000 | S=1 D=0 I=0
2026-01-28 22:01:46,517 | INFO | Chunk: 107 | WER=7.692308 | S=0 D=0 I=1
2026-01-28 22:01:46,517 | INFO | Chunk: 108 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,517 | INFO | Chunk: 109 | WER=18.750000 | S=2 D=0 I=1
2026-01-28 22:01:46,518 | INFO | Chunk: 110 | WER=22.222222 | S=0 D=1 I=1
2026-01-28 22:01:46,518 | INFO | Chunk: 111 | WER=14.285714 | S=2 D=0 I=0
2026-01-28 22:01:46,519 | INFO | Chunk: 112 | WER=18.181818 | S=0 D=0 I=2
2026-01-28 22:01:46,519 | INFO | Chunk: 113 | WER=16.666667 | S=1 D=0 I=0
2026-01-28 22:01:46,519 | INFO | Chunk: 114 | WER=25.000000 | S=1 D=0 I=0
2026-01-28 22:01:46,520 | INFO | Chunk: 115 | WER=12.500000 | S=1 D=0 I=0
2026-01-28 22:01:46,520 | INFO | Chunk: 116 | WER=10.000000 | S=1 D=0 I=0
2026-01-28 22:01:46,520 | INFO | Chunk: 117 | WER=20.000000 | S=0 D=0 I=1
2026-01-28 22:01:46,520 | INFO | Chunk: 118 | WER=0.000000 | S=0 D=0 I=0
2026-01-28 22:01:46,521 | INFO | Chunk: 119 | WER=16.666667 | S=0 D=0 I=2
2026-01-28 22:01:46,522 | INFO | Chunk: 120 | WER=16.666667 | S=2 D=0 I=1
2026-01-28 22:01:46,522 | INFO | Chunk: 121 | WER=80.000000 | S=0 D=1 I=3
2026-01-28 22:01:46,522 | INFO | Chunk: 122 | WER=9.090909 | S=1 D=0 I=0
2026-01-28 22:01:46,523 | INFO | Chunk: 123 | WER=33.333333 | S=0 D=0 I=1
2026-01-28 22:01:46,523 | INFO | Chunk: 124 | WER=100.000000 | S=4 D=0 I=2
2026-01-28 22:01:47,594 | INFO | File: Rhap-M2004.wav | WER=13.751088 | S=54 D=10 I=94
2026-01-28 22:01:47,595 | INFO | ------------------------------
2026-01-28 22:01:47,596 | INFO | hmm_tdnn Done!
2026-01-28 22:01:47,888 | INFO | ==================================Rhap-M2005.wav=========================================
2026-01-28 22:01:48,153 | INFO | Using rVAD model
2026-01-28 22:01:57,914 | INFO | Chunk: 0 | WER=9.195402 | S=6 D=2 I=0
2026-01-28 22:01:57,919 | INFO | Chunk: 1 | WER=16.883117 | S=8 D=5 I=0
2026-01-28 22:01:57,926 | INFO | Chunk: 2 | WER=20.000000 | S=8 D=9 I=1
2026-01-28 22:01:57,933 | INFO | Chunk: 3 | WER=8.695652 | S=4 D=4 I=0
2026-01-28 22:01:58,015 | INFO | File: Rhap-M2005.wav | WER=13.583815 | S=26 D=20 I=1
2026-01-28 22:01:58,015 | INFO | ------------------------------
2026-01-28 22:01:58,016 | INFO | w2vec vad chunk Done!
2026-01-28 22:02:08,939 | INFO | Chunk: 0 | WER=63.218391 | S=2 D=53 I=0
2026-01-28 22:02:08,942 | INFO | Chunk: 1 | WER=63.636364 | S=10 D=39 I=0
2026-01-28 22:02:08,945 | INFO | Chunk: 2 | WER=65.555556 | S=1 D=58 I=0
2026-01-28 22:02:08,949 | INFO | Chunk: 3 | WER=61.956522 | S=3 D=54 I=0
2026-01-28 22:02:08,988 | INFO | File: Rhap-M2005.wav | WER=63.583815 | S=16 D=204 I=0
2026-01-28 22:02:08,988 | INFO | ------------------------------
2026-01-28 22:02:08,988 | INFO | whisper med Done!
2026-01-28 22:02:26,841 | INFO | Chunk: 0 | WER=52.873563 | S=16 D=30 I=0
2026-01-28 22:02:26,844 | INFO | Chunk: 1 | WER=49.350649 | S=12 D=26 I=0
2026-01-28 22:02:26,846 | INFO | Chunk: 2 | WER=64.444444 | S=1 D=57 I=0
2026-01-28 22:02:26,849 | INFO | Chunk: 3 | WER=46.739130 | S=4 D=39 I=0
2026-01-28 22:02:26,887 | INFO | File: Rhap-M2005.wav | WER=53.468208 | S=33 D=152 I=0
2026-01-28 22:02:26,887 | INFO | ------------------------------
2026-01-28 22:02:26,887 | INFO | whisper large Done!
2026-01-28 22:02:27,023 | INFO | config file: /vol/experiments3/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/asr_commonvoice_conformer_FR_config.yaml
2026-01-28 22:02:27,063 | INFO | Vocabulary size: 350
2026-01-28 22:02:28,058 | INFO | Gradient checkpoint layers: []
2026-01-28 22:02:28,778 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 22:02:28,782 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(350, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=350, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 22:02:28,783 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 22:02:28,783 | INFO | Text tokenizer: SentencepiecesTokenizer(model="/vol/experiments/rouas/SpeechRecognition/saved_models/espnet2-commonvoice-conformer-FR/bpe_unigram350/bpe.model")
2026-01-28 22:02:28,783 | INFO | speech length: 446400
2026-01-28 22:02:28,830 | INFO | decoder input length: 697
2026-01-28 22:02:28,830 | INFO | max output length: 697
2026-01-28 22:02:28,830 | INFO | min output length: 69
2026-01-28 22:02:56,084 | INFO | end detected at 234
2026-01-28 22:02:56,086 | INFO | -654.88 * 0.5 = -327.44 for decoder
2026-01-28 22:02:56,086 | INFO | -176.98 * 0.5 = -88.49 for ctc
2026-01-28 22:02:56,086 | INFO | total log probability: -415.93
2026-01-28 22:02:56,086 | INFO | normalized log probability: -1.83
2026-01-28 22:02:56,086 | INFO | total number of ended hypotheses: 165
2026-01-28 22:02:56,089 | INFO | best hypo: ▁le▁succès▁des▁réseaux▁sociaux▁sur▁internet▁n'est▁plus▁à▁démontrer▁il▁n'y▁a▁qu'à▁voir▁les▁soixante▁cinq▁millions▁d'utilisateurs▁de▁facebook▁mais▁connaissez▁vous▁les▁sites▁sociaux▁spécialisés▁pour▁l'ombre▁de▁facebooke▁il▁existe▁en▁ejet▁des▁sites▁communautistes▁et▁qui▁permettent▁de▁communiquer▁avec▁des▁y▁amies▁ou▁de▁faire▁des▁rencontres'▁dans▁des▁cercles▁très▁fermés▁on▁peut▁cit▁par▁exemple▁le▁site▁small▁world▁set▁serveur▁américains▁est▁une▁sorte▁de▁clube▁viay▁per▁résoré▁à▁la▁jet▁set

2026-01-28 22:02:56,093 | INFO | speech length: 390080
2026-01-28 22:02:56,143 | INFO | decoder input length: 609
2026-01-28 22:02:56,143 | INFO | max output length: 609
2026-01-28 22:02:56,143 | INFO | min output length: 60
2026-01-28 22:03:19,285 | INFO | end detected at 221
2026-01-28 22:03:19,287 | INFO | -492.85 * 0.5 = -246.43 for decoder
2026-01-28 22:03:19,287 | INFO | -219.23 * 0.5 = -109.62 for ctc
2026-01-28 22:03:19,287 | INFO | total log probability: -356.04
2026-01-28 22:03:19,287 | INFO | normalized log probability: -1.67
2026-01-28 22:03:19,287 | INFO | total number of ended hypotheses: 165
2026-01-28 22:03:19,290 | INFO | best hypo: ▁smallworld▁compte▁parmi▁ses▁membres▁des▁gens▁comme▁quentin▁tarantino▁ou▁l'incontournable▁paris▁silton▁ou▁encore▁l'un▁déboulonnable▁massimo▁gargia▁bref▁que▁du▁beau▁monde▁version▁cibère▁n'espérait▁pas▁y▁entrerait▁pour▁pouvoir▁tather▁ou▁échanger▁vos▁phottours▁de▁vacnance▁avec▁ces▁gens▁làens▁carand▁même▁si▁internette▁rapproche▁le▁monde▁smallward▁est▁acces▁cible▁uniquement▁sur▁invitonation▁il▁faut▁ou▁déjà▁menaître▁à▁pippple▁pour▁avoir▁le▁droit▁fréquenter'é▁pipe

2026-01-28 22:03:19,292 | INFO | speech length: 474880
2026-01-28 22:03:19,334 | INFO | decoder input length: 741
2026-01-28 22:03:19,334 | INFO | max output length: 741
2026-01-28 22:03:19,334 | INFO | min output length: 74
2026-01-28 22:03:47,488 | INFO | end detected at 230
2026-01-28 22:03:47,491 | INFO | -670.78 * 0.5 = -335.39 for decoder
2026-01-28 22:03:47,491 | INFO | -175.06 * 0.5 = -87.53 for ctc
2026-01-28 22:03:47,491 | INFO | total log probability: -422.92
2026-01-28 22:03:47,491 | INFO | normalized log probability: -1.88
2026-01-28 22:03:47,491 | INFO | total number of ended hypotheses: 181
2026-01-28 22:03:47,494 | INFO | best hypo: ▁en▁france▁moins▁branché▁mais▁plus▁accessible▁au▁commun▁des▁mortels▁on▁peut▁citer▁une▁zickicom▁zeddyki▁un▁site▁communautaire▁dont▁le▁but▁est▁de▁vous▁aider▁et▁à▁créer▁une▁identité▁numérique▁et▁oui▁à▁éviter▁à▁qu'on▁dise▁'importe▁ou▁quo▁à▁propos▁de▁vous▁sur▁internet▁mieux▁vaut▁de▁prendre▁les▁devants▁et▁diffuser▁vous▁même▁les▁infts▁et▁qui▁vous▁me▁concernentre▁enfin▁le▁tout▁récemment▁vient▁de▁s'ouvrire▁le▁site▁famiboocom▁famieboo▁est▁une▁sorte▁de▁facebook▁cent▁pourcent▁français▁réservé▁aux▁familles

2026-01-28 22:03:47,496 | INFO | speech length: 450400
2026-01-28 22:03:47,533 | INFO | decoder input length: 703
2026-01-28 22:03:47,533 | INFO | max output length: 703
2026-01-28 22:03:47,533 | INFO | min output length: 70
2026-01-28 22:04:15,285 | INFO | end detected at 236
2026-01-28 22:04:15,286 | INFO | -585.99 * 0.5 = -293.00 for decoder
2026-01-28 22:04:15,286 | INFO | -171.32 * 0.5 = -85.66 for ctc
2026-01-28 22:04:15,287 | INFO | total log probability: -378.66
2026-01-28 22:04:15,287 | INFO | normalized log probability: -1.65
2026-01-28 22:04:15,287 | INFO | total number of ended hypotheses: 141
2026-01-28 22:04:15,290 | INFO | best hypo: ▁pour▁partager▁vos▁photos▁vos▁vidéos▁ou▁encore▁agenda▁avec▁vos▁parents▁vos▁cousins▁ou▁vos▁tontons▁chaque▁cercle▁familial▁est▁accessible▁uniquement▁à▁ceux▁qui▁en▁sont▁membres▁par▁exemple▁pouror▁organgiser'aniversaire▁de▁votre▁arrière▁grand▁mère▁et▁suffit▁donner▁le▁code▁d'accès▁à▁tous▁les▁membres▁de▁votre▁familles▁et▁de▁vous▁retrouver▁tous▁ou▁ens▁femble▁sur▁famibooscom▁d'autres▁rés'eaux▁sociaux▁thématique▁devraient▁voir▁le▁jour▁prochainement▁notamment▁un▁site▁pour▁les▁amateurs▁sport▁et▁un▁autre▁pour▁les▁clubles▁branch

2026-01-28 22:04:15,300 | INFO | Chunk: 0 | WER=17.241379 | S=11 D=1 I=3
2026-01-28 22:04:15,304 | INFO | Chunk: 1 | WER=36.363636 | S=20 D=5 I=3
2026-01-28 22:04:15,309 | INFO | Chunk: 2 | WER=30.000000 | S=11 D=8 I=8
2026-01-28 22:04:15,314 | INFO | Chunk: 3 | WER=21.739130 | S=14 D=5 I=1
2026-01-28 22:04:15,378 | INFO | File: Rhap-M2005.wav | WER=26.011561 | S=56 D=19 I=15
2026-01-28 22:04:15,378 | INFO | ------------------------------
2026-01-28 22:04:15,378 | INFO | Conf cv Done!
2026-01-28 22:04:15,514 | INFO | config file: /home/rouas/experiments/SpeechRecognition/saved_models/espnet2-conformer-FR/asr_conformer_config.yaml
2026-01-28 22:04:15,536 | INFO | Vocabulary size: 47
2026-01-28 22:04:16,765 | INFO | Gradient checkpoint layers: []
2026-01-28 22:04:19,194 | INFO | BatchBeamSearch implementation is selected.
2026-01-28 22:04:19,200 | INFO | Beam_search: BatchBeamSearch(
  (nn_dict): ModuleDict(
    (decoder): TransformerDecoder(
      (embed): Sequential(
        (0): Embedding(47, 512)
        (1): PositionalEncoding(
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
      (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
      (output_layer): Linear(in_features=512, out_features=47, bias=True)
      (decoders): MultiSequential(
        (0): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (1): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (2): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (3): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (4): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
        (5): DecoderLayer(
          (self_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (src_attn): MultiHeadedAttention(
            (linear_q): Linear(in_features=512, out_features=512, bias=True)
            (linear_k): Linear(in_features=512, out_features=512, bias=True)
            (linear_v): Linear(in_features=512, out_features=512, bias=True)
            (linear_out): Linear(in_features=512, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (q_norm): Identity()
            (k_norm): Identity()
          )
          (feed_forward): PositionwiseFeedForward(
            (w_1): Linear(in_features=512, out_features=2048, bias=True)
            (w_2): Linear(in_features=2048, out_features=512, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
            (activation): ReLU()
          )
          (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)
          (dropout): Dropout(p=0.1, inplace=False)
        )
      )
    )
  )
)
2026-01-28 22:04:19,200 | INFO | Decoding device=cuda, dtype=float32
2026-01-28 22:04:19,201 | INFO | Text tokenizer: CharTokenizer(space_symbol="<space>"non_linguistic_symbols="set()"nonsplit_symbols="set()")
2026-01-28 22:04:19,204 | INFO | speech length: 446400
2026-01-28 22:04:19,250 | INFO | decoder input length: 697
2026-01-28 22:04:19,250 | INFO | max output length: 697
2026-01-28 22:04:19,250 | INFO | min output length: 69
2026-01-28 22:05:11,777 | INFO | end detected at 497
2026-01-28 22:05:11,778 | INFO | -293.14 * 0.5 = -146.57 for decoder
2026-01-28 22:05:11,778 | INFO | -61.02 * 0.5 = -30.51 for ctc
2026-01-28 22:05:11,778 | INFO | total log probability: -177.08
2026-01-28 22:05:11,778 | INFO | normalized log probability: -0.36
2026-01-28 22:05:11,778 | INFO | total number of ended hypotheses: 158
2026-01-28 22:05:11,784 | INFO | best hypo: le<space>succès<space>des<space>réseaux<space>sociaux<space>sur<space>internet<space>n'est<space>plus<space>à<space>démontrer<space>il<space>n'y<space>a<space>qu'à<space>voir<space>les<space>soixante<space>cinq<space>millions<space>d'utilisateurs<space>de<space>feilles<space>book<space>mais<space>connaissez<space>vous<space>les<space>sites<space>euh<space>sociaux<space>spécialisés<space>dans<space>l'ombre<space>de<space>feize<space>book<space>il<space>existe<space>en<space>effet<space>des<space>sites<space>communautaires<space>qui<space>permettent<space>de<space>communiquer<space>avec<space>des<space>amis<space>ou<space>de<space>faire<space>des<space>rencontres<space>dans<space>des<space>cercles<space>très<space>fermés<space>on<space>peut<space>citer<space>par<space>exemple<space>l<space>le<space>site<space>smort<space>worde<space>ce<space>serveur<space>américain<space>est<space>une<space>sorte<space>de<space>club<space>vie<space>pie<space>réservé<space>à<space>ladjet<space>cet

2026-01-28 22:05:11,787 | INFO | speech length: 390080
2026-01-28 22:05:11,824 | INFO | decoder input length: 609
2026-01-28 22:05:11,824 | INFO | max output length: 609
2026-01-28 22:05:11,824 | INFO | min output length: 60
2026-01-28 22:05:51,741 | INFO | end detected at 457
2026-01-28 22:05:51,742 | INFO | -208.99 * 0.5 = -104.49 for decoder
2026-01-28 22:05:51,742 | INFO | -32.69 * 0.5 = -16.35 for ctc
2026-01-28 22:05:51,742 | INFO | total log probability: -120.84
2026-01-28 22:05:51,742 | INFO | normalized log probability: -0.27
2026-01-28 22:05:51,742 | INFO | total number of ended hypotheses: 146
2026-01-28 22:05:51,748 | INFO | best hypo: smol<space>ward<space>compte<space>parmi<space>ses<space>membres<space>des<space>gens<space>comme<space>quantin<space>ta<space>antino<space>ou<space>l'incontournable<space>paris<space>silton<space>ou<space>encore<space>l'un<space>des<space>boulonnables<space>massimo<space>gardia<space>bref<space>que<space>du<space>beau<space>monde<space>version<space>cybere<space>n'espérait<space>pas<space>y<space>entrer<space>pour<space>pouvoir<space>châter<space>ou<space>échanger<space>vos<space>photos<space>de<space>vacances<space>avec<space>ces<space>gens<space>là<space>car<space>même<space>si<space>internet<space>rapproche<space>le<space>monde<space>ce<space>mollward<space>est<space>accessible<space>uniquement<space>sur<space>invitation<space>il<space>faut<space>déjà<space>connaître<space>un<space>peop<space>le<space>pour<space>avoir<space>le<space>droit<space>de<space>fréquenter<space>le<space>pip

2026-01-28 22:05:51,750 | INFO | speech length: 474880
2026-01-28 22:05:51,789 | INFO | decoder input length: 741
2026-01-28 22:05:51,789 | INFO | max output length: 741
2026-01-28 22:05:51,789 | INFO | min output length: 74
2026-01-28 22:06:45,783 | INFO | end detected at 506
2026-01-28 22:06:45,784 | INFO | -426.35 * 0.5 = -213.17 for decoder
2026-01-28 22:06:45,785 | INFO | -33.79 * 0.5 = -16.89 for ctc
2026-01-28 22:06:45,785 | INFO | total log probability: -230.07
2026-01-28 22:06:45,785 | INFO | normalized log probability: -0.46
2026-01-28 22:06:45,785 | INFO | total number of ended hypotheses: 160
2026-01-28 22:06:45,792 | INFO | best hypo: en<space>france<space>moins<space>branché<space>mais<space>plus<space>accessible<space>comme<space>un<space>des<space>mortels<space>on<space>peut<space>citer<space>zicky<space>point<space>com<space>zedick<space>i<space>un<space>site<space>communautaire<space>dont<space>le<space>but<space>est<space>de<space>vous<space>aider<space>à<space>créer<space>une<space>identité<space>numérique<space>et<space>oui<space>pour<space>éviter<space>qu'on<space>dise<space>n'importe<space>quoi<space>à<space>propos<space>de<space>vous<space>sur<space>internet<space>mieux<space>vous<space>prendre<space>les<space>devants<space>et<space>diffuser<space>vous<space>même<space>les<space>infos<space>qui<space>vous<space>concernent<space>enfint<space>tout<space>récemment<space>vient<space>de<space>s'ouvrir<space>le<space>site<space>fami<space>boock<space>point<space>com<space>famil<space>bouqu<space>est<space>une<space>sorte<space>de<space>face<space>book<space>cent<space>pour<space>cent<space>français<space>réservés<space>aux<space>familles

2026-01-28 22:06:45,794 | INFO | speech length: 450400
2026-01-28 22:06:45,837 | INFO | decoder input length: 703
2026-01-28 22:06:45,837 | INFO | max output length: 703
2026-01-28 22:06:45,837 | INFO | min output length: 70
2026-01-28 22:07:40,775 | INFO | end detected at 545
2026-01-28 22:07:40,777 | INFO | -579.70 * 0.5 = -289.85 for decoder
2026-01-28 22:07:40,777 | INFO | -18.36 * 0.5 =  -9.18 for ctc
2026-01-28 22:07:40,777 | INFO | total log probability: -299.03
2026-01-28 22:07:40,777 | INFO | normalized log probability: -0.55
2026-01-28 22:07:40,777 | INFO | total number of ended hypotheses: 182
2026-01-28 22:07:40,784 | INFO | best hypo: pour<space>partager<space>vos<space>photos<space>vos<space>vidéos<space>ou<space>encore<space>un<space>un<space>agenda<space>avec<space>vos<space>parents<space>vos<space>cousins<space>ou<space>vos<space>tontons<space>chaque<space>cercle<space>familial<space>est<space>accessible<space>uniquement<space>à<space>ceux<space>qui<space>en<space>sont<space>membres<space>par<space>exemple<space>pour<space>organiser<space>l'anniversaire<space>de<space>votre<space>arrière<space>grand<space>mère<space>il<space>suffit<space>de<space>donner<space>le<space>code<space>d'accès<space>à<space>tous<space>les<space>membres<space>de<space>votre<space>famille<space>et<space>de<space>vous<space>retrouver<space>tous<space>ensemble<space>sur<space>famie<space>book<space>point<space>com<space>d'autres<space>réseaux<space>sociaux<space>thématiques<space>devrient<space>voir<space>le<space>jour<space>prochainement<space>notamment<space>un<space>site<space>pour<space>les<space>amateurs<space>de<space>sport<space>et<space>un<space>autre<space>pour<space>les<space>clubeurs<space>branches

2026-01-28 22:07:40,796 | INFO | Chunk: 0 | WER=16.091954 | S=7 D=2 I=5
2026-01-28 22:07:40,800 | INFO | Chunk: 1 | WER=27.272727 | S=15 D=2 I=4
2026-01-28 22:07:40,805 | INFO | Chunk: 2 | WER=17.777778 | S=10 D=2 I=4
2026-01-28 22:07:40,810 | INFO | Chunk: 3 | WER=4.347826 | S=3 D=0 I=1
2026-01-28 22:07:40,875 | INFO | File: Rhap-M2005.wav | WER=15.895954 | S=35 D=6 I=14
2026-01-28 22:07:40,875 | INFO | ------------------------------
2026-01-28 22:07:40,876 | INFO | Conf ester Done!
2026-01-28 22:08:31,951 | INFO | Chunk: 0 | WER=8.045977 | S=3 D=1 I=3
2026-01-28 22:08:31,957 | INFO | Chunk: 1 | WER=16.883117 | S=10 D=1 I=2
2026-01-28 22:08:31,964 | INFO | Chunk: 2 | WER=17.777778 | S=10 D=2 I=4
2026-01-28 22:08:31,971 | INFO | Chunk: 3 | WER=13.043478 | S=9 D=1 I=2
2026-01-28 22:08:32,060 | INFO | File: Rhap-M2005.wav | WER=13.872832 | S=32 D=5 I=11
2026-01-28 22:08:32,060 | INFO | ------------------------------
2026-01-28 22:08:32,060 | INFO | hmm_tdnn Done!
2026-01-28 22:08:32,288 | INFO | ==================================Rhap-M2006.wav=========================================
2026-01-28 22:08:32,288 | WARNING | The file /vol/corpora/Rhapsodie/wav16k_corrected/Rhap-M2006.wav does not exist
